{"BEFORE":"        source_ids = source_ids.view(-1, self.args.block_size)\n\n        attention_mask = source_ids.ne(self.tokenizer.pad_token_id)\n        outputs = self.encoder(input_ids=source_ids, attention_mask=attention_mask,\n                               labels=source_ids, decoder_attention_mask=attention_mask, output_hidden_states=True)\n        hidden_states = outputs['decoder_hidden_states'][-1]\n        eos_mask = source_ids.eq(self.config.eos_token_id)\n\n        if len(torch.unique(eos_mask.sum(1))) > 1:\n            raise ValueError(\"All examples must have the same number of <eos> tokens.\")\n        vec = hidden_states[eos_mask, :].view(hidden_states.size(0), -1,\n                                              hidden_states.size(-1))[:, -1, :]\n\n        logits = self.classifier(vec)\n        prob = F.softmax(logits)\n","AFTER":"        source_ids = source_ids.view(-1, self.args.max_source_length)\n\n        if self.args.model_type == 'codet5':\n            vec = self.get_t5_vec(source_ids)\n        elif self.args.model_type == 'bart':\n            vec = self.get_bart_vec(source_ids)\n        elif self.args.model_type == 'roberta':\n            vec = self.get_roberta_vec(source_ids)\n\n        logits = self.classifier(vec)\n        prob = nn.functional.softmax(logits)\n\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n"}