{"BEFORE":"    def forward(self, item_eb: torch.Tensor, mask: torch.Tensor, device: torch.device) -> torch.Tensor:\n        \"\"\"\n        Implements the forward pass of a Capsule Network model.\n\n        Args:\n        item_eb: A tensor of shape (batch_size, seq_len, hidden_size) containing the item embeddings for the input sequence.\n        mask: A tensor of shape (batch_size, seq_len) containing the attention mask for the input sequence.\n        device: The device to store the intermediate computations.\n\n        Returns:\n        interest_capsule: The output interest capsule from the model\n        \"\"\"\n        if self.bilinear_type == 0:\n            # The MIND algorithm uses a single linear layer\n            item_eb_hat = self.linear(item_eb)  # [batch_size, seq_len, hidden_size]\n            item_eb_hat = item_eb_hat.repeat(1, 1, self.interest_num)  # [batch_size, seq_len, hidden_size*interest_num]\n        elif self.bilinear_type == 1:\n            item_eb_hat = self.linear(item_eb)\n        else:  # ComiRec_DR\n            u = torch.unsqueeze(item_eb, dim=2)  # shape=(batch_size, seq_len, 1, embedding_dim)\n            item_eb_hat = torch.sum(self.w[:, :self.seq_len, :, :] * u,\n                                    -1)  # shape=(batch_size, seq_len, hidden_size*interest_num)\n\n        item_eb_hat = item_eb_hat.view(-1, self.seq_len, self.interest_num, self.hidden_size)\n        item_eb_hat = item_eb_hat.permute(0, 2, 1, 3).contiguous()\n        item_eb_hat = item_eb_hat.view(-1, self.interest_num, self.seq_len,\n                                       self.hidden_size)  # [batch_size, num_interest, seq_len, hidden_size]\n\n        # [batch_size, num_interest, seq_len, hidden_size]\n        if self.stop_grad:  # Clip signal for backpropagation, item_emb_hat is not included in gradient calculation\n            item_eb_hat_iter = item_eb_hat.detach()\n        else:\n            item_eb_hat_iter = item_eb_hat\n\n        if self.bilinear_type > 0:\n            # If using specific Capsule Network algorithm rather than ComiRec_DR, initialise capsule_weight to 0\n            capsule_weight = torch.zeros(item_eb_hat.shape[0], self.interest_num, self.seq_len, device=device,\n                                         requires_grad=False)\n        else:  # Use Gaussian distribution to initialise b for the MIND algorithm\n            capsule_weight = torch.randn(item_eb_hat.shape[0], self.interest_num, self.seq_len, device=device,\n                                         requires_grad=False)\n\n        for i in range(self.routing_times):  # Dynamic routing propagation 3 times\n            atten_mask = torch.unsqueeze(mask, 1).repeat(1, self.interest_num, 1)  # [batch_size, num_interest, seq_len]\n            paddings = torch.zeros_like(atten_mask, dtype=torch.float)\n\n            # Calculates c, applies masking, final shape=[batch_size, num_interest, 1, seq_len]\n            capsule_softmax_weight = F.softmax(capsule_weight, dim=-1)\n            capsule_softmax_weight = torch.where(torch.eq(atten_mask, 0), paddings, capsule_softmax_weight)  # Mask\n            capsule_softmax_weight = torch.unsqueeze(capsule_softmax_weight, 2)\n\n            if i < 2:\n                # s=c*u_hat , (batch_size, interest_num, 1, seq_len) * (batch_size, interest_num, seq_len, hidden_size)\n                interest_capsule = torch.matmul(capsule_softmax_weight, item_eb_hat_iter)\n                cap_norm = torch.sum(torch.square(interest_capsule), -1, True)\n                scalar_factor = cap_norm \/ (1 + cap_norm) \/ torch.sqrt(cap_norm + 1e-9)\n                interest_capsule = scalar_factor * interest_capsule\n\n                # Updating b\n                delta_weight = torch.matmul(item_eb_hat_iter, torch.transpose(interest_capsule, 2, 3).contiguous())\n                # u_hat*v, shape=(batch_size, interest_num, seq_len, 1)\n                delta_weight = delta_weight.view(-1, self.interest_num,\n                                                 self.seq_len)  # shape=(batch_size, interest_num, seq_len)\n                capsule_weight = capsule_weight + delta_weight  # Update b\n            else:\n                interest_capsule = torch.matmul(capsule_softmax_weight, item_eb_hat)\n                cap_norm = torch.sum(torch.square(interest_capsule), -1, True)\n                scalar_factor = cap_norm \/ (1 + cap_norm) \/ torch.sqrt(cap_norm + 1e-9)\n                interest_capsule = scalar_factor * interest_capsule\n\n        interest_capsule = interest_capsule.view(-1, self.interest_num, self.hidden_size)\n","AFTER":"    def forward(self, item_eb, mask, device):\n        if self.bilinear_type == 0:  # MIND\n            item_eb_hat = self.linear(item_eb)  # [b, s, h]\n            item_eb_hat = item_eb_hat.repeat(1, 1, self.interest_num)  # [b, s, h*in]\n        elif self.bilinear_type == 1:\n            item_eb_hat = self.linear(item_eb)\n        else:  # ComiRec_DR\n            u = torch.unsqueeze(item_eb, dim=2)  # shape=(batch_size, maxlen, 1, embedding_dim)\n            item_eb_hat = torch.sum(self.w[:, :self.seq_len, :, :] * u,\n                                    dim=3)  # shape=(batch_size, maxlen, hidden_size*interest_num)\n\n        item_eb_hat = torch.reshape(item_eb_hat, (-1, self.seq_len, self.interest_num, self.hidden_size))\n        item_eb_hat = torch.transpose(item_eb_hat, 1, 2).contiguous()\n        item_eb_hat = torch.reshape(item_eb_hat, (-1, self.interest_num, self.seq_len, self.hidden_size))\n\n        # [b, in, s, h]\n        if self.stop_grad:  # 截断反向传播，item_emb_hat不计入梯度计算中\n            item_eb_hat_iter = item_eb_hat.detach()\n        else:\n            item_eb_hat_iter = item_eb_hat\n\n        # b的shape=(b, in, s)\n        if self.bilinear_type > 0:  # b初始化为0（一般的胶囊网络算法）\n            capsule_weight = torch.zeros(item_eb_hat.shape[0], self.interest_num, self.seq_len, device=device,\n                                         requires_grad=False)\n        else:  # MIND使用高斯分布随机初始化b\n            capsule_weight = torch.randn(item_eb_hat.shape[0], self.interest_num, self.seq_len, device=device,\n                                         requires_grad=False)\n\n        for i in range(self.routing_times):  # 动态路由传播3次\n            atten_mask = torch.unsqueeze(mask, 1).repeat(1, self.interest_num, 1)  # [b, in, s]\n            paddings = torch.zeros_like(atten_mask, dtype=torch.float)\n\n            # 计算c，进行mask，最后shape=[b, in, 1, s]\n            capsule_softmax_weight = F.softmax(capsule_weight, dim=-1)\n            capsule_softmax_weight = torch.where(torch.eq(atten_mask, 0), paddings, capsule_softmax_weight)  # mask\n            capsule_softmax_weight = torch.unsqueeze(capsule_softmax_weight, 2)\n\n            if i < 2:\n                # s=c*u_hat , (batch_size, interest_num, 1, seq_len) * (batch_size, interest_num, seq_len, hidden_size)\n                interest_capsule = torch.matmul(capsule_softmax_weight,\n                                                item_eb_hat_iter)  # shape=(batch_size, interest_num, 1, hidden_size)\n                cap_norm = torch.sum(torch.square(interest_capsule), -1, True)  # shape=(batch_size, interest_num, 1, 1)\n                scalar_factor = cap_norm \/ (1 + cap_norm) \/ torch.sqrt(cap_norm + 1e-9)  # shape同上\n                interest_capsule = scalar_factor * interest_capsule  # squash(s)->v,shape=(batch_size, interest_num, 1, hidden_size)\n\n                # 更新b\n                delta_weight = torch.matmul(item_eb_hat_iter,  # shape=(batch_size, interest_num, seq_len, hidden_size)\n                                            torch.transpose(interest_capsule, 2, 3).contiguous()\n                                            # shape=(batch_size, interest_num, hidden_size, 1)\n                                            )  # u_hat*v, shape=(batch_size, interest_num, seq_len, 1)\n                delta_weight = torch.reshape(delta_weight, (\n                -1, self.interest_num, self.seq_len))  # shape=(batch_size, interest_num, seq_len)\n                capsule_weight = capsule_weight + delta_weight  # 更新b\n            else:\n                interest_capsule = torch.matmul(capsule_softmax_weight, item_eb_hat)\n                cap_norm = torch.sum(torch.square(interest_capsule), -1, True)\n                scalar_factor = cap_norm \/ (1 + cap_norm) \/ torch.sqrt(cap_norm + 1e-9)\n                interest_capsule = scalar_factor * interest_capsule\n\n        interest_capsule = torch.reshape(interest_capsule, (-1, self.interest_num, self.hidden_size))\n"}