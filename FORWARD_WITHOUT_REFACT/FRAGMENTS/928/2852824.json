{"BEFORE":"        x = x + pos_emb\n\n        if self.num_streams > 1:\n            x = repeat(x, 'b n d -> b n (s d)', s = self.num_streams)\n\n        x = rearrange(x, 'b n d -> b d n')\n\n        for attn, ff in self.layers:\n            x = attn(x, mask = mask) + x\n            x = ff(x) + x\n\n        if self.num_streams > 1:\n            x = reduce(x, 'b (s d) n -> b d n', 'mean', s = self.num_streams)\n\n        return self.to_logits(x)\n","AFTER":"        x = self.token_emb(x)\n        dim = x.shape[-1]\n\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n        pos_emb = rearrange(pos_emb, 'n d -> () n d')\n\n        x = x + pos_emb\n        x = rearrange(x, 'b n d -> b d n')\n\n        pre_attn, pre_ff = self.pre_layers\n        post_attn, post_ff = self.post_layers\n\n        x = pre_attn(x, mask = mask) + x\n        x = pre_ff(x) + x\n\n        layers = [x]\n\n        if self.num_streams > 1:\n            x = repeat(x, 'b d n -> b (s d) n', s = self.num_streams)\n\n        for attn, ff in self.layers:\n            x = attn(x, mask = mask) + x\n            x = ff(x) + x\n            layers.append(x)\n\n        layers = list(map(lambda t: rearrange(t, 'b (s d) n -> (b n) d s', d = dim), layers))\n        layer_tokens = torch.cat(layers, dim = -1)\n\n        query = repeat(self.query, 'd -> b d ()', b = layer_tokens.shape[0])\n        x = self.attn_pool(query, context = layer_tokens)\n        x = rearrange(x, '(b n) d () -> b d n', n = n)\n\n        x = post_attn(x, mask = mask) + x\n        x = post_ff(x) + x\n"}