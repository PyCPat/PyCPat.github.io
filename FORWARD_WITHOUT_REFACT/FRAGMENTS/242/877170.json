{"BEFORE":"        attn = dots.softmax(dim = -1)\n        out = einsum('b i j, b j d -> b i d', attn, v)\n","AFTER":"    def forward(self, x, context, mask = None):\n        h, scale = self.heads, self.scale\n\n        q = self.to_q(x)\n        kv = self.to_kv(context).chunk(2, dim = -1)\n\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, *kv))\n        dots = einsum('b h i d, b h j d -> b h i j', q, k) * scale\n\n        if exists(mask):\n            mask = rearrange(mask, 'b n -> b () () n')\n            dots.masked_fill_(~mask, float('-inf'))\n\n        attn = dots.softmax(dim = -1)\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n"}