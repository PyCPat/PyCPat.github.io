{"BEFORE":"        padding_mask = tokens.eq(self.padding_idx)\n        if not padding_mask.any():\n            padding_mask = None\n\n        # embed positions\n        positions = (\n            self.embed_positions(tokens)\n            if self.embed_positions is not None else None\n        )\n\n        # embed segments\n        segments = (\n            self.segment_embeddings(segment_labels)\n            if self.segment_embeddings is not None\n            else None\n        )\n\n        x = self.embed_tokens(tokens)\n\n        if positions is not None:\n            x += positions\n        if segments is not None:\n            x += segments\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # account for padding while computing the representation\n        if padding_mask is not None:\n            x *= (1 - padding_mask.unsqueeze(-1).type_as(x))\n","AFTER":"        padding_mask = tokens.eq(self.padding_idx)\n        if not padding_mask.any():\n            padding_mask = None\n\n        x = self.embed_tokens(tokens)\n        if self.embed_scale is not None:\n            x *= self.embed_scale\n\n        if self.embed_positions is not None:\n            x += self.embed_positions(tokens)\n\n        if self.segment_embeddings is not None and segment_labels is not None:\n            x += self.segment_embeddings(segment_labels)\n\n        if self.emb_layer_norm is not None:\n            x = self.emb_layer_norm(x)\n\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # account for padding while computing the representation\n        if padding_mask is not None:\n            x *= (~padding_mask).unsqueeze(-1).type_as(x)\n"}