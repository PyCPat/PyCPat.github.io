<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        sparse_dict = sparse_features.to_dict()
        flattened_sparse_features = torch.cat(
            <a id="change">[sparse_dict[key].values() + offset for key, offset in zip(keys, self.offsets)]</a>)
        batch_offsets = sparse_features.offsets()

        batch_size = len(sparse_features.lengths()) // len(keys)
        feature_size = len(keys)
        flattened_sparse_embeddings<a id="change"> = </a>self.embedding(flattened_sparse_features,
                                                 batch_offsets,
                                                 send_shape=(batch_size, feature_size, -1))
        <a id="change">return </a>flattened_sparse_embeddings
    

class FeatureLinear(nn.Module):</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 print(&quotSaved params (M)&quot,emb_dim*(sum(field_dims) - math.ceil(math.sqrt(sum(field_dims))))//1_000_000)

    def forward(self,sparse_features):
        <a id="change">return </a>self.embedding(sparse_features)
    

class FeatureLinear(nn.Module):</code></pre>