{"BEFORE":"        logit = torch.einsum(\"nhwmd,mkd->nhwmk\", q, k)\n        if first:\n            hard = iGumbelSoftmax((logit \/ self._scale), temperature, False)\n        else:\n            # [n, h, w, m, k]\n            hard = torch.distributions.OneHotCategorical(logits=logit).sample(())\n        quantized = torch.einsum(\"nhwmk,mkc->nhwmc\", hard, v).reshape(n, h, w, -1).permute(0, 3, 1, 2)\n        # [n, c, h, w], [n, h, w, m], [n, h, w, m, k]\n        return quantized, logit.argmax(-1).byte(), logit\n","AFTER":"        logit = torch.einsum(\"nhwmd,mkd->nhwmk\", q, k)\n        if first:\n            # hard = iGumbelSoftmax((logit \/ self._scale), temperature, False)\n            hard = F.gumbel_softmax(logit \/ self._scale * self._temperature, temperature, True)\n        else:\n            # [n, h, w, m, k]\n            hard = torch.distributions.OneHotCategorical(logits=logit).sample(())\n        quantized = torch.einsum(\"nhwmk,mkc->nhwmc\", hard, v).reshape(n, h, w, -1).permute(0, 3, 1, 2)\n        # [n, c, h, w], [n, h, w, m], [n, h, w, m, k]\n        return quantized, logit.argmax(-1), logit\n"}