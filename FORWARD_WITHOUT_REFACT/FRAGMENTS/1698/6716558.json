{"BEFORE":"        cls_token = self.cls_token.expand(x.shape[0], 1, -1)\n        combined = torch.cat((cls_token, x), dim=1)\n        combined = self.norm_1(combined)\n\n        cls_token, _ = self.attn(combined[:,:1], combined, combined, need_weights=False)\n        cls_token = torch.flatten(cls_token, 1)                     # (N, 1, C) -> (N, C)\n        cls_token = cls_token + self.drop_path(cls_token * self.layer_scale_1)\n        cls_token = self.norm_2(cls_token)\n\n        cls_token = self.mlp(cls_token)\n        cls_token = cls_token + self.drop_path(cls_token * self.layer_scale_2)\n        cls_token = self.norm_3(cls_token)\n\n        return cls_token\n","AFTER":"        out = self.norm_1(out)\n        out = self.attn(out[:,:1], out, out, need_weights=False)[0]\n        cls_token = cls_token + self.drop_path(out * self.layer_scale_1)    # residual + layer scale + dropout\n\n        # mlp\n        out = self.norm_2(cls_token)\n        out = self.mlp(out)\n        cls_token = cls_token + self.drop_path(out * self.layer_scale_2)\n        \n        out = self.norm_3(cls_token).squeeze(1)     # (N, 1, C) -> (N, C)\n        return out\n"}