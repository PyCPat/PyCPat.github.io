{"BEFORE":"    def __init__(self, input_dims, batch_size, activation='relu', C=16,\n                 n_residual_blocks=8, channel_norm=True):\n\n        \"\"\" \n        Generator with convolutional architecture proposed in [1].\n        Upscales quantized encoder output into feature map of size C x W x H.\n        Expects input size (C,16,16)\n        ========\n        Arguments:\n        input_dims: Dimensions of quantized representation, (C,H,W)\n        batch_size: Number of instances per minibatch\n        C:          Encoder bottleneck depth, controls bits-per-pixel\n                    C = {2,4,8,16}\n\n        [1] Mentzer et. al., \"High-Fidelity Generative Image Compression\", \n            arXiv:2006.09965 (2020).\n        \"\"\"\n        \n        super(Generator, self).__init__()\n        \n        kernel_dim = 3\n        filters = (960, 480, 240, 120, 60)\n        self.n_residual_blocks = n_residual_blocks\n        assert input_dims == (C, 16, 16), 'Inconsistent input dims to generator'\n\n        # Layer \/ normalization options\n        cnn_kwargs = dict(stride=2, padding=1, output_padding=1)\n        norm_kwargs = dict(momentum=0.1, affine=True, track_running_stats=False)\n        self.activation = getattr(F, activation)  # (leaky_relu, relu, elu)\n        \n        if channel_norm is True:\n            interlayer_norm = normalization.ChannelNorm2D_wrap\n        else:\n            interlayer_norm = normalization.InstanceNorm2D_wrap\n\n        self.pre_pad = nn.ReflectionPad2d(1)\n        self.asymmetric_pad = nn.ReflectionPad2d((0,1,1,0))  # Slower than tensorflow?\n        self.post_pad = nn.ReflectionPad2d(3)\n\n        H0, W0 = input_dims[1:]\n        heights = (2**i for i in range(5,9))\n        widths = heights\n        H1, H2, H3, H4 = heights\n        W1, W2, W3, W4 = widths \n\n        # (16,16) -> (16,16), with implicit padding\n        self.conv_block_init = nn.Sequential(\n            interlayer_norm((batch_size, C, H0, W0), **norm_kwargs),\n            self.pre_pad,\n            nn.Conv2d(C, filters[0], kernel_size=(3,3), stride=1),\n            interlayer_norm((batch_size, filters[0], H0, W0), **norm_kwargs),\n        )\n\n        for m in range(n_residual_blocks):\n            resblock_m = ResidualBlock(in_channels=filters[0], \n                norm_layer=interlayer_norm, activation=activation)\n            self.add_module('resblock_{}'.format(str(m)), resblock_m)\n        \n        # TODO Transposed conv. alternative: https:\/\/distill.pub\/2016\/deconv-checkerboard\/\n        # (16,16) -> (32,32)\n        self.upconv_block1 = nn.Sequential(\n            nn.ConvTranspose2d(filters[0], filters[1], kernel_dim, **cnn_kwargs),\n            interlayer_norm((batch_size, filters[1], H1, W1), **norm_kwargs),\n            self.activation,\n        )\n\n        self.upconv_block2 = nn.Sequential(\n            nn.ConvTranspose2d(filters[1], filters[2], kernel_dim, **cnn_kwargs),\n            interlayer_norm((batch_size, filters[2], H2, W2), **norm_kwargs),\n            self.activation,\n        )\n\n        self.upconv_block3 = nn.Sequential(\n            nn.ConvTranspose2d(filters[2], filters[3], kernel_dim, **cnn_kwargs),\n            interlayer_norm((batch_size, filters[3], H3, W3), **norm_kwargs),\n            self.activation,\n        )\n\n        self.upconv_block4 = nn.Sequential(\n            nn.ConvTranspose2d(filters[3], filters[4], kernel_dim, **cnn_kwargs),\n            interlayer_norm((batch_size, filters[4], H4, W4), **norm_kwargs),\n            self.activation,\n        )\n\n        self.conv_block_out = nn.Sequential(\n            self.post_pad,\n            nn.Conv2d(im_channels, filters[0], kernel_size=(7,7), stride=1),\n","AFTER":"    def __init__(self, input_dims, batch_size, C=16, activation='relu',\n                 n_residual_blocks=8, channel_norm=True):\n\n        \"\"\" \n        Generator with convolutional architecture proposed in [1].\n        Upscales quantized encoder output into feature map of size C x W x H.\n        Expects input size (C,16,16)\n        ========\n        Arguments:\n        input_dims: Dimensions of quantized representation, (C,H,W)\n        batch_size: Number of instances per minibatch\n        C:          Encoder bottleneck depth, controls bits-per-pixel\n                    C = {2,4,8,16}\n\n        [1] Mentzer et. al., \"High-Fidelity Generative Image Compression\", \n            arXiv:2006.09965 (2020).\n        \"\"\"\n        \n        super(Generator, self).__init__()\n        \n        kernel_dim = 3\n        filters = (960, 480, 240, 120, 60)\n        self.n_residual_blocks = n_residual_blocks\n        assert input_dims == (C, 16, 16), 'Inconsistent input dims to generator'\n\n        # Layer \/ normalization options\n        cnn_kwargs = dict(stride=2, padding=1, output_padding=1)\n        norm_kwargs = dict(momentum=0.1, affine=True, track_running_stats=True)\n        activation_d = dict(relu='ReLU', elu='ELU', leaky_relu='LeakyReLU')\n        self.activation = getattr(nn, activation_d[activation])  # (leaky_relu, relu, elu)\n        \n        if channel_norm is True:\n            self.interlayer_norm = normalization.ChannelNorm2D_wrap\n        else:\n            self.interlayer_norm = normalization.InstanceNorm2D_wrap\n\n        self.pre_pad = nn.ReflectionPad2d(1)\n        self.asymmetric_pad = nn.ReflectionPad2d((0,1,1,0))  # Slower than tensorflow?\n        self.post_pad = nn.ReflectionPad2d(3)\n\n        H0, W0 = input_dims[1:]\n        heights = [2**i for i in range(5,9)]\n        widths = heights\n        H1, H2, H3, H4 = heights\n        W1, W2, W3, W4 = widths \n\n        # (16,16) -> (16,16), with implicit padding\n        self.conv_block_init = nn.Sequential(\n            self.interlayer_norm((batch_size, C, H0, W0), **norm_kwargs),\n            self.pre_pad,\n            nn.Conv2d(C, filters[0], kernel_size=(3,3), stride=1),\n            self.interlayer_norm((batch_size, filters[0], H0, W0), **norm_kwargs),\n        )\n\n        for m in range(n_residual_blocks):\n            resblock_m = ResidualBlock(in_channels=filters[0], \n                norm_layer=self.interlayer_norm, activation=activation)\n            self.add_module('resblock_{}'.format(str(m)), resblock_m)\n        \n        # TODO Transposed conv. alternative: https:\/\/distill.pub\/2016\/deconv-checkerboard\/\n        # (16,16) -> (32,32)\n        self.upconv_block1 = nn.Sequential(\n            nn.ConvTranspose2d(filters[0], filters[1], kernel_dim, **cnn_kwargs),\n            self.interlayer_norm((batch_size, filters[1], H1, W1), **norm_kwargs),\n            self.activation(),\n        )\n\n        self.upconv_block2 = nn.Sequential(\n            nn.ConvTranspose2d(filters[1], filters[2], kernel_dim, **cnn_kwargs),\n            self.interlayer_norm((batch_size, filters[2], H2, W2), **norm_kwargs),\n            self.activation(),\n        )\n\n        self.upconv_block3 = nn.Sequential(\n            nn.ConvTranspose2d(filters[2], filters[3], kernel_dim, **cnn_kwargs),\n            self.interlayer_norm((batch_size, filters[3], H3, W3), **norm_kwargs),\n            self.activation(),\n        )\n\n        self.upconv_block4 = nn.Sequential(\n            nn.ConvTranspose2d(filters[3], filters[4], kernel_dim, **cnn_kwargs),\n            self.interlayer_norm((batch_size, filters[4], H4, W4), **norm_kwargs),\n            self.activation(),\n        )\n\n        self.conv_block_out = nn.Sequential(\n            self.post_pad,\n            nn.Conv2d(filters[-1], 3, kernel_size=(7,7), stride=1),\n"}