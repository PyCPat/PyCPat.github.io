{"BEFORE":"        amp = False,\n        group_wd_params = True,\n        device = None,\n        accelerator = None,\n        verbose = True,\n        **kwargs\n    ):\n        super().__init__()\n        assert isinstance(diffusion_prior, DiffusionPrior)\n        assert not exists(accelerator) or isinstance(accelerator, Accelerator)\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # verbosity\n\n        self.verbose = verbose\n\n        # assign some helpful member vars\n\n        self.accelerator = accelerator\n        self.text_conditioned = diffusion_prior.condition_on_text_encodings\n\n        # setting the device\n\n        if not exists(accelerator) and not exists(device):\n            diffusion_prior_device = next(diffusion_prior.parameters()).device\n            self.print(f'accelerator not given, and device not specified: defaulting to device of diffusion prior parameters - {diffusion_prior_device}')\n            self.device = diffusion_prior_device\n        else:\n            self.device = accelerator.device if exists(accelerator) else device\n            diffusion_prior.to(self.device)\n\n        # save model\n\n        self.diffusion_prior = diffusion_prior\n\n        # optimizer and mixed precision stuff\n\n        self.amp = amp\n\n        self.scaler = GradScaler(enabled = amp)\n\n        self.optim_kwargs = dict(lr=lr, wd=wd, eps=eps, group_wd_params=group_wd_params)\n\n        self.optimizer = get_optimizer(\n            self.diffusion_prior.parameters(),\n            **self.optim_kwargs,\n            **kwargs\n        )\n\n        # distribute the model if using HFA\n        if exists(self.accelerator):\n            self.diffusion_prior, self.optimizer = self.accelerator.prepare(self.diffusion_prior, self.optimizer)\n\n        # exponential moving average stuff\n\n        self.use_ema = use_ema\n\n        if self.use_ema:\n            self.ema_diffusion_prior = EMA(self.unwrap_model(self.diffusion_prior), **ema_kwargs)\n","AFTER":"        accelerator,\n        use_ema = True,\n        lr = 3e-4,\n        wd = 1e-2,\n        eps = 1e-6,\n        max_grad_norm = None,\n        group_wd_params = True,\n        warmup_steps = 1,\n        **kwargs\n    ):\n        super().__init__()\n        assert isinstance(diffusion_prior, DiffusionPrior)\n        assert isinstance(accelerator, Accelerator)\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # assign some helpful member vars\n\n        self.accelerator = accelerator\n        self.text_conditioned = diffusion_prior.condition_on_text_encodings\n\n        # setting the device\n\n        self.device = accelerator.device\n        diffusion_prior.to(self.device)\n\n        # save model\n\n        self.diffusion_prior = diffusion_prior\n\n        # mixed precision checks\n\n        if (\n            exists(self.accelerator) \n            and self.accelerator.distributed_type == DistributedType.DEEPSPEED \n            and self.diffusion_prior.clip is not None\n            ):\n            # Then we need to make sure clip is using the correct precision or else deepspeed will error\n            cast_type_map = {\n                \"fp16\": torch.half,\n                \"bf16\": torch.bfloat16,\n                \"no\": torch.float\n            }\n            precision_type = cast_type_map[accelerator.mixed_precision]\n            assert precision_type == torch.float, \"DeepSpeed currently only supports float32 precision when using on the fly embedding generation from clip\"\n            self.diffusion_prior.clip.to(precision_type)\n\n        # optimizer stuff\n\n        self.optim_kwargs = dict(lr=lr, wd=wd, eps=eps, group_wd_params=group_wd_params)\n\n        self.optimizer = get_optimizer(\n            self.diffusion_prior.parameters(),\n            **self.optim_kwargs,\n            **kwargs\n        )\n        \n        self.scheduler = LambdaLR(self.optimizer, lr_lambda = lambda _: 1.0)\n        \n        self.warmup_scheduler = warmup.LinearWarmup(self.optimizer, warmup_period = warmup_steps) if exists(warmup_steps) else None\n\n        # distribute the model if using HFA\n\n        self.diffusion_prior, self.optimizer, self.scheduler = self.accelerator.prepare(self.diffusion_prior, self.optimizer, self.scheduler)\n\n        # exponential moving average stuff\n\n        self.use_ema = use_ema\n\n        if self.use_ema:\n            self.ema_diffusion_prior = EMA(self.accelerator.unwrap_model(self.diffusion_prior), **ema_kwargs)\n"}