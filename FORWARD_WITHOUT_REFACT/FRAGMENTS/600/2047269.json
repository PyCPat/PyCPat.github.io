{"BEFORE":"        assert (hidden is not None) or self.use_attention, \"No use of a decoder with No attention and No Hidden\"\n\n        batch_sz = x.shape[0]\n\n        # x (batch_size, 1, dec_embed_dim) -> after embedding\n        x = self.embedding(x)\n\n        if hidden is None:\n            # hidden: n_layers, batch_size, hidden_dim\n            hidden = torch.zeros((self.dec_layers, batch_sz,\n                                    self.dec_hidden_dim )).to(self.device)\n\n        if self.use_attention:\n            # x (batch_size, 1, dec_embed_dim + hidden_size) -> after attention\n            # aw: (batch_size, max_length, 1)\n            x, aw = self.attention( x, hidden, enc_output)\n        else:\n            x, aw = x, 0\n\n        # passing the concatenated vector to the GRU\n        # output: (batch_size, n_layers, hidden_size)\n        # hidden: n_layers, batch_size, hidden_size\n        output, hidden = self.gru(x, hidden)\n","AFTER":"        if (hidden is None) and (self.use_attention is False):\n            raise Exception( \"No use of a decoder with No attention and No Hidden\")\n\n        batch_sz = x.shape[0]\n\n        if hidden is None:\n            # hidden: n_layers, batch_size, hidden_dim\n            hid_for_att = torch.zeros((self.dec_layers, batch_sz,\n                                    self.dec_hidden_dim )).to(self.device)\n        elif self.dec_rnn_type == 'lstm':\n            hid_for_att = hidden[1] # c_n\n\n        # x (batch_size, 1, dec_embed_dim) -> after embedding\n        x = self.embedding(x)\n\n        if self.use_attention:\n            # x (batch_size, 1, dec_embed_dim + hidden_size) -> after attention\n            # aw: (batch_size, max_length, 1)\n            x, aw = self.attention( x, hidden, enc_output)\n        else:\n            x, aw = x, 0\n\n        # passing the concatenated vector to the GRU\n        # output: (batch_size, n_layers, hidden_size)\n        # hidden: n_layers, batch_size, hidden_size | if LSTM (h_n, c_n)\n        output, hidden = self.dec_rnn(x, hidden) if hidden is not None else self.dec_rnn(x)\n"}