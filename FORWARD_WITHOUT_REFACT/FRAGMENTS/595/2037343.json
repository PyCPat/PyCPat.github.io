{"BEFORE":"        print('entity_input is nan:', torch.isnan(x).any()) if debug else None\r\n\r\n        # calculate there are how many real entities in each batch\r\n        tmp_x = torch.mean(x, dim=2, keepdim=False)\r\n        tmp_y = (tmp_x > self.bias_value + 1e3)\r\n        real_number_tensor = torch.sum(tmp_y, dim=1, keepdim=False)\r\n        # this means for each batch, there are how many real enetities\r\n        print('real_number_tensor:', real_number_tensor) if debug else None\r\n\r\n        x = self.embedd(x)\r\n\r\n        # x is batch_entities_tensor (dim = 3). Shape: batch_size x entities_size x embeding_size\r\n        # change: x is batch_seq_entities_tensor (dim = 4). Shape: batch_size x seq_size x entities_size x embeding_size\r\n        print('x.shape:', x.shape) if debug else None\r\n\r\n        # may found nan here: can use this to do torch.nan_to_num\r\n        out = self.transformer(x)\r\n\r\n        print('out.shape:', out.shape) if debug else None\r\n\r\n        entity_embeddings = F.relu(self.conv1(F.relu(out).transpose(1, 2))).transpose(1, 2)\r\n        print('entity_embeddings.shape:', entity_embeddings.shape) if debug else None\r\n\r\n        # masked by the missing entries\r\n        # note, different batch may contain different number of real entities\r\n        tensor_list = []\r\n        for i, batch in enumerate(out):\r\n            mean_entity = 0.\r\n            real_number = real_number_tensor[i]\r\n            real_number = real_number if real_number != 0 else 1\r\n            for j, entity in enumerate(batch):\r\n                if j >= real_number_tensor[i]:\r\n                    break        \r\n                mean_entity = mean_entity + entity\r\n            mean_entity = mean_entity \/ (real_number)\r\n            tensor_list.append(mean_entity.reshape(1, -1))\r\n        tensor_mean = torch.cat(tensor_list, dim=0)\r\n        print('tensor_mean:', tensor_mean) if debug else None\r\n","AFTER":"    def forward(self, x, debug=True):\r\n        # refactor thanks mostly to the codes from https:\/\/github.com\/opendilab\/DI-star\r\n        batch_size = x.shape[0]\r\n\r\n        # calculate there are how many real entities in each batch\r\n        # tmp_x: [batch_seq_size x entities_size]\r\n        tmp_x = torch.mean(x, dim=2, keepdim=False)\r\n\r\n        # tmp_y: [batch_seq_size x entities_size]\r\n        tmp_y = (tmp_x > self.bias_value + 1e3)\r\n\r\n        # entity_num: [batch_seq_size]\r\n        entity_num = torch.sum(tmp_y, dim=1, keepdim=False)\r\n\r\n        # this means for each batch, there are how many real enetities\r\n        print('entity_num:', entity_num) if debug else None\r\n\r\n        # generate the mask for transformer\r\n        mask = torch.arange(0, self.max_entities).float()\r\n        mask = mask.repeat(batch_size, 1)\r\n        mask = mask < entity_num.unsqueeze(dim=1)\r\n\r\n        print('mask:', mask) if debug else None\r\n        print('mask.shape:', mask.shape) if debug else None\r\n\r\n        # mask: [batch_size, max_entities]\r\n        device = next(self.parameters()).device\r\n        mask = mask.to(device)\r\n\r\n        # assert the input shape is : batch_seq_size x entities_size x embeding_size\r\n        # note: because the feature size of entity is not equal to 256, so it can not fed into transformer directly.\r\n        # thus, we add a embedding layer to transfer it to right size.\r\n        # x is batch_entities_tensor (dim = 3). Shape: batch_seq_size x entities_size x embeding_size\r\n        x = self.embedd(x)\r\n        print('x.shape:', x.shape) if debug else None\r\n\r\n        # mask for transformer need a special format\r\n        mask_seq_len = mask.shape[-1]\r\n        tran_mask = mask.unsqueeze(1)\r\n\r\n        # tran_mask: [batch_seq_size x max_entities x max_entities]\r\n        tran_mask = tran_mask.repeat(1, mask_seq_len, 1)\r\n\r\n        # out: [batch_seq_size x entities_size x embeding_size]\r\n        out = self.transformer(x, mask=tran_mask)\r\n        print('out.shape:', out.shape) if debug else None\r\n\r\n        # entity_embeddings: [batch_seq_size x entities_size x conv1_output_size]\r\n        entity_embeddings = F.relu(self.conv1(F.relu(out).transpose(1, 2))).transpose(1, 2)\r\n        print('entity_embeddings.shape:', entity_embeddings.shape) if debug else None\r\n\r\n        # AlphaStar: The mean of the transformer output across across the units (masked by the missing entries) \r\n        # is fed through a linear layer of size 256 and a ReLU to yield `embedded_entity`\r\n        masked_out = out * mask.unsqueeze(2)\r\n\r\n        # sum over across the units\r\n        # masked_out: [batch_seq_size x entities_size x embeding_size]\r\n        # z: [batch_size, embeding_size]\r\n        z = masked_out.sum(dim=1, keepdim=False)\r\n\r\n        # here we should dived by the entity_num, not the cls.max_entities\r\n        # z: [batch_size, embeding_size]\r\n        z = z \/ entity_num\r\n"}