{"BEFORE":"        x = sub_graph_out.x.view(-1, time_step_len, self.polyline_vec_shape)\n\n        # TODO: compute the adjacency matrix???\n        # reconstruct the batch global interaction graph data\n        if isinstance(data, Batch):\n            # mini-batch case\n            global_g_data = Batch()\n            batch_list = []\n            for idx in range(data.num_graphs):\n                node_list = torch.tensor([i for i in range(valid_lens[idx])]).long()\n                edge_index = torch.combinations(node_list, 2).transpose(1, 0)\n\n                # print(x[idx, :, :].size())\n                batch_list.append(Data(x=F.normalize(x[idx, :, :], dim=1).squeeze(0),\n                                       edge_index=edge_index,\n                                       valid_lens=valid_lens[idx],\n                                       time_step_len=time_step_len))\n            global_g_data = global_g_data.from_data_list(batch_list)\n        elif isinstance(data, Data):\n            # single batch case\n            node_list = torch.tensor([i for i in range(valid_lens[0])]).long()\n            edge_index = torch.combinations(node_list, 2).transpose(1, 0)\n            global_g_data = Data(x=F.normalize(x[0, :, :], dim=3).squeeze(0),\n                                 edge_index=edge_index,\n                                 valid_lens=valid_lens,\n                                 time_step_len=time_step_len)\n        else:\n            raise NotImplementedError\n\n        global_g_data.to(self.device)\n        if self.training:\n            # mask out the features for a random subset of polyline nodes\n            # for one batch, we mask the same polyline features\n\n            global_graph_out, mask_polyline_indices = self.global_graph(global_g_data)\n            global_graph_out = global_graph_out.view(-1, time_step_len, self.polyline_vec_shape)\n\n            pred = self.traj_pred_mlp(global_graph_out[:, [0]].squeeze(1))\n            if self.with_aux:\n                aux_in = torch.empty(\n                    (global_graph_out.size()[0], self.polyline_vec_shape),\n                    device=self.device\n                )\n                aux_gt = torch.empty(\n                    (global_graph_out.size()[0], self.polyline_vec_shape),\n                    device=self.device\n                )\n                for i, idx in enumerate(mask_polyline_indices):\n                    aux_in[i] = global_graph_out[i, idx].squeeze(0)\n                    aux_gt[i] = x[i, idx].squeeze(0)\n                aux_out = self.aux_mlp(aux_in)\n\n                return pred, aux_out, aux_gt\n            else:\n                return pred, None, None\n\n        else:\n            # print(\"x size:\", x.size())\n\n            global_graph_out, _ = self.global_graph(global_g_data)\n","AFTER":"        time_step_len = int(data.time_step_len[0])\n        valid_lens = data.valid_len\n\n        # print(\"valid_lens type:\", type(valid_lens).__name__)\n        # print(\"data batch size:\", data.num_batch)\n\n        sub_graph_out = self.subgraph(data)\n        x = sub_graph_out.x.view(-1, time_step_len, self.subgraph_width)\n\n        if self.training and self.with_aux:\n            mask_polyline_indices = [random.randint(0, time_step_len - 1) + i * time_step_len for i in\n                                     range(x.size()[0])]\n            x = x.view(-1, self.subgraph_width)\n            aux_gt = x[mask_polyline_indices]\n            x[mask_polyline_indices] = 0.0\n            x = x.view(-1, time_step_len, self.subgraph_width)\n\n        # TODO: compute the adjacency matrix???\n        # reconstruct the batch global interaction graph data\n        if isinstance(data, Batch):\n            # mini-batch case\n            global_g_data = Batch()\n            batch_list = []\n            for idx in range(data.num_graphs):\n                node_list = torch.tensor([i for i in range(valid_lens[idx])]).long()\n                edge_index = torch.combinations(node_list, 2).transpose(1, 0)\n\n                # print(x[idx, :, :].size())\n                batch_list.append(Data(x=F.normalize(x[idx, :, :], dim=1).squeeze(0),\n                                       edge_index=edge_index,\n                                       valid_lens=valid_lens[idx],\n                                       time_step_len=time_step_len))\n            global_g_data = global_g_data.from_data_list(batch_list)\n        elif isinstance(data, Data):\n            # single batch case\n            node_list = torch.tensor([i for i in range(valid_lens[0])]).long()\n            edge_index = torch.combinations(node_list, 2).transpose(1, 0)\n            global_g_data = Data(x=F.normalize(x[0, :, :], dim=3).squeeze(0),\n                                 edge_index=edge_index,\n                                 valid_lens=valid_lens,\n                                 time_step_len=time_step_len)\n        else:\n            raise NotImplementedError\n\n        global_g_data.to(self.device)\n        if self.training:\n            # mask out the features for a random subset of polyline nodes\n            # for one batch, we mask the same polyline features\n\n            global_graph_out = self.global_graph(global_g_data)\n            global_graph_out = global_graph_out.view(-1, time_step_len, self.polyline_vec_shape)\n\n            pred = self.traj_pred_mlp(global_graph_out[:, [0]].squeeze(1))\n            if self.with_aux:\n                aux_in = global_graph_out.view(-1, self.global_graph_width)[mask_polyline_indices]\n                aux_out = self.aux_mlp(aux_in)\n                return pred, aux_out, aux_gt\n            else:\n                return pred, None, None\n\n        else:\n            # print(\"x size:\", x.size())\n\n            global_graph_out = self.global_graph(global_g_data)\n"}