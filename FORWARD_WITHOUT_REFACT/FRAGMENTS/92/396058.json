{"BEFORE":"        seq = self.dropout((self.tok_embed(seq) * self.scale) + self.pos_encoding(pos))\n        print(any(torch.isnan(seq).view(-1)))\n        seq = seq.transpose(0, 1)\n        out = self.transformer(seq, seq).transpose(0, 1)\n        print(seq, '\\n', out)\n","AFTER":"        mask = self._generate_square_subsequent_mask(seq.shape[1])\n        print(mask)\n        pos = torch.arange(0, seq.shape[1]).unsqueeze(0).repeat(seq.shape[0], 1).to(self.args.device)\n        seq = (self.tok_embed(seq) * self.scale) + self.pos_encoding(pos)\n"}