{"BEFORE":"        sparse_emb_list = self.embedding_layer(data)\n        feature_emb = torch.stack(sparse_emb_list, dim=1).squeeze(2)\n        print(feature_emb.shape)\n\n        attention_out = self.self_attention(feature_emb)\n        attention_out = attention_out.flatten(start_dim=1)\n        y_pred = self.fc(attention_out)\n        if self.dnn is not None:\n            y_pred += self.dnn(feature_emb.flatten(start_dim=1))\n","AFTER":"        feature_emb = self.embedding_layer(data)\n        attention_out = self.self_attention(feature_emb)\n        attention_out = attention_out.flatten(start_dim=1)\n        y_pred = self.fc(attention_out)\n        if self.dnn is not None:\n            dense_input = get_linear_input(self.enc_dict, data)\n            emb_flatten = feature_emb.flatten(start_dim=1)\n            y_pred += self.dnn(torch.cat([emb_flatten, dense_input], dim=1))\n"}