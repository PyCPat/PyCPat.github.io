{"BEFORE":"            context, self.attention_weights = self.apply_forward_attention(\n                inputs, alignment, query)\n        else:\n            context = torch.bmm(alignment.unsqueeze(1), inputs)\n            context = context.squeeze(1)\n            self.attention_weights = alignment\n","AFTER":"            alignment = self.apply_forward_attention(alignment)\n            self.alpha = alignment\n\n        context = torch.bmm(alignment.unsqueeze(1), inputs)\n        context = context.squeeze(1)\n        self.attention_weights = alignment\n\n        # compute transition agent\n        if self.forward_attn and self.trans_agent:\n            ta_input = torch.cat([context, query.squeeze(1)], dim=-1)\n            self.u = torch.sigmoid(self.ta(ta_input))\n        return context\n"}