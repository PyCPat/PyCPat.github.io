{"BEFORE":"            nei_message = self.W_h(nei_message)\n            if self.master_node:\n                # master_state = self.W_master_in(self.act_func(nei_message.sum(dim=0))) #try something like this to preserve invariance for master node\n                # master_state = self.GRU_master(nei_message.unsqueeze(1))\n                # master_state = master_state[-1].squeeze(0) #this actually doesn't preserve order invariance anymore\n                master_state = self.act_func(self.W_master_in(nei_message.sum(dim=0))).unsqueeze(0)\n                message = self.act_func(binput + nei_message + self.W_master_out(master_state).repeat((nei_message.size(0), 1)))\n                message = self.layer_norm(message)\n","AFTER":"        fatoms, fbonds, agraph, bgraph, scope, bscope = mol_graph\n        if next(self.parameters()).is_cuda:\n            fatoms, fbonds, agraph, bgraph = fatoms.cuda(), fbonds.cuda(), agraph.cuda(), bgraph.cuda()\n\n        # Input\n        binput = self.W_i(fbonds)\n        message = self.act_func(binput)\n\n        # Message passing\n        for i in range(self.depth - 1):\n            nei_message = index_select_ND(message, bgraph)\n            if self.message_attention:\n                message = message.unsqueeze(1).repeat((1, nei_message.size(1), 1))  # num_bonds x 1 x hidden\n                attention_scores = [(self.W_ma[i](nei_message) * message).sum(dim=2)\n                                    for i in range(self.num_heads)]  # num_bonds x maxnb\n                attention_weights = [F.softmax(attention_scores[i], dim=1)\n                                     for i in range(self.num_heads)]  # num_bonds x maxnb\n                message_components = [nei_message * attention_weights[i].unsqueeze(2).repeat((1, 1, self.hidden_size))\n                                      for i in range(self.num_heads)]  # num_bonds x maxnb x hidden\n                message_components = [component.sum(dim=1) for component in message_components]  # num_bonds x hidden\n                nei_message = torch.cat(message_components, dim=1)  # num_bonds x 3*hidden\n            else:\n                nei_message = nei_message.sum(dim=1)  # num_bonds x hidden\n            nei_message = self.W_h(nei_message)\n            if self.master_node:\n                # master_state = self.W_master_in(self.act_func(nei_message.sum(dim=0))) #try something like this to preserve invariance for master node\n                # master_state = self.GRU_master(nei_message.unsqueeze(1))\n                # master_state = master_state[-1].squeeze(0) #this actually doesn't preserve order invariance anymore\n                mol_vecs = [torch.zeros(self.hidden_size).cuda()]\n                for start, size in bscope:\n                    mol_vec = nei_message.narrow(0, start, size)\n                    mol_vec = mol_vec.sum(dim=0) \/ size\n                    mol_vecs += [mol_vec for _ in range(size)]\n                master_state = self.act_func(self.W_master_in(torch.stack(mol_vecs, dim=0)))  # (num_bonds, hidden_size)\n                message = self.act_func(binput + nei_message + self.W_master_out(master_state))\n"}