{"BEFORE":"        first_h_a = self.initHidden_alpha(x.shape[1])\n        first_h_b = self.initHidden_beta(x.shape[1])\n\n        self.emb = self.embedding(x)\n        if self.drop < 1:\n            self.emb = self.dropout(self.emb)\n\n        count = np.arange(x.shape[0]) + 1\n        self.c_t = torch.zeros_like(self.emb)  # shape=(seq_len, batch_size, day_dim)\n        for i, att_timesteps in enumerate(count):\n            # 按时间步迭代，计算每个时间步的经attention的gru输出\n            self.c_t[i] = self.attentionStep(first_h_a, first_h_b, att_timesteps)\n\n        if self.drop < 1.0:\n            self.c_t = self.dropout(self.c_t)\n\n        # # output层\n        # y_hat = self.out(self.c_t)\n        # y_hat = torch.sigmoid(y_hat)\n        return self.c_t\n","AFTER":"        batch_size, time_steps, _ = x.size()\n        x = self.proj(x)\n        x = self.dropout(x)\n\n        out = torch.zeros((batch_size, time_steps, self.hidden_dim))\n\n        for cur_time in range(time_steps):\n            cur_x = x[:, : cur_time + 1, :]\n            out[:, cur_time, :] = self.retain_encoder(cur_x)\n        return out\n"}