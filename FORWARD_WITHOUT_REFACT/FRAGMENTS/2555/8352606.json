{"BEFORE":"        state: Optional[Dict[str, Dict[str, Optional[torch.Tensor]]]] = None,\n    ) -> Tuple[torch.Tensor, None]:\n        \"\"\"Input shape: Time x Batch x Channel\n        Args:\n            padding_mask (ByteTensor, optional): mask to exclude\n                keys that are pads, of shape `(batch, src_len)`, where\n                padding elements are indicated by 1s.\n        \"\"\"\n\n        bsz, seq_len, d_model = x.size()\n        assert d_model == self.d_model\n\n        residual = x * self.omega\n\n        # L x B x D -> B x D x L\n        # x = x.permute(1, 2, 0)\n        x = x.transpose(-1, -2)  # (B D L)\n        if padding_mask is not None:\n            x = x * (1.0 - padding_mask.unsqueeze(1).type_as(x))\n\n        assert not self.bidirectional or state is None, 'Bidirectional EMA does not support incremental state'\n        if state is not None:\n            saved_state = self._get_input_buffer(state)\n            if 'prev_state' in saved_state:\n                h = saved_state['prev_state']\n            else:\n                h = None\n            out, h = self.step(x, seq_len, hx=h)\n            saved_state['prev_state'] = h\n            self._set_input_buffer(state, saved_state)\n            # B x D -> 1 x B x D\n            # out = F.silu(out + residual)\n            out = out + residual\n        else:\n            # D x L\n            k = self.kernel(seq_len)\n            fft_len = seq_len\n            s = 0\n            l_kernel = k.size(1)\n            assert l_kernel == seq_len\n            if self.bidirectional:\n                k1, k2 = torch.split(k, [self.d_model, self.d_model], dim=0)\n                # D x 2*L-1\n                k = F.pad(k1, (l_kernel - 1, 0)) + F.pad(k2.flip(-1), (0, l_kernel - 1))\n                x = F.pad(x, (l_kernel - 1, 0))\n                fft_len = fft_len + l_kernel - 1\n                s = 2 * l_kernel - 2\n\n            k_f = torch.fft.rfft(k.float(), n=2 * fft_len)\n            x_f = torch.fft.rfft(x.float(), n=2 * fft_len)\n            # B x D x L\n            out = torch.fft.irfft(x_f * k_f, n=2 * fft_len)[..., s:s + seq_len]\n            out = out.type_as(x)\n            # B x D x L -> L x B x D\n            # out = F.silu(out.transpose(-1, -2) + residual)\n            out = out.transpose(-1, -2) + residual\n\n        return out, None  # empty state\n","AFTER":"        state: Optional[Dict[str, Dict[str, Optional[torch.Tensor]]]] = None,\n        padding_mask: Optional[torch.Tensor] = None,\n    ) -> Tuple[torch.Tensor, None]:\n        \"\"\"Input shape: Time x Batch x Channel\n        Args:\n            padding_mask (ByteTensor, optional): mask to exclude\n                keys that are pads, of shape `(batch, src_len)`, where\n                padding elements are indicated by 1s.\n        \"\"\"\n\n        B, L, H = u.size()\n        assert H == self.H\n\n        u = u.transpose(-1, -2)  # (B H L)\n        if padding_mask is not None:\n            u = u * (1.0 - padding_mask.unsqueeze(1).type_as(u))\n\n        # assert not self.bidirectional or state is None, 'Bidirectional EMA does not support incremental state'\n        if state is not None:\n            raise NotImplementedError(\n                \"MultiHeadEMA module does not support state passing in this repository.\"\n                \"Use S4D for more functionality such as state passing and better performance.\"\n            )\n        else:\n            k = self.kernel(L)  # (H L)\n            l_fft = L\n            s = 0\n            l_kernel = k.size(1)\n            assert l_kernel == L\n            u_ = u\n            if self.bidirectional:\n                # This is twice as inefficient as it could be\n                # See S4 FFT conv bidirectional implementation for improvement\n                k1, k2 = torch.split(k, [self.H, self.H], dim=0)\n                k = F.pad(k1, (l_kernel - 1, 0)) + F.pad(k2.flip(-1), (0, l_kernel - 1))  # (H 2*L-1)\n                u_ = F.pad(u, (l_kernel - 1, 0))\n                l_fft = l_fft + l_kernel - 1\n                s = 2 * l_kernel - 2\n\n            k_f = torch.fft.rfft(k.float(), n=2 * l_fft)\n            u_f = torch.fft.rfft(u_.float(), n=2 * l_fft)\n            y = torch.fft.irfft(u_f * k_f, n=2 * l_fft)[..., s:s + L]  # (B H L)\n            y = y.type_as(u)\n            y = y + u * self.omega.unsqueeze(-1)  # (B H L)\n            y = y.transpose(-1, -2)\n\n        return y, None  # empty state\n"}