{"BEFORE":"                block_wrapper(Attention(dim = dim, dim_head = dim_head, heads = heads, dropout = attn_dropout)),\n","AFTER":"        memorizing_layers = default(memorizing_layers, (depth \/\/ 2,)) # default KNN attention layer to midpoint of transformer\n        memorizing_layers = cast_tuple(memorizing_layers)\n\n        self.layers = nn.ModuleList([])\n        for idx in range(depth):\n            use_knn_attention = (idx + 1) in memorizing_layers\n\n            if use_knn_attention:\n                attn = KNNAttention(dim = dim, dim_head = dim_head, heads = heads, dropout = attn_dropout, num_retrieved_memories = num_retrieved_memories)\n            else:\n                attn = Attention(dim = dim, dim_head = dim_head, heads = heads, dropout = attn_dropout)\n\n            self.layers.append(nn.ModuleList([\n                block_wrapper(attn),\n"}