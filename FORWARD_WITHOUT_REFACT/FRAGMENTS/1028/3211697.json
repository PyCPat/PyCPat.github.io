{"BEFORE":"        self.to_mem_kv = nn.Linear(dim, heads * dim_head * 2, bias = False)\n\n        # main layers\n\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Residual(PreNorm(dim, Attention(dim = dim, heads = heads, dim_head = dim_head, dropout = attn_dropout))),\n                Residual(PreNorm(dim, FeedForward(dim = dim, dropout = ff_dropout)))\n            ]))\n\n        self.to_logits = nn.Sequential(\n","AFTER":"        self.layers = nn.ModuleList([])\n        shared_kv_proj = None\n\n        for _ in range(depth):\n            attn = Attention(dim = dim, heads = heads, dim_head = dim_head, dropout = attn_dropout)\n            ff = FeedForward(dim = dim, dropout = ff_dropout)\n\n            shared_kv_proj = default(shared_kv_proj, attn.to_kv)\n            attn.to_kv = shared_kv_proj\n\n            self.layers.append(nn.ModuleList([\n                Residual(PreNorm(dim, attn)),\n                Residual(PreNorm(dim, ff))\n            ]))\n\n        # memory parameters\n\n        self.layer_weight = nn.Parameter(torch.ones(depth))\n        self.to_mem_kv = shared_kv_proj\n"}