<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.register_buffer("ema_weight", self.embedding.clone())

    def forward(self, x):
        B, C, L = <a id="change">x.size()</a>
        N, M, D = self.embedding.size()
        assert C == N * D

        x<a id="change"> = </a><a id="change">x.view(</a>B, N, D, L<a id="change">)</a>.permute(1, 0, 3, 2)  &#47&#47 N, B, L, D
        x_flat = x.detach().reshape(N, -1, D)

        distances = torch.cdist(x_flat, self.embedding)
        indices = torch.argmin(distances, dim=-1)

        encodings = F.one_hot(indices, M).float()
        quantized = torch.gather(self.embedding, 1, indices.unsqueeze(-1).expand(-1, -1, D))
        quantized = quantized.view_as(x)

        if self.training:
            self.ema_count = self.decay * self.ema_count + (1 - self.decay) * torch.sum(encodings, dim=1)

            n = torch.sum(self.ema_count, dim=-1, keepdim=True)
            self.ema_count = (self.ema_count + self.epsilon) / (n + M * self.epsilon) * n

            dw = torch.bmm(encodings.transpose(1, 2), x_flat)
            self.ema_weight = self.decay * self.ema_weight + (1 - self.decay) * dw

            self.embedding = self.ema_weight / self.ema_count.unsqueeze(-1)

        e_latent_loss = F.mse_loss(x, quantized.detach())
        loss = self.commitment_cost * e_latent_loss

        quantized = x + (quantized - x).detach()

        avg_probs = torch.mean(encodings, dim=1)
        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10), dim=-1))

        return quantized.permute(1, 0, 3, 2).reshape(B, C, L), loss, <a id="change">perplexity.sum()</a>


class ChannelNorm(nn.Module):
    def __init__(self,</code></pre><h3>After Change</h3><pre><code class='java'>
        M, D = self.embedding.size()
        x_flat = x.detach().reshape(-1, D)

        distances = torch.addmm(torch.sum(self.embedding<a id="change"> ** </a>2, dim=1)<a id="change"> +
                                </a><a id="change">torch.sum(</a>x_flat ** 2<a id="change">, dim=1, keepdim=True)</a>,
                                x_flat, self.embedding.t(),
                                alpha=-2.0, beta=1.0)
</code></pre>