{"BEFORE":"        mem = default(mem, lambda: torch.empty(num_memory_layers, b, 0, d, **to(x)))\n        cmem = default(cmem, lambda: torch.empty(num_memory_layers, b, 0, d, **to(x)))\n\n        total_len = mem.shape[2] + cmem.shape[2] + t\n        pos_emb = self.pos_emb[:, (self.seq_len - t):total_len]\n\n        next_mem = []\n        next_cmem = []\n        aux_loss = torch.tensor(0., requires_grad = True, **to(x))\n\n        mem_iter = iterate_tensor(mem)\n        cmem_iter = iterate_tensor(cmem)\n\n        for ind, (attn, ff, m, c) in enumerate(zip(self.attn_layers, self.ff_layers, mem, cmem)):\n            layer_num = ind + 1\n            use_memory = layer_num in self.memory_layers\n\n            memories = (next(mem_iter), next(cmem_iter)) if use_memory else None\n","AFTER":"            use_memory = layer_num in self.memory_layers\n\n            memories = None\n            if use_memory:\n                memories = (next(mem_iter), next(cmem_iter))\n\n            x, (mem_out, cmem_out), layer_aux_loss = attn(x, memories = memories, calc_memory = use_memory, input_mask = mask, pos_emb = pos_emb)\n"}