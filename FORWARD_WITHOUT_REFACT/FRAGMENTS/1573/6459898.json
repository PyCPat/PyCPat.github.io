{"BEFORE":"        image = self.image_patch_emb(image)  # 1 x 64 x 16 x 16\r\n        if video is not None:\r\n            video = video.permute(0, 2, 1, 3, 4)  # B x T x C x H x W -> B x C x T x H x W\r\n            video = self.video_patch_emb(video)  # 1 x 64 x 20 x 16 x 16\r\n            video = torch.cat([image.unsqueeze(2), video], dim=2)  # 1 x 64 x 21 x 16 x 16\r\n        else:\r\n            video = image.unsqueeze(2)\r\n        video = video.view(*video.shape[:3], -1).permute(0, 2, 3, 1)  # B x T x (H x W) x C -> 1 x 21 x (16*16) x 64\r\n        video = self.attention(video)  # 1 x 21 x 256 x 64\r\n","AFTER":"        video = video.permute(0, 2, 1, 3, 4)\r\n        video = torch.cat([self.learned_frame.unsqueeze(0), image.unsqueeze(2), video], dim=2)\r\n        video = self.stem(video)\r\n        s = None\r\n        if len(self.encoder) > 0:\r\n            for block, remapper in zip(self.encoder, self.remapper):\r\n                if block.scaler is not None:\r\n                    prev_s = nn.functional.interpolate(video, scale_factor=(1, 0.5, 0.5), recompute_scale_factor=False)\r\n                else:\r\n                    prev_s = video\r\n                video = block(video, s)\r\n                if block.scaler is not None:\r\n                    video = remapper(video)\r\n                s = prev_s\r\n        video = self.bottleneck(video)\r\n"}