{"BEFORE":"        bs = q.size(0)\n\n        # perform linear operation and split into h heads\n        if not self.kq_same:\n            q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n        else:\n            q = self.k_linear(q).view(bs, -1, self.h, self.d_k)\n        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n\n        # transpose to get dimensions bs * h * -1 * d_k\n        q = q.transpose(1, 2)\n        k = k.transpose(1, 2)\n        v = v.transpose(1, 2)\n\n        # calculate attention using function we will define next\n        output = self.scaled_dot_product_attention(q, k, v, self.d_k, mask)\n\n        # concatenate heads and put through final linear layer\n        output = output.transpose(1, 2).reshape(bs, -1, self.d_model)\n","AFTER":"            q = self.head_split(self.q_linear(q))\n        else:\n            q = self.head_split(self.k_linear(q))\n        k = self.head_split(self.k_linear(k))\n        v = self.head_split(self.v_linear(v))\n\n        # calculate attention using function we will define next\n        output = self.scaled_dot_product_attention(q, k, v, self.d_k, mask)\n\n        # concatenate heads and put through final linear layer\n        output = torch.cat(torch.unbind(output, dim=-3), dim=-1)\n"}