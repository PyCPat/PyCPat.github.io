{"BEFORE":"        raise NotImplemented\n","AFTER":"        return_loss = False\n    ):\n        if return_loss:\n            labels, ids = ids.clone(), ids[:, :-1]\n\n        tokens = self.semantic_embedding(ids)\n\n        start_tokens = repeat(self.start_token, 'd -> b 1 d', b = ids.shape[0])\n\n        tokens = torch.cat((start_tokens, tokens), dim = 1)\n\n        tokens = self.transformer(tokens)\n        logits = self.to_logits(tokens)\n\n        if not return_loss:\n            return logits\n\n        loss = F.cross_entropy(\n            rearrange(logits, 'b n c -> b c n'),\n            labels\n        )\n\n        return loss\n"}