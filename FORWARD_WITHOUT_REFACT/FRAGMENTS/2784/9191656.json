{"BEFORE":"        raise NotImplemented\n","AFTER":"        coarse_token_ids, fine_token_ids = map(lambda t: rearrange(t, 'b ... -> b (...)'), (coarse_token_ids, fine_token_ids))\n\n        b, n = coarse_token_ids.shape\n\n        coarse_tokens = self.coarse_embedding(coarse_token_ids)\n        fine_tokens = self.fine_embedding(fine_token_ids)\n\n        start_tokens = repeat(self.start_token, 'd -> b 1 d', b = b)\n\n        tokens = torch.cat((start_tokens, coarse_tokens, fine_tokens), dim = 1)\n\n        tokens = self.transformer(tokens)\n\n        pred_coarse_tokens, pred_fine_tokens = tokens[:, :n], tokens[:, n:]\n\n        coarse_logits = self.coarse_logits(pred_coarse_tokens)\n        fine_logits = self.fine_logits(pred_fine_tokens)\n\n        return coarse_logits, fine_logits\n"}