{"BEFORE":"        noise_level = self.log_snr(times)\n        padded_noise_level = right_pad_dims_to(img, noise_level)\n        alpha, sigma =  log_snr_to_alpha_sigma(padded_noise_level)\n\n        noised_img = alpha * img + sigma * noise\n\n        # if doing self-conditioning, 50% of the time, predict x_start from current set of times\n        # and condition with unet with that\n        # this technique will slow down training by 25%, but seems to lower FID significantly\n\n        self_cond = None\n        if random() < 0.5:\n            with torch.no_grad():\n                self_cond = self.model(noised_img, noise_level).detach_()\n\n        # predict and take gradient step\n\n        pred = self.model(noised_img, noise_level, self_cond)\n","AFTER":"        noise_level = self.log_snr(times)\n        padded_noise_level = right_pad_dims_to(img, noise_level)\n        alpha, sigma =  log_snr_to_alpha_sigma(padded_noise_level)\n\n        noised_img = alpha * img + sigma * noise\n\n        # in the paper, they had to use a really high probability of latent self conditioning, up to 90% of the time\n        # slight drawback\n\n        self_cond = self_latents = None\n\n        if random() < self.train_prob_self_cond:\n            with torch.no_grad():\n                self_cond, self_latents = self.model(noised_img, noise_level, return_latents = True)\n                self_cond = self_cond.detach()\n                self_latents = self_latents.detach()\n\n        # predict and take gradient step\n\n        pred = self.model(noised_img, noise_level, self_cond, self_latents)\n"}