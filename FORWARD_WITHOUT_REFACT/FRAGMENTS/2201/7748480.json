{"BEFORE":"            uncertain_infos = []\n            for i, (layer_module, (k, layer_classifier_module)) in enumerate(\n                    zip(self.layer, self.layer_classifiers.items())):\n                hidden_states = layer_module(hidden_states[0], attention_mask)\n                logits = layer_classifier_module(hidden_states[0])\n                prob = F.softmax(logits, dim=-1)\n                log_prob = F.log_softmax(logits, dim=-1)\n                uncertain = torch.sum(prob * log_prob, 1) \/ (-torch.log(self.num_class))\n                uncertain_infos.append([uncertain, prob])\n\n                # return early results\n                if uncertain < inference_speed:\n                    return prob, i, uncertain_infos\n            return prob, i, uncertain_infos\n","AFTER":"    def forward(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor, device: str, inference: bool = False,\n                inference_speed: float = 0.5, training_stage: int = 1):\n        \"\"\"\"\"\"\n        hidden_states = (hidden_states,)\n        # In the Inference stage, if the uncertainty of the i-th student is low it will be\n        # returned earlier.\n        # To handle batches we have to compute the uncertainty and regroup the low-confidence\n        # examples into a new batch that is fed to the next layer.\n        if inference:\n            # positions will keep track of the original position of each element in the\n            # batch when elements will be removed\n            final_probs = torch.zeros((hidden_states[0].shape[0], 2), device=device)\n            positions = torch.arange(start=0, end=hidden_states[0].shape[0], device=device).long()\n\n            for i, (layer_module, (k, layer_classifier_module)) in enumerate(\n                    zip(self.layer, self.layer_classifiers.items())):\n\n                hidden_states = layer_module(hidden_states[0], attention_mask)\n                logits = layer_classifier_module(hidden_states[0])\n                prob = F.softmax(logits, dim=-1)\n                log_prob = F.log_softmax(logits, dim=-1)\n                uncertain = torch.sum(prob * log_prob, 1) \/ (-torch.log(self.num_class))\n\n                # checking if there's enough information\n                enough_info = uncertain < inference_speed\n\n                right_pos = positions[enough_info]\n                final_probs[right_pos] = prob[enough_info]\n\n                hidden_states = (hidden_states[0][~enough_info],)\n                attention_mask = attention_mask[~enough_info]\n\n                # if we have processed all the samples\n                if hidden_states[0].shape[0] == 0:\n                    return final_probs, i\n\n                positions = positions[~enough_info]  # updating the positions to fit the new batch\n\n            return final_probs, i\n"}