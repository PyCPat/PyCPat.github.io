{"BEFORE":"        gate = self.norm(gate)\n\n        weight, bias = self.weight, self.bias\n\n        if self.use_circulant_matrix:\n            dim_seq = weight.shape[-1]\n            weight = F.pad(weight, (0, dim_seq), value = 0)\n            weight = repeat(weight, '... n -> ... (r n)', r = dim_seq)\n            weight = weight[:-dim_seq].reshape(dim_seq, 2 * dim_seq - 1)\n            weight = weight[:, (dim_seq - 1):]\n\n            pos_x, pos_y = self.circulant_pos_x, self.circulant_pos_y\n            weight = weight * rearrange(pos_x, 'i -> i ()') * rearrange(pos_y, 'j -> () j')\n\n        if self.causal:\n            weight, bias = weight[:n, :n], bias[:n]\n            mask = torch.ones(weight.shape[:2], device = device).triu_(1).bool()\n            weight = weight.masked_fill(mask, 0.)\n\n        gate = einsum('b n d, m n -> b m d', gate, weight)\n        gate = gate + rearrange(bias, 'n -> () n ()')\n","AFTER":"        device, n, h = x.device, x.shape[1], self.heads\n\n        res, gate = x.chunk(2, dim = -1)\n        gate = self.norm(gate)\n\n        weight, bias = self.weight, self.bias\n\n        if self.use_circulant_matrix:\n            # build the circulant matrix\n\n            dim_seq = weight.shape[-1]\n            weight = F.pad(weight, (0, dim_seq), value = 0)\n            weight = repeat(weight, '... n -> ... (r n)', r = dim_seq)\n            weight = weight[:, :-dim_seq].reshape(h, dim_seq, 2 * dim_seq - 1)\n            weight = weight[:, :, (dim_seq - 1):]\n\n            # give circulant matrix absolute position awareness\n\n            pos_x, pos_y = self.circulant_pos_x, self.circulant_pos_y\n            weight = weight * rearrange(pos_x, 'h i -> h i ()') * rearrange(pos_y, 'h j -> h () j')\n\n        if self.causal:\n            weight, bias = weight[:, :n, :n], bias[:, :n]\n            mask = torch.ones(weight.shape[-2:], device = device).triu_(1).bool()\n            mask = rearrange(mask, 'i j -> () i j')\n            weight = weight.masked_fill(mask, 0.)\n\n        gate = rearrange(gate, 'b n (h d) -> b h n d', h = h)\n\n        gate = einsum('b h n d, h m n -> b h m d', gate, weight)\n        gate = gate + rearrange(bias, 'h n -> () h n ()')\n\n        gate = rearrange(gate, 'b h n d -> b n (h d)')\n"}