{"BEFORE":"        drop_input = nn.Dropout(0.1)\n        dnn_layers.append(drop_input)\n        for i, (input_dim, hidden_units) in enumerate(zip(layers[:-1], layers[1:])):\n            fc = nn.Linear(input_dim, hidden_units)\n            activation = nn.ReLU()\n            bn = nn.BatchNorm1d(hidden_units)\n            drop = nn.Dropout(0.1)\n            seq = nn.Sequential(fc, bn, activation, drop)\n","AFTER":"        dnn_layers = []\n        drop_input = nn.Dropout(0.05)\n        dnn_layers.append(drop_input)\n        for i, (input_dim, hidden_units) in enumerate(zip(layers[:-1], layers[1:])):\n            fc = nn.Linear(input_dim, hidden_units)\n            activation = nn.ReLU()\n            bn = nn.BatchNorm1d(hidden_units)\n            seq = nn.Sequential(fc, bn, activation)\n            dnn_layers.append(seq)\n        drop_input = nn.Dropout(0.05)\n        dnn_layers.append(drop_input)\n"}