{"BEFORE":"        flatten = rearrange(x, '... d -> (...) d')\n        flatten = l2norm(flatten)\n\n        self.init_embed_(flatten)\n\n        embed = self.embed if not self.learnable_codebook else self.embed.detach()\n        embed = l2norm(embed)\n\n        dist = flatten @ embed.t()\n        embed_ind = gumbel_sample(dist, dim = -1, temperature = self.sample_codebook_temp)\n        embed_onehot = F.one_hot(embed_ind, self.codebook_size).type(dtype)\n        embed_ind = embed_ind.view(*shape[:-1])\n\n        quantize = F.embedding(embed_ind, self.embed)\n\n        if self.training:\n            bins = embed_onehot.sum(0)\n            self.all_reduce_fn(bins)\n\n            ema_inplace(self.cluster_size, bins, self.decay)\n\n            zero_mask = (bins == 0)\n            bins = bins.masked_fill(zero_mask, 1.)\n\n            embed_sum = flatten.t() @ embed_onehot\n            self.all_reduce_fn(embed_sum)\n\n            embed_normalized = (embed_sum \/ bins.unsqueeze(0)).t()\n            embed_normalized = l2norm(embed_normalized)\n            embed_normalized = torch.where(zero_mask[..., None], embed,\n","AFTER":"        needs_codebook_dim = x.ndim < 4\n\n        x = x.float()\n\n        if needs_codebook_dim:\n            x = rearrange(x, '... -> 1 ...')\n\n        shape, dtype = x.shape, x.dtype\n\n        flatten = rearrange(x, 'h ... d -> h (...) d')\n        flatten = l2norm(flatten)\n\n        self.init_embed_(flatten)\n\n        embed = self.embed if not self.learnable_codebook else self.embed.detach()\n        embed = l2norm(embed)\n\n        dist = einsum('h n d, h c d -> h n c', flatten, embed)\n        embed_ind = gumbel_sample(dist, dim = -1, temperature = self.sample_codebook_temp)\n        embed_onehot = F.one_hot(embed_ind, self.codebook_size).type(dtype)\n        embed_ind = embed_ind.view(*shape[:-1])\n\n        quantize = batched_embedding(embed_ind, self.embed)\n\n        if self.training:\n            bins = embed_onehot.sum(dim = 1)\n            self.all_reduce_fn(bins)\n\n            ema_inplace(self.cluster_size, bins, self.decay)\n\n            zero_mask = (bins == 0)\n            bins = bins.masked_fill(zero_mask, 1.)\n\n            embed_sum = einsum('h n d, h n c -> h c d', flatten, embed_onehot)\n            self.all_reduce_fn(embed_sum)\n\n            embed_normalized = embed_sum \/ rearrange(bins, '... -> ... 1')\n            embed_normalized = l2norm(embed_normalized)\n\n            embed_normalized = torch.where(\n                rearrange(zero_mask, '... -> ... 1'),\n                embed,\n                embed_normalized\n            )\n\n            ema_inplace(self.embed, embed_normalized, self.decay)\n            self.expire_codes_(x)\n\n        if needs_codebook_dim:\n            quantize, embed_ind = map(lambda t: rearrange(t, '1 ... -> ...'), (quantize, embed_ind))\n\n        return quantize, embed_ind\n"}