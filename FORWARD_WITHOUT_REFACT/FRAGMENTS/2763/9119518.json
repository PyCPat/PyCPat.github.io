{"BEFORE":"        keys = self.to_k(kv_input)\n        values = self.to_v(kv_input) if not self.share_kv else keys\n\n        kv_projs = (self.proj_k, self.proj_v if not self.share_kv else self.proj_k)\n\n        # project keys and values along the sequence length dimension to k\n        \n        keys, values = map(proj_seq_len, zip((keys, values), kv_projs))\n\n        # merge head into batch for queries and key \/ values\n\n        queries = queries.reshape(b, n, h, -1).transpose(1, 2)\n\n        merge_key_values = lambda t: t.reshape(b, k, -1, d_h).transpose(1, 2).expand(-1, h, -1, -1)\n        keys, values = map(merge_key_values, (keys, values))\n\n        # attention\n\n        dots = torch.einsum('bhnd,bhkd->bhnk', queries, keys)\n","AFTER":"        keys = self.to_k(kv_input)\n        values = self.to_v(kv_input) if not self.share_kv else keys\n\n        kv_projs = (self.proj_k, self.proj_v if not self.share_kv else self.proj_k)\n\n        # project keys and values along the sequence length dimension to k\n        \n        keys, values = map(proj_seq_len, zip((keys, values), kv_projs))\n\n        # merge head into batch for queries and key \/ values\n\n        queries = queries.reshape(b, n, h, -1).transpose(1, 2)\n\n        merge_key_values = lambda t: t.reshape(b, k, -1, d_h).transpose(1, 2).expand(-1, h, -1, -1)\n        keys, values = map(merge_key_values, (keys, values))\n\n        # attention\n\n        dots = torch.einsum('bhnd,bhkd->bhnk', queries, keys) * (d_h ** -0.5)\n        attn = dots.softmax(dim=-1)\n        attn = self.dropout(attn)\n"}