{"BEFORE":"        if self.batch_first:\n            batch_size, n_tokens, _ = x.shape # b, t, c\n        else:\n            n_tokens, batch_size, _ = x.shape # t, b, c\n\n        positions = repeat(self.pos_scale*torch.arange(n_tokens),\n                           't -> new_axis t', new_axis=batch_size).to(x)\n        positions = self.augment_positions(positions)\n\n        positions = rearrange(positions, 'b t -> b t 1')\n        product = positions * self.freq.to(x)\n\n        pos_emb = torch.sin(product + self.cos_shifts.to(x))\n\n        if not self.batch_first:\n            pos_emb = rearrange(pos_emb, 'b t c -> t b c')\n\n        return pos_emb\n","AFTER":"        return x + self.compute_pos_emb(x)\n"}