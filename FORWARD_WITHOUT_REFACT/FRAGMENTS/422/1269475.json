{"BEFORE":"        get_cross_attn = lambda: PreNorm(latent_dim, Attention(latent_dim, dim, heads = cross_heads, dim_head = cross_dim_head), context_dim = dim)\n        get_cross_ff = lambda: PreNorm(latent_dim, FeedForward(latent_dim))\n        get_latent_attn = lambda: PreNorm(latent_dim, Attention(latent_dim, heads = latent_heads, dim_head = latent_dim_head))\n        get_latent_ff = lambda: PreNorm(latent_dim, FeedForward(latent_dim))\n\n        get_cross_attn, get_cross_ff, get_latent_attn, get_latent_ff = map(cache_fn, (get_cross_attn, get_cross_ff, get_latent_attn, get_latent_ff))\n\n        self.layers = nn.ModuleList([])\n        for i in range(depth):\n            should_cache = i > 0 and weight_tie_layers\n            cache_args = {'_cache': should_cache}\n\n            self_attns = nn.ModuleList([])\n\n            for _ in range(self_per_cross_attn):\n                self_attns.append(nn.ModuleList([\n                    get_latent_attn(**cache_args),\n                    get_latent_ff(**cache_args)\n                ]))\n\n            self.layers.append(nn.ModuleList([\n                get_cross_attn(**cache_args),\n                get_cross_ff(**cache_args),\n                self_attns\n            ]))\n\n        self.decoder_cross_attn = PreNorm(queries_dim, Attention(queries_dim, latent_dim, heads = cross_heads, dim_head = cross_dim_head), context_dim = latent_dim)\n","AFTER":"        self.cross_attend_blocks = nn.ModuleList([\n            PreNorm(latent_dim, Attention(latent_dim, dim, heads = cross_heads, dim_head = cross_dim_head), context_dim = dim),\n            PreNorm(latent_dim, FeedForward(latent_dim))\n        ])\n\n        get_latent_attn = lambda: PreNorm(latent_dim, Attention(latent_dim, heads = latent_heads, dim_head = latent_dim_head))\n        get_latent_ff = lambda: PreNorm(latent_dim, FeedForward(latent_dim))\n        get_latent_attn, get_latent_ff = map(cache_fn, (get_latent_attn, get_latent_ff))\n\n        self.layers = nn.ModuleList([])\n        cache_args = {'_cache': weight_tie_layers}\n\n        for i in range(depth):\n            self.layers.append(nn.ModuleList([\n                get_latent_attn(**cache_args),\n                get_latent_ff(**cache_args)\n            ]))\n\n        self.decoder_cross_attn = PreNorm(queries_dim, Attention(queries_dim, latent_dim, heads = cross_heads, dim_head = cross_dim_head), context_dim = latent_dim)\n"}