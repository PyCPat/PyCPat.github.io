{"BEFORE":"        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n        q = q * self.scale\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n\n        sim = self.rel_pos_bias(sim) + sim\n\n        i, j = sim.shape[-2:]\n        causal_mask = torch.ones((i, j), dtype = torch.bool).triu(j - i + 1)\n        sim = sim.masked_fill(causal_mask, -torch.finfo(sim.dtype).max)\n\n        sim = sim - sim.amax(dim = -1, keepdim = True).detach()\n        attn = sim.softmax(dim = -1)\n        attn = self.dropout(attn)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n","AFTER":"        q = rearrange(q, 'b n (h d) -> b h n d', h = h)\n\n        q = q * self.scale\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k)\n\n        sim = self.rel_pos_bias(sim) + sim\n\n        i, j = sim.shape[-2:]\n        causal_mask = torch.ones((i, j), dtype = torch.bool).triu(j - i + 1)\n        sim = sim.masked_fill(causal_mask, -torch.finfo(sim.dtype).max)\n\n        attn = stable_softmax(sim)\n        attn = self.dropout(attn)\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n"}