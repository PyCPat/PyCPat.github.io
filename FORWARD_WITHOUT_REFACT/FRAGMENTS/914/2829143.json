{"BEFORE":"    def forward(self, logits, samples, soft):\n        if samples is None:\n            return self.gumbel_softmax(logits, self._temperature, self._eps, hard=True)\n        else:\n            return -torch.sum(-samples * F.log_softmax(logits, -1), -1)\n","AFTER":"    def forward(self, logits: torch.Tensor, tau: float = 1, hard: bool = False, dim: int = -1):\n        gumbels = -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log()  # ~Gumbel(0,1)\n        gumbels = (logits + gumbels) \/ tau  # ~Gumbel(logits,tau)\n        y_soft = gumbels.softmax(dim)\n\n        if hard:\n            # Straight through.\n            index = y_soft.max(dim, keepdim=True)[1]\n            y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0)\n            ret = y_hard - y_soft.detach() + y_soft\n        else:\n            # Reparametrization trick.\n            ret = y_soft\n        return ret, y_soft\n"}