{"BEFORE":"        x = torch.randn(1, 20, 8, 16, 16)\r\n        print(x.shape)\r\n        bs, tz, cz, wz, hz = x.shape\r\n        tsa = TemporalSpatialAttention(cz, wz, tz)\r\n        x = tsa(x)\r\n        print(x.shape)\r\n","AFTER":"        x = x.permute(0, 2, 1, 3, 4)\r\n        x = self.patch_emb(x)\r\n        x = x.permute(0, 2, 1, 3, 4)\r\n        x = self.attention(x)\r\n        return x\r\n"}