{"BEFORE":"        v = v if torch.is_tensor(v) else k if torch.is_tensor(k) else q\n        k = k if torch.is_tensor(k) else q\n\n        q = q.transpose(0, 1).contiguous()\n        k = k.transpose(0, 1).contiguous()\n        v = v.transpose(0, 1).contiguous()\n\n        b = q.size(1) * self._heads\n\n        q = self.q(q).view(-1, b, self._head_dims).transpose(0, 1)\n        k = self.k(k).view(-1, b, self._head_dims).transpose(0, 1)\n        v = self.v(v).view(-1, b, self._head_dims).transpose(0, 1)\n\n        att = torch.bmm(q, k.transpose(1, 2)) \/ self._h_dims**0.5\n\n        if mask is not None:\n            mask = torch.where(mask > 0, .0, float('-inf'))\n            mask = mask.repeat_interleave(self._heads, dim=0)\n            att += mask.unsqueeze(1).expand(-1, att.size(1), -1)\n\n        att = att.softmax(-1)\n\n        if self.dropout is not None:\n            att = self.dropout(att)\n\n        m = torch.bmm(att, v).transpose(0, 1).contiguous()\n        m = self.m(m).view(m.size(0), -1, self._h_dims).transpose(0, 1)\n","AFTER":"        q = self.q(q).transpose(0, 1).contiguous()\n        k = self.k(k).transpose(0, 1).contiguous()\n        v = self.v(v).transpose(0, 1).contiguous()\n\n        b = q.size(1) * self._heads\n\n        q = q.view(-1, b, self._head_dims).transpose(0, 1)\n        k = k.view(-1, b, self._head_dims).transpose(0, 1)\n        v = v.view(-1, b, self._head_dims).transpose(0, 1)\n\n        att = torch.bmm(q, k.transpose(1, 2)) \/ self._h_dims**0.5\n\n        if mask is not None:\n            mask = torch.where(mask > 0, .0, float('-inf'))\n            mask = mask.repeat_interleave(self._heads, dim=0)\n            att += mask.unsqueeze(1).expand(-1, att.size(1), -1)\n\n        att = att.softmax(-1)\n\n        if self.dropout is not None:\n            att = self.dropout(att)\n\n        m = torch.bmm(att, v).transpose(0, 1).contiguous()\n        m = m.view(m.size(0), -1, self._h_dims).transpose(0, 1)\n        m = self.m(m)\n"}