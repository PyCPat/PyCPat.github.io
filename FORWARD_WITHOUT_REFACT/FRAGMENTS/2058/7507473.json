{"BEFORE":"    def forward(self, x_spt, y_spt, x_qry, y_qry, c_spt, c_qry, graphlets):\n        \"\"\"\n        b: number of tasks\n        setsz: the size for each task\n\n        :param x_spt:   [b], where each unit is a mini-batch of subgraphs, i.e. x_spt[0] is a DGL batch of # setsz subgraphs\n        :param y_spt:   [b, setsz]\n        :param x_qry:   [b], where each unit is a mini-batch of subgraphs, i.e. x_spt[0] is a DGL batch of # setsz subgraphs\n        :param y_qry:   [b, querysz]\n        :return:\n        \"\"\"\n        task_num = len(x_spt)\n        querysz = len(y_qry[0])\n\n        losses_q = [0 for _ in range(self.update_step + 1)]  # losses_q[i] is the loss on step i\n        corrects = [0 for _ in range(self.update_step + 1)]\n        \n\n\n        #logits, _ = self.net(x_spt[0].to(device), c_spt[0].to(device), graphlets, vars=None)\n        #loss = F.cross_entropy(logits, y_spt[0].to(device))\n        #grad = torch.autograd.grad(loss, self.net.parameters())\n        #fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, self.net.parameters())))\n\n        #fast_weights = self.net.parameters()\n\n        for i in range(task_num):\n            '''\n            # this is the loss and accuracy before first update\n            with torch.no_grad():\n                # [setsz, nway]\n                logits_q, _ = self.net(x_qry[i].to(device), c_qry[i].to(device), graphlets, fast_weights)\n                loss_q = F.cross_entropy(logits_q, y_qry[i].to(device))\n                losses_q[0] += loss_q\n\n                pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n                correct = torch.eq(pred_q, y_qry[i].to(device)).sum().item()\n                corrects[0] = corrects[0] + correct\n            '''\n\n            for k in range(1, self.update_step):\n                logits, _ = self.net(x_spt[i].to(device), c_spt[i].to(device), graphlets)\n                loss = F.cross_entropy(logits, y_spt[i].to(device))\n                self.optim.zero_grad()\n                loss.backward()\n                self.optim.step()\n\n                logits_q, _ = self.net(x_qry[i].to(device), c_qry[i].to(device), graphlets)\n                loss_q = F.cross_entropy(logits_q, y_qry[i].to(device))\n\n                with torch.no_grad():\n                    pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n                    correct = torch.eq(pred_q, y_qry[i].to(device)).sum().item()  # convert to numpy\n                    corrects[k] = corrects[k] + correct\n\n\n        # end of all tasks\n        # sum over all losses on query set across all tasks\n        #loss_q = losses_q[-1] \/ task_num\n\n        # optimize theta parameters\n        #self.meta_optim.zero_grad()\n        #loss_q.backward()\n        # print('meta update')\n        # for p in self.net.parameters()[:5]:\n        #   print(torch.norm(p).item())\n        #self.meta_optim.step()\n\n        #print(querysz *task_num)\n        accs = np.array(corrects) \/ (querysz * task_num)\n","AFTER":"        for k in range(1, self.update_step):\n            logits, _ = self.net(x_spt.to(device), c_spt.to(device), graphlets)\n            loss = F.cross_entropy(logits, y_spt.to(device))\n            self.optim.zero_grad()\n            loss.backward()\n            self.optim.step()\n\n            with torch.no_grad():\n                pred = F.softmax(logits, dim=1).argmax(dim=1)\n                correct = torch.eq(pred, y_spt.to(device)).sum().item()  # convert to numpy\n                corrects[k] = corrects[k] + correct\n\n\n        # end of all tasks\n        # sum over all losses on query set across all tasks\n        #loss_q = losses_q[-1] \/ task_num\n\n        # optimize theta parameters\n        #self.meta_optim.zero_grad()\n        #loss_q.backward()\n        # print('meta update')\n        # for p in self.net.parameters()[:5]:\n        #   print(torch.norm(p).item())\n        #self.meta_optim.step()\n\n        #print(querysz *task_num)\n        accs = np.array(corrects) \/ len(x_spt)\n"}