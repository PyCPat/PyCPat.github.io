{"BEFORE":"    def forward(self, inputs: Tensor, memory: Tensor,  inputs_mask: Optional[Tensor] = None,\n                memory_mask: Optional[Tensor] = None) -> Tuple[Tensor, Tensor, Tensor]:\n        self_attns, encoder_attns = list(), list()\n        output = None\n\n        inputs = self.embedding(inputs)\n        inputs = self.positional_encoding(inputs)\n\n        for layer in self.layers:\n            output, self_attn, encoder_attn = layer(inputs, memory, inputs_mask, memory_mask)\n            self_attns.append(self_attn)\n            encoder_attns.append(encoder_attn)\n            inputs = output\n\n        return output, self_attns, encoder_attns\n","AFTER":"                input_lengths: Optional[Tensor] = None,\n                memory: Tensor = None) -> Tuple[Tensor, Tensor, Tensor]:\n        self_attns, memory_attns = list(), list()\n\n        non_pad_mask = get_pad_mask(targets, pad_id=self.pad_id).eq(False)\n        self_attn_mask = get_attn_pad_mask(targets, self.pad_id) | get_subsequent_mask(targets)\n        memory_mask = get_pad_mask(memory, input_lengths).squeeze(-1).unsqueeze(1).expand(-1, targets.size(1), -1)\n\n        output = self.input_dropout(self.embedding(targets) * self.logit_scale + self.pos_encoding(targets.size(1)))\n\n        for layer in self.layers:\n            output, self_attn, memory_attn = layer(output, memory, non_pad_mask, self_attn_mask, memory_mask)\n            self_attns.append(self_attn)\n            memory_attns.append(memory_attn)\n\n        return output, self_attns, memory_attns\n"}