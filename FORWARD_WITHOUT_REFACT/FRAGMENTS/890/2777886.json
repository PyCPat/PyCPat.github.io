{"BEFORE":"    loss_semantic = self.semantic_loss(anchor_output, embedding) + self.semantic_loss(positive_output, embedding) + self.semantic_loss(grad_reverse(anchor_output, self.grl_lambda), embedding)\n    loss_semantic = loss_semantic.mean()\n\n    loss_triplet = self.triplet_loss(anchor_output, positive_output, negative_output)\n\n    # Create targets for the domain loss(adversarial for the main model - as imposed by the GRL after every output)\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    batch_size = anchor_output.shape[0]\n    targets_sketch = torch.zeros(batch_size)\n    targets_photos = torch.ones(batch_size)\n","AFTER":"    loss_semantic = self.semantic_loss(anchor_output, embedding)\n    loss_semantic += self.semantic_loss(positive_output, embedding)  \n    loss_semantic += self.semantic_loss(grad_reverse(negative_output, self.grl_lambda), embedding)\n    loss_semantic = loss_semantic.mean()\n    loss_triplet = self.triplet_loss(anchor_output, positive_output, negative_output)\n\n    # Create targets for the domain loss(adversarial for the main model - as imposed by the GRL after every output)\n    batch_size = anchor_output.shape[0]\n    targets_sketch = torch.zeros(batch_size).to(self.device)\n    targets_photos = torch.ones(batch_size).to(self.device)\n"}