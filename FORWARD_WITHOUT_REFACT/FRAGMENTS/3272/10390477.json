{"BEFORE":"        gt_labels = self.current_data['gt_label']\n\n        student_logits = student \/ self.tau\n        teacher_logits = teacher \/ self.tau\n\n        teacher_probs = self.softmax(teacher_logits)\n\n        ce_loss = -torch.sum(\n            teacher_probs * self.logsoftmax(student_logits), 1, keepdim=True)\n\n        student_detach = student.detach()\n        teacher_detach = teacher.detach()\n        log_softmax_s = self.logsoftmax(student_detach)\n        log_softmax_t = self.logsoftmax(teacher_detach)\n        one_hot_labels = F.one_hot(\n            gt_labels, num_classes=self.num_classes).float()\n        ce_loss_s = -torch.sum(one_hot_labels * log_softmax_s, 1, keepdim=True)\n        ce_loss_t = -torch.sum(one_hot_labels * log_softmax_t, 1, keepdim=True)\n\n        focal_weight = ce_loss_s \/ (ce_loss_t + 1e-7)\n        ratio_lower = torch.zeros(1).cuda()\n","AFTER":"    def forward(self, student, teacher, data_samples):\n\n        # Unpack data samples and pack targets\n        if 'score' in data_samples[0].gt_label:\n            # Batch augmentation may convert labels to one-hot format scores.\n            gt_labels = torch.stack([i.gt_label.score for i in data_samples])\n            one_hot_labels = gt_labels.float()\n        else:\n            gt_labels = torch.hstack([i.gt_label.label for i in data_samples])\n            one_hot_labels = F.one_hot(\n                gt_labels, num_classes=self.num_classes).float()\n\n        student_logits = student \/ self.tau\n        teacher_logits = teacher \/ self.tau\n\n        teacher_probs = self.softmax(teacher_logits)\n\n        ce_loss = -torch.sum(\n            teacher_probs * self.logsoftmax(student_logits), 1, keepdim=True)\n\n        student_detach = student.detach()\n        teacher_detach = teacher.detach()\n        log_softmax_s = self.logsoftmax(student_detach)\n        log_softmax_t = self.logsoftmax(teacher_detach)\n        one_hot_labels = F.one_hot(\n            gt_labels, num_classes=self.num_classes).float()\n        ce_loss_s = -torch.sum(one_hot_labels * log_softmax_s, 1, keepdim=True)\n        ce_loss_t = -torch.sum(one_hot_labels * log_softmax_t, 1, keepdim=True)\n\n        focal_weight = ce_loss_s \/ (ce_loss_t + 1e-7)\n        ratio_lower = torch.zeros_like(focal_weight)\n"}