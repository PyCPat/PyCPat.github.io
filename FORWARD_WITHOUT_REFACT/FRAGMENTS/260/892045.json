{"BEFORE":"        sp = similarity_matrix[one_hot]\n\n        mask = one_hot.logical_not()\n        sn = similarity_matrix[mask]\n\n        sp = sp.view(self.batch_size, -1)\n        sn = sn.view(self.batch_size, -1)\n\n        alpha_p = self.relu(self.O_p - sp)\n        alpha_n = self.relu(sn - self.O_n)\n\n        r_sp_m = alpha_p * (sp - self.Delta_p)\n        r_sn_m = alpha_n * (sn - self.Delta_n)\n\n        _Z = torch.cat((r_sn_m, r_sp_m), 1)\n        _Z = _Z * self.gamma\n\n        logZ = torch.logsumexp(_Z, dim=1, keepdims=True)\n\n        loss =  -r_sp_m * self.gamma + logZ\n\n        return loss.mean()\n","AFTER":"        sp = similarity_matrix[one_hot]\n        mask = one_hot.logical_not()\n        sn = similarity_matrix[mask]\n\n        ap = torch.clamp_min(-sp.detach() + 1 + self.margin, min=0.)\n        an = torch.clamp_min(sn.detach() + self.margin, min=0.)\n\n        delta_p = 1 - self.margin\n        delta_n = self.margin\n\n        logit_p = - ap * (sp - delta_p) * self.gamma\n        logit_n = an * (sn - delta_n) * self.gamma\n\n        loss = self.soft_plus(torch.logsumexp(logit_n, dim=0) + torch.logsumexp(logit_p, dim=0))\n\n        return loss\n"}