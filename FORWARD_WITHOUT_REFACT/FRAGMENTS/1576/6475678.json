{"BEFORE":"        q = self.to_q(x)\n        kv = self.to_kv(kv_input).chunk(2, dim = -1)\n\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, *kv))\n        dots = einsum('b h i d, b h j d -> b h i j', q, k) * scale\n\n        if exists(self.to_pos):\n            p = self.to_pos(pos_emb)\n            pos_attn = einsum('b h i d, j d -> b h i j', q, p) * scale\n            pos_attn = rel_shift(pos_attn)\n","AFTER":"        dots = einsum('b h i d, b h j d -> b h i j', q, k) * scale\n\n        if exists(pos_emb):\n            pos_emb_bias = pos_emb(*dots.shape[-2:])\n"}