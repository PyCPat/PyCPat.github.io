{"BEFORE":"            dots.masked_fill_(~input_mask, float('-inf'))\n            del input_mask\n\n        if self.causal:\n            i, j = dots.shape[-2:]\n            mask = torch.ones((i, j), device = device).triu_(j - i + 1).bool()\n            dots.masked_fill_(mask, float('-inf'))\n            del mask\n\n        if exists(self.sparse_topk) and self.sparse_topk < dots.shape[-1]:\n            top, _ = dots.topk(self.sparse_topk, dim = -1)\n            vk = top[..., -1].unsqueeze(-1).expand_as(dots)\n            mask = dots < vk\n            dots.masked_fill_(mask, float('-inf'))\n","AFTER":"        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        mask_value = max_neg_value(dots)\n\n        if talking_heads:\n            dots = einsum('b h i j, h k -> b k i j', dots, self.pre_softmax_proj).contiguous()\n\n        if exists(rel_pos):\n            dots = rel_pos(dots)\n\n        if exists(input_mask):\n            dots.masked_fill_(~input_mask, mask_value)\n            del input_mask\n\n        if self.causal:\n            i, j = dots.shape[-2:]\n            mask = torch.ones((i, j), device = device).triu_(j - i + 1).bool()\n            dots.masked_fill_(mask, mask_value)\n            del mask\n\n        if exists(self.sparse_topk) and self.sparse_topk < dots.shape[-1]:\n            top, _ = dots.topk(self.sparse_topk, dim = -1)\n            vk = top[..., -1].unsqueeze(-1).expand_as(dots)\n            mask = dots < vk\n            dots.masked_fill_(mask, mask_value)\n"}