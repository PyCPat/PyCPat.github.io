{"BEFORE":"            encoderIn = xRaw.detach().permute(2, 3, 0, 1)\n            # encoderIn = xRaw.permute(2, 3, 0, 1)\n            # [h, w, n, c] -> [h*w, n, c]\n            if False:\n                encoderIn = self._position(encoderIn).reshape(-1, n, c)\n                # encoderIn = encoderIn.reshape(-1, n, c)\n                # [h*w, n, c]\n                x = self._encoder(encoderIn)\n            else:\n                x = encoderIn.reshape(-1, n ,c)\n            # similar to scaled dot-product attention\n            # [h*w, N, Cin],    M * [h*w, n, k]\n            quantized, samples, logits = self._attention(x, temp, True)\n            # quantized = x\n            if False:\n                # [h*w, n, c]\n                posistedQuantized = self._position(quantized.reshape(h, w, n, c)).reshape(-1, n, c)\n                deTransformed = self._decoder(posistedQuantized).reshape(h, w, n, c).permute(2, 3, 0, 1)\n            else:\n                # [h*w, n, c] -> [n, c, h*w] -> [n, c, h, w]\n                deTransformed = quantized.reshape(h, w, n, c).permute(2, 3, 0, 1)\n\n            # mask = torch.rand_like(xRaw) > coeff\n            # mixed = mask * xRaw.detach() + torch.logical_not(mask) * deTransformed\n            # [n, c, h, w]\n            quantizeds.append(deTransformed)\n            samples = [s.argmax(-1).permute(1, 0).reshape(n, h, w) for s in samples]\n            logits = [l.permute(1, 0, 2).reshape(n, h, w, k) for l in logits]\n","AFTER":"        codes = list()\n        logits = list()\n        for i, (xRaw, k) in enumerate(zip(latents, self._k)):\n            n, c, h, w = xRaw.shape\n            # [k, 1, c]\n            codebook = getattr(self, \"codebook\")[:, None, :]\n            # [n, c, h, w] -> [h, w, n, c]\n            encoderIn = xRaw.permute(2, 3, 0, 1)\n            # encoderIn = xRaw.permute(2, 3, 0, 1)\n            # [h, w, n, c] -> [h*w, n, c]\n            encoderIn = self._position(encoderIn).reshape(-1, n, c)\n            # [h*w, n, c]\n            x = self._encoder(codebook, encoderIn)\n            # [h*w, n, k]\n            logit = self._select(x)\n            sample = F.gumbel_softmax(logit, temp, True)\n            # [k, 1, c]\n            codewords = self._codebookEncoder(codebook)\n            # [h*w, n, c]\n            quantized = sample @ codewords[:, 0, ...]\n            # [h*w, n, c]\n            posistedQuantized = self._position(quantized.reshape(h, w, n, c)).reshape(-1, n, c)\n            # [k, 1, c]\n            decodedCodes = self._codebookDecoder(codebook)\n            # [n, c, h, w]\n            deTransformed = self._decoder(decodedCodes, posistedQuantized).reshape(h, w, n, c).permute(2, 3, 0, 1)\n\n            # [n, c, h, w]\n            quantizeds.append(deTransformed)\n            codes.append(sample.argmax(-1).permute(1, 0).reshape(n, h, w))\n            logits.append(logit.permute(1, 0, 2).reshape(n, h, w, k))\n"}