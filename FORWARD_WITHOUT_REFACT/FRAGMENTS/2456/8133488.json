{"BEFORE":"        g_t = gbk_t[:, 0, :]\n        b_t = gbk_t[:, 1, :]\n        k_t = gbk_t[:, 2, :]\n\n        # attention GMM parameters\n        g_t = torch.softmax(g_t, dim=-1) + self.epsilon  # distribution weight\n        sig_t = torch.exp(b_t) + self.epsilon  # variance\n        mu_t = self.mu_tm1 + self.attention_alignment * torch.exp(k_t)  # mean\n\n        g_t = g_t.unsqueeze(2).expand(g_t.size(0),\n                                      g_t.size(1),\n                                      inputs.size(1))\n        sig_t = sig_t.unsqueeze(2).expand_as(g_t)\n        mu_t_ = mu_t.unsqueeze(2).expand_as(g_t)\n        j = self.J[:g_t.size(0), :, :inputs.size(1)]\n\n        # attention weights\n        phi_t = g_t * torch.exp(-0.5 * sig_t * (mu_t_ - j)**2)\n        alpha_t = self.COEF * torch.sum(phi_t, 1)\n\n        # apply masking\n        # if mask is not None:\n        #     alpha_t.data.masked_fill_(~mask, self._mask_value)\n        \n        breakpoint()\n\n        c_t = torch.bmm(alpha_t.unsqueeze(1), inputs).squeeze(1)\n        self.mu_tm1 = mu_t\n        return c_t, mu_t, alpha_t\n","AFTER":"        g_t = gbk_t[:, 0, :]\n        b_t = gbk_t[:, 1, :]\n        k_t = gbk_t[:, 2, :]\n\n        # attention GMM parameters\n        # g_t = torch.softmax(g_t, dim=-1) + self.epsilon  # distribution weight\n        # sig_t = torch.exp(b_t) + self.epsilon  # variance\n        # mu_t = self.mu_prev + self.attention_alignment * torch.exp(k_t)  # mean\n        sig_t = torch.pow(torch.nn.functional.softplus(b_t), 2)\n        mu_t = self.mu_prev + torch.nn.functional.softplus(k_t)\n        g_t = (torch.softmax(g_t, dim=-1) \/ sig_t) * self.COEF\n\n        g_t = g_t.unsqueeze(2).expand(g_t.size(0),\n                                      g_t.size(1),\n                                      inputs.size(1))\n        sig_t = sig_t.unsqueeze(2).expand_as(g_t)\n        mu_t_ = mu_t.unsqueeze(2).expand_as(g_t)\n        j = self.J[:g_t.size(0), :, :inputs.size(1)]\n\n        # attention weights\n        phi_t = g_t * torch.exp(-0.5 * sig_t * (mu_t_ - j)**2)\n        alpha_t = torch.sum(phi_t, 1)\n\n        # apply masking\n        if mask is not None:\n            alpha_t.data.masked_fill_(~mask, self._mask_value)\n\n        context = torch.bmm(alpha_t.unsqueeze(1), inputs).squeeze(1)\n        self.attention_weights = alpha_t\n        self.mu_prev = mu_t\n        breakpoint()\n        return context\n"}