{"BEFORE":"        l2Loss = F.mse_loss(restored, images, reduction='none').mean(axis=(1, 2, 3))\n        l1Loss = F.l1_loss(restored, images, reduction='none').mean(axis=(1, 2, 3))\n        ssimLoss = 1 - ms_ssim((restored + 1), (images + 1), data_range=2.0, size_average=False)\n\n        l2QLoss = list()\n        l1QLoss = list()\n        if not e2e:\n            for latent, q in zip(latents, quantizeds):\n                l2QLoss.append(F.mse_loss(latent, q, reduction='none').mean(axis=(1, 2, 3)))\n                l1QLoss.append(F.l1_loss(latent, q, reduction='none').mean(axis=(1, 2, 3)))\n\n        l1QLoss = sum(l1QLoss)\n        l2QLoss = sum(l2QLoss)\n\n        regs = list()\n        if logits is not None:\n            for logit in logits:\n                # N, H, W, K -> N, HW, K\n                batchWiseLogit = logit.reshape(len(logit), -1, logit.shape[-1])\n\n                # [n, k]\n                summedProb = batchWiseLogit.mean(1).sigmoid()\n\n                target = torch.ones_like(summedProb) \/ 2.0\n                # [n, ]\n                reg = F.binary_cross_entropy(summedProb, target, reduction='none').sum(-1)\n\n                # [n, k] -> [n, ]\n                diversity = batchWiseLogit.var(1).sum(-1)\n                reg -= diversity\n\n                # posterior = OneHotCategorical(logits=summedLogit, validate_args=False)\n                # prior = OneHotCategorical(probs=torch.ones_like(summedLogit) \/ summedLogit.shape[-1], validate_args=False)\n                # reg = cv * torch.distributions.kl_divergence(posterior, prior)\n                # reg = compute_penalties(unNormlogit, allowed_entropy=0.1, individual_entropy_coeff=cv, allowed_js=4.0, js_coeff=cv, cv_coeff=cv, eps=Consts.Eps)\n                regs.append(cv * reg)\n            regs = sum(regs)\n        return ssimLoss, l1Loss + l2Loss + l1QLoss + l2QLoss, regs # + 10 * stdReg\n","AFTER":"        l2Loss = F.mse_loss(restored, images, reduction='none').mean(axis=(1, 2, 3))\n        l1Loss = F.l1_loss(restored, images, reduction='none').mean(axis=(1, 2, 3))\n        ssimLoss = 1 - ms_ssim((restored + 1), (images + 1), data_range=2.0, size_average=False)\n\n        l2QLoss = list()\n        l1QLoss = list()\n        if not e2e:\n            for latent, q in zip(latents, quantizeds):\n                l2QLoss.append(F.mse_loss(latent.detach(), q, reduction='none').mean(axis=(1, 2, 3)))\n                l1QLoss.append(F.l1_loss(latent.detach(), q, reduction='none').mean(axis=(1, 2, 3)))\n                l2QLoss.append(0.1 * F.mse_loss(latent, q.detach(), reduction='none').mean(axis=(1, 2, 3)))\n                l1QLoss.append(0.1 * F.l1_loss(latent, q.detach(), reduction='none').mean(axis=(1, 2, 3)))\n\n        l1QLoss = sum(l1QLoss)\n        l2QLoss = sum(l2QLoss)\n\n        regs = list()\n        if logits is not None:\n            for logit in logits:\n                # N, H, W, K -> N, HW, K\n                batchWiseLogit = logit.reshape(len(logit), -1, logit.shape[-1])\n\n                # [n, k]\n                # summedProb = batchWiseLogit.mean(1).sigmoid()\n\n                # target = torch.ones_like(summedProb) \/ 2.0\n                # [n, ]\n                # reg = F.binary_cross_entropy(summedProb, target, reduction='none').sum(-1)\n\n                # var = batchWiseLogit.var(1).sum(-1)\n\n                # [n, k] -> [n, ]\n                # diversity = torch.minimum(var, torch.ones_like(var))\n                # reg -= diversity\n\n                diversity = batchWiseLogit.var(1).sum(-1).sigmoid()\n\n                summedProb = batchWiseLogit.sum(1)\n                posterior = OneHotCategorical(logits=summedProb)\n                prior = OneHotCategorical(probs=torch.ones_like(summedProb) \/ summedProb.shape[-1])\n                reg = torch.distributions.kl_divergence(posterior, prior) \/ diversity\n                # reg += compute_penalties(unNormlogit, allowed_entropy=0.1, individual_entropy_coeff=cv, allowed_js=4.0, js_coeff=cv, cv_coeff=cv, eps=Consts.Eps)\n                regs.append(reg)\n            regs = sum(regs)\n        return ssimLoss, l1Loss + l2Loss, l1QLoss + l2QLoss, regs # + 10 * stdReg\n"}