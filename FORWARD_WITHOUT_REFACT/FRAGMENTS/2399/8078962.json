{"BEFORE":"        if mode in [\"multimodal\", \"fusion\"] and hasattr(self, \"crossattention\"):\n            assert (\n                encoder_hidden_states is not None\n            ), \"encoder_hidden_states must be given for cross-attention layers\"\n\n            cross_attention_outputs = self.crossattention(\n                attention_output,\n                attention_mask,\n                head_mask,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                output_attentions=output_attentions,\n            )\n            attention_output = cross_attention_outputs[0]\n            outputs = (\n                outputs + cross_attention_outputs[1:-1]\n            )  # add cross attentions if we output attention weights\n        layer_output = apply_chunking_to_forward(\n","AFTER":"        attention_output = self_attention_outputs[0]\n\n        outputs = self_attention_outputs[1:-1]\n        present_key_value = self_attention_outputs[-1]\n\n        # TODO line 482 in albef\/models\/xbert.py\n        # compatibility for ALBEF and BLIP\n        if mode in [\"multimodal\", \"fusion\"] and hasattr(self, \"crossattention\"):\n            assert (\n                encoder_hidden_states is not None\n            ), \"encoder_hidden_states must be given for cross-attention layers\"\n\n            if isinstance(encoder_hidden_states, list):\n                cross_attention_outputs = self.crossattention(\n                    attention_output,\n                    attention_mask,\n                    head_mask,\n                    encoder_hidden_states[\n                        (self.layer_num - self.config.fusion_layer)\n                        % len(encoder_hidden_states)\n                    ],\n                    encoder_attention_mask[\n                        (self.layer_num - self.config.fusion_layer)\n                        % len(encoder_hidden_states)\n                    ],\n                    output_attentions=output_attentions,\n                )\n                attention_output = cross_attention_outputs[0]\n                outputs = outputs + cross_attention_outputs[1:-1]\n\n            else:\n                cross_attention_outputs = self.crossattention(\n                    attention_output,\n                    attention_mask,\n                    head_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                    output_attentions=output_attentions,\n                )\n                attention_output = cross_attention_outputs[0]\n                outputs = (\n                    outputs + cross_attention_outputs[1:-1]\n                )  # add cross attentions if we output attention weights\n        layer_output = apply_chunking_to_forward(\n"}