{"BEFORE":"        inputs = inputs.contiguous()\n        reconstructions = reconstructions.contiguous()       \n        \n        # now the GAN part\n        if optimizer_idx == 0:\n            # generator update\n            loglaplace_loss = (reconstructions - inputs).abs().mean()\n            loggaussian_loss = (reconstructions - inputs).pow(2).mean()\n            perceptual_loss = self.perceptual_loss(inputs*2-1, reconstructions*2-1).mean()\n\n            nll_loss = self.loglaplace_weight * loglaplace_loss + self.loggaussian_weight * loggaussian_loss + self.perceptual_weight * perceptual_loss\n        \n            logits_fake = self.discriminator(reconstructions)\n            g_loss = self.disc_loss(logits_fake)\n            \n            try:\n                d_weight = self.adversarial_weight\n                \n                if self.use_adaptive_adv:\n                    d_weight *= self.calculate_adaptive_factor(nll_loss, g_loss, last_layer=last_layer)\n            except RuntimeError:\n                assert not self.training\n                d_weight = torch.tensor(0.0)\n\n            disc_factor = 1 if global_step >= self.discriminator_iter_start else 0\n            loss = nll_loss + disc_factor * d_weight * g_loss + self.codebook_weight * codebook_loss\n\n            log = {\"{}\/total_loss\".format(split): loss.clone().detach(),\n                   \"{}\/quant_loss\".format(split): codebook_loss.detach(),\n                   \"{}\/rec_loss\".format(split): nll_loss.detach(),\n                   \"{}\/loglaplace_loss\".format(split): loglaplace_loss.detach(),\n                   \"{}\/loggaussian_loss\".format(split): loggaussian_loss.detach(),\n                   \"{}\/perceptual_loss\".format(split): perceptual_loss.detach(),\n                   \"{}\/g_loss\".format(split): g_loss.detach(),\n                   }\n\n            if self.use_adaptive_adv:\n                log[\"{}\/d_weight\".format(split)] = d_weight.detach()\n            \n            return loss, log\n\n        if optimizer_idx == 1:\n            # second pass for discriminator update\n            inputs.requires_grad_()\n\n            logits_real = self.discriminator(inputs)\n            logits_fake = self.discriminator(reconstructions.detach())\n            \n            disc_factor = 1 if global_step >= self.discriminator_iter_start else 0\n            d_loss = disc_factor * self.disc_loss(logits_fake, logits_real)\n\n            log = {\"{}\/disc_loss\".format(split): d_loss.clone().detach(),\n                   \"{}\/logits_real\".format(split): logits_real.detach().mean(),\n                   \"{}\/logits_fake\".format(split): logits_fake.detach().mean()\n                   }\n\n            if self.training and disc_factor and global_step % 16 == 0:\n                gradients, = torch.autograd.grad(outputs=logits_real.sum(), inputs=inputs, create_graph=True)\n                gradients = gradients.view(inputs.shape[0], -1)\n\n                gradients_norm = gradients.norm(2, dim=1).pow(2).mean()\n                d_loss += 10 * gradients_norm\/2\n\n                log[\"{}\/r1_reg\".format(split)] = gradients_norm.detach()\n            \n            return d_loss, log\n","AFTER":"        inputs = inputs.contiguous()\n        reconstructions = reconstructions.contiguous()       \n        \n        # now the GAN part\n        if optimizer_idx == 0:\n            # generator update\n            loglaplace_loss = (reconstructions - inputs).abs().mean()\n            loggaussian_loss = (reconstructions - inputs).pow(2).mean()\n            perceptual_loss = self.perceptual_loss(inputs*2-1, reconstructions*2-1).mean()\n\n            nll_loss = self.loglaplace_weight * loglaplace_loss + self.loggaussian_weight * loggaussian_loss + self.perceptual_weight * perceptual_loss\n        \n            logits_fake = self.discriminator(reconstructions)\n            g_loss = self.disc_loss(logits_fake)\n            \n            try:\n                d_weight = self.adversarial_weight\n                \n                if self.use_adaptive_adv:\n                    d_weight *= self.calculate_adaptive_factor(nll_loss, g_loss, last_layer=last_layer)\n            except RuntimeError:\n                assert not self.training\n                d_weight = torch.tensor(0.0)\n\n            disc_factor = 1 if global_step >= self.discriminator_iter_start else 0\n            loss = nll_loss + disc_factor * d_weight * g_loss + self.codebook_weight * codebook_loss\n\n            log = {\"{}\/total_loss\".format(split): loss.clone().detach(),\n                   \"{}\/quant_loss\".format(split): codebook_loss.detach(),\n                   \"{}\/rec_loss\".format(split): nll_loss.detach(),\n                   \"{}\/loglaplace_loss\".format(split): loglaplace_loss.detach(),\n                   \"{}\/loggaussian_loss\".format(split): loggaussian_loss.detach(),\n                   \"{}\/perceptual_loss\".format(split): perceptual_loss.detach(),\n                   \"{}\/g_loss\".format(split): g_loss.detach(),\n                   }\n\n            if self.use_adaptive_adv:\n                log[\"{}\/d_weight\".format(split)] = d_weight.detach()\n            \n            return loss, log\n\n        if optimizer_idx == 1:\n            # second pass for discriminator update\n            disc_factor = 1 if global_step >= self.discriminator_iter_start else 0\n            do_r1 = self.training and bool(disc_factor) and global_step % 16 == 0\n\n            logits_real = self.discriminator(inputs.detach().requires_grad_(do_r1))\n            logits_fake = self.discriminator(reconstructions.detach())\n            \n            d_loss = disc_factor * self.disc_loss(logits_fake, logits_real)\n            if do_r1:\n                gradients, = torch.autograd.grad(outputs=logits_real.sum(), inputs=inputs, create_graph=True)\n                gradients = gradients.view(inputs.shape[0], -1)\n\n                gradients_norm = gradients.norm(2, dim=1).pow(2).mean()\n                d_loss += 10 * gradients_norm\/2\n\n            log = {\"{}\/disc_loss\".format(split): d_loss.detach(),\n                   \"{}\/logits_real\".format(split): logits_real.detach().mean(),\n                   \"{}\/logits_fake\".format(split): logits_fake.detach().mean()\n                   }\n\n            if do_r1:\n                log[\"{}\/r1_reg\".format(split)] = gradients_norm.detach()\n            \n            return d_loss, log\n"}