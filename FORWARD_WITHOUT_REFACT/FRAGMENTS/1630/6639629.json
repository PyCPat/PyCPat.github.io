{"BEFORE":"            mask = rearrange(mask, 'b i -> b 1 i 1') * rearrange(mask, 'b j -> b 1 1 j')\n\n        # expand values\n\n        v = repeat(v, 'b h j d -> b h i j d', i = n)\n\n        # determine k nearest neighbors for each point, if specified\n\n        if exists(num_neighbors) and num_neighbors < n:\n            rel_dist = rel_pos.norm(dim = -1)\n\n            if exists(mask):\n                mask_value = max_value(rel_dist)\n                rel_dist.masked_fill_(~mask, mask_value)\n\n            dist, indices = rel_dist.topk(num_neighbors, largest = False)\n\n            v = batched_index_select(v, indices, dim = 2)\n            qk_rel = batched_index_select(qk_rel, indices, dim = 2)\n            rel_pos_emb = batched_index_select(rel_pos_emb, indices, dim = 2)\n            mask = batched_index_select(mask, indices, dim = 2) if exists(mask) else None\n\n        # add relative positional embeddings to value\n\n        v = v + rel_pos_emb\n\n        # use attention mlp, making sure to add relative positional embedding first\n\n        attn_mlp_input = qk_rel + rel_pos_emb\n        attn_mlp_input = rearrange(attn_mlp_input, 'b h i j d -> b (h d) i j')\n\n        sim = self.attn_mlp(attn_mlp_input)\n\n        # masking\n\n        if exists(mask):\n","AFTER":"            mask = rearrange(mask, 'b i -> b i 1') * rearrange(mask, 'b j -> b 1 j')\n\n        # expand values\n\n        v = repeat(v, 'b h j d -> b h i j d', i = n)\n\n        # determine k nearest neighbors for each point, if specified\n\n        if exists(num_neighbors) and num_neighbors < n:\n            rel_dist = rel_pos.norm(dim = -1)\n\n            if exists(mask):\n                mask_value = max_value(rel_dist)\n                rel_dist.masked_fill_(~mask, mask_value)\n\n            dist, indices = rel_dist.topk(num_neighbors, largest = False)\n\n            indices_with_heads = repeat(indices, 'b i j -> b h i j', h = h)\n\n            v = batched_index_select(v, indices_with_heads, dim = 3)\n            qk_rel = batched_index_select(qk_rel, indices_with_heads, dim = 3)\n            rel_pos_emb = batched_index_select(rel_pos_emb, indices_with_heads, dim = 3)\n\n            if exists(mask):\n                mask = batched_index_select(mask, indices, dim = 2)\n\n        # add relative positional embeddings to value\n\n        v = v + rel_pos_emb\n\n        # use attention mlp, making sure to add relative positional embedding first\n\n        attn_mlp_input = qk_rel + rel_pos_emb\n        attn_mlp_input = rearrange(attn_mlp_input, 'b h i j d -> b (h d) i j')\n\n        sim = self.attn_mlp(attn_mlp_input)\n\n        # masking\n\n        if exists(mask):\n            mask_value = -max_value(sim)\n            mask = rearrange(mask, 'b i j -> b 1 i j')\n"}