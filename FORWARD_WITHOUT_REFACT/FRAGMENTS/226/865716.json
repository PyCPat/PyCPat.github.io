{"BEFORE":"        mask = self.pad_masking(x, offset)\n        if not self.bidirectional:\n            mask = mask + self.future_masking(x, offset)\n\n        # Use token embedding and positional embedding layers.\n        x = self.token_embedding(x) + self.positional_embedding(x, offset)\n        x = self.dropout_embedding(x)\n\n        # Apply transformer layers sequentially.\n        present = []\n        for i, transformer in enumerate(self.transformers):\n            if self.training and use_grad_ckpt:\n                x = torch.utils.checkpoint.checkpoint(\n                    transformer,\n                    x, past[i] if past is not None else None, mask)\n            else:\n                x, p = transformer(\n                    x, past[i] if past is not None else None, mask)\n                present.append(p)\n","AFTER":"                transformer = partial(torch.utils.checkpoint.checkpoint,\n                                      transformer)\n\n            x = transformer(x, past[i] if past is not None else None, mask)\n\n            if not self.training:\n                present.append(x[1])\n                x = x[0]\n\n        x = self.ln_head(x)\n"}