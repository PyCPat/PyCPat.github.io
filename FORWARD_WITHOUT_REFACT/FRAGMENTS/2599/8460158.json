{"BEFORE":"        current_context_vec = Variable(\n            inputs.data.new(B, 256).zero_())\n\n        # Time first (T_decoder, B, memory_dim)\n        if memory is not None:\n            memory = memory.transpose(0, 1)\n\n        outputs = []\n        alignments = []\n        stop_outputs = []\n\n        t = 0\n        memory_input = initial_memory\n        while True:\n            if t > 0:\n                if greedy:\n                    memory_input = outputs[-1]\n                else:\n                    # combine prev. model output and prev. real target\n                    # memory_input = torch.div(outputs[-1] + memory[t-1], 2.0)\n                    # add a random noise\n                    # noise = torch.autograd.Variable(\n                        # memory_input.data.new(memory_input.size()).normal_(0.0, 0.5))\n                    # memory_input = memory_input + noise\n                    memory_input = memory[t-1]\n\n            # Prenet\n            processed_memory = self.prenet(memory_input)\n\n            # Attention RNN\n            attention_rnn_hidden, current_context_vec, alignment = self.attention_rnn(\n                processed_memory, current_context_vec, attention_rnn_hidden,\n                inputs)\n\n            # Concat RNN output and attention context vector\n            decoder_input = self.project_to_decoder_in(\n                torch.cat((attention_rnn_hidden, current_context_vec), -1))\n\n            # Pass through the decoder RNNs\n            for idx in range(len(self.decoder_rnns)):\n                decoder_rnn_hiddens[idx] = self.decoder_rnns[idx](\n                    decoder_input, decoder_rnn_hiddens[idx])\n                # Residual connectinon\n                decoder_input = decoder_rnn_hiddens[idx] + decoder_input\n            \n            output = decoder_input\n            stop_token_input = decoder_input\n            \n            # stop token prediction\n            stop_token_input = torch.cat((output, current_context_vec), -1)\n            stop_output = self.stop_token(stop_token_input)\n\n            # predict mel vectors from decoder vectors\n            output = self.proj_to_mel(output)\n\n            outputs += [output]\n            alignments += [alignment]\n            stop_outputs += [stop_output]\n\n            t += 1\n\n            if (not greedy and self.training) or (greedy and memory is not None):\n                if t >= T_decoder:\n                    break\n            else:\n                if t > 1 and is_end_of_frames(output, self.eps):\n                    break\n                elif t > self.max_decoder_steps:\n                    print(\" !! Decoder stopped with 'max_decoder_steps'. \\\n                          Something is probably wrong.\")\n                    break\n                           \n        assert greedy or len(outputs) == T_decoder\n\n        # Back to batch first\n        alignments = torch.stack(alignments).transpose(0, 1)\n        outputs = torch.stack(outputs).transpose(0, 1).contiguous()\n        stop_outputs = torch.stack(stop_outputs).transpose(0, 1).contiguous()\n\n        return outputs, alignments, stop_outputs\n","AFTER":"        alignments = torch.stack(alignments).transpose(0, 1)\n        outputs = torch.stack(outputs).transpose(0, 1).contiguous()\n\n        return outputs, alignments\n"}