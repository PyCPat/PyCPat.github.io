{"BEFORE":"        batch, permutation_dim, _ = x.shape\n\n        # if no NaNs for padding varying trial lengths we can batch the computation\n        if not torch.isnan(x).any():\n            trial_embeddings = self.trial_net(x.view(batch * permutation_dim, -1)).view(\n                batch, permutation_dim, -1\n            )\n            combined_embedding = self.combining_function(trial_embeddings, dim=1)\n            trial_counts = torch.ones(batch, 1, dtype=torch.float32) * permutation_dim\n\n        # otherwise we need to loop over the batch to account for varying trial lengths\n        else:\n            combined_embedding = []\n            trial_counts = torch.zeros(batch, 1)\n            for i in range(batch):\n                # remove NaNs\n                valid_x = x[i, ~torch.isnan(x[i, :, 0]), :]\n                trial_counts[i] = valid_x.shape[0]\n                trial_embeddings = self.trial_net(valid_x)\n                # apply combining operation over permutation dimension\n                combined_embedding.append(\n                    self.combining_function(trial_embeddings, dim=0)\n                )\n\n            combined_embedding = torch.stack(combined_embedding, dim=0)\n\n        assert not torch.isnan(combined_embedding).any(), \"NaNs in embedding.\"\n","AFTER":"        num_batch, max_num_trials = x.shape[0], x.shape[self.aggregation_dim]\n        nan_counts = (\n            torch.isnan(x)\n            .sum(dim=self.aggregation_dim)  # count nans over trial dimension\n            .reshape(-1)[:num_batch]  # counts are the same across data dims\n            .unsqueeze(-1)  # make it (batch, 1) to match embeddings below\n        )\n        # number of non-nan trials\n        trial_counts = max_num_trials - nan_counts\n\n        # get nan entries\n        is_nan = torch.isnan(x)\n        # apply trial net with nan entries replaced with 0\n        masked_x = torch.nan_to_num(x, nan=0.0)\n        trial_embeddings = self.trial_net(masked_x)\n        # replace previous nan entries with zeros\n        trial_embeddings = trial_embeddings * (~is_nan.all(-1, keepdim=True)).float()\n\n        # Take mean over permutation dimension divide by number of trials\n        # (instead of just taking torch.mean) to account for masking.\n        if self.aggregation_fn == \"mean\":\n            combined_embedding = (\n                trial_embeddings.sum(dim=self.aggregation_dim) \/ trial_counts\n            )\n        else:\n            combined_embedding = trial_embeddings.sum(dim=self.aggregation_dim)\n\n        assert not torch.isnan(combined_embedding).any(), \"NaNs in embedding.\"\n"}