{"BEFORE":"        num_classes = hparams_net.get(\"num_classes\", 6)\n        d_in = hparams_net.get(\"d_in\", 6)  # xyz + features\n        self.num_neighbors = hparams_net.get(\"num_neighbors\", 16)\n        self.decimation = hparams_net.get(\"decimation\", 4)\n\n        self.fc_start = nn.Linear(d_in, d_in * 2)\n        self.bn_start = nn.Sequential(\n            nn.BatchNorm2d(d_in * 2, eps=1e-6, momentum=0.99), nn.LeakyReLU(0.2)\n        )\n\n        # encoding layers\n        self.encoder = nn.ModuleList(\n            [\n                LocalFeatureAggregation(d_in * 2, 16, self.num_neighbors),\n                LocalFeatureAggregation(32, 64, self.num_neighbors),\n                LocalFeatureAggregation(128, 128, self.num_neighbors),\n                LocalFeatureAggregation(256, 256, self.num_neighbors),\n            ]\n        )\n\n        self.mlp = SharedMLP(512, 512, activation_fn=nn.ReLU())\n\n        # decoding layers\n        decoder_kwargs = dict(transpose=True, bn=True, activation_fn=nn.ReLU())\n        self.decoder = nn.ModuleList(\n            [\n                SharedMLP(1024, 256, **decoder_kwargs),\n                SharedMLP(512, 128, **decoder_kwargs),\n                SharedMLP(256, 32, **decoder_kwargs),\n                SharedMLP(64, d_in * 2, **decoder_kwargs),\n            ]\n        )\n\n        # final semantic prediction\n        parts = [\n            SharedMLP(d_in * 2, 64, bn=True, activation_fn=nn.ReLU()),\n            SharedMLP(64, 32, bn=True, activation_fn=nn.ReLU()),\n        ]\n        dropout = hparams_net.get(\"dropout\", 0.0)\n        if dropout:\n            parts.append(nn.Dropout(p=dropout))\n        parts.append(SharedMLP(32, num_classes))\n        self.fc_end = nn.Sequential(*parts)\n","AFTER":"        self.d_in = hparams_net.get(\"d_in\", 6)  # xyz + features\n        self.num_neighbors = hparams_net.get(\"num_neighbors\", 16)\n        self.decimation = hparams_net.get(\"decimation\", 4)\n        self.dropout = hparams_net.get(\"dropout\", 0.0)\n        self.num_classes = hparams_net.get(\"num_classes\", 6)\n\n        self.fc_start = nn.Linear(self.d_in, self.d_in * 2)\n        self.bn_start = nn.Sequential(\n            nn.BatchNorm2d(self.d_in * 2, eps=1e-6, momentum=0.99), nn.LeakyReLU(0.2)\n        )\n\n        # encoding layers\n        self.encoder = nn.ModuleList(\n            [\n                LocalFeatureAggregation(self.d_in * 2, 16, self.num_neighbors),\n                LocalFeatureAggregation(32, 64, self.num_neighbors),\n                LocalFeatureAggregation(128, 128, self.num_neighbors),\n                LocalFeatureAggregation(256, 256, self.num_neighbors),\n            ]\n        )\n\n        self.mlp = SharedMLP(512, 512, activation_fn=nn.ReLU())\n\n        # decoding layers\n        decoder_kwargs = dict(transpose=True, bn=True, activation_fn=nn.ReLU())\n        self.decoder = nn.ModuleList(\n            [\n                SharedMLP(1024, 256, **decoder_kwargs),\n                SharedMLP(512, 128, **decoder_kwargs),\n                SharedMLP(256, 32, **decoder_kwargs),\n                SharedMLP(64, self.d_in * 2, **decoder_kwargs),\n            ]\n        )\n        self.set_fc_end(self.d_in, self.dropout, self.num_classes)\n"}