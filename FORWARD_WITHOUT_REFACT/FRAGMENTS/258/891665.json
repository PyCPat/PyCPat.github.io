{"BEFORE":"        padding = effective_kernel_size \/\/ 2\n\n        k_img, v_img = map(lambda t: rearrange(t, 'b (h w) c -> b c h w', h = img_size), (k_img, v_img))\n        k_img, v_img = map(lambda t: F.unfold(t, kernel_size, padding = padding, dilation = dilation), (k_img, v_img))\n        k_img, v_img = map(lambda t: rearrange(t, 'b (d j) i -> b i j d', j = kernel_size ** 2), (k_img, v_img))\n\n        # let image attend to all of text\n\n        dots_image = einsum('b i d, b i j d -> b i j', q_img, k_img)\n        dots_image_to_text = einsum('b i d, b j d -> b i j', q_img, k_text)\n\n        # calculate causal attention for local convolution\n\n        i, j = dots_image.shape[-2:]\n        img_seq = torch.arange(img_seq_len, device = device)\n        k_img_indices = rearrange(img_seq.float(), '(h w) -> () () h w', h = img_size)\n        k_img_indices = F.pad(k_img_indices, (padding,) * 4, value = img_seq_len) # padding set to be max, so it is never attended to\n        k_img_indices = F.unfold(k_img_indices, kernel_size, dilation = dilation)\n        k_img_indices = rearrange(k_img_indices, 'b j i -> b i j')\n\n        # mask image attention\n\n        q_img_indices = rearrange(img_seq, 'i -> () i ()')\n        causal_mask =  q_img_indices < k_img_indices\n","AFTER":"        img_seq_len = img_size ** 2\n        text_len = seq_len + 1 - img_seq_len\n\n        # padding\n\n        padding = seq_len - n + 1\n        mask = default(mask, lambda: torch.ones(b, text_len, device = device).bool())\n\n        x = F.pad(x, (0, 0, 0, padding), value = 0)\n        mask = mask[:, :text_len]\n\n        # derive query \/ keys \/ values\n\n        qkv = self.to_qkv(x).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), qkv)\n\n        if exists(rotary_pos_emb):\n            q, k, v = apply_pos_emb(rotary_pos_emb, (q, k, v))\n\n        q *= self.scale\n\n        ((q_text, q_img), (k_text, k_img), (v_text, v_img)) = map(lambda t: (t[:, :-img_seq_len], t[:, -img_seq_len:]), (q, k, v))\n\n        # text attention\n\n        dots_text = einsum('b i d, b j d -> b i j', q_text, k_text)\n        mask_value = max_neg_value(dots_text)\n\n        i, j = dots_text.shape[-2:]\n        text_causal_mask = torch.ones(i, j, device = device).triu_(j - i + 1).bool()\n        dots_text.masked_fill_(text_causal_mask, mask_value)\n\n        attn_text = softmax(dots_text, dim = -1)\n        out_text = einsum('b i j, b j d -> b i d', attn_text, v_text)\n\n        # image attention\n\n        effective_kernel_size = (kernel_size - 1) * dilation + 1\n        same_padding = effective_kernel_size \/\/ 2\n        causal_padding = (same_padding * 2, 0, same_padding * 2, 0)\n\n        k_img, v_img = map(lambda t: rearrange(t, 'b (h w) c -> b c h w', h = img_size), (k_img, v_img))\n        k_img, v_img = map(lambda t: F.pad(t, causal_padding), (k_img, v_img))\n        k_img, v_img = map(lambda t: F.unfold(t, kernel_size, dilation = dilation), (k_img, v_img))\n        k_img, v_img = map(lambda t: rearrange(t, 'b (d j) i -> b i j d', j = kernel_size ** 2), (k_img, v_img))\n\n        # let image attend to all of text\n\n        dots_image = einsum('b i d, b i j d -> b i j', q_img, k_img)\n        dots_image_to_text = einsum('b i d, b j d -> b i j', q_img, k_text)\n\n        # calculate causal attention for local convolution\n\n        i, j = dots_image.shape[-2:]\n        img_seq = torch.arange(img_seq_len, device = device)\n        k_img_indices = rearrange(img_seq.float(), '(h w) -> () () h w', h = img_size)\n        k_img_indices = F.pad(k_img_indices, causal_padding, value = img_seq_len) # padding set to be max, so it is never attended to\n        k_img_indices = F.unfold(k_img_indices, kernel_size, dilation = dilation)\n        k_img_indices = rearrange(k_img_indices, 'b j i -> b i j')\n\n        # mask image attention\n\n        padding_mask =  k_img_indices == img_seq_len\n"}