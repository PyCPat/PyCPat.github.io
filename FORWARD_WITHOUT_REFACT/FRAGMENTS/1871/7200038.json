{"BEFORE":"        decoder_vocab = batch.decoder_vocab\n\n        self.map_to_full = decoder_vocab.decode\n\n        context_padding = context.data == self.pad_idx\n        question_padding = question.data == self.pad_idx\n\n        if self.training:\n            if self.args.rnn_layers > 0:\n                self.rnn_decoder.applyMasks(context_padding, question_padding)\n            else:\n                self.context_attn.applyMasks(context_padding)\n                self.question_attn.applyMasks(question_padding)\n\n            answer_padding = (answer.data == self.pad_idx)[:, :-1]\n\n            answer_embedded = self.decoder_embeddings(answer[:, :-1], padding=answer_padding).last_layer\n\n            if self.args.transformer_layers > 0:\n                self_attended_decoded = self.self_attentive_decoder(answer_embedded,\n                                                                    self_attended_context,\n                                                                    context_padding=context_padding,\n                                                                    answer_padding=answer_padding,\n                                                                    positional_encodings=True)\n            else:\n                self_attended_decoded = answer_embedded\n\n            if self.args.rnn_layers > 0:\n                rnn_decoder_outputs = self.rnn_decoder(self_attended_decoded, final_context, final_question,\n                                                       hidden=context_rnn_state)\n                decoder_output, vocab_pointer_switch_input, context_question_switch_input, context_attention, \\\n                question_attention, rnn_state = rnn_decoder_outputs\n            else:\n                context_decoder_output, context_attention = self.context_attn(self_attended_decoded, final_context)\n                question_decoder_output, question_attention = self.question_attn(self_attended_decoded, final_question)\n\n                vocab_pointer_switch_input = torch.cat((context_decoder_output, self_attended_decoded), dim=-1)\n                context_question_switch_input = torch.cat((question_decoder_output, self_attended_decoded), dim=-1)\n\n                decoder_output = self.dropout(context_decoder_output)\n\n            vocab_pointer_switch = self.vocab_pointer_switch(vocab_pointer_switch_input)\n            context_question_switch = self.context_question_switch(context_question_switch_input)\n\n            probs = self.probs(decoder_output, vocab_pointer_switch, context_question_switch,\n                               context_attention, question_attention,\n                               context_limited, question_limited,\n                               decoder_vocab)\n\n            probs, targets = mask(answer_limited[:, 1:].contiguous(), probs.contiguous(), pad_idx=decoder_vocab.pad_idx)\n            loss = F.nll_loss(probs.log(), targets)\n            if encoder_loss is not None:\n                loss += self.args.encoder_loss_weight * encoder_loss\n            return loss, None\n\n        else:\n            return None, self.decode(self_attended_context, final_context, context_padding, final_question, question_padding,\n                                     context_limited, question_limited,\n                                     decoder_vocab, rnn_state=context_rnn_state).data\n","AFTER":"                question_rnn_state, encoder_loss, current_token_id=None, decoder_wrapper=None):\n\n        context, context_lengths, context_limited = batch.context.value, batch.context.length, batch.context.limited\n        question, question_lengths, question_limited = batch.question.value, batch.question.length, batch.question.limited\n        answer, answer_lengths, answer_limited = batch.answer.value, batch.answer.length, batch.answer.limited\n        decoder_vocab = batch.decoder_vocab\n        self.map_to_full = decoder_vocab.decode\n        context_padding = context.data == self.pad_idx\n        question_padding = question.data == self.pad_idx\n            \n        if self.training:\n            if self.args.rnn_layers > 0:\n                self.rnn_decoder.applyMasks(context_padding, question_padding)\n            else:\n                self.context_attn.applyMasks(context_padding)\n                self.question_attn.applyMasks(question_padding)\n\n            answer_padding = (answer.data == self.pad_idx)[:, :-1]\n\n            answer_embedded = self.decoder_embeddings(answer[:, :-1], padding=answer_padding).last_layer\n\n            if self.args.transformer_layers > 0:\n                self_attended_decoded = self.self_attentive_decoder(answer_embedded,\n                                                                    self_attended_context,\n                                                                    context_padding=context_padding,\n                                                                    answer_padding=answer_padding,\n                                                                    positional_encodings=True)\n            else:\n                self_attended_decoded = answer_embedded\n\n            if self.args.rnn_layers > 0:\n                rnn_decoder_outputs = self.rnn_decoder(self_attended_decoded, final_context, final_question,\n                                                        hidden=context_rnn_state)\n                decoder_output, vocab_pointer_switch_input, context_question_switch_input, context_attention, \\\n                question_attention, rnn_state = rnn_decoder_outputs\n            else:\n                context_decoder_output, context_attention = self.context_attn(self_attended_decoded, final_context)\n                question_decoder_output, question_attention = self.question_attn(self_attended_decoded, final_question)\n\n                vocab_pointer_switch_input = torch.cat((context_decoder_output, self_attended_decoded), dim=-1)\n                context_question_switch_input = torch.cat((question_decoder_output, self_attended_decoded), dim=-1)\n\n                decoder_output = self.dropout(context_decoder_output)\n\n            vocab_pointer_switch = self.vocab_pointer_switch(vocab_pointer_switch_input)\n            context_question_switch = self.context_question_switch(context_question_switch_input)\n\n            probs = self.probs(decoder_output, vocab_pointer_switch, context_question_switch,\n                                context_attention, question_attention,\n                                context_limited, question_limited,\n                                decoder_vocab)\n\n        \n            probs, targets = mask(answer_limited[:, 1:].contiguous(), probs.contiguous(), pad_idx=decoder_vocab.pad_idx)\n            loss = F.nll_loss(probs.log(), targets)\n            if encoder_loss is not None:\n                loss += self.args.encoder_loss_weight * encoder_loss\n            return loss, None\n        else:\n            if decoder_wrapper is None:\n                decoder_wrapper = self.decoder_wrapper(self_attended_context, final_context, context_padding, final_question, question_padding,\n                                                    context_limited, question_limited, decoder_vocab, rnn_state=context_rnn_state)\n            else:\n                current_token_id = current_token_id.cpu().apply_(self.map_to_full).to(current_token_id.device)\n            # return (next_token_logits, past) where `past` includes all the states needed to continue generation\n            logits = decoder_wrapper.next_token_probs(current_token_id)\n            # print('logits', logits.shape)\n            return logits, decoder_wrapper\n"}