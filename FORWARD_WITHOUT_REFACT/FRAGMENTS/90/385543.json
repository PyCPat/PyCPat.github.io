{"BEFORE":"        seq_len = history.size()[1]\n        state_len = out_state.size()[1]\n        batch_size = history.size()[0]\n        attn_energies = torch.zeros(\n            batch_size, state_len, seq_len).to(self.device)\n        for i in range(state_len):\n            for j in range(seq_len):\n                for k in range(batch_size):\n                    attn_energies[k, i, j] = self.score(\n                        out_state[k][i], history[k][j])\n        return F.softmax(attn_energies, dim=2)\n","AFTER":"        if self.method == 'dot':\n            history = history.permute(0, 2, 1)  # batch_size * hidden_size * history_len\n            attn_energies = torch.bmm(out_state, history)\n        elif self.method == 'general':\n            history = self.attn(history)\n            history = history.permute(0, 2, 1)\n            attn_energies = torch.bmm(out_state, history)\n        return F.softmax(attn_energies, dim=2)\n"}