<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                feat = self._cached_h
            else:
                &#47&#47 compute normalization
                degs = <a id="change">graph.in_degrees().float()</a>.clamp(min=1)
                norm = th.pow(degs, -0.5).to(feat.device).unsqueeze(1)

                &#47&#47 compute (D^-0.5 * A * D^-0.5)^k X
                for _ in range(self._k):
                    feat = feat * norm
                    graph.ndata[&quoth&quot] = feat
                    graph.update_all(fn.copy_src(&quoth&quot, &quotm&quot),
                                     fn.sum(&quotm&quot, &quoth&quot))
                    feat = graph.ndata.pop(&quoth&quot)
                    feat = feat * norm

                &#47&#47 cache feature
                if self._cached:
                    self._cached_h = feat

            if weight is not None:
                if self.weight is not None:
                    raise DGLError(&quotExternal weight is provided while at the same time the&quot
                                   &quot module has defined its own weight parameter. Please&quot
                                   &quot create the module with flag weight=False.&quot)
            else:
                weight<a id="change"> = </a>self.weight

            if weight is not None:
                feat = th.matmul(feat, weight)

            if self.bias is not None:
                feat<a id="change"> = </a>feat<a id="change"> + </a>self.bias
            <a id="change">return </a>feat
</code></pre><h3>After Change</h3><pre><code class='java'>
            assert edge_weight is None or edge_weight.size(0) == graph.num_edges()

            if self._add_self_loop:
                graph<a id="change"> = </a>graph.add_self_loop()
                if edge_weight is not None:
                    size = (graph.num_nodes(),) + edge_weight.size()[1:]
                    self_loop = edge_weight.new_ones(size)
                    edge_weight = torch.cat([edge_weight, self_loop])
            else:
                graph = graph.local_var()

            edge_weight = dgl_normalize(graph, self._norm, edge_weight)
            graph.edata[&quot_edge_weight&quot] = edge_weight

            for _ in range(self._k):
                graph.ndata[&quoth&quot] = feat
                graph.update_all(fn.u_mul_e(&quoth&quot, &quot_edge_weight&quot, &quotm&quot),
                                 fn.sum(&quotm&quot, &quoth&quot))
                feat = graph.ndata.pop(&quoth&quot)

            &#47&#47 cache feature
            if self._cached:
                self._cached_h = feat

        <a id="change">return </a>self.linear(feat)
    
    def extra_repr(self):
        Set the extra representation of the module,</code></pre>