{"BEFORE":"        if hasattr(self, 'position_embeddings'):\r\n            seq_length = token_ids.size(1)\r\n            if position_ids is None:\r\n                position_ids = torch.arange(seq_length, dtype=torch.long, device=token_ids.device)\r\n                position_ids = position_ids.unsqueeze(0).repeat(token_ids.shape[0], 1)\r\n            position_embeddings = self.position_embeddings(position_ids)\r\n","AFTER":"        if hasattr(self, 'position_embeddings') and (position_ids is not None):\r\n            position_ids = position_ids.unsqueeze(0).repeat(token_ids.shape[0], 1)\r\n"}