{"BEFORE":"        B, C, L = x.size()\n        N, M, D = self.embedding.size()\n        assert C == N * D\n\n        x = x.view(B, N, D, L).permute(1, 0, 3, 2)  # N, B, L, D\n        x_flat = x.detach().reshape(N, -1, D)\n\n        distances = torch.cdist(x_flat, self.embedding)\n        indices = torch.argmin(distances, dim=-1)\n\n        encodings = F.one_hot(indices, M).float()\n        quantized = torch.gather(self.embedding, 1, indices.unsqueeze(-1).expand(-1, -1, D))\n        quantized = quantized.view_as(x)\n\n        if self.training:\n            self.ema_count = self.decay * self.ema_count + (1 - self.decay) * torch.sum(encodings, dim=1)\n\n            n = torch.sum(self.ema_count, dim=-1, keepdim=True)\n            self.ema_count = (self.ema_count + self.epsilon) \/ (n + M * self.epsilon) * n\n\n            dw = torch.bmm(encodings.transpose(1, 2), x_flat)\n            self.ema_weight = self.decay * self.ema_weight + (1 - self.decay) * dw\n\n            self.embedding = self.ema_weight \/ self.ema_count.unsqueeze(-1)\n\n        e_latent_loss = F.mse_loss(x, quantized.detach())\n        loss = self.commitment_cost * e_latent_loss\n\n        quantized = x + (quantized - x).detach()\n\n        avg_probs = torch.mean(encodings, dim=1)\n        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10), dim=-1))\n\n        return quantized.permute(1, 0, 3, 2).reshape(B, C, L), loss, perplexity.sum()\n","AFTER":"        x_flat = x.detach().reshape(-1, D)\n\n        distances = torch.addmm(torch.sum(self.embedding ** 2, dim=1) +\n                                torch.sum(x_flat ** 2, dim=1, keepdim=True),\n                                x_flat, self.embedding.t(),\n                                alpha=-2.0, beta=1.0)\n\n        indices = torch.argmin(distances.float(), dim=-1)\n        encodings = F.one_hot(indices, M).float()\n        quantized = F.embedding(indices, self.embedding)\n        quantized = quantized.view_as(x)\n\n        if self.training:\n            self.ema_count = self.decay * self.ema_count + (1 - self.decay) * torch.sum(encodings, dim=0)\n\n            n = torch.sum(self.ema_count)\n            self.ema_count = (self.ema_count + self.epsilon) \/ (n + M * self.epsilon) * n\n\n            dw = torch.matmul(encodings.t(), x_flat)\n            self.ema_weight = self.decay * self.ema_weight + (1 - self.decay) * dw\n\n            self.embedding = self.ema_weight \/ self.ema_count.unsqueeze(-1)\n\n        e_latent_loss = F.mse_loss(x, quantized.detach())\n        loss = self.commitment_cost * e_latent_loss\n\n        quantized = x + (quantized - x).detach()\n\n        avg_probs = torch.mean(encodings, dim=0)\n        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n\n        return quantized, loss, perplexity\n"}