{"BEFORE":"    def forward(self, x, layer_past=None, attention_mask=None, head_mask=None):\n        output_attn = self.attn(self.ln_1(x),\n                                layer_past=layer_past,\n                                attention_mask=attention_mask,\n                                head_mask=head_mask)\n        a = output_attn[0]  # output_attn: a, present, (attentions)\n\n        x = x + a\n        m = self.mlp(self.ln_2(x))\n        x = x + m\n\n        outputs = [x] + output_attn[1:]\n        return outputs  # x, present, (attentions)\n","AFTER":"    def forward(self, x):\r\n        output_attn = self.attn(self.ln_1(x))\r\n        x = x + output_attn\r\n        m = self.mlp(self.ln_2(x))\r\n        return x + m\r\n"}