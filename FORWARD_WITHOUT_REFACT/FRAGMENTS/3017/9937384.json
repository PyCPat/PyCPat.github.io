{"BEFORE":"            assert label.ndim == 1, ('The label shoube be in shape of (n, )'\n                                     f'but got {label.shape}.')\n            label_batch = label\n        elif callable(label):\n            label_generator = label\n            assert num_batches > 0\n            label_batch = label_generator((num_batches, ))\n        else:\n            assert num_batches > 0\n            label_batch = torch.randint(0, self.num_classes, (num_batches, ))\n\n        # dirty code for putting data on the right device\n        noise_batch = noise_batch.to(get_module_device(self))\n        if label_batch is not None:\n            label_batch = label_batch.to(get_module_device(self))\n            class_vector = self.shared_embedding(label_batch)\n","AFTER":"                truncation=-1.0,\n                use_outside_embedding=False):\n        \"\"\"Forward function.\n\n        Args:\n            noise (torch.Tensor | callable | None): You can directly give a\n                batch of noise through a ``torch.Tensor`` or offer a callable\n                function to sample a batch of noise data. Otherwise, the\n                ``None`` indicates to use the default noise sampler.\n            label (torch.Tensor | callable | None): You can directly give a\n                batch of label through a ``torch.Tensor`` or offer a callable\n                function to sample a batch of label data. Otherwise, the\n                ``None`` indicates to use the default label sampler.\n                Defaults to None.\n            num_batches (int, optional): The number of batch size.\n                Defaults to 0.\n            return_noise (bool, optional): If True, ``noise_batch`` and\n                ``label`` will be returned in a dict with ``fake_img``.\n                Defaults to False.\n            truncation (float, optional): Truncation factor. Give value not\n                less than 0., the truncation trick will be adopted.\n                Otherwise, the truncation trick will not be adopted.\n                Defaults to -1..\n            use_outside_embedding (bool, optional): Whether to use outside\n                embedding or use `shared_embedding`. Set to `True` if\n                embedding has already be performed outside this function.\n                Default to False.\n\n        Returns:\n            torch.Tensor | dict: If not ``return_noise``, only the output image\n                will be returned. Otherwise, a dict contains ``fake_img``,\n                ``noise_batch`` and ``label`` will be returned.\n        \"\"\"\n        if isinstance(noise, torch.Tensor):\n            assert noise.shape[1] == self.noise_size\n            assert noise.ndim == 2, ('The noise should be in shape of (n, c), '\n                                     f'but got {noise.shape}')\n            noise_batch = noise\n        # receive a noise generator and sample noise.\n        elif callable(noise):\n            noise_generator = noise\n            assert num_batches > 0\n            noise_batch = noise_generator((num_batches, self.noise_size))\n        # otherwise, we will adopt default noise sampler.\n        else:\n            assert num_batches > 0\n            noise_batch = torch.randn((num_batches, self.noise_size))\n\n        # perform truncation\n        if truncation >= 0.0:\n            noise_batch = torch.clamp(noise_batch, -1. * truncation,\n                                      1. * truncation)\n\n        if self.num_classes == 0:\n            label_batch = None\n\n        elif isinstance(label, torch.Tensor):\n            if not use_outside_embedding:\n                assert label.ndim == 1, (\n                    'The label shoube be in shape of (n, )'\n                    f'but got {label.shape}.')\n            label_batch = label\n        elif callable(label):\n            label_generator = label\n            assert num_batches > 0\n            label_batch = label_generator((num_batches, ))\n        else:\n            assert num_batches > 0\n            label_batch = torch.randint(0, self.num_classes, (num_batches, ))\n\n        # dirty code for putting data on the right device\n        noise_batch = noise_batch.to(get_module_device(self))\n        if label_batch is not None:\n            label_batch = label_batch.to(get_module_device(self))\n            if not use_outside_embedding:\n                class_vector = self.shared_embedding(label_batch)\n            else:\n                class_vector = label_batch\n        else:\n            class_vector = None\n        # If 'split noise', concat class vector and noise chunk\n        if self.split_noise:\n            zs = torch.split(noise_batch, self.noise_chunk_size, dim=1)\n            z = zs[0]\n            if class_vector is not None:\n                ys = [torch.cat([class_vector, item], 1) for item in zs[1:]]\n            else:\n                ys = zs[1:]\n        else:\n            ys = [class_vector] * len(self.conv_blocks)\n            z = noise_batch\n\n        # First linear layer\n        x = self.noise2feat(z)\n        # Reshape\n        x = x.view(x.size(0), -1, self.input_scale, self.input_scale)\n\n        # Loop over blocks\n        counter = 0\n        for conv_block in self.conv_blocks:\n            if isinstance(conv_block, SelfAttentionBlock):\n                x = conv_block(x)\n            else:\n                x = conv_block(x, ys[counter])\n                counter += 1\n\n        # Apply batchnorm-relu-conv-tanh at output\n        out_img = torch.tanh(self.output_layer(x))\n\n        if self.rgb2bgr:\n            out_img = out_img[:, [2, 1, 0], ...]\n\n        if return_noise:\n"}