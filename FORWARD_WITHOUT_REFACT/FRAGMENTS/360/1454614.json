{"BEFORE":"        attn_logits_scalar = einsum('b i d, b j d -> b i j', q_scalar, k_scalar) * self.scalar_attn_logits_scale\n        attn_logits_pairwise = self.to_pairwise_attn_bias(pairwise_repr) * self.pairwise_attn_logits_scale\n\n        attn_logits = attn_logits_scalar + attn_logits_pairwise\n\n        # mask\n\n        if exists(mask):\n            mask = rearrange(mask, 'b i -> b i ()') * rearrange(mask, 'b j -> b () j')\n            mask_value = max_neg_value(attn_logits)\n            attn_logits = attn_logits.masked_fill(~mask, mask_value)\n\n        # attention\n\n        attn = attn_logits.softmax(dim = - 1)\n\n        # aggregate values\n\n        results_scalar = einsum('b i j, b j d -> b i d', attn, v_scalar)\n\n        attn_with_heads = rearrange(attn, '(b h) i j -> b h i j', h = h)\n        results_pairwise = einsum('b h i j, b i j d -> b h i d', attn_with_heads, pairwise_repr)\n\n        # merge back heads\n\n        results_scalar = rearrange(results_scalar, '(b h) n d -> b n (h d)', h = h)\n        results_pairwise = rearrange(results_pairwise, 'b h n d -> b n (h d)', h = h)\n\n        results = torch.cat((results_scalar, results_pairwise), dim = -1)\n        return single_repr\n","AFTER":"        x, b, h, eps = single_repr, single_repr.shape[0], self.heads, self.eps\n\n        # get queries, keys, values for scalar and point (coordinate-aware) attention pathways\n\n        q_scalar, k_scalar, v_scalar = self.to_scalar_q(x), self.to_scalar_k(x), self.to_scalar_v(x)\n\n        q_point, k_point, v_point = self.to_point_q(x), self.to_point_k(x), self.to_point_v(x)\n\n        # split out heads\n\n        q_scalar, k_scalar, v_scalar = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q_scalar, k_scalar, v_scalar))\n        q_point, k_point, v_point = map(lambda t: rearrange(t, 'b n (h d c) -> (b h) n d c', h = h, c = 3), (q_point, k_point, v_point))\n\n        # derive attn logits for scalar and pairwise\n\n        attn_logits_scalar = einsum('b i d, b j d -> b i j', q_scalar, k_scalar) * self.scalar_attn_logits_scale\n        attn_logits_pairwise = self.to_pairwise_attn_bias(pairwise_repr) * self.pairwise_attn_logits_scale\n\n        # derive attn logits for point attention\n\n        point_qk_diff = rearrange(q_point, 'b i d c -> b i () d c') - rearrange(k_point, 'b j d c -> b () j d c')\n        point_dist = (point_qk_diff ** 2).sum(dim = -2)\n\n        point_weights = F.softplus(self.point_weights)\n        point_weights = repeat(point_weights, 'h -> (b h) () () ()', b = b)\n\n        attn_logits_points = -0.5 * (point_dist * point_weights).sum(dim = -1)\n\n        # combine attn logits\n\n        attn_logits = attn_logits_scalar + attn_logits_pairwise + attn_logits_points\n\n        # mask\n\n        if exists(mask):\n            mask = rearrange(mask, 'b i -> b i ()') * rearrange(mask, 'b j -> b () j')\n            mask_value = max_neg_value(attn_logits)\n            attn_logits = attn_logits.masked_fill(~mask, mask_value)\n\n        # attention\n\n        attn = attn_logits.softmax(dim = - 1)\n\n        # aggregate values\n\n        results_scalar = einsum('b i j, b j d -> b i d', attn, v_scalar)\n\n        attn_with_heads = rearrange(attn, '(b h) i j -> b h i j', h = h)\n        results_pairwise = einsum('b h i j, b i j d -> b h i d', attn_with_heads, pairwise_repr)\n\n        # aggregate point values\n\n        results_points = einsum('b i j, b j d c -> b i d c', attn, v_point)\n\n        # merge back heads\n\n        results_scalar = rearrange(results_scalar, '(b h) n d -> b n (h d)', h = h)\n        results_pairwise = rearrange(results_pairwise, 'b h n d -> b n (h d)', h = h)\n        results_points = rearrange(results_points, '(b h) n d c -> b n (h d c)', h = h)\n\n        results = torch.cat((results_scalar, results_pairwise, results_points), dim = -1)\n        return self.to_out(results)\n"}