{"BEFORE":"    def forward(self, data, mems, padding_mask, mem_padding):\n        #padding_mask should be shape 1 X (mlen+qlen) X batch_size,\n        #which we apply row wise\n\n        if not mems:\n            # print('INITIALIZED MEMS')\n            mems = self.init_mems()\n\n        #Concatenate mem_padding and padding_mask (slight modifications if None)\n        padding_mask2 = mem_padding\n        #print('mem_padding', mem_padding.shape if mem_padding is not None else None)\n        #print('PADDING MASK: ', padding_mask.shape if padding_mask is not None else None)\n        if padding_mask2 is None:\n            padding_mask2 = padding_mask\n        elif padding_mask is not None:\n            padding_mask2 = torch.cat([mem_padding, padding_mask], dim=1)\n\n        '''\n        if mem_padding is not None and padding_mask is not None:\n            print('Adding orig: ', padding_mask[:,:,0])\n            print('mem_padding: ', mem_padding[:,:,0])\n            print('Result: ', padding_mask2[:,:,0])\n            print('DATA shape: ', data.shape)\n            print('mems shape: ', mems[0].shape)\n        '''\n        hidden, new_mems = self._forward(data, padding_mask=padding_mask2, mems=mems)\n\n        if padding_mask2 is not None:\n            padding_mask2 = padding_mask2[:,-self.mem_len:,:] #will me memory_padding at next iteration.\n\n        return hidden, new_mems, padding_mask2\n","AFTER":"        return hidden, new_mems\n"}