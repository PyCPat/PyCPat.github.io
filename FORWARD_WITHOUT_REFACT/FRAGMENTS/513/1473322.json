{"BEFORE":"        layers = nn.ModuleList([])\n\n        def cast_tuple(val):\n            return (val,) if not isinstance(val, tuple) else val\n\n        for ind, local_heads in zip(range(depth), n_local_attn_heads):\n            layer_num = ind + 1\n            use_pkm = layer_num in cast_tuple(pkm_layers)\n\n            parallel_net = Chunk(ff_chunks, FeedForward(dim), along_dim = 1) if not use_pkm else PKM(dim)\n\n            layer = nn.ModuleList([\n                PreNorm(dim, SelfAttention(dim, heads, causal, one_kv_head = one_kv_head, blindspot_size = blindspot_size, n_local_attn_heads = local_heads, local_attn_window_size = local_attn_window_size, psi_fn = psi_fn)),\n                PreNorm(dim, parallel_net)\n            ])\n            layers.append(layer)\n\n            if not receives_context:\n                continue\n\n            layer = nn.ModuleList([\n                PreNorm(dim, SelfAttention(dim, heads, one_kv_head = one_kv_head, psi_fn = psi_fn, receives_context = True)),\n                PreNorm(dim, Chunk(ff_chunks, FeedForward(dim), along_dim = 1))\n            ])\n            layers.append(layer)\n\n        execute_type = ReversibleSequence if reversible else SequentialSequence\n\n        attn_context_layer = ((True, False),) if receives_context else tuple()\n        route_attn = ((True, False), *attn_context_layer) * depth\n","AFTER":"    def __init__(self, dim, depth, max_seq_len, heads = 8, bucket_size = 64, causal = False, one_kv_head = False, ff_chunks = 1, reversible = False, blindspot_size = 1, n_local_attn_heads = 0, local_attn_window_size = 128, psi_fn = DEFAULT_PSI, receives_context = False, attend_axially = False, pkm_layers = tuple(), pkm_num_keys = 128):\n        super().__init__()\n        if type(n_local_attn_heads) is not tuple:\n            n_local_attn_heads = tuple([n_local_attn_heads] * depth)\n\n        assert len(n_local_attn_heads) == depth, 'local attention heads tuple must have the same length as the depth'\n        assert all([(local_heads <= heads) for local_heads in n_local_attn_heads]), 'number of local attn heads must be less than the maximum number of heads'\n\n        layers = nn.ModuleList([])\n\n        for ind, local_heads in zip(range(depth), n_local_attn_heads):\n            layer_num = ind + 1\n            use_pkm = layer_num in cast_tuple(pkm_layers)\n\n            parallel_net = Chunk(ff_chunks, FeedForward(dim), along_dim = 1) if not use_pkm else PKM(dim)\n\n            layers.append(nn.ModuleList([\n                PreNorm(dim, SelfAttention(dim, heads, causal, one_kv_head = one_kv_head, blindspot_size = blindspot_size, n_local_attn_heads = local_heads, local_attn_window_size = local_attn_window_size, psi_fn = psi_fn)),\n                PreNorm(dim, parallel_net)\n            ]))\n\n            if attend_axially:\n                layers.append(nn.ModuleList([\n                    PreNorm(dim, FoldAxially(local_attn_window_size, SelfAttention(dim, heads, causal, one_kv_head = one_kv_head, psi_fn = psi_fn))),\n                    PreNorm(dim, Chunk(ff_chunks, FeedForward(dim), along_dim = 1))\n                ]))\n\n            if receives_context:\n                layers.append(nn.ModuleList([\n                    PreNorm(dim, SelfAttention(dim, heads, one_kv_head = one_kv_head, psi_fn = psi_fn, receives_context = True)),\n                    PreNorm(dim, Chunk(ff_chunks, FeedForward(dim), along_dim = 1))\n                ]))\n\n        execute_type = ReversibleSequence if reversible else SequentialSequence\n\n        axial_layer = ((True, False),) if attend_axially else tuple()\n        attn_context_layer = ((True, False),) if receives_context else tuple()\n        route_attn = ((True, False), *axial_layer, *attn_context_layer) * depth\n        route_context = ((False, False), *axial_layer, *attn_context_layer) * depth\n"}