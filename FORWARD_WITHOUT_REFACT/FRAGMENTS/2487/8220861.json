{"BEFORE":"        x = F.relu(self.fc(x))\n\n        one_hot_last_action = F.one_hot(\n            inputs[\"last_action\"].view(T * B), self.num_actions\n        ).float()\n\n        #what's happening here?\n        print('REWARD SHAPE: ', inputs['reward'].shape)\n        print('X shape: ', x.shape)\n        clipped_reward = torch.clamp(inputs[\"reward\"], -1, 1).view(T * B, 1)\n        core_input = torch.cat([x, clipped_reward, one_hot_last_action], dim=-1)\n        ###############################################################transformer\n        # if self.use_lstm:\n        #print('CoreInput shape: ', core_input.shape)\n\n        #BE CAREFUL WITH THIS WAY OF RESHAPING (should transpose instead)\n        core_input = core_input.view(T, B, -1)\n        core_output_list = []\n        notdone = (~inputs[\"done\"]).float()\n\n        # TODO : We need to pass everything at once to the transformer, and not\n        #         iterate over each timestep. Check how this should be done here\n\n        # TODO : seems like core_input does have all the timesteps into it since\n        #       an unbind is being called on it. It should be safe to pass core_input\n        #       directly to the transformer. Check dimensions here\n        # TODO : the memory has been put as None here, this will be changed in the upcoming codes\n\n        # TODO DEBUG : This line is giving all nans XD\n        core_output = self.core(core_input)   # core_input is of shape (T, B, ...)\n                                              # core_output is (B, ...)\n        print('CORE OUTPUT: ',core_output[0,:10])\n        print('Core output shpae: ',core_output.shape)\n        # TODO : The current memory is put as None since I've instantiated TransformerLM with\n        #  mem_len = 0 above\n\n        # for input, nd in zip(core_input.unbind(), notdone.unbind()):\n        #     # Reset core state to zero whenever an episode ended.\n        #     # Make `done` broadcastable with (num_layers, B, hidden_size)\n        #     # states:\n        #     nd = nd.view(1, -1, 1)\n        #     core_state = tuple(nd * s for s in core_state)\n        #     output, core_state = self.core(input.unsqueeze(0), core_state)\n        #     core_output_list.append(output)\n\n        # TODO : I dont think the following line is required on core_output anymore\n        # core_output = torch.flatten(torch.cat(core_output_list), 0, 1)\n\n        # else:\n        #     core_output = core_input\n        #     core_state = tuple()\n\n        policy_logits = self.policy(core_output)\n        baseline = self.baseline(core_output)\n\n        print('POLICY SHAPE: ',policy_logits.shape)\n","AFTER":"        policy_logits = self.policy(core_output)\n        baseline = self.baseline(core_output)\n\n        # print('POLICY SHAPE: ',policy_logits.shape)\n        policy_logits = policy_logits.reshape(T*B, self.num_actions)\n        # print('TMP : {} Original : {}'.format(policy_logits_tmp[:3, :], policy_logits[:3, :3, :]))\n        if self.training:\n            # Sample from multinomial distribution for explorationx\n            action = torch.multinomial(F.softmax(policy_logits, dim=1), num_samples=1)\n        else:\n            # Don't sample when testing.\n            action = torch.argmax(policy_logits, dim=1)\n\n        #IS THIS NECESSARY? If yes then switch to transpose\n        # print('')\n        # print('policy logits : {} and T : {} B : {}'.format(policy_logits.shape, T, B))\n        policy_logits = policy_logits.view(T, B, self.num_actions)\n        baseline = baseline.view(T, B)\n\n        print('policy logits : {} and T : {} B : {} action : {}'.format(policy_logits.shape, T, B, action.shape))\n"}