{"BEFORE":"        img_seq_len = img_size ** 2\n        text_len = n - img_seq_len\n        ((q_text, q_img), (k_text, k_img), (v_text, v_img)) = map(lambda t: (t[:, img_seq_len:], t[:, -img_seq_len:]), (q, k, v))\n\n        # text attention\n\n        dots_text = einsum('b i d, b j d -> b i j', q_text, k_text)\n        mask_value = max_neg_value(dots_text)\n\n        i, j = dots_text.shape[-2:]\n        mask = torch.ones(i, j, device = device).triu_(j - i + 1).bool()\n        dots_text.masked_fill(mask, mask_value)\n\n        attn_text = dots_text.softmax(dim = -1)\n        out_text = einsum('b i j, b j d -> b i d', attn_text, v_text)\n\n        # image attention\n\n        split_axis_einops = 'b (h w) c -> (b h) w c' if axis == 0 else 'b (h w) c -> (b w) h c'\n        merge_axis_einops = '(b ax) n d -> b (ax n) d' if axis == 0 else '(b ax) n d -> b (n ax) d'\n\n        # split out axis\n\n        q_img, k_img, v_img = map(lambda t: rearrange(t, split_axis_einops, h = img_size), (q_img, k_img, v_img))\n\n        # prepare text key \/ values for the image tokens to attend to\n\n        k_text, v_text = map(lambda t: repeat(t, 'b n d -> (b ax) n d', ax = img_size), (k_text, v_text))\n        k_img = torch.cat((k_text, k_img), dim = 1)\n        v_img = torch.cat((v_text, v_img), dim = 1)\n\n        # similarity\n\n        dots_image = einsum('b i d, b j d -> b i j', q_img, k_img)\n\n        # mask so image has full attention to text, but causal along axis\n\n        i, j = dots_image.shape[-2:]\n        mask = torch.ones(i, j, device = device).triu_(j - i + 1).bool()\n        dots_image.masked_fill_(mask, mask_value)\n\n        # attention.\n\n        attn_image = dots_image.softmax(dim = -1)\n        out_image = einsum('b i j, b j d -> b i d', attn_image, v_img)\n\n        # merge back axis\n\n        out_image = rearrange(out_image, merge_axis_einops, ax = img_size)\n\n        # combine attended values for both text and image\n\n        out = torch.cat((out_text, out_image), dim = 1)\n\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n        out =  self.to_out(out)\n        return out\n","AFTER":"        b, n, _, h, img_size, axis, seq_len, device = *x.shape, self.heads, self.image_size, self.axis, self.seq_len, x.device\n\n        if n < seq_len:\n            padding = seq_len - n\n            x = F.pad(x, (0, 0, 0, padding), value = 0)\n\n            if exists(mask):\n                mask = F.pad(x, (0, padding), value = False)\n\n        qkv = self.to_qkv(x).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), qkv)\n\n        q *= self.scale\n\n        img_seq_len = img_size ** 2\n        text_len = seq_len - img_seq_len\n\n        ((q_text, q_img), (k_text, k_img), (v_text, v_img)) = map(lambda t: (t[:, img_seq_len:], t[:, -img_seq_len:]), (q, k, v))\n\n        # text attention\n\n        dots_text = einsum('b i d, b j d -> b i j', q_text, k_text)\n        mask_value = max_neg_value(dots_text)\n\n        i, j = dots_text.shape[-2:]\n        mask = torch.ones(i, j, device = device).triu_(j - i + 1).bool()\n        dots_text.masked_fill(mask, mask_value)\n\n        attn_text = dots_text.softmax(dim = -1)\n        out_text = einsum('b i j, b j d -> b i d', attn_text, v_text)\n\n        # image attention\n\n        split_axis_einops = 'b (h w) c -> (b h) w c' if axis == 0 else 'b (h w) c -> (b w) h c'\n        merge_axis_einops = '(b ax) n d -> b (ax n) d' if axis == 0 else '(b ax) n d -> b (n ax) d'\n\n        # split out axis\n\n        q_img, k_img, v_img = map(lambda t: rearrange(t, split_axis_einops, h = img_size), (q_img, k_img, v_img))\n\n        # prepare text key \/ values for the image tokens to attend to\n\n        k_text, v_text = map(lambda t: repeat(t, 'b n d -> (b ax) n d', ax = img_size), (k_text, v_text))\n        k_img = torch.cat((k_text, k_img), dim = 1)\n        v_img = torch.cat((v_text, v_img), dim = 1)\n\n        # similarity\n\n        dots_image = einsum('b i d, b j d -> b i j', q_img, k_img)\n\n        # mask so image has full attention to text, but causal along axis\n\n        i, j = dots_image.shape[-2:]\n        mask = torch.ones(i, j, device = device).triu_(j - i + 1).bool()\n        dots_image.masked_fill_(mask, mask_value)\n\n        # attention.\n\n        attn_image = dots_image.softmax(dim = -1)\n        out_image = einsum('b i j, b j d -> b i d', attn_image, v_img)\n\n        # merge back axis\n\n        out_image = rearrange(out_image, merge_axis_einops, ax = img_size)\n\n        # combine attended values for both text and image\n\n        out = torch.cat((out_text, out_image), dim = 1)\n\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n        out =  self.to_out(out)\n        return out[:, :n]\n"}