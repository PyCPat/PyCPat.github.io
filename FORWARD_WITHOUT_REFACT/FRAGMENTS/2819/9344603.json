{"BEFORE":"    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n        if self.training:\n            teacher_forcing_ratio = self.teacher_forcing_ratio\n        else:\n            teacher_forcing_ratio = 0\n        #x_enc = [x_enc len, batch size, n_features]\n        #x_dec = [x_dec len, batch size, n_features]\n        #teacher_forcing_ratio is probability to use teacher forcing\n        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n        \n        batch_size, x_dec_len, dec_in = x_dec.shape\n        #tensor to store decoder outputs\n        outputs = torch.zeros(batch_size, x_dec_len-1, dec_in).to(x_enc.device)\n        \n        #last hidden state of the encoder is used as the initial hidden state of the decoder\n        hidden, cell = self.encoder(x_enc, x_mark_enc)\n        \n        #first input to the decoder is the <sos> tokens\n        input, input_mark = x_dec[:, 0, :].unsqueeze(dim=1), x_mark_dec[:, 0, :].unsqueeze(dim=1)\n\n        for t in range(1, x_dec_len):\n            \n            #insert input token embedding, previous hidden and previous cell states\n            #receive output tensor (predictions) and new hidden and cell states\n            output, hidden, cell = self.decoder(input, input_mark, hidden, cell)\n            \n            #place predictions in a tensor holding predictions for each token\n            outputs[:, t-1, :] = output.squeeze(dim=1)\n            \n            #decide if we are going to use teacher forcing or not\n            teacher_force = random.random() < teacher_forcing_ratio\n\n            #if teacher forcing, use actual next token as next input\n            #if not, use predicted token\n            input = x_dec[:, t, :].unsqueeze(dim=1) if teacher_force else output\n            input_mark = x_mark_dec[:, t, :].unsqueeze(dim=1)\n","AFTER":"        if isinstance(x_dec, list):\n            input = [i[:, 0, :].unsqueeze(dim=1) for i in x_dec]\n        else:\n            input = x_dec[:, 0, :].unsqueeze(dim=1)\n        for t in range(1, x_dec_len):\n            \n            #insert input token embedding, previous hidden and previous cell states\n            #receive output tensor (predictions) and new hidden and cell states\n            output, hidden, cell = self.decoder(input, hidden, cell)\n            \n            #place predictions in a tensor holding predictions for each token\n            outputs[:, t-1, :] = output.squeeze(dim=1)\n            \n            #decide if we are going to use teacher forcing or not\n            teacher_force = random.random() < teacher_forcing_ratio\n\n            #if teacher forcing, use actual next token as next input\n            #if not, use predicted token\n            if isinstance(x_dec, list):\n                input0 = x_dec[0][:, t, :].unsqueeze(dim=1) if teacher_force else output\n                input = [input0, x_dec[1][:, t, :].unsqueeze(dim=1)]\n            else:\n                input = x_dec[:, t, :].unsqueeze(dim=1) if teacher_force else output\n        return outputs[:, :, -self.out_size:]\n"}