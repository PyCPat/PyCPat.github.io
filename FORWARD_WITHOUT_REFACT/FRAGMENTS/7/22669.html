<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.avgpool = nn.AvgPool2d(kernel_size=4)
        self.fc = nn.Linear(512 * block.expansion, num_classes)

        for <a id="change">m</a> in self.modules():
            if isinstance(m, nn.Conv2d):
                n<a id="change"> = </a><a id="change">m.kernel_size[0]</a><a id="change"> * m.kernel_size[1] * </a>m.out_channels
                <a id="change">m.weight.data.normal_(0</a>, <a id="change">math.sqrt(2.</a><a id="change"> / </a>n<a id="change">)</a><a id="change">)</a>
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                <a id="change">m.bias.data.zero_()</a>

    def _make_layer(self, block, planes, blocks, stride=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:</code></pre><h3>After Change</h3><pre><code class='java'>
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512 * block.expansion, num_classes)

        for <a id="change">m</a> in self.modules():
            if isinstance(m, nn.Conv2d):
                <a id="change">nn.init.kaiming_normal_(
                    </a>m.weight<a id="change">, mode=&quotfan_out&quot, nonlinearity=&quotrelu&quot)</a>
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                <a id="change">nn.init.constant_(</a>m.bias, <a id="change">0</a><a id="change">)</a>

        &#47&#47 Zero-initialize the last BN in each residual branch,
        &#47&#47 so that the residual branch starts with zeros, and each residual block behaves like an identity.
        &#47&#47 This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677</code></pre>