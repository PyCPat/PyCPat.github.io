{"BEFORE":"            msa_mask = default(msa_mask, torch.ones_like(msa).bool())\n\n        elif exists(embedds):\n            m = self.embedd_project(embedds)\n            \n            # get msa_mask to all ones if none was passed\n            msa_mask = default(msa_mask, torch.ones_like(embedds[..., -1]).bool())\n        else:\n            raise Error('either MSA or embeds must be given')\n\n        # derive pairwise representation\n\n        x_left, x_right = self.to_pairwise_repr(x).chunk(2, dim = -1)\n        x = rearrange(x_left, 'b i d -> b i () d') + rearrange(x_right, 'b j d-> b () j d') # create pair-wise residue embeds\n        x_mask = rearrange(mask, 'b i -> b i ()') + rearrange(mask, 'b j -> b () j') if exists(mask) else None\n\n        # add relative positional embedding\n\n        seq_index = default(seq_index, torch.arange(n, device = device))\n        seq_rel_dist = rearrange(seq_index, 'i -> () i ()') - rearrange(seq_index, 'j -> () () j')\n        seq_rel_dist = seq_rel_dist.clamp(-self.max_rel_dist, self.max_rel_dist) + self.max_rel_dist\n        rel_pos_emb = self.pos_emb(seq_rel_dist)\n\n        # embed templates, if present\n\n        if exists(templates_feats):\n            _, num_templates, *_ = templates_feats.shape\n\n            # embed template\n\n            t = self.to_template_embed(templates_feats)\n            templates_mask_crossed = rearrange(templates_mask, 'b t i -> b t i ()') * rearrange(templates_mask, 'b t j -> b t () j')\n\n            t = rearrange(t, 'b t ... -> (b t) ...')\n            templates_mask_crossed = rearrange(templates_mask_crossed, 'b t ... -> (b t) ...')\n\n            for _ in range(self.template_embed_layers):\n                t = self.template_pairwise_embedder(t, mask = templates_mask_crossed) + t\n\n            t = rearrange(t, '(b t) ... -> b t ...', t = num_templates)\n            templates_mask_crossed = rearrange(templates_mask_crossed, '(b t) ... -> b t ...', t = num_templates)\n\n            # template pos emb\n\n            assert t.shape[-2:] == x.shape[-2:]\n            x = x + t.mean(dim = 1)\n","AFTER":"        x = rearrange(x_left, 'b i d -> b i () d') + rearrange(x_right, 'b j d-> b () j d') # create pair-wise residue embeds\n        x_mask = rearrange(mask, 'b i -> b i ()') + rearrange(mask, 'b j -> b () j') if exists(mask) else None\n\n        # add relative positional embedding\n\n        seq_index = default(seq_index, lambda: torch.arange(n, device = device))\n        seq_rel_dist = rearrange(seq_index, 'i -> () i ()') - rearrange(seq_index, 'j -> () () j')\n        seq_rel_dist = seq_rel_dist.clamp(-self.max_rel_dist, self.max_rel_dist) + self.max_rel_dist\n        rel_pos_emb = self.pos_emb(seq_rel_dist)\n\n        # embed templates, if present\n\n        if exists(templates_feats):\n            _, num_templates, *_ = templates_feats.shape\n\n            # embed template\n\n            t = self.to_template_embed(templates_feats)\n            t_mask_crossed = rearrange(templates_mask, 'b t i -> b t i ()') * rearrange(templates_mask, 'b t j -> b t () j')\n\n            t = rearrange(t, 'b t ... -> (b t) ...')\n            t_mask_crossed = rearrange(t_mask_crossed, 'b t ... -> (b t) ...')\n\n            for _ in range(self.template_embed_layers):\n                t = self.template_pairwise_embedder(t, mask = t_mask_crossed) + t\n\n            t = rearrange(t, '(b t) ... -> b t ...', t = num_templates)\n            t_mask_crossed = rearrange(t_mask_crossed, '(b t) ... -> b t ...', t = num_templates)\n\n            # template pos emb\n\n            x_point = rearrange(x, 'b i j d -> (b i j) () d')\n            t_point = rearrange(t, 'b t i j d -> (b i j) t d')\n            x_mask_point = rearrange(x_mask, 'b i j -> (b i j) ()')\n            t_mask_point = rearrange(t_mask_crossed, 'b t i j -> (b i j) t')\n\n            template_pooled = self.template_pointwise_attn(\n                x_point,\n                context = t_point,\n                mask = x_mask_point,\n                context_mask = t_mask_point\n            )\n\n            template_pooled_mask = rearrange(t_mask_point.sum(dim = -1) > 0, 'b -> b () ()')\n            template_pooled = template_pooled * template_pooled_mask\n\n            template_pooled = rearrange(template_pooled, '(b i j) () d -> b i j d', i = n, j = n)\n            x = x + template_pooled\n"}