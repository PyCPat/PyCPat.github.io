{"BEFORE":"        x_emb = self.emb(x)\n        # [B, C, T]\n        x_emb = torch.transpose(x_emb, 1, -1)\n\n        # compute sequence masks\n        x_mask = torch.unsqueeze(sequence_mask(x_lengths, x.shape[1]),\n                                 1).to(x.dtype)\n\n        y_mask = torch.unsqueeze(sequence_mask(y_lengths, None),\n                                 1).to(x_mask.dtype)\n\n        # encoder pass\n        o_en = self.encoder(x_emb, x_mask)\n\n        # duration predictor pass\n        o_dr_log = self.duration_predictor(o_en.detach(), x_mask)\n\n        # expand o_en with durations\n        o_en_ex, attn = self.expand_encoder_outputs(o_en, dr, x_mask, y_mask)\n\n        # positional encoding\n        if hasattr(self, 'pos_encoder'):\n            o_en_ex = self.pos_encoder(o_en_ex, y_mask)\n\n        # decoder pass\n        o_de = self.decoder(o_en_ex, y_mask)\n\n        return o_de, o_dr_log.squeeze(1), attn.transpose(1, 2)\n","AFTER":"        o_en, o_en_dp, x_mask, g = self._forward_encoder(x, x_lengths, g)\n        o_dr_log = self.duration_predictor(o_en_dp.detach(), x_mask)\n        o_de, attn= self._forward_decoder(o_en, o_en_dp, dr, x_mask, y_lengths, g=g)\n        return o_de, o_dr_log.squeeze(1), attn\n"}