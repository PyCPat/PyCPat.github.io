{"BEFORE":"        if isinstance(x_dec, list):\n            input = [i[:, 0, :].unsqueeze(dim=1) for i in x_dec]\n        else:\n            input = x_dec[:, 0, :].unsqueeze(dim=1)\n        for t in range(1, x_dec_len):\n            \n            #insert input token embedding, previous hidden state and the context state\n            #receive output tensor (predictions) and new hidden state\n            output, hidden = self.decoder(input, hidden, context)\n            \n            #place predictions in a tensor holding predictions for each token\n            outputs[:, t-1, :] = output.squeeze(dim=1)\n            \n            #decide if we are going to use teacher forcing or not\n            teacher_force = random.random() < teacher_forcing_ratio\n            \n            #if teacher forcing, use actual next token as next input\n            #if not, use predicted token\n            if isinstance(x_dec, list):\n                input0 = x_dec[0][:, t, :].unsqueeze(dim=1) if teacher_force else output\n                input = [input0, x_dec[1][:, t, :].unsqueeze(dim=1)]\n            else:\n                input = x_dec[:, t, :].unsqueeze(dim=1) if teacher_force else output\n        return outputs[:, :, -self.out_size:]\n","AFTER":"    def forward(self, x_enc, x_enc_mark, x_dec, x_dec_mark):\n        if self.training:\n            teacher_forcing_ratio = self.teacher_forcing_ratio\n        else:\n            teacher_forcing_ratio = 0\n            \n        batch_size, x_dec_len, dec_in = x_dec.shape\n        #tensor to store decoder outputs\n        outputs = torch.zeros(batch_size, x_dec_len-1, dec_in).to(x_enc.device)\n        \n        #last hidden state of the encoder is the context\n        context = self.encoder(x_enc, x_enc_mark)\n        \n        #context also used as the initial hidden state of the decoder\n        hidden = context\n        \n        input = x_dec[:, 0, :].unsqueeze(dim=1)\n        input_mark = x_dec_mark[:, 0, :].unsqueeze(dim=1)\n        for t in range(1, x_dec_len):\n            #insert input token embedding, previous hidden state and the context state\n            #receive output tensor (predictions) and new hidden state\n            output, hidden = self.decoder(input, input_mark, hidden, context)\n            \n            #place predictions in a tensor holding predictions for each token\n            outputs[:, t-1, :] = output.squeeze(dim=1)\n            \n            #decide if we are going to use teacher forcing or not\n            teacher_force = random.random() < teacher_forcing_ratio\n            \n            #if teacher forcing, use actual next token as next input\n            input = x_dec[:, t, :].unsqueeze(dim=1) if teacher_force else output\n            input_mark = x_dec_mark[:, t, :].unsqueeze(dim=1)\n\n        return outputs[:, -self.pred_len:, -self.out_size:]\n"}