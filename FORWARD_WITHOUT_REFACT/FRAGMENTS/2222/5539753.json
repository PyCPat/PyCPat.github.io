{"BEFORE":"                PreNorm(dim, Chunk(ff_chunks, FeedForward(dim), along_dim = 1))\n","AFTER":"    def __init__(self, dim, depth, max_seq_len, heads = 8, bucket_size = 64, causal = False, one_kv_head = False, ff_chunks = 1, reversible = False, blindspot_size = 1, n_local_attn_heads = 0, local_attn_window_size = 128, psi_fn = DEFAULT_PSI, receives_context = False, pkm_layers = tuple(), pkm_num_keys = 128):\n        super().__init__()\n        if type(n_local_attn_heads) is not tuple:\n            n_local_attn_heads = tuple([n_local_attn_heads] * depth)\n\n        assert len(n_local_attn_heads) == depth, 'local attention heads tuple must have the same length as the depth'\n        assert all([(local_heads <= heads) for local_heads in n_local_attn_heads]), 'number of local attn heads must be less than the maximum number of heads'\n\n        layers = nn.ModuleList([])\n\n        def cast_tuple(val):\n            return (val,) if not isinstance(val, tuple) else val\n\n        for ind, local_heads in zip(range(depth), n_local_attn_heads):\n            layer_num = ind + 1\n            use_pkm = layer_num in cast_tuple(pkm_layers)\n\n            parallel_net = Chunk(ff_chunks, FeedForward(dim), along_dim = 1) if not use_pkm else PKM(dim)\n\n            layer = nn.ModuleList([\n                PreNorm(dim, SelfAttention(dim, heads, causal, one_kv_head = one_kv_head, blindspot_size = blindspot_size, n_local_attn_heads = local_heads, local_attn_window_size = local_attn_window_size, psi_fn = psi_fn)),\n                PreNorm(dim, parallel_net)\n"}