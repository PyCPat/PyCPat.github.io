{"BEFORE":"        h = torch.cat((h, x), dim=1)\n        q = self.w_q(x)\n        k = self.w_ke(h)\n        v = self.w_v(h)\n        \n        _, ctx_len, embed_dim = x.shape\n        mem_len = h.shape[1] - ctx_len\n        \n        b = q @ self.w_kr(self.r).T\n        b = self.circulant_shift(b, -ctx_len+1) # do we need to tril here or is att.tril fine?\n                \n        att = q @ k.T + b + self.u @ k.T + self.v @ self.w_kr(self.r).T\n        att = att.tril(mem_len) \/ embed_dim**0.5\n        att = torch.softmax(att, dim=-1)\n        \n        out = self.layer_norm(self.mlp(att @ v))\n        return self.pos_ff(out)\n","AFTER":"        h = torch.cat((mem, x), dim=1)\n        \n        batch_size, seg_len, embed_dim = x.shape\n        mem_len = h.shape[1] - seg_len\n        total_len = h.shape[1]\n        \n        # compute projections of output from previous layer and the memory\n        q = self.w_q(x).reshape(batch_size, -1, seg_len, embed_dim)\n        k = self.w_ke(h).reshape(batch_size, -1, total_len, embed_dim)\n        v = self.w_v(h).reshape(batch_size, -1, total_len, embed_dim)\n        r = self.w_kr(self.pos).reshape(-1, total_len, embed_dim)\n        \n        # compute relative positional encodings\n        b = q @ r.transpose(1, 2)\n        b = self.circulant_shift(b, -seg_len+1)\n        \n        # this is the XL specific way of computing the attention score\n        k = k.transpose(2, 3)\n        att = q @ k + b + self.u @ k + self.v @ r.transpose(1, 2)\n        att = att.tril(mem_len) \/ embed_dim**0.5\n        att = torch.softmax(att, dim=-1)\n        \n        # compute the output of the layer and save to memory\n        out = self.layer_norm(self.mlp(att @ v) + x)\n        out = self.pos_ff(out)\n        self.save_to_memory(out)\n        \n        return out\n"}