{"BEFORE":"        m = self.layer_norm_m(m)\n\n        n_seq, n_res = m.shape[-3:-1]\n        if mask is None:\n            # [*, N_seq, N_res]\n            mask = m.new_ones(\n                m.shape[:-3] + (n_seq, n_res),\n            )\n\n        # [*, N_seq, 1, 1, N_res]\n        bias = (self.inf * (mask - 1))[..., :, None, None, :]\n\n        # This step simply returns a larger view of the bias, and does not\n        # consume additional memory.\n        # [*, N_seq, no_heads, N_res, N_res]\n        #bias = bias.expand(\n        #    ((-1,) * len(bias.shape[:-4])) + (-1, self.no_heads, n_res, -1)\n        #)\n        \n        biases = [bias]\n\n        if (self.pair_bias and \n            z is not None and                       # For the \n            self.layer_norm_z is not None and       # benefit of\n            self.linear_z is not None               # TorchScript\n        ):\n            # [*, N_res, N_res, C_z]\n            z = self.layer_norm_z(z)\n\n            # [*, N_res, N_res, no_heads]\n            z = self.linear_z(z)\n\n            # [*, 1, no_heads, N_res, N_res]\n            z = permute_final_dims(z, (2, 0, 1)).unsqueeze(-4)\n\n            biases.append(z)\n\n        if chunk_size is not None:\n","AFTER":"        _checkpoint_chunks: Optional[bool] = None,\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            m:\n                [*, N_seq, N_res, C_m] MSA embedding\n            z:\n                [*, N_res, N_res, C_z] pair embedding. Required only if\n                pair_bias is True\n            mask:\n                [*, N_seq, N_res] MSA mask\n            chunk_size:\n                Size of chunks into which the inputs are split along their\n                batch dimensions. A low value decreases memory overhead at the \n                cost of slower execution. Chunking is not performed by default.\n                \n        \"\"\"\n        if(_chunk_logits is not None):\n            return self._chunked_msa_attn(\n                m=m, z=z, mask=mask, \n                chunk_logits=_chunk_logits, checkpoint=_checkpoint_chunks\n            )           \n\n        m, mask_bias, z = self._prep_inputs(m, z, mask)\n\n        biases = [mask_bias]\n        if(z is not None):\n            biases.append(z)\n\n        if chunk_size is not None:\n"}