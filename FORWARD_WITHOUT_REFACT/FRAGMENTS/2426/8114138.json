{"BEFORE":"        t_32 = torch.add(input=t_23, other=self.l_25(self.l_24(Tensor.view(t_31, size=[Tensor.size(t_31, dim=0), Tensor.size(t_31, dim=1), torch.mul(input=Tensor.size(t_31, dim=-2), other=Tensor.size(t_31, dim=-1))]))))\n        # calling GPT2Model\/Block[2]\/MLP[mlp]\/Conv1D[c_fc] with arguments:\n        # GPT2Model\/Block[2]\/LayerNorm[ln_2]\n        t_33 = self.l_27(self.l_26(t_32))\n        # returing:\n        # GPT2Model\/Block[2]\/MLP[mlp]\/Dropout[dropout]\n        # GPT2Model\/Block[2]\/aten::add5567\n        return (self.l_29(self.l_28(torch.mul(input=torch.mul(input=t_33, other=0.5), other=torch.add(input=Tensor.tanh(torch.mul(input=torch.add(input=t_33, other=torch.mul(input=Tensor.pow(t_33, exponent=3), other=0.044715)), other=0.7978845608028654)), other=1)))), t_32)\n","AFTER":"        t_34 = torch.add(input=t_32, other=self.l_29(self.l_28(torch.mul(input=torch.mul(input=t_33, other=0.5), other=torch.add(input=Tensor.tanh(torch.mul(input=torch.add(input=t_33, other=torch.mul(input=Tensor.pow(t_33, exponent=3), other=0.044715)), other=0.7978845608028654)), other=1)))))\n        # calling torch.split with arguments:\n        # GPT2LMHeadModel\/GPT2Model[transformer]\/Block[3]\/Attention[attn]\/Conv1D[c_attn]\n        # GPT2LMHeadModel\/GPT2Model[transformer]\/Block[3]\/Attention[attn]\/prim::Constant5779\n        # GPT2LMHeadModel\/GPT2Model[transformer]\/Block[3]\/Attention[attn]\/prim::Constant5780\n        t_35 = Tensor.split(self.l_31(self.l_30(t_34)), split_size=768, dim=2)\n        t_36 = t_35[0]\n        t_37 = t_35[1]\n        t_38 = t_35[2]\n        # calling torch.div with arguments:\n        # GPT2LMHeadModel\/GPT2Model[transformer]\/Block[3]\/Attention[attn]\/aten::matmul5854\n        # GPT2LMHeadModel\/GPT2Model[transformer]\/Block[3]\/Attention[attn]\/prim::Constant5855\n        t_39 = torch.div(input=Tensor.matmul(Tensor.permute(Tensor.view(t_36, size=[Tensor.size(t_36, dim=0), Tensor.size(t_36, dim=1), 12, torch.div(input=Tensor.size(t_36, dim=-1), other=12)]), dims=[0, 2, 1, 3]), other=Tensor.permute(Tensor.view(t_37, size=[Tensor.size(t_37, dim=0), Tensor.size(t_37, dim=1), 12, torch.div(input=Tensor.size(t_37, dim=-1), other=12)]), dims=[0, 2, 3, 1])), other=8.0)\n        # calling Tensor.size with arguments:\n        # GPT2LMHeadModel\/GPT2Model[transformer]\/Block[3]\/Attention[attn]\/aten::div5856\n        # GPT2LMHeadModel\/GPT2Model[transformer]\/Block[3]\/Attention[attn]\/prim::Constant5860\n        t_40 = Tensor.size(t_39, dim=-1)\n        # calling Tensor.slice with arguments:\n        # GPT2LMHeadModel\/GPT2Model[transformer]\/Block[3]\/Attention[attn]\/aten::slice5880\n        # GPT2LMHeadModel\/GPT2Model[transformer]\/Block[3]\/Attention[attn]\/prim::Constant5881\n        # GPT2LMHeadModel\/GPT2Model[transformer]\/Block[3]\/Attention[attn]\/prim::Constant5882\n        # GPT2LMHeadModel\/GPT2Model[transformer]\/Block[3]\/Attention[attn]\/aten::size5861\n        # GPT2LMHeadModel\/GPT2Model[transformer]\/Block[3]\/Attention[attn]\/prim::Constant5883\n        t_41 = self.b_3[0:9223372036854775807:1][:, 0:9223372036854775807:1][:, :, torch.sub(input=t_40, other=Tensor.size(t_39, dim=-2)):t_40:1][:, :, :, 0:t_40:1]\n        # calling Tensor.contiguous with arguments:\n        # GPT2LMHeadModel\/GPT2Model[transformer]\/Block[3]\/Attention[attn]\/aten::permute5905\n        t_42 = Tensor.contiguous(Tensor.permute(Tensor.matmul(self.l_32(Tensor.softmax(torch.sub(input=torch.mul(input=t_39, other=t_41), other=torch.mul(input=torch.rsub(t_41, other=1, alpha=1), other=10000.0)), dim=-1, dtype=None)), other=Tensor.permute(Tensor.view(t_38, size=[Tensor.size(t_38, dim=0), Tensor.size(t_38, dim=1), 12, torch.div(input=Tensor.size(t_38, dim=-1), other=12)]), dims=[0, 2, 1, 3])), dims=[0, 2, 1, 3]))\n        # calling torch.add with arguments:\n        # GPT2LMHeadModel\/GPT2Model[transformer]\/Block[2]\/aten::add5739\n        # GPT2LMHeadModel\/GPT2Model[transformer]\/Block[3]\/Attention[attn]\/Dropout[resid_dropout]\n        t_43 = torch.add(input=t_34, other=self.l_34(self.l_33(Tensor.view(t_42, size=[Tensor.size(t_42, dim=0), Tensor.size(t_42, dim=1), torch.mul(input=Tensor.size(t_42, dim=-2), other=Tensor.size(t_42, dim=-1))]))))\n        # returing:\n        # GPT2LMHeadModel\/GPT2Model[transformer]\/Block[3]\/LayerNorm[ln_2]\n        # GPT2LMHeadModel\/GPT2Model[transformer]\/Block[3]\/aten::add5953\n        return (self.l_35(t_43), t_43)\n"}