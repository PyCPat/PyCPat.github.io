{"BEFORE":"    def forward(self, data, target, mems):\n        # nn.DataParallel does not allow size(0) tensors to be broadcasted.\n        # So, have to initialize size(0) mems inside the model forward.\n        # Moreover, have to return new_mems to allow nn.DataParallel to piece\n        # them together.\n        if mems is None:\n            mems = self.init_mems()\n\n        tgt_len = target.size(0)\n        hidden, new_mems = self._forward(data, mems=mems)\n\n        pred_hid = hidden[-tgt_len:]\n        if self.sample_softmax > 0 and self.training:\n            raise NotImplementedError('Computing log probabilities is not implemented for sample_softmax mode')\n            assert self.tie_weight\n            logit = sample_logits(self.word_emb, self.out_layer.bias, target,\n                                  pred_hid, self.sampler)\n            loss = -F.log_softmax(logit, -1)[:, :, 0]\n        else:\n            loss, log_prob = self.crit(pred_hid.view(-1, pred_hid.size(-1)), target.view(-1))\n            loss = loss.view(tgt_len, -1)\n","AFTER":"    def forward(self, data:torch.Tensor, target:Optional[torch.Tensor], mems:Optional[torch.Tensor]):\n        # data -> [seq_len, batch_size], target -> [seq_len, batch_size]\n        # Returns:\n        # loss -> [seq_len, batch_size], log_prob -> [seq_len, batch_size, vocab_size]\n        # nn.DataParallel does not allow size(0) tensors to be broadcasted.\n        # So, have to initialize size(0) mems inside the model forward.\n        # Moreover, have to return new_mems to allow nn.DataParallel to piece\n        # them together.\n        if mems is None:\n            mems = self.init_mems()\n\n        hidden, new_mems = self._forward(data, mems=mems)\n\n        tgt_len = target.size(0) if target is not None else data.size(0)\n\n        pred_hid = hidden[-tgt_len:]\n        if self.sample_softmax > 0 and self.training:\n            raise NotImplementedError('Computing log probabilities is not implemented for sample_softmax mode')\n            assert self.tie_weight\n            logit = sample_logits(self.word_emb, self.out_layer.bias, target,\n                                  pred_hid, self.sampler)\n            loss = -F.log_softmax(logit, -1)[:, :, 0]\n        else:\n            loss, log_prob = self.crit(hidden=pred_hid.view(-1, pred_hid.size(-1)),\n                                       target=target.view(-1) if target is not None else None)\n            # loss -> [target_len, batch_size]\n            # log_prob -> [target_len, batch_size, vocab_size]\n            loss = loss.view(tgt_len, -1) if target is not None else None\n            log_prob = log_prob.view(tgt_len, data.size(1), -1)\n"}