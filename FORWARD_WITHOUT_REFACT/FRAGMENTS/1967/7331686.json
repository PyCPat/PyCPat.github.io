{"BEFORE":"        batch_count = keys.shape[0]\n\n        # b(hc)1q -> bqhc\n        # print(keys.shape, \"keys\", values.shape, \"values\", queries.shape, \"queries\")\n        keys = keys.transpose(1, 3)\n        keys = keys.reshape(keys.shape[:2] + (self.head_count, -1))\n\n        # b(hc)1q -> bchq\n        shape = (batch_count, self.head_count, self.head_dim, -1)\n        values = values.reshape(shape)\n        values = values.transpose(1, 2)\n        queries = queries.reshape(shape)\n        queries = queries.transpose(1, 2)\n\n        # print(keys.shape, \"keys\", values.shape, \"values\", queries.shape, \"queries\")\n\n        attention_bias = torch.where(\n            attention_mask,\n            torch.zeros([1, 1]),\n            torch.ones([1, 1]) * (-torch.inf),\n        )\n        attention_weights: FloatTensor = torch.einsum(\n            'bchq,bkhc->bkhq',\n            queries \/ self.head_dim ** 0.5, \n            keys\n        )\n        attention_weights += attention_bias[:, :, None, None]\n        attention_weights = torch.softmax(attention_weights, 1)\n        # print(attention_weights.shape, \"attention_weights\")\n        hidden_state: FloatTensor = torch.einsum(\n            \"bkhq,bchk->bchq\",\n            attention_weights, \n            values\n        )\n        # bchq -> b(hc)1q\n        # print(hidden_state.shape, \"hidden_state\")\n        hidden_state = hidden_state.transpose(1, 2)\n        hidden_state = hidden_state.reshape(batch_count, self.embed_count, 1, -1)\n        hidden_state = self.out_proj.forward(hidden_state)\n        # print(hidden_state.shape, \"hidden_state\")\n        return hidden_state\n","AFTER":"            torch.full(attention_mask.shape, 0.0),\n            torch.full(attention_mask.shape, -torch.inf),\n        )\n        attention_weights: FloatTensor = torch.einsum(\n            'bqhc,bkhc->bhqk',\n            queries, \n            keys\n        )\n        attention_weights += attention_bias[:, None, None, :]\n        attention_weights = torch.softmax(attention_weights, -1)\n        attention_output: FloatTensor = torch.einsum(\n            \"bhqk,bkhc->bqhc\",\n            attention_weights, \n            values\n        )\n        shape = attention_output.shape[:2] + (self.embed_count,)\n        attention_output = attention_output.reshape(shape)\n        attention_output = self.out_proj.forward(attention_output)\n        return attention_output\n"}