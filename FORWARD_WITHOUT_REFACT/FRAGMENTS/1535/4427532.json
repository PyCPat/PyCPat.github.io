{"BEFORE":"        o = self.x2h(g.x)\n        e = self.e2h(g.edge_attr)\n        c = self.c2h(cond)\n        num_total_nodes = g.x.shape[0]\n        # Augment the edges with a new edge to the conditioning\n        # information node. This new node is connected to every node\n        # within its graph.\n        u, v = torch.arange(num_total_nodes, device=o.device), g.batch + num_total_nodes\n        aug_edge_index = torch.cat([g.edge_index, torch.stack([u, v]), torch.stack([v, u])], 1)\n        e_p = torch.zeros((num_total_nodes * 2, e.shape[1]), device=g.x.device)\n        e_p[:, 0] = 1  # Manually create a bias term\n        aug_e = torch.cat([e, e_p], 0)\n        aug_edge_index, aug_e = add_self_loops(aug_edge_index, aug_e, 'mean')\n        aug_batch = torch.cat([g.batch, torch.arange(c.shape[0], device=o.device)], 0)\n\n        # Append the conditioning information node embedding to o\n        o = torch.cat([o, c], 0)\n        for i in range(self.num_layers):\n            # Run the graph transformer forward\n            gen, trans, linear, norm1, ff, norm2 = self.graph2emb[i * 6:(i + 1) * 6]\n            agg = gen(o, aug_edge_index, aug_e)\n            o = norm1(o + linear(trans(torch.cat([o, agg], 1), aug_edge_index, aug_e)), aug_batch)\n            o = norm2(o + ff(o), aug_batch)\n\n        glob = torch.cat([gnn.global_mean_pool(o[:-c.shape[0]], g.batch), o[-c.shape[0]:], c], 1)\n        o_final = torch.cat([o[:-c.shape[0]], c[g.batch]], 1)\n","AFTER":"        if self.num_noise > 0:\n            x = torch.cat([g.x, torch.rand(g.x.shape[0], self.num_noise, device=g.x.device)], 1)\n        else:\n            x = g.x\n        o = self.x2h(x)\n        e = self.e2h(g.edge_attr)\n        c = self.c2h(cond)\n        num_total_nodes = g.x.shape[0]\n        # Augment the edges with a new edge to the conditioning\n        # information node. This new node is connected to every node\n        # within its graph.\n        u, v = torch.arange(num_total_nodes, device=o.device), g.batch + num_total_nodes\n        aug_edge_index = torch.cat([g.edge_index, torch.stack([u, v]), torch.stack([v, u])], 1)\n        e_p = torch.zeros((num_total_nodes * 2, e.shape[1]), device=g.x.device)\n        e_p[:, 0] = 1  # Manually create a bias term\n        aug_e = torch.cat([e, e_p], 0)\n        aug_edge_index, aug_e = add_self_loops(aug_edge_index, aug_e, 'mean')\n        aug_batch = torch.cat([g.batch, torch.arange(c.shape[0], device=o.device)], 0)\n\n        # Append the conditioning information node embedding to o\n        o = torch.cat([o, c], 0)\n        for i in range(self.num_layers):\n            # Run the graph transformer forward\n            gen, trans, linear, norm1, ff, norm2, cscale = self.graph2emb[i * 7:(i + 1) * 7]\n            agg = gen(o, aug_edge_index, aug_e)\n            l_h = linear(trans(torch.cat([o, agg], 1), aug_edge_index, aug_e))\n            cs = cscale(c[aug_batch])\n            scale, shift = cs[:, :l_h.shape[1]], cs[:, l_h.shape[1]:]\n            o = norm1(o + l_h * scale + shift, aug_batch)\n"}