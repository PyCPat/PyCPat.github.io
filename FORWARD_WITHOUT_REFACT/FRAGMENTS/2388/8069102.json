{"BEFORE":"        batch_size = x.size(0)\n        embedding = self.dropout(self.embed(x))\n        conv_in = embedding.permute(0, 2, 1)\n        conv_out = self.conv(conv_in)\n        values, indices = conv_out.max(dim=-1)\n        conv_out = conv_out.permute(2, 0, 1)\n        rnn_out, _ = self.rnn(conv_out)\n        attention = (self.mask * rnn_out).mean(dim=0)\n        output = self.fc(attention).squeeze(1)\n","AFTER":"        conv_in = embedding.permute(0, 2, 1)\n        conv_out = self.conv(conv_in).permute(2, 0, 1)\n        rnn_out, _ = self.rnn(conv_out)\n        global_rnn_out = rnn_out.mean(dim=0)\n        attention = torch.tanh(\n            self.local2attn(rnn_out) + self.global2attn(global_rnn_out)\n        ).permute(1, 0, 2)\n        alpha = F.softmax(attention.matmul(self.attn_scale), dim=-1)\n        rnn_out = rnn_out.permute(1, 0, 2)\n        memory = (alpha * rnn_out).sum(dim=1)\n        output = self.fc(memory).squeeze(1)\n"}