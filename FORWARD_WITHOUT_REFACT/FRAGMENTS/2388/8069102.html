<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.fc = nn.Linear(num_layers * hidden_size, 1)

    def forward(self, x):
        batch_size<a id="change"> = </a><a id="change">x.size(</a>0<a id="change">)</a>
        embedding = self.dropout(self.embed(x))
        conv_in = embedding.permute(0, 2, 1)
        conv_out = self.conv(conv_in)
        values, indices = conv_out.max(dim=-1)</code></pre><h3>After Change</h3><pre><code class='java'>
        attention = torch.tanh(
            self.local2attn(rnn_out) + self.global2attn(global_rnn_out)
        ).permute(1, 0, 2)
        alpha = <a id="change">F.softmax(</a>attention.matmul(self.attn_scale)<a id="change">, dim=-1)</a>
        rnn_out = rnn_out.permute(1, 0, 2)
        memory = (alpha * rnn_out).sum(dim=1)
        output = self.fc(memory).squeeze(1)
        return output</code></pre>