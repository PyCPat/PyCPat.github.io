{"BEFORE":"    def forward(self,ct,gi,et,ei,ht):\n        '''\n        Forward pass through Attended memory decoder.\n        we run this one step (word) at a time.\n        Args:\n            ct : contect vector from decoder at time step t. (B,F')\n            gi : The visual context from memory for current(tth) word. (B,F') \n            et : word embedding from decoder for current word. (B,E)\n            ei : word context from memory for current word. (B,E)\n            ht : Last hidden memory of decoder LSTM. (B,h)\n            \n        \n        '''\n        decoder_visual = self.decoder_visual_context_projection(ct)\n        memory_visual = self.memory_visual_context_projection(gi)\n        decoder_word_embd = self.decoder_word_embed_projection(et)\n        memory_word_embd = self.memory_word_embed_projection(ei)\n        decoder_hidden = self.decoder_hidden_projection(ht)\n        out = decoder_visual+memory_visual+decoder_word_embd+memory_word_embd+decoder_hidden\n        output = self.output(out)\n","AFTER":"    def forward(self,ct,et,ht,memory):\n        '''\n        Forward pass through Attended memory decoder.\n        we run this one step (word) at a time.\n        Args:\n            ct : contect vector from decoder at time step t. (B,2F')\n            gi : The visual context from memory for current(tth) word. (B,F') \n            et : word embedding from decoder for current word. (B,E)\n            ei : word context from memory for current word. (B,E)\n            ht : Last hidden memory of decoder LSTM. (B,h)\n            \n        \n        '''\n        \n        ei,gi = self._create_memory_representation(memory)\n        \n        memory_word_embd = self.memory_word_embed_projection(ei)\n        memory_visual = self.memory_visual_context_projection(gi)\n        \n        \n        decoder_visual = self.decoder_visual_context_projection(ct).unsqueeze(1).expand_as(memory_visual)\n        decoder_word_embd = self.decoder_word_embed_projection(et).unsqueeze(1).expand_as(memory_word_embd)\n        decoder_hidden = self.decoder_hidden_projection(ht).unsqueeze(1).expand_as(memory_visual)\n        \n        out = torch.tanh(decoder_visual+memory_visual+decoder_word_embd+memory_word_embd+decoder_hidden) \n        output = self.output(out).squeeze(2) #(B,V,1) -> (B,V)\n"}