{"BEFORE":"        b, n, num_retrieved, chunk_size = x.shape[0], x.shape[-2], context.shape[-3], context.shape[-2]\n        causal_padding = chunk_size - 1\n\n        # causal padding\n\n        x = F.pad(x, (0, 0, -causal_padding, causal_padding), value = 0.)\n\n        if exists(pos_emb):\n            q_pos_emb, k_pos_emb = cast_tuple(pos_emb, num = 2)\n\n            # make sure queries positions are properly shifted\n            q_pos_emb = F.pad(q_pos_emb, (0, 0, -causal_padding, causal_padding), value = 0.)\n\n            # properly chunk positional embeddings\n\n            q_pos_emb = repeat(q_pos_emb, '1 h (k n) d -> (b k) h n d', n = chunk_size, b = b)\n            k_pos_emb = repeat(k_pos_emb, '1 h (k n) d -> (b k) h (r n) d', n = chunk_size, b = b, r = num_retrieved)\n\n            pos_emb = (q_pos_emb, k_pos_emb)\n\n        # reshape so we have chunk to chunk attention, without breaking causality\n\n        x = rearrange(x, 'b (k n) d -> (b k) n d', n = chunk_size)\n        context = rearrange(context, 'b k r n d -> (b k) (r n) d')\n\n        # cross attention\n\n        out = self.cross_attn(x, context = context, pos_emb = pos_emb, **kwargs)\n\n        # reshape back to original sequence\n\n        out = rearrange(out, '(b k) n d -> b (k n) d', k = n \/\/ chunk_size)\n","AFTER":"        b, n, num_chunks, num_retrieved, chunk_size = x.shape[0], x.shape[-2], *context.shape[-4:-1]\n        causal_padding = chunk_size - 1\n\n        # causal padding\n\n        x = F.pad(x, (0, 0, -causal_padding, causal_padding), value = 0.)\n\n        # take care of rotary positional embedding\n        # make sure queries positions are properly shifted to the future\n\n        q_pos_emb, k_pos_emb = pos_emb\n        q_pos_emb = F.pad(q_pos_emb, (0, 0, -causal_padding, causal_padding), value = 0.)\n        pos_emb = (q_pos_emb, k_pos_emb)\n\n        # reshape so we have chunk to chunk attention, without breaking causality\n\n        x = rearrange(x, 'b (k n) d -> (b k) n d', k = num_chunks)\n        context = rearrange(context, 'b k r n d -> (b k) (r n) d')\n\n        # cross attention\n\n        out = self.cross_attn(x, context = context, pos_emb = pos_emb, **kwargs)\n\n        # reshape back to original sequence\n\n        out = rearrange(out, '(b k) n d -> b (k n) d', k = num_chunks)\n"}