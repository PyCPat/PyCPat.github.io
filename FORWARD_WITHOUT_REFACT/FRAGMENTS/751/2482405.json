{"BEFORE":"            if self.dropout > 0:\n                message = F.dropout(message, self.dropout, self.training)\n\n        nei_message = index_select_ND(message, 0, agraph)\n        nei_message = nei_message.sum(dim=1)\n        ainput = torch.cat([fatoms, nei_message], dim=1)\n        atom_hiddens = self.act_func(self.W_o(ainput))\n        if self.dropout > 0:\n            atom_hiddens = F.dropout(atom_hiddens, self.dropout, self.training)\n\n        mol_vecs = []\n        for st, le in scope:\n            mol_vec = atom_hiddens.narrow(0, st, le).sum(dim=0) \/ le\n","AFTER":"            message = self.dropout_layer(message)\n\n        nei_message = index_select_ND(message, 0, agraph)\n        nei_message = nei_message.sum(dim=1)\n        ainput = torch.cat([fatoms, nei_message], dim=1)\n        atom_hiddens = self.act_func(self.W_o(ainput))\n        atom_hiddens = self.dropout_layer(atom_hiddens)\n\n        mol_vecs = []\n        for st, le in scope:\n            cur_hiddens = atom_hiddens.narrow(0, st, le)\n\n            if self.attention:\n                att_w = torch.matmul(self.W_a(cur_hiddens), cur_hiddens.t())\n                att_w = F.softmax(att_w, dim=1)\n                att_hiddens = torch.matmul(att_w, cur_hiddens)\n                att_hiddens = self.act_func(self.W_b(att_hiddens))\n                att_hiddens = self.dropout_layer(att_hiddens)\n                mol_vec = (cur_hiddens + att_hiddens)\n            else:\n                mol_vec = cur_hiddens\n\n            mol_vec = mol_vec.sum(dim=0) \/ le\n"}