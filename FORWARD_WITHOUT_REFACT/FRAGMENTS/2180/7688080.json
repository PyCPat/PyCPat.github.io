{"BEFORE":"        gate = self.norm(gate)\n\n        weight, bias = self.proj.weight, self.proj.bias\n        if self.causal:\n            weight, bias = weight[:n, :n], bias[:n]\n            mask = torch.ones(weight.shape[:2], device = device).triu_(1).bool()\n            weight = weight.masked_fill(mask[..., None], 0.)\n\n        gate = F.conv1d(gate, weight, bias)\n","AFTER":"        gate = self.norm(gate)\n\n        weight, bias = self.weight, self.bias\n\n        if self.use_circulant_matrix:\n            dim_seq = weight.shape[-1]\n            weight = F.pad(weight, (0, dim_seq), value = 0)\n            weight = repeat(weight, '... n -> ... (r n)', r = dim_seq)\n            weight = weight[:-dim_seq].reshape(dim_seq, 2 * dim_seq - 1)\n            weight = weight[:, (dim_seq - 1):]\n\n        if self.causal:\n            weight, bias = weight[:n, :n], bias[:n]\n            mask = torch.ones(weight.shape[:2], device = device).triu_(1).bool()\n            weight = weight.masked_fill(mask, 0.)\n\n        gate = einsum('b n d, m n -> b m d', gate, weight)\n        gate = gate + rearrange(bias, 'n -> () n ()')\n"}