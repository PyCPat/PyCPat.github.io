{"BEFORE":"        if n < seq_len:\n            padding = seq_len - n\n            mask = default(mask, lambda: torch.ones(b, n, device = device).bool())\n            x = F.pad(x, (0, 0, 0, padding), value = 0)\n            mask = F.pad(x, (0, padding), value = False)\n\n        qkv = self.to_qkv(x).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), qkv)\n\n        q *= self.scale\n\n        img_seq_len = img_size ** 2\n        text_len = seq_len - img_seq_len\n        ((q_text, q_img), (k_text, k_img), (v_text, v_img)) = map(lambda t: (t[:, img_seq_len:], t[:, -img_seq_len:]), (q, k, v))\n\n        # text attention\n\n        dots_text = einsum('b i d, b j d -> b i j', q_text, k_text)\n        mask_value = max_neg_value(dots_text)\n\n        i, j = dots_text.shape[-2:]\n        mask = torch.ones(i, j, device = device).triu_(j - i + 1).bool()\n        dots_text.masked_fill(mask, mask_value)\n\n        attn_text = dots_text.softmax(dim = -1)\n        out_text = einsum('b i j, b j d -> b i d', attn_text, v_text)\n\n        # image attention\n\n        effective_kernel_size = (kernel_size - 1) * dilation + 1\n        padding = effective_kernel_size \/\/ 2\n\n        k_img, v_img = map(lambda t: rearrange(t, 'b (h w) c -> b c h w', h = img_size), (k_img, v_img))\n        k_img, v_img = map(lambda t: F.unfold(t, kernel_size, padding = padding, dilation = dilation), (k_img, v_img))\n        k_img, v_img = map(lambda t: rearrange(t, 'b (j d) i -> b i j d', j = kernel_size ** 2), (k_img, v_img))\n\n        k_text, v_text = map(lambda t: repeat(t, 'b j d -> b i j d', i = img_seq_len), (k_text, v_text))\n\n        # let image attend to all of text\n\n        k_img = torch.cat((k_text, k_img), dim = 2)\n        v_img = torch.cat((v_text, v_img), dim = 2)\n\n        dots_image = einsum('b i d, b i j d -> b i j', q_img, k_img)\n\n        # calculate causal attention for local convolution\n\n        i, j = dots_image.shape[-2:]\n        img_seq = torch.arange(img_seq_len, device = device)\n        k_img_indices = rearrange(img_seq.float(), '(h w) -> () () h w', h = img_size)\n        k_img_indices = F.pad(k_img_indices, (padding,) * 4, value = img_seq_len) # padding set to be max, so it is never attended to\n        k_img_indices = F.unfold(k_img_indices, kernel_size, dilation = dilation)\n        k_img_indices = rearrange(k_img_indices, 'b j i -> b i j')\n\n        # mask image attention\n\n        q_img_indices = rearrange(img_seq, 'i -> () i ()')\n        mask =  q_img_indices >= k_img_indices\n\n        # image can attend to all of text\n\n        mask = F.pad(mask, (text_len, 0), value = True)\n        dots_image.masked_fill_(~mask, mask_value)\n","AFTER":"        img_seq_len = img_size ** 2\n        text_len = seq_len + 1 - img_seq_len\n\n        # padding\n\n        padding = seq_len - n + 1\n        mask = default(mask, lambda: torch.ones(b, text_len, device = device).bool())\n\n        x = F.pad(x, (0, 0, 0, padding), value = 0)\n        mask = mask[:, :text_len]\n\n        # derive query \/ keys \/ values\n\n        qkv = self.to_qkv(x).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), qkv)\n\n        q *= self.scale\n\n        ((q_text, q_img), (k_text, k_img), (v_text, v_img)) = map(lambda t: (t[:, img_seq_len:], t[:, -img_seq_len:]), (q, k, v))\n\n        # text attention\n\n        dots_text = einsum('b i d, b j d -> b i j', q_text, k_text)\n        mask_value = max_neg_value(dots_text)\n\n        i, j = dots_text.shape[-2:]\n        text_causal_mask = torch.ones(i, j, device = device).triu_(j - i + 1).bool()\n        dots_text.masked_fill(text_causal_mask, mask_value)\n\n        attn_text = dots_text.softmax(dim = -1)\n        out_text = einsum('b i j, b j d -> b i d', attn_text, v_text)\n\n        # image attention\n\n        effective_kernel_size = (kernel_size - 1) * dilation + 1\n        padding = effective_kernel_size \/\/ 2\n\n        k_img, v_img = map(lambda t: rearrange(t, 'b (h w) c -> b c h w', h = img_size), (k_img, v_img))\n        k_img, v_img = map(lambda t: F.unfold(t, kernel_size, padding = padding, dilation = dilation), (k_img, v_img))\n        k_img, v_img = map(lambda t: rearrange(t, 'b (j d) i -> b i j d', j = kernel_size ** 2), (k_img, v_img))\n\n        k_text, v_text = map(lambda t: repeat(t, 'b j d -> b i j d', i = img_seq_len), (k_text, v_text))\n\n        # let image attend to all of text\n\n        k_img = torch.cat((k_text, k_img), dim = 2)\n        v_img = torch.cat((v_text, v_img), dim = 2)\n\n        dots_image = einsum('b i d, b i j d -> b i j', q_img, k_img)\n\n        # calculate causal attention for local convolution\n\n        i, j = dots_image.shape[-2:]\n        img_seq = torch.arange(img_seq_len, device = device)\n        k_img_indices = rearrange(img_seq.float(), '(h w) -> () () h w', h = img_size)\n        k_img_indices = F.pad(k_img_indices, (padding,) * 4, value = img_seq_len) # padding set to be max, so it is never attended to\n        k_img_indices = F.unfold(k_img_indices, kernel_size, dilation = dilation)\n        k_img_indices = rearrange(k_img_indices, 'b j i -> b i j')\n\n        # mask image attention\n\n        q_img_indices = rearrange(img_seq, 'i -> () i ()')\n        causal_mask =  q_img_indices >= k_img_indices\n\n        # concat text mask with image causal mask\n\n        causal_mask = repeat(causal_mask, '() i j -> b i j', b = b * h)\n        mask = repeat(mask, 'b j -> (b h) i j', i = i, h = h)\n        mask = torch.cat((~mask, causal_mask), dim = -1)\n\n        # image can attend to all of text\n\n        dots_image.masked_fill_(mask, mask_value)\n"}