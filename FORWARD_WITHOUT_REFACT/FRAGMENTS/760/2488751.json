{"BEFORE":"        assert x.size(1) == self.max_seq_length\n\n        x = self.embedding(x)\n        x = x.unsqueeze(dim=1)\n\n        out_tensors = []\n        for conv, pool in zip(self.convs, self.pools):\n            activation = pool(F.relu(conv(x)))\n            out_tensors.append(activation)\n\n        x = torch.cat(out_tensors, dim=1)\n\n        batch_size = x.size(0)\n        x = x.view(batch_size, -1)\n        x = self.dropout(x)\n\n        return self.fc(x)\n","AFTER":"        x = self._forward_pooled(x)\n        return self._dropout_and_fc(x)\n"}