{"BEFORE":"        dh1 = self.pyr.down(dh)\n        dh2 = self.pyr.down(dh1)\n        dh3 = self.pyr.down(dh2)\n            \n        B, _, H, W = x.shape\n        x1, x2, x3, x4, x5 = self.backbone(x)\n        \n        x1 = self.context1(x1) #4\n        x2 = self.context2(x2) #4\n        x3 = self.context3(x3) #8\n        x4 = self.context4(x4) #16\n        x5 = self.context5(x5) #32\n\n        f3, d3 = self.decoder(x5, x4, x3) #16\n\n        f3 = self.res(f3, (H \/\/ 4,  W \/\/ 4 ))\n        f3, _  = self.attention2_1(torch.cat([x2, f3], dim=1), d3.detach())\n        f2, p2 = self.attention2_2(torch.cat([x2, f3], dim=1), dh3)\n        d2 = self.pyr.rec(d3.detach(), p2) #4\n\n        x1 = self.res(x1, (H \/\/ 2, W \/\/ 2))\n        f2 = self.res(f2, (H \/\/ 2, W \/\/ 2))\n        f2, _  = self.attention1_1(torch.cat([x1, f2], dim=1), d2.detach()) #2\n        f2, _  = self.attention1_2(torch.cat([x1, f2], dim=1), p2.detach()) #2\n        f1, p1 = self.attention1_3(torch.cat([x1, f2], dim=1), p2.detach()) #2\n        d1 = self.pyr.rec(d2.detach(), p1) #2\n        \n        f1 = self.res(f1, (H, W))\n        f1, _ = self.attention0_1(f1, d1.detach()) #2\n        f1, _ = self.attention0_2(f1, p1.detach()) #attention0_2\n        _, p0 = self.attention0_3(f1, p1.detach()) #2\n","AFTER":"        x = self.resize(x)\n        dh = self.resize(dh)\n        B, _, H, W = x.shape\n            \n        x = torch.cat([x, dh], dim=1)\n        x = self.reduce(x)\n    \n        x1, x2, x3, x4, x5 = self.backbone(x)\n        \n        x1 = self.context1(x1) #4\n        x2 = self.context2(x2) #4\n        x3 = self.context3(x3) #8\n        x4 = self.context4(x4) #16\n        x5 = self.context5(x5) #32\n\n        f3, d3 = self.decoder(x5, x4, x3) #16\n\n        f3 = self.res(f3, (H \/\/ 4,  W \/\/ 4 ))\n        f2, p2 = self.attention2(torch.cat([x2, f3], dim=1), d3.detach())\n        d2 = self.pyr.rec(d3.detach(), p2) #4\n\n        x1 = self.res(x1, (H \/\/ 2, W \/\/ 2))\n        f2 = self.res(f2, (H \/\/ 2, W \/\/ 2))\n        f1, p1 = self.attention1(torch.cat([x1, f2], dim=1), d2.detach(), p2.detach()) #2\n        d1 = self.pyr.rec(d2.detach(), p1) #2\n        \n        f1 = self.res(f1, (H, W))\n        _, p0 = self.attention0(f1, d1.detach(), p1.detach()) #2\n        d0 = self.pyr.rec(d1.detach(), p0) #2\n        \n        if type(sample) == dict and 'gt' in sample.keys() and sample['gt'] is not None:\n            y = sample['gt']\n            \n            y1 = self.pyr.down(y)\n            y2 = self.pyr.down(y1)\n            y3 = self.pyr.down(y2)\n\n            ploss =  self.pyramidal_consistency_loss_fn(self.des(d3, (H, W)), self.des(self.pyr.down(d2), (H, W)).detach()) * 0.0001\n            ploss += self.pyramidal_consistency_loss_fn(self.des(d2, (H, W)), self.des(self.pyr.down(d1), (H, W)).detach()) * 0.0001\n            ploss += self.pyramidal_consistency_loss_fn(self.des(d1, (H, W)), self.des(self.pyr.down(d0), (H, W)).detach()) * 0.0001\n            \n            closs =  self.loss_fn(self.des(d3, (H, W)), self.des(y3, (H, W)))\n            closs += self.loss_fn(self.des(d2, (H, W)), self.des(y2, (H, W)))\n            closs += self.loss_fn(self.des(d1, (H, W)), self.des(y1, (H, W)))\n            closs += self.loss_fn(self.des(d0, (H, W)), self.des(y, (H, W)))\n            \n            loss = ploss + closs\n\n        else:\n            loss = 0\n\n        if self.resize is True:\n            d0 = self.res(d0, (h, w))\n\n        if type(sample) == dict:\n"}