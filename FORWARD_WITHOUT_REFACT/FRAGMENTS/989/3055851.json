{"BEFORE":"        semantic_embeddings = []\n        for i, g in enumerate(gs):\n            semantic_embeddings.append(self.gat_layers[i](g, h).flatten(1))\n        semantic_embeddings = torch.stack(semantic_embeddings, dim=1)  # (N, M, D * K)\n        return self.semantic_attention(semantic_embeddings)  # (N, D * K)\n","AFTER":"        zp = [gat(g, h).flatten(start_dim=1) for gat, g in zip(self.gats, gs)]  # 基于元路径的嵌入\n        zp = torch.stack(zp, dim=1)  # (N, M, K*d_out)\n        z = self.semantic_attention(zp)  # (N, K*d_out)\n        return z\n"}