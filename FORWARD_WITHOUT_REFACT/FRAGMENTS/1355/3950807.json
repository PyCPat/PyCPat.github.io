{"BEFORE":"        for xRaw, prob, squeeze, codebook, k in zip(latents, self._prob, self._squeeze, self._codebook, self._k):\n            n, c, h, w = xRaw.shape\n            # [c, k]\n            codewords = codebook.weight\n            # [n, c, h, w] -> [h, w, n, c]\n            encoderIn = xRaw.permute(2, 3, 0, 1)\n            # [h, w, n, c] -> [h*w, n, c]\n            encoderIn = self._position(encoderIn).reshape(-1, n, c)\n            # [h*w, n, c]\n            # x = self._encoder(posisted)\n            x = self._encoder(encoderIn)\n            # x = self._dePosition(x.reshape(h, w, n, c)).reshape(-1, n, c)\n            # x = encoderIn\n            # [h*w, n, k]\n            # logit = prob(x, h, w)\n            logit = torch.matmul(x, codewords)\n            soft = (logit \/ temperature).softmax(-1)\n            if hard:\n                hard = logit.argmax(-1)\n                hard = F.one_hot(hard, k)\n                sample = (hard - soft).detach() + soft\n            else:\n                sample = soft\n            # sample = F.gumbel_softmax(logit, temperature, hard)\n            # sample = logit\n            # [h*w, N, c] <- [h*w, N, k] @ [k, C]\n            quantized = codebook(sample)\n","AFTER":"            logit = torch.matmul(x, codewords)\n            # soft = (logit \/ temperature).softmax(-1)\n            # if hard:\n            #     hard = logit.argmax(-1)\n            #     hard = F.one_hot(hard, k)\n            #     sample = (hard - soft).detach() + soft\n            # else:\n            #     sample = soft\n            sample = F.gumbel_softmax(logit, temperature, hard)\n"}