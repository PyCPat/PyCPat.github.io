<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            batched_output_per_clip.append(x.squeeze(0))
            &#47&#47 input should be (seq_len, batch, input_size)

        output_seq = <a id="change">torch.stack(</a>batched_output_per_clip<a id="change">, dim=0)</a>.transpose_(0, 1)
        gru_output, h_n = self.rnn(output_seq.unsqueeze(1)[:, :, :config.GRU_TEMPORAL_WINDOW])

        &#47&#47 fc_out = self.fc(gru_output.flatten())
        &#47&#47
        &#47&#47 return output_seq, gru_output.squeeze(0), fc_out
        <a id="change">return </a>output_seq, gru_output

    def name(self):
        return "RhythmNet"</code></pre><h3>After Change</h3><pre><code class='java'>
            &#47&#47 input should be (seq_len, batch, input_size)

        &#47&#47 the features extracted from the backbone CNN are fed to a one-layer GRU structure.
        output_seq = <a id="change">torch.stack(</a>batched_output_per_clip<a id="change">, dim=0)</a>
        gru_output, h_n = self.rnn(output_seq.unsqueeze(1))
        gru_output = gru_output.squeeze(1)
        for i in range(gru_output.size(0)):
            hr = self.fc_resnet(gru_output[i, :])
            &#47&#47 hr = hr * 25.0
            hr_per_clip.append(hr)

        output_seq = torch.stack(hr_per_clip, dim=0).permute(1,0)
        &#47&#47 return output_seq, gru_output.squeeze(0), fc_out
        <a id="change">return </a>output_seq, output_seq.squeeze(0)[:6]

    def name(self):
        return "RhythmNet"</code></pre>