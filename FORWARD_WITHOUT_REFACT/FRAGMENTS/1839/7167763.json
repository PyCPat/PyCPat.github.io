{"BEFORE":"    def forward(self, x):\n        return x\n","AFTER":"        self,\n        proprio,\n        extero,\n        hiddens = None\n    ):\n        check_shape(proprio, 'b d', d = self.proprio_dim)\n        check_shape(extero, 'b n d', n = self.num_legs, d = self.extero_dim)\n\n        latent_extero = self.extero_encoder(extero)\n        latent_extero = rearrange(latent_extero, 'b ... -> b (...)')\n\n        # RNN\n\n        if not exists(hiddens):\n            hiddens = (None,) * len(self.gru_cells)\n\n        gru_input = torch.cat((proprio, latent_extero), dim = -1)\n\n        next_hiddens = []\n        for gru_cell, prev_hidden in zip(self.gru_cells, hiddens):\n            gru_input = gru_cell(gru_input, prev_hidden)\n            next_hiddens.append(gru_input)\n\n        gru_output = gru_input\n\n        # attention gating of exteroception\n\n        attention_gate = self.to_extero_attn_gate(gru_output)\n        gated_extero = latent_extero * attention_gate.sigmoid()\n\n        # belief state and add gated exteroception\n\n        belief_state = self.belief_state_encoder(gru_output)\n        belief_state = sum_with_zeropad(belief_state, gated_extero)\n\n        # to action logits\n\n        action_logits = self.to_action_logits(belief_state)\n        return action_logits, next_hiddens\n"}