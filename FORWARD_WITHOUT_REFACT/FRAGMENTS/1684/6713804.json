{"BEFORE":"        x = inputs[0]\n        enc_self_attn_mask = self.get_attn_mask(x, x)\n\n        enc_outputs = self.encoder(x, enc_self_attn_mask, \n                                   return_states=return_states)\n\n        enc_output = enc_outputs[0]\n        \n        if self.share_emb_out_proj == False:\n            W = self.W\n        else:\n            W = self.encoder.src_embedding.get_embedding().T\n\n        mask = x.view(-1).eq(self.MASK)\n        enc_output = enc_output.view(-1, self.d_model)[mask]\n","AFTER":"    def forward(self, inputs, return_states=False, targets=None, compute_loss=False):\n        \"\"\"\n        \"\"\"\n        x = inputs[0]\n\n        enc_self_attn_mask = self.get_attn_mask(x, x)\n\n        enc_outputs = self.encoder(x, enc_self_attn_mask, \n                                   return_states=return_states)\n\n        enc_output = enc_outputs[0]\n\n        if self.activation == \"relu\":\n            enc_output = F.relu(enc_output)\n        elif self.activation == \"gelu\":\n            enc_output = gelu_new(enc_output)\n        enc_output = self.norm(enc_output)\n            \n        if self.share_emb_out_proj == False:\n            W = self.W\n        else:\n            W = self.encoder.src_embedding.get_embedding().T\n        \n        logits = torch.matmul(enc_output, W)\n        \n        outputs = [logits]\n        \n        if return_states == True:\n            outputs = outputs + enc_outputs \n            \n        if compute_loss == True:\n            loss = self.loss_fn(outputs, targets)\n            outputs = [loss] + outputs\n            \n        return outputs\n"}