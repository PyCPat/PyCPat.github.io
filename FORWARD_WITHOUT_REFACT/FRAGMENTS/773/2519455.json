{"BEFORE":"    def forward(self, batch: Union[Dict, InputFeatures]) -> torch.Tensor:\n        r\"\"\" \n        This is the forward method of the generation in prompt-learning framework. \n        \n        Args:\n            batch (:obj:`Union[Dict, InputFeatures]`): The input features of batchified data sequences.\n        \n        Returns:\n            loss(:obj:torch.Tensor): The loss of the current generation procedure.\n        \"\"\"\n        outputs = self.prompt_model(batch)\n        logits = outputs.logits\n        logits, labels = self.shift_logits_and_labels(logits, batch)\n        batch_size, seq_len, vocab_size = logits.shape\n        loss = self.loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n        loss = loss.view(batch_size, -1).sum(dim=-1) #TODO support more objectives\n        loss = loss.mean()\n        return loss\n","AFTER":"    def forward(self, *args, **kwargs):\n        r\"\"\"In generation process, it will use the plm's forward function.\n        This is because, in the first step we will directly call the process_batch function to \n        generate initial input with the template, after that the all template\n        have been processed into the past_key_value,\n        then we can use the normal generation function. \n        In learning process, the forward is linked to ``_forward`` functions.\n        in which the loss will be calcated for all the postions in the same time. \n        \"\"\"\n        if self.in_generation_function:\n            return self.prompt_model.model.forward(*args, **kwargs)\n        else:\n            return self._forward(*args, **kwargs)\n\n    def _forward(self, batch: Union[Dict, InputFeatures]) -> torch.Tensor:\n"}