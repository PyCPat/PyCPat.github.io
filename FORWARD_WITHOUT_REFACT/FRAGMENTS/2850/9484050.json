{"BEFORE":"        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n","AFTER":"    def forward(self, x, mask = None, attn_bias = None, context = None, context_mask = None, tie_dim = None):\n        device, orig_shape, h, has_context = x.device, x.shape, self.heads, exists(context)\n\n        context = default(context, x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        i, j = q.shape[-2], k.shape[-2]\n\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n\n        # scale\n\n        q = q * self.scale\n\n        # query \/ key similarities\n\n        if exists(tie_dim):\n            # as in the paper, for the extra MSAs\n            # they average the queries along the rows of the MSAs\n            # they named this particular module MSAColumnGlobalAttention\n\n            q, k = map(lambda t: rearrange(t, '(b r) ... -> b r ...', r = tie_dim), (q, k))\n            q = q.mean(dim = 1)\n\n            dots = einsum('b h i d, b r h j d -> b r h i j', q, k)\n            dots = rearrange(dots, 'b r ... -> (b r) ...')\n        else:\n            dots = einsum('b h i d, b h j d -> b h i j', q, k)\n\n        # add attention bias, if supplied (for pairwise to msa attention communication)\n\n        if exists(attn_bias):\n"}