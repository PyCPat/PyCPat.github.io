<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

            &#47&#47 encoder: y_t, h_t -&gt; z_t
            enc_t = self.enc(torch.cat([phi_y_t, h[-1]], 1))
            <a id="change">enc_mean_t</a> = self.enc_mean(enc_t)
            enc_logvar_t = self.enc_logvar(enc_t)

            &#47&#47 prior: z_t ~ N(0,1) (for KLD loss)
            prior_mean_t<a id="change"> = </a><a id="change">torch.zeros_like(</a>enc_mean_t<a id="change">)</a>
            prior_logvar_t<a id="change"> = </a><a id="change">torch.zeros_like(</a>enc_mean_t<a id="change">)</a>

            &#47&#47 sampling and reparameterization: get a new z_t
            temp = tdist.Normal(enc_mean_t, enc_logvar_t.exp().sqrt())
            z_t = tdist.Normal.rsample(temp)</code></pre><h3>After Change</h3><pre><code class='java'>

    def forward(self, u, y):
        &#47&#47  batch size
        <a id="change">batch_size</a> = y.shape[0]
        seq_len = y.shape[2]
        &#47&#47 allocation
        loss = 0
        KLD_total = 0
        &#47&#47 initialization
        h = torch.zeros(self.n_layers, batch_size, self.h_dim, device=self.device)

        &#47&#47 constant so can be outside of loop
        &#47&#47 prior: z_t ~ N(0,1) (for KLD loss)
        prior_mean_t<a id="change"> = torch.zeros(</a><a id="change">[</a>batch_size, self.z_dim<a id="change"></a>]<a id="change">, device=self.device)</a>
        prior_logvar_t<a id="change"> = </a><a id="change">torch.zeros(</a><a id="change">[</a>batch_size, self.z_dim<a id="change"></a>]<a id="change">, device=self.device)</a>

        &#47&#47 for all time steps
        for t in range(seq_len):
            &#47&#47 feature extraction: y_t</code></pre>