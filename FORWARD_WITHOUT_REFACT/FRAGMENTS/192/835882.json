{"BEFORE":"        adj_knn = self.from_cache(adj_knn=adj_knn)\r\n        gamma = self.gamma\r\n        h = None\r\n\r\n        for ix, (layer, act) in enumerate(zip(self.layers, self.act_layers)):\r\n            s = torch.sigmoid(x @ self.scores[ix] + self.bias[ix])\r\n            Dk = x @ self.D_k[ix] + self.D_bias[ix]\r\n            x = s * act(layer(x, adj)) + (1 - s) * act(layer(x, adj_knn)) + gamma * Dk * act(layer(x))\r\n\r\n            if ix < len(self.layers) - 1:\r\n                x = self.dropout(x)\r\n\r\n            if ix == len(self.layers) - 2:\r\n                h = x.clone()\r\n\r\n        z = x\r\n        # self.ss = torch.cat((s_i.view(1, -1), s_o.view(1, -1), gamma * Dk_i.view(1, -1), gamma * Dk_o.view(1, -1)), dim=0)\r\n        return dict(z=z, h=h)\r\n","AFTER":"        if adj_knn is None:\r\n            adj_knn = self._adj_knn\r\n        else:\r\n            self._adj_knn = adj_knn\r\n\r\n        gamma = self.gamma\r\n        h = None\r\n\r\n        for ix, (layer, act) in enumerate(zip(self.layers, self.act_layers)):\r\n            s = torch.sigmoid(x @ self.scores[ix] + self.bias[ix])\r\n            Dk = x @ self.D_k[ix] + self.D_bias[ix]\r\n            x = s * act(layer(x, adj)) + (1 - s) * act(layer(x, adj_knn)) + gamma * Dk * act(layer(x))\r\n\r\n            if ix < len(self.layers) - 1:\r\n                x = self.dropout(x)\r\n\r\n            if ix == len(self.layers) - 2:\r\n                h = x.clone()\r\n\r\n        z = x\r\n        # self.ss = torch.cat((s_i.view(1, -1), s_o.view(1, -1), gamma * Dk_i.view(1, -1), gamma * Dk_o.view(1, -1)), dim=0)\r\n        if self.training:\r\n            return z, h\r\n        else:\r\n            return z\r\n\r\n    def cache_clear(self):\r\n"}