{"BEFORE":"        batch_size = subject_batch.shape[0]\n\n        # batch_size, num_input_channels, width, height\n        e1_embedded = self.entity_embeddings(subject_batch).view(-1, 1, self.img_height, self.img_width)\n        rel_embedded = self.relation_embeddings(relation_batch).view(-1, 1, self.img_height, self.img_width)\n\n        # batch_size, num_input_channels, 2*height, width\n        stacked_inputs = torch.cat([e1_embedded, rel_embedded], 2)\n\n        # batch_size, num_input_channels, 2*height, width\n        stacked_inputs = self.bn0(stacked_inputs)\n\n        # batch_size, num_input_channels, 2*height, width\n        x = self.inp_drop(stacked_inputs)\n        # (N,C_out,H_out,W_out)\n        x = self.conv1(x)\n\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = self.feature_map_drop(x)\n        # batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)\n        x = x.view(batch_size, -1)\n        x = self.fc(x)\n        x = self.hidden_drop(x)\n\n        if batch_size > 1:\n            x = self.bn2(x)\n        x = F.relu(x)\n\n        x = torch.mm(x, self.entity_embeddings.weight.transpose(1, 0))\n\n        # TODO: Why this?\n        x += self.b.expand_as(x)\n        pred = F.sigmoid(x)\n\n        return pred\n","AFTER":"        batch_size = batch.shape[0]\n\n        heads = batch[:, 0:1]\n        relations = batch[:, 1:2]\n        tails = batch[:, 2:3]\n\n        # batch_size, num_input_channels, width, height\n        heads_embs = self.entity_embeddings(heads).view(-1, 1, self.img_height, self.img_width)\n        relation_embs = self.relation_embeddings(relations).view(-1, 1, self.img_height, self.img_width)\n        tails_embs = self.entity_embeddings(tails).view(-1, self.embedding_dim)\n\n        # batch_size, num_input_channels, 2*height, width\n        stacked_inputs = torch.cat([heads_embs, relation_embs], 2)\n\n        # batch_size, num_input_channels, 2*height, width\n        stacked_inputs = self.bn0(stacked_inputs)\n\n        # batch_size, num_input_channels, 2*height, width\n        x = self.inp_drop(stacked_inputs)\n        # (N,C_out,H_out,W_out)\n        x = self.conv1(x)\n\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = self.feature_map_drop(x)\n        # batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)\n        x = x.view(batch_size, -1)\n        x = self.fc(x)\n        x = self.hidden_drop(x)\n\n        if batch_size > 1:\n            x = self.bn2(x)\n        x = F.relu(x)\n\n        scores = torch.sum(torch.mm(x, tails_embs.transpose(1, 0)), dim=1)\n\n        predictions = F.sigmoid(scores)\n        loss = self.compute_loss(predictions, labels)\n\n        return loss\n"}