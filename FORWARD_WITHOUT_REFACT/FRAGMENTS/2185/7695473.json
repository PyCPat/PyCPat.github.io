{"BEFORE":"        output = DeepSpeedSelfAttentionFunction.apply(\n            input,\n            input_mask,\n            head_mask,\n            layer_past,\n            get_present,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            output_attentions,\n            norm_w,\n            norm_b,\n            self.config,\n            self.attn_qkvw,\n            self.attn_qkvb,\n            self.num_attention_heads_per_partition,\n            self.norm_factor,\n            self.hidden_size_per_partition,\n            self.attn_ow,\n            self.attn_ob,\n            self.mp_group,\n            self.q_scales,\n            self.q_groups,\n            self.merge_count,\n            self.qkv_merging,\n            self.score_context_func,\n            alibi)\n\n        return output\n","AFTER":"        if not self.config.pre_layer_norm:\n            qkv_out = self.linear_func(input=input,\n                                       weight=self.attn_qkvw,\n                                       bias=self.attn_qkvb,\n                                       add_bias=self.attn_qkvb is not None,\n                                       do_flash_attn=False,\n                                       num_heads=self.num_attention_heads_per_partition)\n        else:\n            qkv_out = self.qkv_func(\n                input=input,\n                weight=self.attn_qkvw,\n                bias=(self.attn_qkvb if self.attn_qkvb is not None else norm_b),\n                gamma=norm_w,\n                beta=norm_b,\n                add_bias=(self.attn_qkvb is not None),\n                num_layers=DeepSpeedSelfAttention.num_layers)\n\n        context_layer, key_layer, value_layer = self.compute_attention(\n            qkv_out=qkv_out,\n            input_mask=input_mask,\n            layer_past=layer_past,\n            alibi=alibi)\n\n        output = self.vector_matmul_func(input=context_layer, weight=self.attn_ow)\n\n        inp_norm = qkv_out[-1]\n\n        if self.config.mlp_after_attn and self.mp_group is not None and dist.get_world_size(\n                group=self.mp_group) > 1:\n            dist.all_reduce(output, group=self.mp_group)\n\n        return (output, key_layer, value_layer, context_layer, inp_norm)\n"}