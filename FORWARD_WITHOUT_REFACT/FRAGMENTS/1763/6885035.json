{"BEFORE":"        x_in = self.align(x)[:, :, self.Kt - 1:, :]\r\n        x_causal_conv = self.causal_conv(x)\r\n\r\n        if self.enable_gated_act_func == True:\r\n            x_p = x_causal_conv[:, : self.c_out, :, :]\r\n            x_q = x_causal_conv[:, -self.c_out:, :, :]\r\n\r\n            # Temporal Convolution Layer (GLU)\r\n            if self.act_func == 'glu':\r\n                # GLU was first purposed in\r\n                # Language Modeling with Gated Convolutional Networks\r\n                # https:\/\/arxiv.org\/abs\/1612.08083\r\n                # Input tensor X was split by a certain dimension into tensor X_a and X_b\r\n                # In original paper, GLU as Linear(X_a) ⊙ Sigmoid(Linear(X_b))\r\n                # However, in PyTorch, GLU as X_a ⊙ Sigmoid(X_b)\r\n                # https:\/\/pytorch.org\/docs\/master\/nn.functional.html#torch.nn.functional.glu\r\n                # Because in original paper, the representation of GLU and GTU is ambiguous\r\n                # So, it is arguable which one version is correct\r\n\r\n                # (x_p + x_in) ⊙ Sigmoid(x_q)\r\n                x_glu = torch.mul((x_p + x_in), self.sigmoid(x_q))\r\n                x_tc_out = x_glu\r\n\r\n            # Temporal Convolution Layer (GTU)\r\n            elif self.act_func == 'gtu':\r\n                # Tanh(x_p + x_in) ⊙ Sigmoid(x_q)\r\n                x_gtu = torch.mul(self.tanh(x_p + x_in), self.sigmoid(x_q))\r\n                x_tc_out = x_gtu\r\n\r\n            else:\r\n                raise ValueError(f'ERROR: activation function {self.act_func} is not defined.')\r\n\r\n        else:\r\n            # Temporal Convolution Layer (Linear)\r\n            if self.act_func == 'linear':\r\n                x_linear = self.linear(x_causal_conv + x_in)\r\n                x_tc_out = x_linear\r\n            \r\n            # Temporal Convolution Layer (Sigmoid)\r\n            elif self.act_func == 'sigmoid':\r\n                x_sigmoid = self.sigmoid(x_causal_conv + x_in)\r\n                x_tc_out = x_sigmoid\r\n\r\n            # Temporal Convolution Layer (Tanh)\r\n            elif self.act_func == 'tanh':\r\n                x_tanh = self.tanh(x_causal_conv + x_in)\r\n                x_tc_out = x_tanh\r\n\r\n            # Temporal Convolution Layer (ReLU)\r\n            elif self.act_func == 'relu':\r\n                x_relu = self.relu(x_causal_conv + x_in)\r\n                x_tc_out = x_relu\r\n        \r\n            # Temporal Convolution Layer (LeakyReLU)\r\n            elif self.act_func == 'leaky_relu':\r\n                x_leaky_relu = self.leaky_relu(x_causal_conv + x_in)\r\n                x_tc_out = x_leaky_relu\r\n\r\n            # Temporal Convolution Layer (ELU)\r\n            elif self.act_func == 'elu':\r\n                x_elu = self.elu(x_causal_conv + x_in)\r\n                x_tc_out = x_elu\r\n\r\n            else:\r\n                raise ValueError(f'ERROR: activation function {self.act_func} is not defined.')\r\n        \r\n        return x_tc_out\r\n","AFTER":"        if self.act_func == 'glu' or self.act_func == 'gtu':\r\n            x_p = x_causal_conv[:, : self.c_out, :, :]\r\n            x_q = x_causal_conv[:, -self.c_out:, :, :]\r\n\r\n            if self.act_func == 'glu':\r\n                # GLU was first purposed in\r\n                # *Language Modeling with Gated Convolutional Networks*.\r\n                # URL: https:\/\/arxiv.org\/abs\/1612.08083\r\n                # Input tensor X is split by a certain dimension into tensor X_a and X_b.\r\n                # In the original paper, GLU is defined as Linear(X_a) ⊙ Sigmoid(Linear(X_b)).\r\n                # However, in PyTorch, GLU is defined as X_a ⊙ Sigmoid(X_b).\r\n                # URL: https:\/\/pytorch.org\/docs\/master\/nn.functional.html#torch.nn.functional.glu\r\n                # Because in original paper, the representation of GLU and GTU is ambiguous.\r\n                # So, it is arguable which one version is correct.\r\n\r\n                # (x_p + x_in) ⊙ Sigmoid(x_q)\r\n                x = torch.mul((x_p + x_in), self.sigmoid(x_q))\r\n\r\n            else:\r\n                # Tanh(x_p + x_in) ⊙ Sigmoid(x_q)\r\n                x = torch.mul(self.tanh(x_p + x_in), self.sigmoid(x_q))\r\n\r\n        elif self.act_func == 'relu':\r\n            x = self.relu(x_causal_conv + x_in)\r\n        \r\n        elif self.act_func == 'leaky_relu':\r\n            x = self.leaky_relu(x_causal_conv + x_in)\r\n\r\n        elif self.act_func == 'silu':\r\n            x = self.silu(x_causal_conv + x_in)\r\n        \r\n        else:\r\n            raise NotImplementedError(f'ERROR: The activation function {self.act_func} is not implemented.')\r\n        \r\n        return x\r\n"}