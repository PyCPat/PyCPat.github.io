{"BEFORE":"        head = self.dropout(self.activation(self.head(x)))\n        dependent = self.dropout(self.activation(self.dependent(x)))\n\n        # Compute biaffine attention matrix. This computes from the hidden\n        # representations of the shape [batch_size, seq_len, hidden_size] the\n        # attention matrices [batch_size, seq_len, seq_len].\n        logits = self.bilinear(head, dependent).squeeze(-1)\n\n        # Mask out head candidates that are padding time steps. The logits mask\n        # has shape [batch_size, seq_len], we reshape it to [batch_size, 1,\n        # seq_len] to mask out the head predictions.\n        logits += logits_mask.unsqueeze(1)\n\n        if self.training:\n            # Compute head probability distribution.\n            logits = logits.softmax(-1)\n\n        return logits\n","AFTER":"        logits_mask = logits_mask.unsqueeze(1)\n\n        # Create representations of tokens as heads and dependents.\n        head_arc = self.dropout(self.activation(self.head_arc(x)))\n        dependent_arc = self.dropout(self.activation(self.dependent_arc(x)))\n        head_label = self.dropout(self.activation(self.head_label(x)))\n        dependent_label = self.dropout(self.activation(self.dependent_label(x)))\n\n        # Compute biaffine attention matrix. This computes from the hidden\n        # representations of the shape [batch_size, seq_len, hidden_size] the\n        # attention matrices [batch_size, seq_len, seq_len].\n        logits_arc = self.bilinear_arc(head_arc, dependent_arc).squeeze(-1)\n\n        logits_label = self.bilinear_label(head_label, dependent_label)\n\n        # Mask out head candidates that are padding time steps. The logits mask\n        # has shape [batch_size, seq_len], we reshape it to [batch_size, 1,\n        # seq_len] to mask out the head predictions.\n        logits_arc += logits_mask\n        logits_label += logits_mask.unsqueeze(-1)\n\n        if self.training:\n            # Compute head probability distribution.\n            logits_arc = logits_arc.softmax(-1)\n            logits_label = logits_label.softmax(-1)\n\n        return logits_arc, logits_label\n"}