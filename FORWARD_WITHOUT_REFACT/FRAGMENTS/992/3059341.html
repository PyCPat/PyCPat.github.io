<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        return nn.Sequential(*layers)

    def forward(self, x):  &#47&#47 224x224
        features<a id="change"> = </a><a id="change">[]</a>
        x = self.conv1(x)  &#47&#47 112x112
        features.append(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)  &#47&#47 56x56 ignore

        x = self.layer1(x)  &#47&#47 56x56
        features.append(x)
        x = self.layer2(x)  &#47&#47 28x28
        features.append(x)
        x = self.layer3(x)  &#47&#47 14x14 ignore (maybe not)
        <a id="change">features.append(</a>x<a id="change">)</a>
        x = self.layer4(x)  &#47&#47 7x7
        features.append(x)

        if not self.include_top:
            <a id="change">return </a>features

        x = self.avgpool(x)  &#47&#47 1x1
        features.append(x)</code></pre><h3>After Change</h3><pre><code class='java'>
        Normalizing the features and applying spatial resolution was taken from LPIPS and wasn&quott mentioned in the paper.
        
        images = torch.concat([x, x_rec], dim=0)  &#47&#47 batch
        features<a id="change"> = </a>self._forward(images)
        features = [f.chunk(2) for f in features]
        &#47&#47 diffs = [a * torch.abs(p[0] - p[1]).sum() for a, p in zip(self.alphas, features)]
        diffs = [a * torch.abs(p[0] - <a id="change">p[1]</a>).mean() for a, p in zip(self.alphas, features)]
        &#47&#47 diffs = [a*torch.abs(self.norm_tensor(tf) - self.norm_tensor(rf)) for a, tf, rf in zip(self.alphas, true_features, rec_features)]

        &#47&#47 diffs = [a * torch.mean(torch.abs(tf - rf)) for a, tf, rf in zip(self.alphas, features)]</code></pre>