{"BEFORE":"        num_clusters = self.means.shape[1]\n        window_size = min(wsz, t)\n\n        k_routing = torch.einsum('bhtd,hdr->bhtr', qk, self.router) if self.router is not None else qk\n        k_routing = F.normalize(qk, dim=-1)\n\n        with torch.no_grad():\n            means, buckets, dists, se = kmeans(k_routing, self.means, training=self.training, init=not self.initted)\n            indices = distribution(dists, window_size)\n\n        routed_means = batched_index_select(expand_dim(self.means, 0, b), buckets)\n        commitment_loss = F.mse_loss(k_routing, routed_means) * self.commitment\n\n        if self.training:\n            self.new_means.copy_(means)\n\n        indices = indices.contiguous().view(*indices.size()[:2], -1)\n        \n        qk = batched_index_select(qk, indices)\n        v = batched_index_select(v, indices)\n\n        qk, v = map(lambda x: x.reshape(b, h, num_clusters, window_size, d), (qk, v))\n\n        q = qk\n        k = F.normalize(qk, 2, dim=-1).type(qk.dtype)\n\n        dots = torch.einsum('bhnid,bhnjd->bhnij', q, k) * (d ** -0.5)\n        dots = dots + self.rel_pos(q)\n\n        mask_value = max_neg_value(dots)\n\n        if input_mask is not None:\n            qk_mask = expand_dim(input_mask, 1, h).gather(2, indices)\n            qk_mask = qk_mask.reshape(b, h, num_clusters, window_size)\n","AFTER":"        b, h, t, d, wsz, num_clusters, device, dtype = *qk.shape, self.window_size, self.num_clusters, qk.device, qk.dtype\n        out = torch.zeros_like(qk, dtype=dtype)\n\n        wsz = min(wsz, t)\n\n        k_routing = torch.einsum('bhtd,hdr->bhtr', qk, self.router) if self.router is not None else qk\n        k_routing = F.normalize(qk, dim=-1)\n\n        indices, commitment_loss = self.kmeans(k_routing, wsz)\n        \n        qk = batched_index_select(qk, indices)\n        v = batched_index_select(v, indices)\n\n        qk, v = map(lambda x: x.reshape(b, h, num_clusters, wsz, d), (qk, v))\n\n        q = qk\n        k = F.normalize(qk, 2, dim=-1).type(qk.dtype)\n\n        dots = torch.einsum('bhnid,bhnjd->bhnij', q, k) * (d ** -0.5)\n        dots = dots + self.rel_pos(q)\n\n        mask_value = max_neg_value(dots)\n\n        if input_mask is not None:\n            qk_mask = expand_dim(input_mask, 1, h).gather(2, indices)\n            qk_mask = qk_mask.reshape(b, h, num_clusters, wsz)\n            mask = qk_mask[:, :, :, :, None] * qk_mask[:, :, :, None, :]\n            dots.masked_fill_(~mask, mask_value)\n            del mask\n\n        if self.causal:\n            mask = torch.ones(wsz, wsz, device=device).byte().triu_(1).bool()\n            dots.masked_fill_(mask, mask_value)\n            del mask\n\n        mask = torch.eye(wsz, device=dots.device).bool()\n        dots.masked_fill_(mask, TOKEN_SELF_ATTN_VALUE)\n        del mask\n\n        dots = dots.softmax(dim=-1)\n        dots = self.dropout(dots)\n\n        bo = torch.einsum('bhcij,bhcjd->bhcid', dots, v)\n        so = torch.reshape(bo, (b, h, -1, bo.shape[-1])).type(dtype)\n"}