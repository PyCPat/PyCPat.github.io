{"BEFORE":"        if d_spectral_norm:\n            self.conv0 = snconv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n            self.conv1 = snconv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=4, stride=2, padding=1)\n        else:\n            self.conv0 = conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n            self.conv1 = conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=4, stride=2, padding=1)\n\n            self.bn0 = batchnorm_2d(in_features=out_channels)\n            self.bn1 = batchnorm_2d(in_features=out_channels)\n\n        if activation_fn == \"ReLU\":\n            self.activation = nn.ReLU(inplace=True)\n        elif activation_fn == \"Leaky_ReLU\":\n            self.activation = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n        elif activation_fn == \"ELU\":\n            self.activation = nn.ELU(alpha=1.0, inplace=True)\n        elif activation_fn == \"GELU\":\n            self.activation = nn.GELU()\n        else:\n            raise NotImplementedError\n\n    def forward(self, x):\n","AFTER":"        self.conv0 = MODULES.d_conv2d(in_channels=in_channels,\n                                      out_channels=out_channels,\n                                      kernel_size=3,\n                                      stride=1,\n                                      padding=1)\n\n        self.conv1 = MODULES.d_conv2d(in_channels=out_channels,\n                                      out_channels=out_channels,\n                                      kernel_size=4,\n                                      stride=2,\n                                      padding=1)\n\n        if not apply_d_sn:\n            self.bn0 = MODULES.d_bn(in_features=out_channels)\n            self.bn1 = MODULES.d_bn(in_features=out_channels)\n\n        self.activation = MODULES.d_act_fn\n"}