{"BEFORE":"            q1 = self.relative_positions_encoding(q1)\r\n            k1 = self.relative_positions_encoding(k1)\r\n            position_ids = torch.cat((torch.zeros(q1.shape[2]-1, dtype=torch.long, device=q1.device),\r\n                                     torch.arange(1, dtype=torch.long, device=q1.device) + 1))\r\n            q2 = self.relative_positions_encoding(q2, position_ids)\r\n            k2 = self.relative_positions_encoding(k2, position_ids)\r\n","AFTER":"            q2 = self.relative_positions_encoding(q2, model_kwargs['position_ids'])\r\n            k2 = self.relative_positions_encoding(k2, model_kwargs['position_ids'])\r\n            query_layer = torch.concat([q1, q2], dim=(q1.ndim - 1))\r\n            key_layer = torch.concat([k1, k2], dim=(k1.ndim - 1))\r\n        elif self.p_bias == 'rotary' and not self.position_encoding_2d:  # 原rotary逻辑\r\n            query_layer = self.relative_positions_encoding(query_layer, model_kwargs['position_ids'])\r\n            key_layer = self.relative_positions_encoding(key_layer, model_kwargs['position_ids'])\r\n"}