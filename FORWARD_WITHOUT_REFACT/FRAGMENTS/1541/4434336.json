{"BEFORE":"        l2QLoss = list()\n        l1QLoss = list()\n        if not e2e:\n            for latent, q in zip(latents, quantizeds):\n                l2QLoss.append(F.mse_loss(latent.detach(), q, reduction='none').mean(axis=(1, 2, 3)))\n                l1QLoss.append(F.l1_loss(latent.detach(), q, reduction='none').mean(axis=(1, 2, 3)))\n                l2QLoss.append(0.1 * F.mse_loss(latent, q.detach(), reduction='none').mean(axis=(1, 2, 3)))\n                l1QLoss.append(0.1 * F.l1_loss(latent, q.detach(), reduction='none').mean(axis=(1, 2, 3)))\n\n        l1QLoss = sum(l1QLoss)\n        l2QLoss = sum(l2QLoss)\n\n        regs = list()\n        if logits is not None:\n            for logit in logits:\n                # N, H, W, K -> N, HW, K\n                batchWiseLogit = logit.reshape(len(logit), -1, logit.shape[-1])\n\n                # [n, k]\n                # summedProb = batchWiseLogit.mean(1).sigmoid()\n\n                # target = torch.ones_like(summedProb) \/ 2.0\n                # [n, ]\n                # reg = F.binary_cross_entropy(summedProb, target, reduction='none').sum(-1)\n\n                # var = batchWiseLogit.var(1).sum(-1)\n\n                # [n, k] -> [n, ]\n                # diversity = torch.minimum(var, torch.ones_like(var))\n                # reg -= diversity\n\n                diversity = batchWiseLogit.var(1).sum(-1).sigmoid()\n\n                summedProb = batchWiseLogit.sum(1)\n                posterior = OneHotCategorical(logits=summedProb)\n                prior = OneHotCategorical(probs=torch.ones_like(summedProb) \/ summedProb.shape[-1])\n                reg = torch.distributions.kl_divergence(posterior, prior) \/ diversity\n","AFTER":"                batchWiseLogit = logit.reshape(len(logit), -1, logit.shape[-1])\n\n                # [n, k]\n                # summedProb = batchWiseLogit.mean(1).sigmoid()\n\n                # target = torch.ones_like(summedProb) \/ 2.0\n                # [n, ]\n                # reg = F.binary_cross_entropy(summedProb, target, reduction='none').sum(-1)\n\n                # var = batchWiseLogit.var(1).sum(-1)\n\n                # [n, k] -> [n, ]\n                # diversity = torch.minimum(var, torch.ones_like(var))\n                # reg -= diversity\n\n                diversity = batchWiseLogit.std(1).mean(-1).sigmoid()\n\n                # summedProb = batchWiseLogit.sum(1)\n                # posterior = OneHotCategorical(logits=summedProb)\n                # prior = OneHotCategorical(probs=torch.ones_like(summedProb) \/ summedProb.shape[-1])\n                # reg = torch.distributions.kl_divergence(posterior, prior) \/ diversity\n                reg = compute_penalties(batchWiseLogit, allowed_entropy=0.1, individual_entropy_coeff=cv, allowed_js=4.0, js_coeff=cv, cv_coeff=cv, eps=Consts.Eps)\n                reg = reg \/ diversity\n"}