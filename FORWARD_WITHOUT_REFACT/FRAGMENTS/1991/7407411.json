{"BEFORE":"        x = einsum('... d, h d -> ... h d', x, self.expansion)\n\n        # weights derived from alphas (learned exponential smoothing decay rate)\n\n        alphas = self.alphas.sigmoid()\n        dampen_factors = self.dampen_factors.sigmoid()\n\n        reversed_powers = torch.arange(seq_len - 1, -1, -1, device = device)\n        K = alphas * (((1 - alphas) * dampen_factors) ** rearrange(reversed_powers, '... l -> ... l 1'))\n\n        # conv1d fft O(nlog(n))\n\n        out = conv1d_fft(x, K, dim = -3, weight_dim = -2)\n\n        # combine heads and out\n\n        return einsum('... h d, h d -> ... d', out, self.reduction)\n","AFTER":"        x = einsum('... d, h d -> ... h d', x, self.expansion)\n\n        # weights derived from alphas (learned exponential smoothing decay rate)\n\n        def apply_learned_ema_with_damping(x, alphas, dampen_factors):\n            alphas = alphas.sigmoid()\n            dampen_factors = dampen_factors.sigmoid()\n\n            reversed_powers = torch.arange(seq_len - 1, -1, -1, device = device)\n            K = alphas * (((1 - alphas) * dampen_factors) ** rearrange(reversed_powers, '... l -> ... l 1'))\n\n            # conv1d fft O(nlog(n))\n\n            return conv1d_fft(x, K, dim = -3, weight_dim = -2)\n\n        x = apply_learned_ema_with_damping(x, self.alphas, self.dampen_factors)\n\n        if self.bidirectional:\n            x = torch.flip(x, dims = (1,))\n            x = apply_learned_ema_with_damping(x, self.reverse_alphas, self.reverse_dampen_factors)\n            x = torch.flip(x, dims = (1,))\n\n        # combine heads and out\n\n        return einsum('... h d, h d -> ... d', x, self.reduction)\n"}