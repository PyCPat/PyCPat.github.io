{"BEFORE":"        k = self.k_proj(k).reshape(B, N_kv, self.num_heads, C \/\/ self.num_heads)\n        v = self.v_proj(v).reshape(B, N_kv, self.num_heads, C \/\/ self.num_heads)\n        q = self.k_proj(q).reshape(B, N_q, self.num_heads, C \/\/ self.num_heads)\n        q = q * self.scale\n\n        if mask is not None:\n            attn = torch.matmul(q.permute(0, 2, 1, 3), k.permute(0, 2, 3, 1)).transpose(0, 1)\n            mask[mask > 0] = - torch.inf\n            attn = (attn + mask).transpose(0, 1)\n        else:\n            attn = torch.matmul(q.permute(0, 2, 1, 3), k.permute(0, 2, 3, 1))\n\n        attn = self.softmax(attn)\n","AFTER":"        attn = torch.matmul(q.permute(0, 2, 1, 3), k.permute(0, 2, 3, 1))\n\n        if self.pos_bias is not None:\n            attn += self.pos_bias\n\n        if mask is not None:\n            mask[mask > 0] = - torch.inf\n            attn += mask\n\n        attn = self.softmax(attn)\n"}