{"BEFORE":"            self.precomputed_enc_h = self.precomputed_enc_h.view(\n                B, L, self.nhead, self.attn_dim\n            ).permute(0, 2, 1, 3)\n            self.precomputed_enc_h = self.precomputed_enc_h.reshape(\n                B * self.nhead, L, self.attn_dim\n            )\n            self.mask = (\n                length_to_mask(\n                    enc_len,\n                    max_len=enc_states.size(1),\n                    device=enc_states.device,\n                )\n                .unsqueeze(1)\n                .repeat(1, self.nhead, 1)\n            )\n\n            # multiply mask by 1\/Ln for each row\n            self.prev_attn = self.mask * (1 \/ enc_len.float()).unsqueeze(\n                1\n            ).unsqueeze(2)\n\n        # compute location-aware features\n        # [B, nhead, L] -> [B, C, L]\n        attn_conv = self.conv_loc(self.prev_attn)\n        # [B, C, L] -> [B, L, C] -> [B, L, F]\n        attn_conv = self.mlp_loc(attn_conv.transpose(1, 2))\n        # [B, L, F] -> [B*nhead, L, F]\n        attn_conv = (\n            attn_conv.unsqueeze(1)\n            .repeat(1, self.nhead, 1, 1)\n            .view(-1, L, self.attn_dim)\n        )\n        # Location-aware attention\n        attn = self.mlp_attn(\n            torch.tanh(self.precomputed_enc_h + dec_h + attn_conv)\n        ).squeeze(-1)\n\n        # mask the padded frames\n        attn = attn.masked_fill(self.mask.view(-1, L) == 0, -np.inf)\n        attn = self.softmax(attn * self.scaling)\n\n        # compute context vectors\n        # [B*nhead, 1, L] X [B*nhead, L, F]\n        context = torch.bmm(\n            attn.unsqueeze(1), enc_states.repeat(self.nhead, 1, 1)\n        ).squeeze(1)\n        context = context.view(B, self.nhead * self.enc_dim)\n        context = self.mlp_out(context)\n\n        # set prev_attn to current attn for the next timestep\n        attn = attn.view(B, self.nhead, L)\n        self.prev_attn = attn.detach()\n","AFTER":"            self.key = self.key.view(B, L, self.nhead, self.attn_dim).permute(\n                0, 2, 1, 3\n            )\n            self.key = self.key.contiguous().view(\n                B * self.nhead, L, self.attn_dim\n            )\n            self.mask = (\n                length_to_mask(\n                    enc_len,\n                    max_len=enc_states.size(1),\n                    device=enc_states.device,\n                )\n                .unsqueeze(1)\n                .repeat(1, self.nhead, 1)\n            )\n\n            # value: [B, T, enc_dim] -> [B, T, nhead*enc_dim] -> [B*nhead, T, enc_dim]\n            self.value = torch.tanh(self.mlp_v(enc_states))\n            self.value = self.value.view(\n                B, L, self.nhead, self.enc_dim\n            ).permute(0, 2, 1, 3)\n            self.value = self.value.contiguous().view(\n                B * self.nhead, L, self.enc_dim\n            )\n\n            # multiply mask by 1\/Ln for each row\n            self.prev_attn = self.mask * (1 \/ enc_len.float()).unsqueeze(\n                1\n            ).unsqueeze(2)\n\n        # compute location-aware features\n        # [B, nhead, L] -> [B, C, L]\n        attn_conv = torch.tanh(self.conv_loc(self.prev_attn))\n        # [B, C, L] -> [B, L, C] -> [B, L, F]\n        attn_conv = self.mlp_loc(attn_conv.transpose(1, 2))\n        # [B, L, F] -> [B*nhead, L, F]\n        attn_conv = (\n            attn_conv.unsqueeze(1)\n            .repeat(1, self.nhead, 1, 1)\n            .view(-1, L, self.attn_dim)\n        )\n        # Location-aware attention\n        attn = self.mlp_attn(torch.tanh(self.key + query + attn_conv)).squeeze(\n            -1\n        )\n\n        # mask the padded frames\n        attn = attn.masked_fill(self.mask.view(-1, L) == 0, -np.inf)\n        attn = self.softmax(attn * self.scaling)\n\n        # set prev_attn to current attn for the next timestep\n        self.prev_attn = attn.detach()\n        self.prev_attn = self.prev_attn.view(B, self.nhead, L)\n\n        # compute context vectors\n        # [B*nhead, 1, L] X [B*nhead, L, F]\n        context = torch.bmm(attn.unsqueeze(1), self.value).squeeze(1)\n"}