{"BEFORE":"        self.dropout = Dropout(dropout)\n        self.pos_embedding = PositionEmbedding(trg_max_len, d_model)\n        \n        if share_layer_params == False:\n            self.layers = nn.ModuleList([\n                    DecoderLayer(n_heads, d_model, d_ff, d_qk, d_v,\n                                 dropout=dropout,\n                                 use_pre_norm=use_pre_norm,\n                                 activation=activation)\n                    for _ in range(n_layers)])\n        else:\n            layers = []\n            for i in range(n_layers):\n                if i % n_share_across_layers == 0:\n                    layer = DecoderLayer(n_heads, d_model, d_ff, d_qk, d_v,\n                                         dropout=dropout,\n                                         use_pre_norm=use_pre_norm,\n                                         activation=activation)\n                    layers.append(layer)\n            self.layers = nn.ModuleList(layers)\n    \n        self.share_emb_out_proj = share_emb_out_proj\n        if share_emb_out_proj == False: \n            self.W = nn.Parameter(torch.Tensor(trg_vocab_size, d_model))\n        self.b = nn.Parameter(torch.Tensor(trg_vocab_size))\n","AFTER":"                 attn_dropout=0,\n                 emb_dropout=0,\n                 ln_eps=1e-5,\n                 embedding_size=None,\n                 share_layer_params=False, \n                 n_share_across_layers=1,\n                 share_src_trg_emb=False, \n                 share_emb_out_proj=False,\n                 use_pre_norm=True, \n                 activation=\"relu\", \n                 scale_embedding=False,\n                 norm_before_pred=False,\n                 norm_after_embedding=False,\n                 pos_need_train=False,\n                 use_proj_bias=False):\n        \"\"\"\n        \"\"\"\n        super(Decoder, self).__init__()\n        self.trg_vocab_size = trg_vocab_size\n        self.n_heads = n_heads\n        self.d_model = d_model\n        self.n_layers = n_layers\n        self.share_layer_params = share_layer_params\n        self.n_share_across_layers = n_share_across_layers\n        self.scale_embedding = scale_embedding\n        self.norm_before_pred = norm_before_pred\n        self.norm_after_embedding = norm_after_embedding\n        \n        if share_src_trg_emb == False:\n            self.trg_embedding = Embedding(trg_vocab_size, \n                                           d_model, \n                                           scale_embedding)\n            \n        self.emb_dropout = Dropout(emb_dropout)\n        self.pos_embedding = PositionEmbedding(trg_max_len, \n                                               d_model, \n                                               pos_need_train)\n        \n        if self.norm_after_embedding == True:\n            self.norm_emb = LayerNorm(self.d_model)\n        \n        if share_layer_params == False:\n            self.layers = nn.ModuleList([\n                    DecoderLayer(n_heads, \n                                 d_model, \n                                 d_ff, \n                                 d_qk, \n                                 d_v,\n                                 dropout=dropout,\n                                 attn_dropout=attn_dropout,\n                                 ln_eps=ln_eps,\n                                 use_pre_norm=use_pre_norm,\n                                 activation=activation)\n                    for _ in range(n_layers)])\n        else:\n            layers = []\n            for i in range(n_layers):\n                if i % n_share_across_layers == 0:\n                    layer = DecoderLayer(n_heads,\n                                         d_model, \n                                         d_ff, \n                                         d_qk, \n                                         d_v,\n                                         dropout=dropout,\n                                         attn_dropout=attn_dropout,\n                                         ln_eps=ln_eps,\n                                         use_pre_norm=use_pre_norm,\n                                         activation=activation)\n                    layers.append(layer)\n            self.layers = nn.ModuleList(layers)\n    \n        self.share_emb_out_proj = share_emb_out_proj\n        if share_emb_out_proj == False: \n            self.W = nn.Parameter(torch.Tensor(trg_vocab_size, d_model))\n        self.use_proj_bias = use_proj_bias\n        if use_proj_bias == True:\n            self.b = nn.Parameter(torch.Tensor(trg_vocab_size))\n\n        if self.norm_before_pred == True:\n            self.norm = LayerNorm(d_model)\n        \n        self.reset_parameters()\n"}