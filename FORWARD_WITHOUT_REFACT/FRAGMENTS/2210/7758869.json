{"BEFORE":"        quantizeds = list()\n        codes = list()\n        logits = list()\n        # probability = mixin \/ (mixin + 1.0)\n        # rolloutDistribution = Bernoulli(probs=torch.tensor(probability).to(latents[0].device))\n        for xRaw, prob, squeeze, codebook, k in zip(latents, self._prob, self._squeeze, self._codebook, self._k):\n            n, c, h, w = xRaw.shape\n            # [c, k]\n            codewords = codebook.weight\n            # [n, c, h, w] -> [h, w, n, c]\n            encoderIn = xRaw.permute(2, 3, 0, 1)\n            # [h, w, n, c] -> [h*w, n, c]\n            encoderIn = self._position(encoderIn).reshape(-1, n, c)\n            # [h*w, n, c]\n            # x = self._encoder(posisted)\n            x = self._encoder(encoderIn)\n            # x = self._dePosition(x.reshape(h, w, n, c)).reshape(-1, n, c)\n            # x = encoderIn\n            # [h*w, n, k]\n            # logit = prob(x, h, w)\n            logit = torch.matmul(x, codewords)\n            # soft = (logit \/ temperature).softmax(-1)\n            # if hard:\n            #     hard = logit.argmax(-1)\n            #     hard = F.one_hot(hard, k)\n            #     sample = (hard - soft).detach() + soft\n            # else:\n            #     sample = soft\n            sample = F.gumbel_softmax(logit, temperature, hard)\n            # sample = logit\n            # [h*w, N, c] <- [h*w, N, k] @ [k, C]\n            quantized = codebook(sample)\n\n            # quantized = sample\n\n            # normalize\n            # quantized \/= (k - 0.5) \/ (2 * k - 2)\n            # quantized -= 0.5 \/ (k - 1)\n            # [h*w, n, c]\n            # quantized = squeeze(sample, h, w)\n            posistedQuantized = self._position(quantized.reshape(h, w, n, c)).reshape(-1, n, c)\n\n            # mixed = (mixin * encoderIn \/ (mixin + 1)) + (quantized \/ (mixin + 1))\n\n            # mask = rolloutDistribution.sample((h*w, n, 1)).bool()\n\n            # mixed = mask * encoderIn.detach() + torch.logical_not(mask) * quantized\n            # [h*w, n, c] -> [n, c, h*w] -> [n, c, h, w]\n            deTransformed = self._decoder(posistedQuantized, posistedQuantized).reshape(h, w, n, c).permute(2, 3, 0, 1)\n            # deTransformed = quantized.permute(1, 2, 0).reshape(n, c, h, w)\n            # deTransformed = self._dePosition(deTransformed.reshape(h, w, n, c)).permute(2, 3, 0, 1)\n            # [n, c, h, w]\n            quantizeds.append(deTransformed)\n            codes.append(sample.argmax(-1).permute(1, 0).reshape(n, h, w))\n            logits.append(logit.reshape(n, h, w, k))\n        return quantizeds, codes, logits\n","AFTER":"        quantizeds = list()\n        codes = list()\n        logits = list()\n        allCodewords = list()\n        # probability = mixin \/ (mixin + 1.0)\n        # rolloutDistribution = Bernoulli(probs=torch.tensor(probability).to(latents[0].device))\n        for xRaw, prob, squeeze, codebook, k in zip(latents, self._prob, self._squeeze, self._codebook, self._k):\n            n, c, h, w = xRaw.shape\n            # [c, k]\n            codewords = codebook.weight\n            # [n, c, h, w] -> [h, w, n, c]\n            encoderIn = xRaw.permute(2, 3, 0, 1)\n            # [h, w, n, c] -> [h*w, n, c]\n            encoderIn = self._position(encoderIn).reshape(-1, n, c)\n            # [h*w, n, c]\n            # x = self._encoder(posisted)\n            x = self._encoder(encoderIn)\n            # x = self._dePosition(x.reshape(h, w, n, c)).reshape(-1, n, c)\n            # x = encoderIn\n            # [h*w, n, k]\n            # logit = prob(x, h, w)\n            # logit = torch.matmul(x \/ (x ** 2).sum(-1, keepdim=True), codewords \/ (codewords ** 2).sum(0, keepdim=True))\n            logit = x @ codewords\n            soft = (logit \/ temperature).softmax(-1)\n            if hard:\n                hard = logit.argmax(-1)\n                hard = F.one_hot(hard, k)\n                sample = (hard - soft).detach() + soft\n            else:\n                sample = soft\n            # sample = F.gumbel_softmax(logit, temperature, hard)\n            # sample = logit\n            # [h*w, N, c] <- [h*w, N, k] @ [k, C]\n            quantized = codebook(sample)\n\n            quantized += torch.randn_like(quantized)\n            # quantized = sample\n\n            # normalize\n            # quantized \/= (k - 0.5) \/ (2 * k - 2)\n            # quantized -= 0.5 \/ (k - 1)\n            # [h*w, n, c]\n            # quantized = squeeze(sample, h, w)\n            posistedQuantized = self._position(quantized.reshape(h, w, n, c)).reshape(-1, n, c)\n\n            # mixed = (mixin * encoderIn \/ (mixin + 1)) + (quantized \/ (mixin + 1))\n\n            # mask = rolloutDistribution.sample((h*w, n, 1)).bool()\n\n            # mixed = mask * encoderIn.detach() + torch.logical_not(mask) * quantized\n            # [h*w, n, c] -> [n, c, h*w] -> [n, c, h, w]\n            deTransformed = self._decoder(posistedQuantized, posistedQuantized).reshape(h, w, n, c).permute(2, 3, 0, 1)\n            # deTransformed = quantized.permute(1, 2, 0).reshape(n, c, h, w)\n            # deTransformed = self._dePosition(deTransformed.reshape(h, w, n, c)).permute(2, 3, 0, 1)\n            # [n, c, h, w]\n            quantizeds.append(deTransformed)\n            codes.append(sample.argmax(-1).permute(1, 0).reshape(n, h, w))\n            logits.append(logit.reshape(n, h, w, k))\n            allCodewords.append(codewords.t())\n        return quantizeds, codes, logits, allCodewords\n"}