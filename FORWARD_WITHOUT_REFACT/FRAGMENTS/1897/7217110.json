{"BEFORE":"        key_mapping = self.key_convolution(input)\n        value_mapping = self.value_convolution(input)\n        # Reshape and transpose query mapping\n        query_mapping = query_mapping.view(batch_size, -1, height * width).permute(0, 2, 1)\n        # Reshape key mapping\n        key_mapping = key_mapping.view(batch_size, -1, height * width)\n        # Calc attention maps\n        attention = F.softmax(torch.bmm(query_mapping, key_mapping), dim=1)\n        # Reshape value mapping\n        value_mapping = value_mapping.view(batch_size, -1, height * width)\n        # Attention features\n        attention_features = torch.bmm(value_mapping, attention)\n        # Reshape to original shape\n        attention_features = attention_features.view(batch_size, channels, height, width)\n        # Residual mapping and gamma multiplication\n        output = self.gamma * attention_features + input\n","AFTER":"        key = self.key_convolution(self.max_pooling(input))\n        value = self.value_convolution(self.max_pooling(input))\n        # Reshape key, query and value mapping\n        query = query.view(-1, channels \/\/ 8, height * width).permute(0, 2, 1)\n        key = key.view(-1, channels \/\/ 8, height * width \/\/ 4)\n        value = value.view(-1, channels \/\/ 2, height * width \/\/ 4)\n        # Calc attention map\n        attention_map = torch.bmm(query, key).softmax(dim=-1).permute(0, 2, 1)\n        # Apply attention map to value to obtain the attention output features\n        attention_features = torch.bmm(value, attention_map)\n        # Reshape attention features\n        attention_features = attention_features.view(-1, channels \/\/ 2, height, width)\n        # Get output features\n        output = self.attention_convolution(attention_features)\n        # Residual mapping and gamma multiplication\n        output = self.gamma * output + input\n"}