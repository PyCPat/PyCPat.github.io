{"BEFORE":"        positions = (torch.full((batch_size, 1), self.pos_scale)\n                     * torch.arange(n_tokens).unsqueeze(0)).to(x)\n        positions = self.augment_positions(positions) # B, T\n\n        positions = positions.unsqueeze(-1) # B, T, 1\n        product = positions * self.freq # (B, T, 1) * (C) = (B, T, C)\n\n        pos_emb = torch.zeros(batch_size, n_tokens, n_feats, device=x.device)\n        pos_emb[:, :, 0::2] = torch.sin(product)\n        pos_emb[:, :, 1::2] = torch.cos(product)\n\n        if not self.batch_first:\n            pos_emb = pos_emb.transpose(0, 1)\n","AFTER":"        positions = repeat(self.pos_scale*torch.arange(n_tokens),\n                           't -> new_axis t', new_axis=batch_size).to(x)\n        positions = self.augment_positions(positions)\n\n        positions = rearrange(positions, 'b t -> b t 1')\n        product = positions * self.freq\n\n        pos_emb = torch.zeros(batch_size, n_tokens, n_feats, device=x.device)\n        pos_emb[:, :, 0::2] = torch.sin(product)\n        pos_emb[:, :, 1::2] = torch.cos(product)\n\n        if not self.batch_first:\n            pos_emb = rearrange(pos_emb, 'b t c -> t b c')\n"}