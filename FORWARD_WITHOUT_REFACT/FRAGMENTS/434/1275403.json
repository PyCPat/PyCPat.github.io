{"BEFORE":"        self.depth = depth\n\n        attn_kwargs = {**attn_kwargs, 'time_cond_dim': time_dim}\n\n        self.latents_attend_to_patches = Attention(dim, norm = True, norm_context = True, **attn_kwargs)\n\n        self.latent_self_attns = nn.ModuleList([])\n        for _ in range(latent_self_attn_depth):\n            self.latent_self_attns.append(nn.ModuleList([\n                Attention(dim, norm = True, **attn_kwargs),\n                FeedForward(dim)\n            ]))\n\n        self.patches_peg = PEG(dim)\n        self.patches_self_attn = LinearAttention(dim, norm = True, **attn_kwargs)\n        self.patches_self_attn_ff = FeedForward(dim)\n\n        self.patches_attend_to_latents = Attention(dim, norm = True, norm_context = True, **attn_kwargs)\n        self.patches_cross_attn_ff = FeedForward(dim)\n","AFTER":"        self.blocks = nn.ModuleList([RINBlock(dim, latent_self_attn_depth = latent_self_attn_depth, **attn_kwargs) for _ in range(depth)])\n"}