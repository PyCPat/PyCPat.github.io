{"BEFORE":"        a = torch.matmul(\n            permute_final_dims(q, (1, 0, 2)),  # [*, H, N_res, C_hidden]\n            permute_final_dims(k, (1, 2, 0)),  # [*, H, C_hidden, N_res]\n        )\n        a = a * math.sqrt(1.0 \/ (3 * self.c_hidden))\n        a = a + (math.sqrt(1.0 \/ 3) * permute_final_dims(b, (2, 0, 1)))\n\n        # [*, N_res, N_res, H, P_q, 3]\n        pt_att = q_pts.unsqueeze(-4) - k_pts.unsqueeze(-5)\n        pt_att = pt_att ** 2\n\n        # [*, N_res, N_res, H, P_q]\n        pt_att = sum(torch.unbind(pt_att, dim=-1))\n        head_weights = self.softplus(self.head_weights).view(\n            *((1,) * len(pt_att.shape[:-2]) + (-1, 1))\n        )\n        head_weights = head_weights * math.sqrt(\n            1.0 \/ (3 * (self.no_qk_points * 9.0 \/ 2))\n        )\n        pt_att = pt_att * head_weights\n\n        # [*, N_res, N_res, H]\n        pt_att = torch.sum(pt_att, dim=-1) * (-0.5)\n        # [*, N_res, N_res]\n        square_mask = mask.unsqueeze(-1) * mask.unsqueeze(-2)\n        square_mask = self.inf * (square_mask - 1)\n\n        # [*, H, N_res, N_res]\n        pt_att = permute_final_dims(pt_att, (2, 0, 1))\n        a = a + pt_att\n        a = a + square_mask.unsqueeze(-3)\n        a = self.softmax(a)\n\n        ################\n        # Compute output\n        ################\n        # [*, N_res, H, C_hidden]\n        o = torch.matmul(a, v.transpose(-2, -3)).transpose(-2, -3)\n\n        # [*, N_res, H * C_hidden]\n        o = flatten_final_dims(o, 2)\n\n        # As DeepMind explains, this manual matmul ensures that the operation\n        # happens in float32.\n        # [*, H, 3, N_res, P_v]\n        o_pt = torch.sum(\n            (\n                a[..., None, :, :, None]\n                * permute_final_dims(v_pts, (1, 3, 0, 2))[..., None, :, :]\n            ),\n            dim=-2,\n        )\n\n        # [*, N_res, H, P_v, 3]\n        o_pt = permute_final_dims(o_pt, (2, 0, 3, 1))\n        o_pt = r[..., None, None].invert_apply(o_pt)\n\n        # [*, N_res, H * P_v]\n        o_pt_norm = flatten_final_dims(\n            torch.sqrt(torch.sum(o_pt ** 2, dim=-1) + self.eps), 2\n        )\n\n        # [*, N_res, H * P_v, 3]\n        o_pt = o_pt.reshape(*o_pt.shape[:-3], -1, 3)\n\n        # [*, N_res, H, C_z]\n        o_pair = torch.matmul(a.transpose(-2, -3), z)\n\n        # [*, N_res, H * C_z]\n        o_pair = flatten_final_dims(o_pair, 2)\n\n        # [*, N_res, C_s]\n        s = self.linear_out(\n            torch.cat(\n                (o, *torch.unbind(o_pt, dim=-1), o_pt_norm, o_pair), dim=-1\n            )\n","AFTER":"        a = torch.matmul(\n            permute_final_dims(q, (1, 0, 2)),  # [*, H, N_res, C_hidden]\n            permute_final_dims(k, (1, 2, 0)),  # [*, H, C_hidden, N_res]\n        )\n        a *= math.sqrt(1.0 \/ (3 * self.c_hidden))\n        a += (math.sqrt(1.0 \/ 3) * permute_final_dims(b, (2, 0, 1)))\n\n        # [*, N_res, N_res, H, P_q, 3]\n        pt_att = q_pts.unsqueeze(-4) - k_pts.unsqueeze(-5)\n        pt_att = pt_att ** 2\n\n        # [*, N_res, N_res, H, P_q]\n        pt_att = sum(torch.unbind(pt_att, dim=-1))\n        head_weights = self.softplus(self.head_weights).view(\n            *((1,) * len(pt_att.shape[:-2]) + (-1, 1))\n        )\n        head_weights = head_weights * math.sqrt(\n            1.0 \/ (3 * (self.no_qk_points * 9.0 \/ 2))\n        )\n        pt_att = pt_att * head_weights\n\n        # [*, N_res, N_res, H]\n        pt_att = torch.sum(pt_att, dim=-1) * (-0.5)\n        # [*, N_res, N_res]\n        square_mask = mask.unsqueeze(-1) * mask.unsqueeze(-2)\n        square_mask = self.inf * (square_mask - 1)\n\n        # [*, H, N_res, N_res]\n        pt_att = permute_final_dims(pt_att, (2, 0, 1))\n        a = a + pt_att \n        a = a + square_mask.unsqueeze(-3)\n        a = self.softmax(a)\n\n        ################\n        # Compute output\n        ################\n        # [*, N_res, H, C_hidden]\n        o = torch.matmul(\n            a, v.transpose(-2, -3).to(dtype=a.dtype)\n        ).transpose(-2, -3)\n\n        # [*, N_res, H * C_hidden]\n        o = flatten_final_dims(o, 2)\n\n        # As DeepMind explains, this manual matmul ensures that the operation\n        # happens in float32.\n        # [*, H, 3, N_res, P_v]\n        o_pt = torch.sum(\n            (\n                a[..., None, :, :, None]\n                * permute_final_dims(v_pts, (1, 3, 0, 2))[..., None, :, :]\n            ),\n            dim=-2,\n        )\n\n        # [*, N_res, H, P_v, 3]\n        o_pt = permute_final_dims(o_pt, (2, 0, 3, 1))\n        o_pt = r[..., None, None].invert_apply(o_pt)\n\n        # [*, N_res, H * P_v]\n        o_pt_norm = flatten_final_dims(\n            torch.sqrt(torch.sum(o_pt ** 2, dim=-1) + self.eps), 2\n        )\n\n        # [*, N_res, H * P_v, 3]\n        o_pt = o_pt.reshape(*o_pt.shape[:-3], -1, 3)\n\n        # [*, N_res, H, C_z]\n        o_pair = torch.matmul(a.transpose(-2, -3), z.to(dtype=a.dtype))\n\n        # [*, N_res, H * C_z]\n        o_pair = flatten_final_dims(o_pair, 2)\n\n        # [*, N_res, C_s]\n        s = self.linear_out(\n            torch.cat(\n                (o, *torch.unbind(o_pt, dim=-1), o_pt_norm, o_pair), dim=-1\n            ).to(dtype=z.dtype)\n"}