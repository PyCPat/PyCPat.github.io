{"BEFORE":"        if type(sample) == dict:\n            x = sample['image']\n            dh = sample['depth']\n        else:\n            x, dh = sample\n\n        B, _, H, W = x.shape\n            \n        x = torch.cat([x, dh], dim=1)\n        x = self.reduce(x)\n        \n        dh1 = self.pyr.down(dh)\n        dh2 = self.pyr.down(dh1)\n        dh3 = self.pyr.down(dh2)\n            \n        B, _, H, W = x.shape\n        x1, x2, x3, x4, x5 = self.backbone(x)\n        \n        x1 = self.context1(x1) #4\n        x2 = self.context2(x2) #4\n        x3 = self.context3(x3) #8\n        x4 = self.context4(x4) #16\n        x5 = self.context5(x5) #32\n\n        f3, d3 = self.decoder(x5, x4, x3) #16\n\n        f3 = self.res(f3, (H \/\/ 4,  W \/\/ 4 ))\n        f2, _ = self.attention2_1(torch.cat([x2, f3], dim=1), d3.detach())\n        f2, p2 = self.attention2_2(torch.cat([f2, f3], dim=1), dh3)\n        d2 = self.pyr.rec(d3.detach(), p2) #4\n\n        x1 = self.res(x1, (H \/\/ 2, W \/\/ 2))\n        f2 = self.res(f2, (H \/\/ 2, W \/\/ 2))\n        f1, _ = self.attention1_1(torch.cat([x1, f2], dim=1), d2.detach()) #2\n        f1, p1 = self.attention1_2(torch.cat([f1, f2], dim=1), dh2) #2\n        d1 = self.pyr.rec(d2.detach(), p1) #2\n        \n        f1 = self.res(f1, (H, W))\n        f1, _ = self.attention0_1(f1, d1.detach()) #2\n        f1, p0 = self.attention0_2(f1, dh1) #2\n        d0 = self.pyr.rec(d1.detach(), p0) #2\n        \n        if type(sample) == dict and 'gt' in sample.keys() and sample['gt'] is not None:\n            y = sample['gt']\n            \n            y1 = self.pyr.down(y)\n            y2 = self.pyr.down(y1)\n            y3 = self.pyr.down(y2)\n\n            ploss =  self.pyramidal_consistency_loss_fn(self.des(d3, (H, W)), self.des(self.pyr.down(d2), (H, W)).detach()) * 0.0001\n            ploss += self.pyramidal_consistency_loss_fn(self.des(d2, (H, W)), self.des(self.pyr.down(d1), (H, W)).detach()) * 0.0001\n            ploss += self.pyramidal_consistency_loss_fn(self.des(d1, (H, W)), self.des(self.pyr.down(d0), (H, W)).detach()) * 0.0001\n            \n            closs =  self.loss_fn(self.des(d3, (H, W)), self.des(y3, (H, W)))\n            closs += self.loss_fn(self.des(d2, (H, W)), self.des(y2, (H, W)))\n            closs += self.loss_fn(self.des(d1, (H, W)), self.des(y1, (H, W)))\n            closs += self.loss_fn(self.des(d0, (H, W)), self.des(y, (H, W)))\n            \n            loss = ploss + closs\n\n        else:\n            loss = 0\n\n        if type(sample) == dict:\n            return {'pred': d0, \n                    'loss': loss, \n                    'gaussian': [d3, d2, d1, d0], \n                    'laplacian': [p2, p1, p0]}\n        \n        else:\n            return d0\n    \n    \ndef InSPyReNetV4_Res2Net50(depth, pretrained, base_size, **kwargs):\n","AFTER":"        x = sample['image']\n        dh = sample['depth']\n        B, _, H, W = x.shape\n            \n        x = torch.cat([x, dh], dim=1)\n        x = self.reduce(x)\n        \n        dh1 = self.pyr.down(dh)\n        dh2 = self.pyr.down(dh1)\n        dh3 = self.pyr.down(dh2)\n            \n        B, _, H, W = x.shape\n        x1, x2, x3, x4, x5 = self.backbone(x)\n        \n        x1 = self.context1(x1) #4\n        x2 = self.context2(x2) #4\n        x3 = self.context3(x3) #8\n        x4 = self.context4(x4) #16\n        x5 = self.context5(x5) #32\n\n        f3, d3 = self.decoder(x3, x4, x5) #16\n\n        f3 = self.res(f3, (H \/\/ 4,  W \/\/ 4 ))\n        f2, _ = self.attention2_1(torch.cat([x2, f3], dim=1), d3.detach())\n        f2, p2 = self.attention2_2(torch.cat([f2, f3], dim=1), dh3)\n        d2 = self.pyr.rec(d3.detach(), p2) #4\n\n        x1 = self.res(x1, (H \/\/ 2, W \/\/ 2))\n        f2 = self.res(f2, (H \/\/ 2, W \/\/ 2))\n        f1, _ = self.attention1_1(torch.cat([x1, f2], dim=1), d2.detach()) #2\n        f1, p1 = self.attention1_2(torch.cat([f1, f2], dim=1), dh2) #2\n        d1 = self.pyr.rec(d2.detach(), p1) #2\n        \n        f1 = self.res(f1, (H, W))\n        f1, _ = self.attention0_1(f1, d1.detach()) #2\n        f1, p0 = self.attention0_2(f1, dh1) #2\n        d0 = self.pyr.rec(d1.detach(), p0) #2\n        \n        if type(sample) == dict and 'gt' in sample.keys() and sample['gt'] is not None:\n            y = sample['gt']\n            \n            y1 = self.pyr.down(y)\n            y2 = self.pyr.down(y1)\n            y3 = self.pyr.down(y2)\n\n            ploss =  self.pyramidal_consistency_loss_fn(self.des(d3, (H, W)), self.des(self.pyr.down(d2), (H, W)).detach()) * 0.0001\n            ploss += self.pyramidal_consistency_loss_fn(self.des(d2, (H, W)), self.des(self.pyr.down(d1), (H, W)).detach()) * 0.0001\n            ploss += self.pyramidal_consistency_loss_fn(self.des(d1, (H, W)), self.des(self.pyr.down(d0), (H, W)).detach()) * 0.0001\n            \n            closs =  self.loss_fn(self.des(d3, (H, W)), self.des(y3, (H, W)))\n            closs += self.loss_fn(self.des(d2, (H, W)), self.des(y2, (H, W)))\n            closs += self.loss_fn(self.des(d1, (H, W)), self.des(y1, (H, W)))\n            closs += self.loss_fn(self.des(d0, (H, W)), self.des(y, (H, W)))\n            \n            loss = ploss + closs\n\n        else:\n            loss = 0\n\n        sample['pred'] = d0\n        sample['loss'] = loss\n        sample['gaussian'] = [d3, d2, d1, d0]\n        sample['laplacian'] = [p2, p1, p0]\n        return sample\n"}