{"BEFORE":"        if type(sample) == dict:\n            x = sample['image']\n            dx = sample['depth']\n        else:\n            x, dx = sample\n\n        B, _, H, W = x.shape\n            \n        # x = torch.cat([x, dx], dim=1)\n        # x = self.reduce(x)\n    \n        x1, x2, x3, x4, x5 = self.backbone(x)\n        \n        x1 = self.context1(x1) #4\n        x2 = self.context2(x2) #4\n        x3 = self.context3(x3) #8\n        x4 = self.context4(x4) #16\n        x5 = self.context5(x5) #32\n\n        f3, dh3 = self.decoder(x5, x4, x3) #16\n\n        f3 = self.res(f3, (H \/\/ 4,  W \/\/ 4 ))\n        f2, ph2 = self.d_attention2(torch.cat([x2, f3], dim=1), dh3.detach())\n        dh2 = self.pyr.rec(dh3.detach(), ph2) #4\n\n        x1 = self.res(x1, (H \/\/ 2, W \/\/ 2))\n        f2 = self.res(f2, (H \/\/ 2, W \/\/ 2))\n        f1, ph1 = self.d_attention1(torch.cat([x1, f2], dim=1), dh2.detach(), ph2.detach()) #2\n        dh1 = self.pyr.rec(dh2.detach(), ph1) #2\n        \n        f1 = self.res(f1, (H, W))\n        _, ph0 = self.d_attention0(f1, dh1.detach(), ph1.detach()) #2\n        dh0 = self.pyr.rec(dh1.detach(), ph0) #2\n        \n        x = torch.cat([x, dh0], dim=1)\n        x = self.reduce(x)\n        \n        x1, x2, x3, x4, x5 = self.backbone(x)\n        \n        x1 = self.context1(x1) #4\n        x2 = self.context2(x2) #4\n        x3 = self.context3(x3) #8\n        x4 = self.context4(x4) #16\n        x5 = self.context5(x5) #32\n\n        f3, d3 = self.decoder(x5, x4, x3) #16\n\n        f3 = self.res(f3, (H \/\/ 4,  W \/\/ 4 ))\n        f2, p2 = self.attention2(torch.cat([x2, f3], dim=1), d3.detach())\n        d2 = self.pyr.rec(d3.detach(), p2) #4\n\n        x1 = self.res(x1, (H \/\/ 2, W \/\/ 2))\n        f2 = self.res(f2, (H \/\/ 2, W \/\/ 2))\n        f1, p1 = self.attention1(torch.cat([x1, f2], dim=1), d2.detach(), p2.detach()) #2\n        d1 = self.pyr.rec(d2.detach(), p1) #2\n        \n        f1 = self.res(f1, (H, W))\n        _, p0 = self.attention0(f1, d1.detach(), p1.detach()) #2\n        d0 = self.pyr.rec(d1.detach(), p0) #2\n        \n        if type(sample) == dict and 'gt' in sample.keys() and sample['gt'] is not None:\n            y = self.resize(sample['gt'])\n            \n            y1 = self.pyr.down(y)\n            y2 = self.pyr.down(y1)\n            y3 = self.pyr.down(y2)\n            \n            dx1 = self.pyr.down(dx)\n            dx2 = self.pyr.down(dx1)\n            dx3 = self.pyr.down(dx2)\n\n            ploss =  self.pyramidal_consistency_loss_fn(self.des(d3, (H, W)), self.des(self.pyr.down(d2), (H, W)).detach()) * 0.0001\n            ploss += self.pyramidal_consistency_loss_fn(self.des(d2, (H, W)), self.des(self.pyr.down(d1), (H, W)).detach()) * 0.0001\n            ploss += self.pyramidal_consistency_loss_fn(self.des(d1, (H, W)), self.des(self.pyr.down(d0), (H, W)).detach()) * 0.0001\n            \n            ploss += self.pyramidal_consistency_loss_fn(self.des(dh3, (H, W)), self.des(self.pyr.down(dh2), (H, W)).detach()) * 0.0001\n            ploss += self.pyramidal_consistency_loss_fn(self.des(dh2, (H, W)), self.des(self.pyr.down(dh1), (H, W)).detach()) * 0.0001\n            ploss += self.pyramidal_consistency_loss_fn(self.des(dh1, (H, W)), self.des(self.pyr.down(dh0), (H, W)).detach()) * 0.0001\n            \n            closs =  self.loss_fn(self.des(d3, (H, W)), self.des(y3, (H, W)))\n            closs += self.loss_fn(self.des(d2, (H, W)), self.des(y2, (H, W)))\n            closs += self.loss_fn(self.des(d1, (H, W)), self.des(y1, (H, W)))\n            closs += self.loss_fn(self.des(d0, (H, W)), self.des(y, (H, W)))\n            \n            closs +=  self.loss_fn(self.des(dh3, (H, W)), self.des(dx3, (H, W)))\n            closs += self.loss_fn(self.des(dh2, (H, W)), self.des(dx2, (H, W)))\n            closs += self.loss_fn(self.des(dh1, (H, W)), self.des(dx1, (H, W)))\n            closs += self.loss_fn(self.des(dh0, (H, W)), self.des(dx, (H, W)))\n            \n            loss = ploss + closs\n\n        else:\n            loss = 0\n\n        if type(sample) == dict:\n            return {'pred': d0, \n                    'loss': loss, \n                    'gaussian': [d3, d2, d1, d0], \n                    'laplacian': [p2, p1, p0]}\n        \n        else:\n            return d0\n    \n    \ndef InSPyReNetV3_Res2Net50(depth, pretrained, base_size, **kwargs):\n","AFTER":"        x = sample['image']\n        dx = sample['depth']\n        B, _, H, W = x.shape\n            \n        # x = torch.cat([x, dx], dim=1)\n        # x = self.reduce(x)\n    \n        x1, x2, x3, x4, x5 = self.backbone(x)\n        \n        x1 = self.context1(x1) #4\n        x2 = self.context2(x2) #4\n        x3 = self.context3(x3) #8\n        x4 = self.context4(x4) #16\n        x5 = self.context5(x5) #32\n\n        f3, dh3 = self.decoder(x3, x4, x5) #16\n\n        f3 = self.res(f3, (H \/\/ 4,  W \/\/ 4 ))\n        f2, ph2 = self.d_attention2(torch.cat([x2, f3], dim=1), dh3.detach())\n        dh2 = self.pyr.rec(dh3.detach(), ph2) #4\n\n        x1 = self.res(x1, (H \/\/ 2, W \/\/ 2))\n        f2 = self.res(f2, (H \/\/ 2, W \/\/ 2))\n        f1, ph1 = self.d_attention1(torch.cat([x1, f2], dim=1), dh2.detach(), ph2.detach()) #2\n        dh1 = self.pyr.rec(dh2.detach(), ph1) #2\n        \n        f1 = self.res(f1, (H, W))\n        _, ph0 = self.d_attention0(f1, dh1.detach(), ph1.detach()) #2\n        dh0 = self.pyr.rec(dh1.detach(), ph0) #2\n        \n        x = torch.cat([x, dh0], dim=1)\n        x = self.reduce(x)\n        \n        x1, x2, x3, x4, x5 = self.backbone(x)\n        \n        x1 = self.context1(x1) #4\n        x2 = self.context2(x2) #4\n        x3 = self.context3(x3) #8\n        x4 = self.context4(x4) #16\n        x5 = self.context5(x5) #32\n\n        f3, d3 = self.decoder(x3, x4, x5) #16\n\n        f3 = self.res(f3, (H \/\/ 4,  W \/\/ 4 ))\n        f2, p2 = self.attention2(torch.cat([x2, f3], dim=1), d3.detach())\n        d2 = self.pyr.rec(d3.detach(), p2) #4\n\n        x1 = self.res(x1, (H \/\/ 2, W \/\/ 2))\n        f2 = self.res(f2, (H \/\/ 2, W \/\/ 2))\n        f1, p1 = self.attention1(torch.cat([x1, f2], dim=1), d2.detach(), p2.detach()) #2\n        d1 = self.pyr.rec(d2.detach(), p1) #2\n        \n        f1 = self.res(f1, (H, W))\n        _, p0 = self.attention0(f1, d1.detach(), p1.detach()) #2\n        d0 = self.pyr.rec(d1.detach(), p0) #2\n        \n        if type(sample) == dict and 'gt' in sample.keys() and sample['gt'] is not None:\n            y = self.resize(sample['gt'])\n            \n            y1 = self.pyr.down(y)\n            y2 = self.pyr.down(y1)\n            y3 = self.pyr.down(y2)\n            \n            dx1 = self.pyr.down(dx)\n            dx2 = self.pyr.down(dx1)\n            dx3 = self.pyr.down(dx2)\n\n            ploss =  self.pyramidal_consistency_loss_fn(self.des(d3, (H, W)), self.des(self.pyr.down(d2), (H, W)).detach()) * 0.0001\n            ploss += self.pyramidal_consistency_loss_fn(self.des(d2, (H, W)), self.des(self.pyr.down(d1), (H, W)).detach()) * 0.0001\n            ploss += self.pyramidal_consistency_loss_fn(self.des(d1, (H, W)), self.des(self.pyr.down(d0), (H, W)).detach()) * 0.0001\n            \n            ploss += self.pyramidal_consistency_loss_fn(self.des(dh3, (H, W)), self.des(self.pyr.down(dh2), (H, W)).detach()) * 0.0001\n            ploss += self.pyramidal_consistency_loss_fn(self.des(dh2, (H, W)), self.des(self.pyr.down(dh1), (H, W)).detach()) * 0.0001\n            ploss += self.pyramidal_consistency_loss_fn(self.des(dh1, (H, W)), self.des(self.pyr.down(dh0), (H, W)).detach()) * 0.0001\n            \n            closs =  self.loss_fn(self.des(d3, (H, W)), self.des(y3, (H, W)))\n            closs += self.loss_fn(self.des(d2, (H, W)), self.des(y2, (H, W)))\n            closs += self.loss_fn(self.des(d1, (H, W)), self.des(y1, (H, W)))\n            closs += self.loss_fn(self.des(d0, (H, W)), self.des(y, (H, W)))\n            \n            closs +=  self.loss_fn(self.des(dh3, (H, W)), self.des(dx3, (H, W)))\n            closs += self.loss_fn(self.des(dh2, (H, W)), self.des(dx2, (H, W)))\n            closs += self.loss_fn(self.des(dh1, (H, W)), self.des(dx1, (H, W)))\n            closs += self.loss_fn(self.des(dh0, (H, W)), self.des(dx, (H, W)))\n            \n            loss = ploss + closs\n\n        else:\n            loss = 0\n            \n        sample['pred'] = d0\n        sample['loss'] = loss\n        sample['gaussian'] = [d3, d2, d1, d0]\n        sample['laplacian'] = [p2, p1, p0]\n        return sample\n"}