{"BEFORE":"        B = encoder_outputs.size(0)\n\n        # Get processed memory for attention key\n        processed_memory = self.memory_layer(encoder_outputs)\n        if memory_lengths is not None:\n            mask = get_mask_from_lengths(processed_memory, memory_lengths)\n        else:\n            mask = None\n\n        # Run greedy decoding if inputs is None\n        greedy = inputs is None\n\n        # (B, T, mel_dim) -> (B, T', mel_dim*r)\n        if inputs is not None:\n            # Grouping multiple frames if necessary\n            if inputs.size(-1) == self.mel_dim:\n                inputs = inputs.reshape(B, inputs.size(1) \/\/ self.r, -1)\n            assert inputs.size(-1) == self.mel_dim * self.r\n            T_decoder = inputs.size(1)\n\n        # Time first (T', B, mel_dim*r)\n        if inputs is not None:\n            inputs = inputs.transpose(0, 1)\n\n        # <GO> frames\n        initial_input = encoder_outputs.data.new(B, self.mel_dim * self.r).zero_()\n\n        # Init decoder states\n        attention_rnn_hidden = encoder_outputs.data.new(B, self.attention_rnn_units).zero_()\n        decoder_rnn_hiddens = [encoder_outputs.data.new(B, self.decoder_rnn_units).zero_()\n                               for _ in range(len(self.decoder_rnns))]\n        attention_context = encoder_outputs.data.new(B, self.attention_context_dim).zero_()\n\n        # To save the result\n        mel_outputs, attn_scores, stop_tokens = [], [], []\n\n        # Run the decoder loop\n        t = 0\n        current_input = initial_input\n        while True:\n            if t > 0:\n                current_input = mel_outputs[-1] if greedy else inputs[t - 1]\n            t += 1\n\n            # Prenet\n            current_input = self.prenet(current_input)\n\n            # Attention RNN\n            attention_rnn_hidden, attention_context, attention_score = self.attention_rnn(\n                current_input, attention_context, attention_rnn_hidden,\n                encoder_outputs, processed_memory=processed_memory, mask=mask)\n\n            # Concat RNN output and attention context vector\n            decoder_input = self.project_to_decoder_in(\n                torch.cat((attention_rnn_hidden, attention_context), -1))\n\n            # Pass through the decoder RNNs\n            for idx in range(len(self.decoder_rnns)):\n                decoder_rnn_hiddens[idx] = self.decoder_rnns[idx](\n                    decoder_input, decoder_rnn_hiddens[idx])\n                # Residual connectinon\n                decoder_input = decoder_rnn_hiddens[idx] + decoder_input\n\n            # Contact RNN output and context vector to form projection input\n            proj_input = torch.cat((decoder_input, attention_context), -1)\n\n            # Project to mel\n            output = self.mel_proj(proj_input)\n\n            # Stop token prediction\n            stop = self.stop_proj(proj_input)\n            stop = torch.sigmoid(stop)\n\n            # Store predictions\n            mel_outputs += [output]\n            attn_scores += [attention_score]\n            stop_tokens += [stop] * self.r\n\n            if greedy:\n                if stop > self.stop_threshold:\n                    break\n                elif t > 1 and is_end_of_frames(output):\n                    print(\"Warning: End with low power.\")\n                    break\n                elif t > self.max_decoder_steps:\n                    print(\"Warning: Reached max decoder steps.\")\n                    break\n            else:\n                if t >= T_decoder:\n                    break\n\n        # Validation check\n        assert greedy or len(mel_outputs) == T_decoder\n\n        # Back to batch first\n        attn_scores = torch.stack(attn_scores).transpose(0, 1)\n        mel_outputs = torch.stack(mel_outputs).transpose(0, 1).contiguous()\n        stop_tokens = torch.stack(stop_tokens).transpose(0, 1).squeeze(2)\n\n        # (B, T', mel_dim*r) -> (B, T, mel_dim)\n        mel_outputs = mel_outputs.reshape(B, -1, self.mel_dim)\n","AFTER":"        B = encoder_outputs.size(0)\n\n        # Get processed memory for attention key\n        processed_memory = self.memory_layer(encoder_outputs)\n        if memory_lengths is not None:\n            mask = get_mask_from_lengths(processed_memory, memory_lengths)\n        else:\n            mask = None\n\n        # Run greedy decoding if inputs is None\n        greedy = inputs is None\n\n        # Time first: (B, T, mel_dim) -> (T, B, mel_dim)\n        if inputs is not None:\n            inputs = inputs.transpose(0, 1)\n            T_decoder = inputs.size(0)\n\n        # <GO> frames\n        initial_input = encoder_outputs.data.new(B, self.mel_dim).zero_()\n\n        # Init decoder states\n        attention_rnn_hidden = encoder_outputs.data.new(B, self.attention_rnn_units).zero_()\n        decoder_rnn_hiddens = [encoder_outputs.data.new(B, self.decoder_rnn_units).zero_()\n                               for _ in range(len(self.decoder_rnns))]\n        attention_context = encoder_outputs.data.new(B, self.attention_context_dim).zero_()\n\n        # To save the result\n        mel_outputs, attn_scores, stop_tokens = [], [], []\n\n        # Run the decoder loop\n        t = 0\n        current_input = initial_input\n        while True:\n            if t > 0:\n                current_input = mel_outputs[-1][:, -1, :] if greedy else inputs[t - 1]\n            t += self.r\n\n            # Prenet\n            current_input = self.prenet(current_input)\n\n            # Attention RNN\n            attention_rnn_hidden, attention_context, attention_score = self.attention_rnn(\n                current_input, attention_context, attention_rnn_hidden,\n                encoder_outputs, processed_memory=processed_memory, mask=mask)\n\n            # Concat RNN output and attention context vector\n            decoder_input = self.project_to_decoder_in(\n                torch.cat((attention_rnn_hidden, attention_context), -1))\n\n            # Pass through the decoder RNNs\n            for idx in range(len(self.decoder_rnns)):\n                decoder_rnn_hiddens[idx] = self.decoder_rnns[idx](\n                    decoder_input, decoder_rnn_hiddens[idx])\n                # Residual connectinon\n                decoder_input = decoder_rnn_hiddens[idx] + decoder_input\n\n            # Contact RNN output and context vector to form projection input\n            proj_input = torch.cat((decoder_input, attention_context), -1)\n\n            # Project to mel\n            # (B, mel_dim*r) -> (B, r, mel_dim)\n            output = self.mel_proj(proj_input)\n            output = output.view(B, -1, self.mel_dim)\n\n            # Stop token prediction\n            stop = self.stop_proj(proj_input)\n            stop = torch.sigmoid(stop)\n\n            # Store predictions\n            mel_outputs.append(output)\n            attn_scores.append(attention_score.unsqueeze(1))\n            stop_tokens.extend([stop] * self.r)\n\n            if greedy:\n                if stop > self.stop_threshold:\n                    break\n                elif t > 1 and is_end_of_frames(output):\n                    print(\"Warning: End with low power.\")\n                    break\n                elif t > self.max_decoder_steps:\n                    print(\"Warning: Reached max decoder steps.\")\n                    break\n            else:\n                if t >= T_decoder:\n                    break\n\n        # To tensor\n        mel_outputs = torch.cat(mel_outputs, dim=1) # (B, T_decoder, mel_dim)\n        attn_scores = torch.cat(attn_scores, dim=1) # (B, T_decoder\/r, T_encoder)\n        stop_tokens = torch.cat(stop_tokens, dim=1) # (B, T_decoder)\n\n        # Validation check\n        assert greedy or mel_outputs.size(1) == T_decoder\n"}