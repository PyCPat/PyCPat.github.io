{"BEFORE":"        sparse_emb_list = self.embedding_layer(data)\n        feature_emb = torch.stack(sparse_emb_list, dim=1).squeeze(2)\n        lr_logit = self.lr_layer(data)\n        cin_logit = self.cin(feature_emb)\n        if self.dnn is not None:\n            dnn_logit = self.dnn(feature_emb.flatten(start_dim=1))\n","AFTER":"        feature_emb = self.embedding_layer(data)\n        lr_logit = self.lr_layer(data)\n        cin_logit = self.cin(feature_emb)\n        if self.dnn is not None:\n            dense_input = get_linear_input(self.enc_dict, data)\n            emb_flatten = feature_emb.flatten(start_dim=1)\n            dnn_logit = self.dnn(torch.cat([emb_flatten, dense_input], dim=1))\n"}