{"BEFORE":"    def forward(self, x):\n        return x\n","AFTER":"        self,\n        seq,\n        retrieved,\n        return_loss = False\n    ):\n        assert not (return_loss and not self.training), 'must be training if returning loss'\n\n        n, device = seq.shape[-1], seq.device\n\n        # use 0 as bos\n\n        seq = F.pad(seq, (1, 0), value = 0)\n\n        # if training, derive labels\n\n        if return_loss:\n            seq, labels = seq[:, :-1], seq[:, 1:]\n\n        # embed both sequence and retrieved chunks\n\n        embed = self.token_emb(seq)\n        retrieved = self.token_emb(retrieved)\n\n        # get positional embedding\n\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n        pos_emb = rearrange(pos_emb, 'n d -> 1 n d')\n        embed = embed + pos_emb\n\n        logits = self.to_logits(embed)\n\n        if not return_loss:\n            return logits\n\n        loss = F.cross_entropy(rearrange(logits, 'b n c -> b c n'), labels)\n        return loss\n"}