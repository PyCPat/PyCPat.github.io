{"BEFORE":"        attention = self.softmax(energy) # BX (N) X (N)\n        neg_attention = (1-attention.clone()) #for negative, we attend on those dissimilar tasks (1-attention)\n\n        # attention =  F.gumbel_softmax(energy,hard=True,dim=-1)\n        # print('attention: ',attention)\n        proj_value = self.value_conv(x).view(m_batchsize,width,height) # B X C X N\n\n        out = torch.bmm(proj_value,attention.permute(0,2,1) )\n        out = out.view(m_batchsize,width,height)\n\n        neg_out = torch.bmm(proj_value,neg_attention.permute(0,2,1) )\n        neg_out = neg_out.view(m_batchsize,width,height)\n\n        out = self.gamma*out + x\n        neg_out = self.gamma*neg_out + x\n\n\n        return out,neg_out\n","AFTER":"        out = self.gamma*out + x\n\n\n        return out\n"}