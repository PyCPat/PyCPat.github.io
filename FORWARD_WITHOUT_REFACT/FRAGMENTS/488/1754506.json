{"BEFORE":"        a = F.relu(self.fc1(state))\n        a = F.relu(self.fc2(a))\n        mu = self.mu_head(a)\n        mu = torch.clip(mu, MEAN_MIN, MEAN_MAX)\n        log_sigma = self.sigma_head(a)\n        log_sigma = torch.clip(log_sigma, LOG_STD_MIN, LOG_STD_MAX)\n        sigma = torch.exp(log_sigma)\n\n        a_distribution = Normal(mu, sigma)\n        action = a_distribution.rsample()\n\n        logp_pi = a_distribution.log_prob(action).sum(axis=-1)\n        logp_pi -= (2 * (np.log(2) - action - F.softplus(-2 * action))).sum(axis=1)\n        logp_pi = torch.unsqueeze(logp_pi, dim=1)\n\n        action = self.max_action * torch.tanh(action)\n        mu = torch.tanh(mu) * self.max_action\n        return action, logp_pi, mu\n","AFTER":"        a_dist, a_tanh_mode = self._get_outputs(state)\n        action = a_dist.rsample()\n        logp_pi = a_dist.log_prob(action).sum(axis=-1)\n        return action, logp_pi, a_tanh_mode\n"}