{"BEFORE":"        h = self.heads\n        q, k, v = self.to_qkv(feats).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n\n        if exists(mask):\n            mask = rearrange(mask, 'b n -> b () n ()')\n            k.masked_fill_(~mask, -torch.finfo(k.dtype).max)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        if exists(mask):\n            v.masked_fill_(~mask, 0.)\n\n        context = einsum('b h n d, b h n e -> b h d e', k, v)\n        out = einsum('b h d e, b h n d -> b h n e', context, q)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out), 0\n","AFTER":"        induced = self.attn1(queries, x, mask = mask)\n        out     = self.attn2(x, induced)\n        return out, 0\n"}