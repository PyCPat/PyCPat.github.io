{"BEFORE":"        regs = list()\n\n        n, h, w, k = logits[0].shape\n        # ths = torch.tensor(float(h * w) \/ k, device=device).clamp(1.0, h * w)\n\n        # codes: [m, n, h, w]; logits: m * list(n, h, w, k); codeFreqMap: m * list([n, h, w]), binCounts: m * list([n, k])\n        for code, logit, freqMap, binCount in zip(codes.permute(1, 0, 2, 3), logits, codeFreqMap, binCounts):\n            # binCount = binCount.float()\n            # maxFreq, _ = binCount.max(-1, keepdim=True)\n\n            # prob = ths \/ freqMap\n            # Expectation of number of each bin not masked: (h * w) \/ k\n            # maskProb = torch.distributions.Bernoulli(probs=(1.0 - prob).clamp(0.05, 0.95))\n            maskProb = torch.distributions.Bernoulli(probs=(freqMap \/ float(h * w)).clamp(0.01, 0.99))\n            # [n, h, w]\n            needRegMask = maskProb.sample().bool()\n\n            # # adjust final code\n            # # fm, bc, regMask shape: [h, w]\n            # for i, (c, fm, bc, regMask) in enumerate(zip(code, freqMap, binCount, needRegMask)):\n            #     # needRegMask codes will be re-allocated\n            #     newCount = torch.bincount(c[~regMask].flatten(), minlength=k)\n            #     # contain negative numbers\n            #     remain = ths - newCount\n            #     # totally, have j negatives\n            #     negativeTotalCount = (remain[remain < 0]).sum()\n            #     # set negative to zero, then, we remain j numbers to remove\n            #     remain[remain < 0] = 0\n            #     # [h*w - sum(freqMap > ths)] sequence, the remaining codes to allocate\n            #     sequence = torch.repeat_interleave(torch.arange(k, device=device), remain)\n            #     # remove the last j numbers\n            #     sequence = sequence[torch.randperm(len(sequence))]\n            #     if negativeTotalCount < 0:\n            #         sequence = sequence[:negativeTotalCount]\n            #     code[i][regMask] = sequence\n            # relaxedFreq = maxFreq # + binCount.mean(-1, keepdim=True)\n            # reverse frequencies\n            # max bin -> 0\n            # min bin -> maxbin - minbin\n            # [n, k]\n            # reverseBin = relaxedFreq - binCount\n            # [n, h, w], auto convert freq to prob in pytorch implementation\n            # sample = torch.distributions.Categorical(probs=reverseBin).sample((h, w)).permute(2, 0, 1)\n            # sample = torch.distributions.Categorical(logits=torch.zeros_like(logit)).sample()\n            sample = torch.distributions.Categorical(probs=(binCount < 1).float()).sample((h, w)).permute(2, 0, 1)\n            logit = logit.permute(0, 3, 1, 2)\n            # [n, 1, 1], normalize then sigmoid, higher frequency -> higher weight\n            # weight = freqMap \/ float(h * w) # ((freqMap \/ float(h * w) - 0.5)* 4).sigmoid()\n            ceReg = F.cross_entropy(logit, sample, reduction=\"none\") * needRegMask # * (float(h * w) \/ needRegMask.float().sum(dim=(1, 2), keepdims=True))\n            # # cePush = 0.001 * F.cross_entropy(logit, code, reduction=\"none\") * (~needRegMask)\n            # cePush = F.cross_entropy(logit, code, reduction=\"none\") * (~needRegMask) * weight\n            regs.append((ceReg).mean())\n\n\n        regs = sum(regs) \/ len(regs)\n","AFTER":"    def forward(self, images, restored, codes, trueCodes, logits, codeFreqMap, binCounts):\n        device = images.device\n\n        l2Loss = F.mse_loss(restored, images)\n        l1Loss = F.l1_loss(restored, images)\n        ssimLoss = 1 - self._msssim(restored + 1, images + 1)\n\n        # ssimLoss = (1 - self._msssim((restored + 1), (images + 1))).log10().mean()\n        # ssimLoss = -F.binary_cross_entropy(ssimLoss, torch.ones_like(ssimLoss))\n\n        regs = list()\n        \"\"\"\n        n, h, w, k = logits[0].shape\n        # ths = torch.tensor(float(h * w) \/ k, device=device).clamp(1.0, h * w)\n\n        # codes: [m, n, h, w]; logits: m * list(n, h, w, k); codeFreqMap: m * list([n, h, w]), binCounts: m * list([n, k])\n        for code, logit, freqMap, binCount in zip(trueCodes.permute(1, 0, 2, 3), logits, codeFreqMap, binCounts):\n            # binCount = binCount.float()\n            # maxFreq, _ = binCount.max(-1, keepdim=True)\n\n            # prob = ths \/ freqMap\n            # Expectation of number of each bin not masked: (h * w) \/ k\n            # maskProb = torch.distributions.Bernoulli(probs=(1.0 - prob).clamp(0.05, 0.95))\n            maskProb = torch.distributions.Bernoulli(probs=(freqMap \/ float(h * w)).clamp(0.01, 0.99))\n            # [n, h, w]\n            needRegMask = maskProb.sample().bool()\n\n            # # adjust final code\n            # # fm, bc, regMask shape: [h, w]\n            # for i, (c, fm, bc, regMask) in enumerate(zip(code, freqMap, binCount, needRegMask)):\n            #     # needRegMask codes will be re-allocated\n            #     newCount = torch.bincount(c[~regMask].flatten(), minlength=k)\n            #     # contain negative numbers\n            #     remain = ths - newCount\n            #     # totally, have j negatives\n            #     negativeTotalCount = (remain[remain < 0]).sum()\n            #     # set negative to zero, then, we remain j numbers to remove\n            #     remain[remain < 0] = 0\n            #     # [h*w - sum(freqMap > ths)] sequence, the remaining codes to allocate\n            #     sequence = torch.repeat_interleave(torch.arange(k, device=device), remain)\n            #     # remove the last j numbers\n            #     sequence = sequence[torch.randperm(len(sequence))]\n            #     if negativeTotalCount < 0:\n            #         sequence = sequence[:negativeTotalCount]\n            #     code[i][regMask] = sequence\n            # relaxedFreq = maxFreq # + binCount.mean(-1, keepdim=True)\n            # reverse frequencies\n            # max bin -> 0\n            # min bin -> maxbin - minbin\n            # [n, k]\n            # reverseBin = relaxedFreq - binCount\n            # [n, h, w], auto convert freq to prob in pytorch implementation\n            # sample = torch.distributions.Categorical(probs=reverseBin).sample((h, w)).permute(2, 0, 1)\n            # sample = torch.distributions.Categorical(logits=torch.zeros_like(logit)).sample()\n            sample = torch.distributions.Categorical(probs=(binCount < 1).float()).sample((h, w)).permute(2, 0, 1)\n            logit = logit.permute(0, 3, 1, 2)\n            # [n, 1, 1], normalize then sigmoid, higher frequency -> higher weight\n            # weight = freqMap \/ float(h * w) # ((freqMap \/ float(h * w) - 0.5)* 4).sigmoid()\n            ceReg = F.cross_entropy(logit, sample, reduction=\"none\") * needRegMask # * (float(h * w) \/ needRegMask.float().sum(dim=(1, 2), keepdims=True))\n            # # cePush = 0.001 * F.cross_entropy(logit, code, reduction=\"none\") * (~needRegMask)\n            # cePush = F.cross_entropy(logit, code, reduction=\"none\") * (~needRegMask) * weight\n            regs.append((ceReg).mean())\n        regs = sum(regs) \/ len(regs)\n        \"\"\"\n\n        for logit, code in zip(logits, codes.permute(1, 0, 2, 3)):\n            # [N, H, W, K] -> [N, K]\n            # logit = logit.mean(dim=(1, 2))\n            posterior = Categorical(logits=logit)\n            prior = Categorical(logits=torch.zeros_like(logit))\n            reg = torch.distributions.kl_divergence(posterior, prior).mean()\n            # [n, h, w, k]\n            # weight = (-logit).detach().softmax(-1)\n            # oneHot = F.one_hot(code, num_classes=logit.shape[-1]).float()\n            # [n, h, w]\n            # targetWeight = (weight * oneHot).sum(-1)\n            code = torch.randint_like(code, logit.shape[-1])\n            logit = logit.permute(0, 3, 1, 2)\n            mle = F.cross_entropy(logit, code)\n            regs.append(reg + 0.01 * mle)\n        regs = sum(regs) \/ len(logits)\n"}