{"BEFORE":"        k = torch.cat((k_context, k_input), dim = 1)\n        v = torch.cat((v_context, v_input), dim = 1)\n\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), (q, k, v))\n\n        q = q * self.scale\n\n        if exists(rotary_pos_emb):\n            q = apply_rotary_pos_emb(rotary_pos_emb, q)\n            k = apply_rotary_pos_emb(rotary_pos_emb, k)\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n\n        i, j = sim.shape[-2:]\n\n        mask_value = -torch.finfo(sim.dtype).max\n\n        if exists(context_mask):\n            mask_len = context_mask.shape[-1]\n            context_mask = F.pad(context_mask, (0, max(j - mask_len, 0)), value = True)\n            context_mask = rearrange(context_mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~context_mask, mask_value)\n\n        causal_mask = torch.ones((i, j), device = x.device, dtype = torch.bool).triu(j - i + 1)\n        sim = sim.masked_fill(causal_mask, mask_value)\n\n        attn = sim.softmax(dim = -1)\n        attn = self.dropout(attn)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n","AFTER":"        k = torch.cat((k_context, k_input), dim = 1)\n        v = torch.cat((v_context, v_input), dim = 1)\n\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), (q, k, v))\n\n        q = q * self.scale\n\n        if exists(rotary_pos_emb):\n            q = apply_rotary_pos_emb(rotary_pos_emb, q)\n            k = apply_rotary_pos_emb(rotary_pos_emb, k)\n\n        # take care of masking\n\n        i, j = q.shape[-2], k.shape[-2]\n        mask_value = -torch.finfo(q.dtype).max\n\n        if exists(context_mask):\n            mask_len = context_mask.shape[-1]\n            context_mask = F.pad(context_mask, (0, max(j - mask_len, 0)), value = True)\n            context_mask = rearrange(context_mask, 'b j -> b 1 1 j')\n\n        causal_mask = torch.ones((i, j), device = x.device, dtype = torch.bool).triu(j - i + 1)\n\n        # process in chunks of heads\n\n        out = []\n\n        max_heads = self.max_heads_process\n\n        for q_chunk, k_chunk, v_chunk in zip(q.split(max_heads, dim = 1), k.split(max_heads, dim = 1), v.split(max_heads, dim = 1)):\n            sim = einsum('b h i d, b h j d -> b h i j', q_chunk, k_chunk)\n\n            if exists(context_mask):\n                sim = sim.masked_fill(~context_mask, mask_value)\n\n            sim = sim.masked_fill(causal_mask, mask_value)\n\n            attn = sim.softmax(dim = -1)\n            attn = self.dropout(attn)\n\n            out_chunk = einsum('b h i j, b h j d -> b h i d', attn, v_chunk)\n            out.append(out_chunk)\n\n        # concat all the heads together\n\n        out = torch.cat(out, dim = 1)\n"}