{"BEFORE":"            action = mean\n            std = torch.zeros_like(log_std)\n        else:\n            # todo: is clamp really necessary?\n            log_std = self.std_clamp(log_std)\n            std = log_std.exp()\n            m = MultivariateNormal(mean.reshape(-1), torch.diag(std.reshape(-1)))\n            action = m.sample()\n            action = action.reshape(mean.shape)\n\n        action = action.tanh()\n\n        return action, std\n","AFTER":"            action = mean.tanh()\n            log_prob = torch.zeros_like(log_std)\n        else:\n            # todo: is clamp really necessary?\n            log_std = self.std_clamp(log_std)\n            std = log_std.exp()\n            covariance = torch.diag_embed(std)\n            m = MultivariateNormal(mean, covariance)\n            action_base = m.sample()\n            log_prob = m.log_prob(action_base)\n            log_prob.unsqueeze_(-1)\n\n            action = action_base.tanh()\n\n            # According to \"Soft Actor-Critic\" (Haarnoja et. al) Appendix C\n            action_bound_compensation = torch.log(1. - action.tanh().pow(2) + 1e-6)\n            action_bound_compensation = action_bound_compensation.sum(dim=-1, keepdim=True)\n            log_prob.sub_(action_bound_compensation)\n\n        return action, log_prob\n"}