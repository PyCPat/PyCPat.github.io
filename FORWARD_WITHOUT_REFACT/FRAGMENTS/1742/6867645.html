<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.embedding.weight.data.uniform_(1./self.K, 1./self.K)

    def forward(self, input: Tensor):
        input = <a id="change">input.permute(</a>0, <a id="change">2</a>, <a id="change">3</a>, <a id="change">1</a><a id="change">)</a> &#47&#47 [B x D x H x W] -&gt; [B x H x W x D]
        input_shape = input.shape
        flat_input = <a id="change">input.contiguous()</a>.view(-1, self.D)    &#47&#47 [BHW x D]

        &#47&#47 Compute L2 distance between input and embedding weights
        dist = torch.sum(flat_input**2, dim = 1, keepdim=True) + \</code></pre><h3>After Change</h3><pre><code class='java'>
        self.embedding.weight.data.uniform_(-1 / self.K, 1 / self.K)

    def forward(self, latents: Tensor) -&gt; Tensor:
        latents = <a id="change">latents.permute(0, 2, 3, 1).contiguous()</a>  &#47&#47 [B x D x H x W] -&gt; [B x H x W x D]
        latents_shape = latents.shape
        flat_latents = latents.view(-1, self.D)  &#47&#47 [BHW x D]

        &#47&#47 Compute L2 distance between latents and embedding weights
        dist = torch.sum(flat_latents ** 2, dim=1, keepdim=True) + \
               torch.sum(self.embedding.weight ** 2, dim=1) - \
               2 * torch.matmul(flat_latents, self.embedding.weight.t())  &#47&#47 [BHW x K]

        &#47&#47 Get the encoding that has the min distance
        encoding_inds = torch.argmin(dist, dim=1).unsqueeze(1)  &#47&#47 [BHW, 1]

        &#47&#47 Convert to one-hot encodings
        device = latents.device
        encoding_one_hot = torch.zeros(encoding_inds.size(0), self.K, device=device)
        encoding_one_hot.scatter_(1, encoding_inds, 1)  &#47&#47 [BHW x K]

        &#47&#47 Quantize the latents
        quantized_latents = torch.matmul(encoding_one_hot, self.embedding.weight)  &#47&#47 [BHW, D]
        quantized_latents = quantized_latents.view(latents_shape)  &#47&#47 [B x H x W x D]

        &#47&#47 Compute the VQ Losses
        commitment_loss = F.mse_loss(quantized_latents.detach(), latents)
        embedding_loss<a id="change"> = </a>F.mse_loss(quantized_latents, latents.detach())

        vq_loss = commitment_loss * self.beta + embedding_loss
</code></pre>