{"BEFORE":"        print('######USING ATTENTION STYLE: ', attention_style)\n        if hybrid_backbone is not None:\n            raise NotImplementedError('hybrid backbone not implemented')\n        else:\n            # self.patch_embed = VideoPatchEmbed(\n            #     img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim, num_frames=num_frames)\n\n            self.patch_embed = PatchEmbed2D(\n                img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim, flatten=False)\n\n        num_patches = self.patch_embed.num_patches*num_frames\n        self.patches_per_frame = num_patches \/\/ num_frames\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(\n            torch.zeros(1, self.patches_per_frame + 1,\n                        embed_dim))  # remember to take pos_embed[1:] for tiling over time\n        self.temporal_embed = nn.Parameter(torch.zeros(1, num_frames, embed_dim))\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.blocks = nn.ModuleList([\n            SpaceTimeBlock(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, time_init=time_init,\n                attention_style=attention_style)\n            for i in range(depth)])\n        self.norm = norm_layer(embed_dim)\n","AFTER":"        self.attention_style = attention_style\n        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n        logging.info('######USING ATTENTION STYLE: %s', self.attention_style)\n        if hybrid_backbone is not None:\n            raise NotImplementedError('hybrid backbone not implemented')\n        else:\n            # self.patch_embed = VideoPatchEmbed(\n            #     img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim, num_frames=num_frames)\n\n            self.patch_embed = PatchEmbed2D(\n                img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim, flatten=False)\n\n        num_patches = self.patch_embed.num_patches*num_frames\n        self.patches_per_frame = num_patches \/\/ num_frames\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(\n            torch.zeros(1, self.patches_per_frame + 1,\n                        embed_dim))  # remember to take pos_embed[1:] for tiling over time\n        if self.attention_style != 'bridge_former':\n            self.temporal_embed = nn.Parameter(torch.zeros(1, num_frames, embed_dim))\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.blocks = nn.ModuleList([\n            SpaceTimeBlock(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, time_init=time_init,\n                attention_style=attention_style)\n            for i in range(depth)])\n        if self.attention_style == 'bridge_former':\n            self.norm1 = norm_layer(embed_dim)\n        else:\n            self.norm = norm_layer(embed_dim)\n\n        # Representation layer\n        if representation_size:\n"}