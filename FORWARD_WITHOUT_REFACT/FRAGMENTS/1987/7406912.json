{"BEFORE":"        residual = u * self.param_D\n\n        # weights derived from alphas (learned exponential smoothing decay rate)\n\n        alphas = self.alphas.sigmoid()\n        reversed_powers = torch.arange(seq_len - 1, -1, -1, device = device)\n        K = alphas * ((1 - alphas) ** rearrange(reversed_powers, '... l -> ... l 1'))\n\n        # conv1d fft O(nlog(n))\n\n        u = rearrange(u, '... (h d) -> ... h d', h = self.heads)\n\n        out = conv1d_fft(u, K, dim = -3, weight_dim = -2)\n\n        out = rearrange(out, '... h d -> ... (h d)')\n\n        return out + residual\n","AFTER":"        if self.reverse_seq:\n            x = torch.flip(x, dims = (1,))\n\n        device, seq_len = x.device, x.shape[1]\n        u = self.norm(x)\n\n        # learned weighted residual\n\n        residual = u * self.param_D\n\n        # weights derived from alphas (learned exponential smoothing decay rate)\n\n        alphas = self.alphas.sigmoid()\n        reversed_powers = torch.arange(seq_len - 1, -1, -1, device = device)\n        K = alphas * ((1 - alphas) ** rearrange(reversed_powers, '... l -> ... l 1'))\n\n        # conv1d fft O(nlog(n))\n\n        u = rearrange(u, '... (h d) -> ... h d', h = self.heads)\n\n        out = conv1d_fft(u, K, dim = -3, weight_dim = -2)\n\n        out = rearrange(out, '... h d -> ... (h d)')\n\n        out = out + residual\n\n        if self.reverse_seq:\n            out = torch.flip(out, dims = (1,))\n\n        return out\n"}