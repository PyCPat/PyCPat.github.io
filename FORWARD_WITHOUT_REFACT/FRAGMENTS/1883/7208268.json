{"BEFORE":"        coor_attn = coor_weights.softmax(dim = -2)\n        rel_coors = self.norm_rel_coors(rel_coors)\n        coors_out = einsum('b i j h, b i j c -> b i c h', coor_attn, rel_coors)\n","AFTER":"        coors_mlp_input = rearrange(qk, 'b h i j -> b i j h')\n        coor_weights = self.coors_mlp(coors_mlp_input)\n\n        if exists(mask):\n            mask_value = max_neg_value(coor_weights)\n            coor_mask = rearrange(mask, 'b h i j -> b i j h')\n            coor_weights.masked_fill_(~coor_mask, mask_value)\n\n        coor_attn = coor_weights.softmax(dim = -2)\n\n        rel_coors_sign = rearrange(self.coors_gate(coors_mlp_input), 'b i j h -> b h i j ()')\n\n        rel_coors = self.norm_rel_coors(rel_coors)\n        rel_coors = repeat(rel_coors, 'b i j c -> b h i j c', h = h) * rel_coors_sign\n\n        coors_out = einsum('b i j h, b h i j c -> b i c h', coor_attn, rel_coors)\n"}