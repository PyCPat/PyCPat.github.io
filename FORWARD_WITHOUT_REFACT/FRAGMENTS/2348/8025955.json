{"BEFORE":"        token_embeddings = self.tok_emb(x)\r\n        t = token_embeddings.shape[1]\r\n        position_embeddings = self.pos_emb[:, :t, :]\r\n        embed = token_embeddings + position_embeddings\r\n        for enc_layer in self.EncoderLayers:\r\n            embed = enc_layer(embed)\r\n        tkn_prd = self.Token_Prediction(embed)\r\n        return tkn_prd\r\n","AFTER":"        token_embeddings = self.tok_emb(x)\r\n        t = token_embeddings.shape[1]\r\n        position_embeddings = self.pos_emb[:t, :]\r\n        # position_embeddings = self.pos_emb(x)\r\n        embed = self.drop(self.ln(token_embeddings + position_embeddings))\r\n        embed = self.blocks(embed)\r\n        embed = self.Token_Prediction(embed)\r\n        logits = torch.matmul(embed, self.tok_emb.weight.T) + self.bias\r\n\r\n        return logits\r\n"}