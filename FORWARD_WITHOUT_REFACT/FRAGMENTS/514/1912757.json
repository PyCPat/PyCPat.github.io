{"BEFORE":"        x = self._pos_embeding(inputs, x, self.pos_embed)\n        x = self.blocks(x)\n        x = self.norm(x)\n        B, _, C = x.shape\n        x = x.reshape(B, inputs.shape[2] \/\/ self.patch_size,\n                      inputs.shape[3] \/\/ self.patch_size,\n                      C).permute(0, 3, 1, 2)\n        return [x]\n","AFTER":"        B = inputs.shape[0]\n\n        x = self.patch_embed(inputs)\n\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = self._pos_embeding(inputs, x, self.pos_embed)\n\n        if not self.with_cls_token:\n            # Remove class token for transformer input\n            x = x[:, 1:]\n\n        outs = []\n        for i, blk in enumerate(self.blocks):\n            x = blk(x)\n            if i == len(self.blocks) - 1:\n                if self.final_norm:\n                    x = self.norm(x)\n            if i in self.out_indices:\n                if self.with_cls_token:\n                    # Remove class token and reshape token for decoder head\n                    out = x[:, 1:]\n                else:\n                    out = x\n                B, _, C = out.shape\n                out = out.reshape(B, inputs.shape[2] \/\/ self.patch_size,\n                                  inputs.shape[3] \/\/ self.patch_size,\n                                  C).permute(0, 3, 1, 2)\n                outs.append(out)\n\n        return tuple(outs)\n"}