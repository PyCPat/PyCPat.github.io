{"BEFORE":"        if n < seq_len:\n            padding = seq_len - n\n            mask = default(mask, lambda: torch.ones(b, n, device = device).bool())\n            x = F.pad(x, (0, 0, 0, padding), value = 0)\n            mask = F.pad(x, (0, padding), value = False)\n\n        qkv = self.to_qkv(x).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), qkv)\n\n        q *= self.scale\n\n        img_seq_len = img_size ** 2\n        text_len = seq_len - img_seq_len\n\n        ((q_text, q_img), (k_text, k_img), (v_text, v_img)) = map(lambda t: (t[:, img_seq_len:], t[:, -img_seq_len:]), (q, k, v))\n\n        # text attention\n\n        dots_text = einsum('b i d, b j d -> b i j', q_text, k_text)\n        mask_value = max_neg_value(dots_text)\n\n        i, j = dots_text.shape[-2:]\n        mask = torch.ones(i, j, device = device).triu_(j - i + 1).bool()\n        dots_text.masked_fill(mask, mask_value)\n\n        attn_text = dots_text.softmax(dim = -1)\n        out_text = einsum('b i j, b j d -> b i d', attn_text, v_text)\n\n        # image attention\n\n        split_axis_einops = 'b (h w) c -> (b h) w c' if axis == 0 else 'b (h w) c -> (b w) h c'\n        merge_axis_einops = '(b ax) n d -> b (ax n) d' if axis == 0 else '(b ax) n d -> b (n ax) d'\n\n        # split out axis\n\n        q_img, k_img, v_img = map(lambda t: rearrange(t, split_axis_einops, h = img_size), (q_img, k_img, v_img))\n\n        # prepare text key \/ values for the image tokens to attend to\n\n        k_text, v_text = map(lambda t: repeat(t, 'b n d -> (b ax) n d', ax = img_size), (k_text, v_text))\n        k_img = torch.cat((k_text, k_img), dim = 1)\n        v_img = torch.cat((v_text, v_img), dim = 1)\n\n        # similarity\n\n        dots_image = einsum('b i d, b j d -> b i j', q_img, k_img)\n\n        # mask so image has full attention to text, but causal along axis\n\n        i, j = dots_image.shape[-2:]\n        mask = torch.ones(i, j, device = device).triu_(j - i + 1).bool()\n","AFTER":"        img_seq_len = img_size ** 2\n        text_len = seq_len + 1 - img_seq_len\n\n        # padding\n\n        padding = seq_len - n + 1\n        mask = default(mask, lambda: torch.ones(b, text_len, device = device).bool())\n\n        x = F.pad(x, (0, 0, 0, padding), value = 0)\n        mask = mask[:, :text_len]\n\n        # derive queries \/ keys \/ values\n\n        qkv = self.to_qkv(x).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), qkv)\n\n        q *= self.scale\n\n        ((q_text, q_img), (k_text, k_img), (v_text, v_img)) = map(lambda t: (t[:, img_seq_len:], t[:, -img_seq_len:]), (q, k, v))\n\n        # text attention\n\n        dots_text = einsum('b i d, b j d -> b i j', q_text, k_text)\n        mask_value = max_neg_value(dots_text)\n\n        i, j = dots_text.shape[-2:]\n        text_causal_mask = torch.ones(i, j, device = device).triu_(j - i + 1).bool()\n        dots_text.masked_fill(text_causal_mask, mask_value)\n\n        attn_text = dots_text.softmax(dim = -1)\n        out_text = einsum('b i j, b j d -> b i d', attn_text, v_text)\n\n        # image attention\n\n        split_axis_einops = 'b (h w) c -> (b h) w c' if axis == 0 else 'b (h w) c -> (b w) h c'\n        merge_axis_einops = '(b ax) n d -> b (ax n) d' if axis == 0 else '(b ax) n d -> b (n ax) d'\n\n        # split out axis\n\n        q_img, k_img, v_img = map(lambda t: rearrange(t, split_axis_einops, h = img_size), (q_img, k_img, v_img))\n\n        # prepare text key \/ values for the image tokens to attend to\n\n        k_text, v_text = map(lambda t: repeat(t, 'b n d -> (b ax) n d', ax = img_size), (k_text, v_text))\n        k_img = torch.cat((k_text, k_img), dim = 1)\n        v_img = torch.cat((v_text, v_img), dim = 1)\n\n        # similarity\n\n        dots_image = einsum('b i d, b j d -> b i j', q_img, k_img)\n\n        # mask so image has full attention to text, but causal along axis\n\n        bh, i, j = dots_image.shape\n        causal_mask = torch.ones(i, img_size, device = device).triu_(img_size - i + 1).bool()\n        causal_mask = repeat(causal_mask, 'i j -> b i j', b = bh)\n\n        mask = repeat(mask, 'b j -> (b r) i j', r = (bh \/\/ b), i = i)\n        mask = torch.cat((~mask, causal_mask), dim = -1)\n"}