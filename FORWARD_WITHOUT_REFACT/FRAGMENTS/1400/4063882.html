<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    &#47&#47 Function which sends some input data through the network and returns the network&quots output. In this example, a ReLU activation function is used for both hidden layers, but the output layer has no activation function (it is just a linear layer).
    def forward(self, input):
        input = <a id="change">input.to(</a>self.device<a id="change">)</a>
        input = input.view(-1, self.layer_1.in_features)
        layer_1_output = nn.functional.relu(self.layer_1(input))
        layer_2_output = nn.functional.relu(self.layer_2(layer_1_output))
        output = self.output_layer(layer_2_output)</code></pre><h3>After Change</h3><pre><code class='java'>

    &#47&#47 Function which sends some input data through the network and returns the network&quots output. In this example, a ReLU activation function is used for both hidden layers, but the output layer has no activation function (it is just a linear layer).
    def forward(self, input):
        input = <a id="change">input.to(</a>self.device<a id="change">)/255.0</a>
        input = input.view(-1, self.layer_1.in_features)
        layer_1_output = nn.functional.relu(self.layer_1(input))
        layer_2_output = nn.functional.relu(self.layer_2(layer_1_output))
        output = self.output_layer(layer_2_output)</code></pre>