{"BEFORE":"        after_attn = self.multihead_attention(x, x, x, mask=encoder_mask) # (B, L, d_model) => (B, L, d_model)\r\n        after_norm_1 = self.layer_norm_1(after_attn + x) # (B, L, d_model) => (B, L, d_model)\r\n        after_ff = self.feed_forward(after_norm_1) # (B, L, d_model) => (B, L, d_ff) => (B, L, d_model)\r\n        after_norm_2 = self.layer_norm_2(after_ff + after_norm_1) # (B, L, d_model) => (B, L, d_model)\r\n\r\n        return after_norm_2\r\n","AFTER":"        after_norm_1 = self.layer_norm_1(x) # (B, L, d_model)\r\n        x += self.drop_out_1(self.multihead_attention(after_norm_1, after_norm_1, after_norm_1, mask=encoder_mask)) # (B, L, d_model)\r\n        after_norm_2 = self.layer_norm_2(x) # (B, L, d_model)\r\n        x += self.drop_out_2(self.feed_forward(after_norm_2)) # (B, L, d_model)\r\n\r\n        return self.layer_norm_3(x) # (B, L, d_model)\r\n"}