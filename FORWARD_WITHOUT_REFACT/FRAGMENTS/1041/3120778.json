{"BEFORE":"        with graph.local_scope():\r\n\r\n            if self._cached and self._cached_h is not None:\r\n                feat = self._cached_h\r\n            else:\r\n                # compute normalization\r\n                degs = graph.in_degrees().float().clamp(min=1)\r\n                norm = th.pow(degs, -0.5).to(feat.device).unsqueeze(1)\r\n\r\n                # compute (D^-0.5 * A * D^-0.5)^k X\r\n                for _ in range(self._k):\r\n                    feat = feat * norm\r\n                    graph.ndata['h'] = feat\r\n                    graph.update_all(fn.copy_src('h', 'm'),\r\n                                     fn.sum('m', 'h'))\r\n                    feat = graph.ndata.pop('h')\r\n                    feat = feat * norm\r\n\r\n                # cache feature\r\n                if self._cached:\r\n                    self._cached_h = feat\r\n\r\n            if weight is not None:\r\n                if self.weight is not None:\r\n                    raise DGLError('External weight is provided while at the same time the'\r\n                                   ' module has defined its own weight parameter. Please'\r\n                                   ' create the module with flag weight=False.')\r\n            else:\r\n                weight = self.weight\r\n\r\n            if weight is not None:\r\n                feat = th.matmul(feat, weight)\r\n\r\n            if self.bias is not None:\r\n                feat = feat + self.bias\r\n            return feat\r\n","AFTER":"        if self._cached and self._cached_h is not None:\r\n            feat = self._cached_h\r\n        else:\r\n            assert edge_weight is None or edge_weight.size(0) == graph.num_edges()\r\n\r\n            if self._add_self_loop:\r\n                graph = graph.add_self_loop()\r\n                if edge_weight is not None:\r\n                    size = (graph.num_nodes(),) + edge_weight.size()[1:]\r\n                    self_loop = edge_weight.new_ones(size)\r\n                    edge_weight = torch.cat([edge_weight, self_loop])\r\n            else:\r\n                graph = graph.local_var()\r\n\r\n            edge_weight = dgl_normalize(graph, self._norm, edge_weight)\r\n            graph.edata['_edge_weight'] = edge_weight\r\n\r\n            for _ in range(self._k):\r\n                graph.ndata['h'] = feat\r\n                graph.update_all(fn.u_mul_e('h', '_edge_weight', 'm'),\r\n                                 fn.sum('m', 'h'))\r\n                feat = graph.ndata.pop('h')\r\n\r\n            # cache feature\r\n            if self._cached:\r\n                self._cached_h = feat\r\n\r\n        return self.linear(feat)\r\n"}