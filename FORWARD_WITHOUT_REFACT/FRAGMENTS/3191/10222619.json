{"BEFORE":"        x = self.to_model_dim(x)\n        b, t, d = x.shape\n\n        assert t <= self.seq_len, f'input contains a sequence length {t} that is greater than the designated maximum sequence length {self.seq_len}'\n\n        mem = lmem = None\n        if memories is not None:\n            mem, lmem = memories\n\n        num_memory_layers = len(self.memory_layers)\n        mem = default(mem, lambda: torch.empty(num_memory_layers, b, 0, d, **to(x)))\n        lmem = default(lmem, lambda: torch.empty(num_memory_layers, b, 0, d, **to(x)))\n\n        total_len = mem.shape[2] + lmem.shape[2] + self.seq_len\n        pos_emb = self.pos_emb[:, (self.seq_len - t):total_len]\n\n        next_mem = []\n        next_lmem = []\n\n        mem_iter, lmem_iter = map(iterate_tensor, (mem, lmem))\n\n        for ind, (attn, ff) in enumerate(zip(self.attn_layers, self.ff_layers)):\n            layer_num = ind + 1\n            use_memory = layer_num in self.memory_layers\n\n            memories = None\n            if use_memory:\n                memories = (next(mem_iter), next(lmem_iter))\n\n            x, (mem_out, lmem_out) = attn(x, memories = memories, calc_memory = use_memory, input_mask = mask, pos_emb = pos_emb)\n            x, = ff(x)\n\n            if use_memory:\n                next_mem.append(mem_out)\n                next_lmem.append(lmem_out)\n\n        out = self.to_logits(x)\n\n        next_mem, next_lmem = map(torch.stack, (next_mem, next_lmem))\n        next_mem, next_lmem = map(torch.detach, (next_mem, next_lmem))\n\n        return out, Memory(short = next_mem, long = next_lmem)\n","AFTER":"        x = self.to_model_dim(x)\n        b, t, d = x.shape\n\n        assert t <= self.seq_len, f'input contains a sequence length {t} that is greater than the designated maximum sequence length {self.seq_len}'\n\n        memories = default(memories, (None, None))\n        mem, lmem = memories\n\n        num_memory_layers = len(self.memory_layers)\n        init_mem = lambda: torch.empty(num_memory_layers, b, 0, d, **to(x))\n\n        mem = default(mem, init_mem)\n        lmem = default(lmem, init_mem)\n\n        mem_len, lmem_len = map(lambda t: t.shape[2], (mem, lmem))\n        total_len = mem_len + lmem_len + self.seq_len\n\n        pos_emb = self.pos_emb[:, (self.seq_len - t):total_len]\n        mem_iter, lmem_iter = map(iterate_tensor, (mem, lmem))\n\n        hiddens = []\n\n        for ind, (attn, ff) in enumerate(zip(self.attn_layers, self.ff_layers)):\n            layer_num = ind + 1\n            use_memory = layer_num in self.memory_layers\n            memories = map(next, (mem_iter, lmem_iter)) if use_memory else None\n\n            if use_memory:\n                hiddens.append(x)\n\n            x = attn(x, memories = memories, calc_memory = use_memory, input_mask = mask, pos_emb = pos_emb)\n            x = ff(x)\n\n        hiddens = torch.stack(hiddens)\n        out = self.to_logits(x)\n\n        # calculate next memory state\n\n        next_memory = self.memory_network(lmem, mem, hiddens)\n        return out, next_memory\n"}