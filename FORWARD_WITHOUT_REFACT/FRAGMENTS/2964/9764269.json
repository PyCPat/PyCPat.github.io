{"BEFORE":"        q = self.to_q(x)\n        k, v = self.to_kv(context).chunk(2, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n\n        q, k = map(lambda t: self.qk_activation(t), (q, k))\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        if exists(mask) or exists(context_mask):\n            if not exists(mask):\n                mask = torch.ones(b, q.shape[-2], dtype = torch.bool, device = device)\n\n            if not exists(context_mask):\n                context_mask = torch.ones(b, k.shape[-2], dtype = torch.bool, device = device)\n","AFTER":"        null_k, null_v = map(lambda t: repeat(t, 'd -> b h () d', b = b, h = h), (self.null_key, self.null_value))\n        k = torch.cat((null_k, k), dim = -2)\n        v = torch.cat((null_v, v), dim = -2)\n\n        q, k = map(lambda t: self.qk_activation(t), (q, k))\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        if exists(mask) or exists(context_mask):\n            i, j = sim.shape[-2:]\n\n            if not exists(mask):\n                mask = torch.ones(b, i, dtype = torch.bool, device = device)\n\n            if exists(context_mask):\n                context_mask = F.pad(context_mask, (1, 0), value = True)\n            else:\n                context_mask = torch.ones(b, j, dtype = torch.bool, device = device)\n"}