<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            logits = [l.permute(1, 0, 2).reshape(n, h, w, k) for l in logits]
            &#47&#47 codes.append(samples.argmax(-1).permute(1, 0).reshape(n, h, w))
            &#47&#47 logits.append(logit.permute(1, 0, 2).reshape(n, h, w, k))
        <a id="change">return </a>quantizeds<a id="change">, codes, logits</a>


class TransformerQuantizer(nn.Module):
    def __init__(self, k: List[int], cin: int, rate: float = 0.1):</code></pre><h3>After Change</h3><pre><code class='java'>
    def forward(self, latents, temp, *_):
        quantizeds = list()
        codes = list()
        <a id="change">softQs</a><a id="change"> = </a><a id="change">list()</a>
        &#47&#47 logits = list()
        for _, (xRaw, k) in enumerate(zip(latents, self._k)):
            n, c, h, w = xRaw.shape
            &#47&#47 [n, c, h, w] -&gt; [h, w, n, c]
             *************** TODO: NEED DETACH? ******************* 
            &#47&#47 encoderIn = xRaw.detach().permute(2, 3, 0, 1)
            encoderIn = xRaw.permute(2, 3, 0, 1)
            &#47&#47 [h, w, n, c] -&gt; [h*w, n, c]
            x = encoderIn.reshape(-1, n ,c)
            &#47&#47 similar to scaled dot-product attention
            &#47&#47 [h*w, N, Cin],    M * [h*w, n, k]
            quantized, samples, softQ, logits = self._attention(x, temp, True)
            &#47&#47 [h*w, n, c] -&gt; [n, c, h*w] -&gt; [n, c, h, w]
            deTransformed = quantized.reshape(h, w, n, c).permute(2, 3, 0, 1)

            &#47&#47 mask = torch.rand_like(xRaw) &gt; coeff
            &#47&#47 mixed = mask * xRaw.detach() + torch.logical_not(mask) * deTransformed
            &#47&#47 [n, c, h, w]
            quantizeds.append(deTransformed)
            <a id="change">softQs.append(</a>softQ.reshape(h, w, n, c).permute(2, 3, 0, 1)<a id="change">)</a>
            samples = [s.argmax(-1).permute(1, 0).reshape(n, h, w) for s in samples]
            logits = [l.permute(1, 0, 2).reshape(n, h, w, k) for l in logits]
            &#47&#47 codes.append(samples.argmax(-1).permute(1, 0).reshape(n, h, w))
            &#47&#47 logits.append(logit.permute(1, 0, 2).reshape(n, h, w, k))
        <a id="change">return </a>quantizeds<a id="change">, softQs, codes, logits</a>


class TransformerQuantizer(nn.Module):
    def __init__(self, k: List[int], cin: int, rate: float = 0.1):</code></pre>