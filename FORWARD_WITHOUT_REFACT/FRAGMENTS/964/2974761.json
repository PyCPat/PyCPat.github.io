{"BEFORE":"        regs = list()\n        if logits is not None:\n            for logit in logits:\n                # N, H, W, K -> N, HW, K\n                batchWiseLogit = logit.reshape(len(logit), -1, logit.shape[-1])\n\n                # [n, k]\n                # summedProb = batchWiseLogit.mean(1).sigmoid()\n\n                # target = torch.ones_like(summedProb) \/ 2.0\n                # [n, ]\n                # reg = F.binary_cross_entropy(summedProb, target, reduction='none').sum(-1)\n\n                # var = batchWiseLogit.var(1).sum(-1)\n\n                # [n, k] -> [n, ]\n                # diversity = torch.minimum(var, torch.ones_like(var))\n                # reg -= diversity\n\n                # diversity = batchWiseLogit.std(1).mean(-1).sigmoid()\n\n                # summedProb = batchWiseLogit.sum(1)\n                posterior = OneHotCategorical(logits=batchWiseLogit)\n                prior = OneHotCategorical(probs=torch.ones_like(batchWiseLogit) \/ batchWiseLogit.shape[-1])\n                reg = torch.distributions.kl_divergence(posterior, prior).sum(-1)\n                reg += compute_penalties(batchWiseLogit, allowed_entropy=0.1, individual_entropy_coeff=1.0, allowed_js=4.0, js_coeff=1.0, cv_coeff=1.0, eps=Consts.Eps)\n                # reg = reg \/ diversity\n                regs.append(reg)\n","AFTER":"        l2QLoss = list()\n        l1QLoss = list()\n        regs = list()\n        for latent, q in zip(latents, quantizeds):\n            l2QLoss.append(F.mse_loss(latent.detach(), q, reduction='none').mean(axis=(1, 2, 3)))\n            l1QLoss.append(F.l1_loss(latent.detach(), q, reduction='none').mean(axis=(1, 2, 3)))\n            l2QLoss.append(0.25 * F.mse_loss(latent, q.detach(), reduction='none').mean(axis=(1, 2, 3)))\n            l1QLoss.append(0.25 * F.l1_loss(latent, q.detach(), reduction='none').mean(axis=(1, 2, 3)))\n            regs.append(-1e-4 * ((latent ** 2).mean((1, 2, 3)) + (q ** 2).mean((1, 2, 3))))\n\n        l1QLoss = sum(l1QLoss)\n        l2QLoss = sum(l2QLoss)\n\n        if logits is not None:\n            for logit in logits:\n                # N, H, W, K -> N, HW, K\n                batchWiseLogit = logit.reshape(len(logit), -1, logit.shape[-1])\n\n                # [n, k]\n                # summedProb = batchWiseLogit.mean(1).sigmoid()\n\n                # target = torch.ones_like(summedProb) \/ 2.0\n                # [n, ]\n                # reg = F.binary_cross_entropy(summedProb, target, reduction='none').sum(-1)\n\n                # var = batchWiseLogit.var(1).sum(-1)\n\n                # [n, k] -> [n, ]\n                # diversity = torch.minimum(var, torch.ones_like(var))\n                # reg -= diversity\n\n                # diversity = batchWiseLogit.std(1).mean(-1).sigmoid()\n\n                # summedProb = batchWiseLogit.sum(1)\n                posterior = OneHotCategorical(logits=batchWiseLogit)\n                prior = OneHotCategorical(probs=torch.ones_like(batchWiseLogit) \/ batchWiseLogit.shape[-1])\n                regs.append(torch.distributions.kl_divergence(posterior, prior).sum(-1) + compute_penalties(batchWiseLogit, allowed_entropy=0.1, individual_entropy_coeff=1.0, allowed_js=4.0, js_coeff=1.0, cv_coeff=1.0, eps=Consts.Eps))\n"}