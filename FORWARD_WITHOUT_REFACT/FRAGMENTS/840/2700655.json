{"BEFORE":"        get_attn = lambda: SinkhornSelfAttention(dim, causal = causal, heads = heads, buckets = buckets, kv_buckets = kv_buckets, non_permutative = non_permutative, sinkhorn_iter = sinkhorn_iter, n_sortcut = n_sortcut, temperature = temperature, attn_dropout = attn_dropout, dropout = attn_layer_dropout, attn_sort_net = attn_sort_net)\n        get_ff = lambda: FeedForward(dim, dropout = ff_dropout, glu = ff_glu)\n\n        if weight_tie:\n            get_attn = cache_fn(get_attn)\n            get_ff = cache_fn(get_ff)\n\n        for _ in range(depth):\n","AFTER":"    def __init__(self, dim, depth, causal = False, heads = 8, buckets = 64, kv_buckets = None, non_permutative = False, sinkhorn_iter = 5, n_sortcut = 0, temperature = 0.75, reversible = False, ff_chunks = 1, ff_dropout = 0., attn_dropout = 0., attn_layer_dropout = 0., weight_tie = False, ff_glu = False, attn_sort_net = False, receives_context = False):\n        super().__init__()\n        layers = nn.ModuleList([])\n\n        get_attn = lambda: SinkhornSelfAttention(dim, causal = causal, heads = heads, buckets = buckets, kv_buckets = kv_buckets, non_permutative = non_permutative, sinkhorn_iter = sinkhorn_iter, n_sortcut = n_sortcut, temperature = temperature, attn_dropout = attn_dropout, dropout = attn_layer_dropout, attn_sort_net = attn_sort_net)\n        get_ff = lambda: FeedForward(dim, dropout = ff_dropout, glu = ff_glu)\n\n        get_attn_context = lambda: SinkhornSelfAttention(dim, context_only = True, heads = heads, buckets = buckets, kv_buckets = kv_buckets, non_permutative = non_permutative, sinkhorn_iter = sinkhorn_iter, n_sortcut = n_sortcut, temperature = temperature, attn_dropout = attn_dropout, dropout = attn_layer_dropout, attn_sort_net = attn_sort_net)\n        get_ff_context = lambda: FeedForward(dim, dropout = ff_dropout, glu = ff_glu)\n\n        if weight_tie:\n            get_attn, get_attn_context, get_ff, get_ff_context = map(cache_fn, (get_attn, get_attn_context, get_ff, get_ff_context))\n\n        for _ in range(depth):\n            layers.append(nn.ModuleList([\n                PreNorm(nn.LayerNorm, dim, get_attn()),\n                PreNorm(nn.LayerNorm, dim, Chunk(ff_chunks, get_ff(), along_dim=1))\n            ]))\n\n            if receives_context:\n                layers.append(nn.ModuleList([\n                    PreNorm(nn.LayerNorm, dim, get_attn_context()),\n                    PreNorm(nn.LayerNorm, dim, Chunk(ff_chunks, get_ff_context(), along_dim=1))\n                ]))\n\n        execute_type = Reversible if reversible else Sequential\n\n        context_map = ((False, False), (True, False)) * depth\n        args_route = {'context': context_map} if receives_context else {}\n        self.layers = execute_type(layers, args_route = args_route)\n        self.receives_context = receives_context\n"}