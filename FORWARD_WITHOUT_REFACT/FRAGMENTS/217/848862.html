<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.ff_layers = nn.ModuleList([PreNorm(dim, FeedForward(dim)) for _ in range(depth)])

    def forward(self, x, **kwargs):
        <a id="change">for </a>attn, <a id="change">ff</a> in zip(self.attn_layers, self.ff_layers)<a id="change">:
            </a>x = attn(x, **kwargs) + x
            x<a id="change"> = </a>ff(x) + x
        return x

class LinearAttentionTransformerLM(nn.Module):</code></pre><h3>After Change</h3><pre><code class='java'>
    def forward(self, x, **kwargs):
        x = torch.cat([x, x], dim = -1)
        x = self.layers(x, **kwargs)
        return <a id="change">torch.stack(x.chunk(2, dim=-1)).mean(dim=0)</a>

class LinearAttentionTransformerLM(nn.Module):
    def __init__(self, num_tokens, dim, depth, max_seq_len, heads = 8, causal = False, one_kv_head = False):
        super().__init__()</code></pre>