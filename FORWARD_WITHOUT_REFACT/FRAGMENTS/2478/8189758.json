{"BEFORE":"        x = self.token_emb(x)\n        out = self.layers(x)\n        out = self.to_logits(x)\n        return out\n","AFTER":"    def forward(self, x, mem = None):\n        x = self.token_emb(x)\n        b, t, d = x.shape\n\n        mem = default(mem, torch.empty(self.depth, b, 0, d))\n        hidden_states = []\n\n        for attn, ff, m in zip(self.attn_layers, self.ff_layers, mem):\n            hidden_states.append(x)\n            x = attn(x, mem = m)\n            x = ff(x)\n\n        out = self.to_logits(x)\n\n        hidden_states = torch.stack(hidden_states)\n        new_mem = torch.cat((mem, hidden_states), dim=2)[:, :, -self.mem_len:, :].detach()\n        return out, new_mem\n"}