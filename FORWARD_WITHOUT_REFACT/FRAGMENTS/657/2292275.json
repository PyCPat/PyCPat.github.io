{"BEFORE":"        feat_projection = self.feat_conv(features)\n        # shape (N, rnn_units, 1, 1) -> (N, attention_units, 1, 1)\n        state_projection = self.state_conv(hidden_state)\n        projection = torch.tanh(feat_projection + state_projection)\n        # shape (N, attention_units, H, W) -> (N, 1, H, W)\n        attention = self.attention_projector(projection)\n        # shape (N, 1, H, W) -> (N, H * W)\n        attention = torch.flatten(attention, 1)\n        # shape (N, H * W) -> (N, 1, H, W)\n        attention = torch.softmax(attention, 1).reshape(-1, 1, features.shape[-2], features.shape[-1])\n\n        glimpse = (features * attention).sum(dim=(2, 3))\n\n        return glimpse\n","AFTER":"        feat_projection = self.feat_conv(features)\n        # shape (N, L, rnn_units) -> (N, L, attention_units)\n        state_projection = self.state_conv(hidden_state).unsqueeze(-1).unsqueeze(-1)\n        # (N, L, attention_units, H, W)\n        projection = torch.tanh(feat_projection.unsqueeze(1) + state_projection)\n        # (N, L, H, W, 1)\n        attention = self.attention_projector(projection.permute(0, 1, 3, 4, 2))\n        # shape (N, L, H, W, 1) -> (N, L, H * W)\n        attention = torch.flatten(attention, 2)\n        attention = torch.softmax(attention, -1)\n        # shape (N, L, H * W) -> (N, L, 1, H, W)\n        attention = attention.reshape(-1, hidden_state.shape[1], features.shape[-2], features.shape[-1])\n\n        # (N, L, C)\n        return (features.unsqueeze(1) * attention.unsqueeze(2)).sum(dim=(3, 4))\n"}