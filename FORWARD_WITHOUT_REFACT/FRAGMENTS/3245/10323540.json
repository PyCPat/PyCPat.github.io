{"BEFORE":"        rel_coors = rearrange(coors, 'b i d -> b i () d') - rearrange(coors, 'b j d -> b () j d')\n        rel_dist = rel_coors.norm(p = 2, dim = -1)\n\n        # calculate neighborhood indices\n\n        nbhd_indices = None\n        nbhd_masks = None\n        nbhd_ranking = rel_dist.clone()\n\n        if exists(adj_mat):\n            if len(adj_mat.shape) == 2:\n                adj_mat = repeat(adj_mat, 'i j -> b i j', b = b)\n\n            self_mask = torch.eye(n, device = device).bool()\n            self_mask = rearrange(self_mask, 'i j -> () i j')\n            adj_mat.masked_fill_(self_mask, False)\n\n            max_adj_neighbors = adj_mat.long().sum(dim = -1).max().item() + 1\n\n            num_nn = max_adj_neighbors if only_sparse_neighbors else (num_nn + max_adj_neighbors)\n            valid_neighbor_radius = 0 if only_sparse_neighbors else valid_neighbor_radius\n\n            nbhd_ranking = nbhd_ranking.masked_fill(self_mask, -1.)\n            nbhd_ranking = nbhd_ranking.masked_fill(adj_mat, 0.)\n\n        if num_nn > 0:\n            # make sure padding does not end up becoming neighbors\n            if exists(mask):\n                ranking_mask = mask[:, :, None] * mask[:, None, :]\n                nbhd_ranking = nbhd_ranking.masked_fill(~ranking_mask, 1e5)\n\n            nbhd_values, nbhd_indices = nbhd_ranking.topk(num_nn, dim = -1, largest = False)\n            nbhd_masks = nbhd_values <= valid_neighbor_radius\n\n        # derive queries keys and values\n\n        q, k, v = self.to_qkv(feats).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n\n        # calculate nearest neighbors\n\n        i = j = n\n\n        if exists(nbhd_indices):\n            i, j = nbhd_indices.shape[-2:]\n            nbhd_indices_with_heads = repeat(nbhd_indices, 'b n d -> b h n d', h = h)\n            k         = batched_index_select(k, nbhd_indices_with_heads, dim = 2)\n            v         = batched_index_select(v, nbhd_indices_with_heads, dim = 2)\n            rel_dist  = batched_index_select(rel_dist, nbhd_indices, dim = 2)\n            rel_coors = batched_index_select(rel_coors, nbhd_indices, dim = 2)\n        else:\n            k = repeat(k, 'b h j d -> b h n j d', n = n)\n            v = repeat(v, 'b h j d -> b h n j d', n = n)\n\n        # prepare mask\n\n        if exists(mask):\n            q_mask = rearrange(mask, 'b i -> b () i ()')\n            k_mask = repeat(mask, 'b j -> b i j', i = n)\n\n            if exists(nbhd_indices):\n                k_mask = batched_index_select(k_mask, nbhd_indices, dim = 2)\n\n            k_mask = rearrange(k_mask, 'b i j -> b () i j')\n\n            mask = q_mask * k_mask\n\n            if exists(nbhd_masks):\n                mask &= rearrange(nbhd_masks, 'b i j -> b () i j')\n\n        # generate and apply rotary embeddings\n\n        rot_null = torch.zeros_like(rel_dist)\n\n        q_pos_emb_rel_dist = self.rotary_emb(torch.zeros(n, device = device))\n        k_pos_emb_rel_dist = self.rotary_emb(rel_dist * 1e2)\n\n        q_pos_emb = repeat(q_pos_emb_rel_dist, 'i d -> b () i d', b = b)\n        k_pos_emb = repeat(k_pos_emb_rel_dist, 'b i j d -> b () i j d')\n\n        if exists(self.rotary_emb_seq):\n            pos_emb = self.rotary_emb_seq(torch.arange(n, device = device))\n\n            q_pos_emb_seq = repeat(pos_emb, 'n d -> b () n d', b = b)\n            k_pos_emb_seq = repeat(pos_emb, 'n d -> b () n j d', b = b, j = j)\n\n            q_pos_emb = safe_cat(q_pos_emb, q_pos_emb_seq, dim = -1)\n            k_pos_emb = safe_cat(k_pos_emb, k_pos_emb_seq, dim = -1)\n\n        q = apply_rotary_pos_emb(q, q_pos_emb)\n        k = apply_rotary_pos_emb(k, k_pos_emb)\n        v = apply_rotary_pos_emb(v, k_pos_emb)\n\n        # calculate inner product for queries and keys\n\n        qk = einsum('b h i d, b h i j d -> b h i j', q, k) * self.scale\n\n        # add edge information and pass through edges MLP if needed\n\n        if exists(edges):\n            if exists(nbhd_indices):\n                edges = batched_index_select(edges, nbhd_indices, dim = 2)\n\n            qk = rearrange(qk, 'b h i j -> b i j h')\n            qk = torch.cat((qk, edges), dim = -1)\n            qk = self.edge_mlp(qk)\n            qk = rearrange(qk, 'b i j h -> b h i j')\n\n        # coordinate MLP and calculate coordinate updates\n\n        coors_mlp_input = rearrange(qk, 'b h i j -> b i j h')\n        coor_weights = self.coors_mlp(coors_mlp_input)\n\n        if exists(mask):\n            coor_weights.masked_fill_(~mask.squeeze(1), 0.)\n\n        rel_coors = self.rel_coors_norm(rel_coors)\n\n        coors_out = einsum('b i j, b i j c -> b i c', coor_weights, rel_coors)\n\n        # derive attention\n\n        sim = qk.clone()\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n","AFTER":"        rel_coors = rearrange(coors, 'b i d -> b i () d') - rearrange(coors, 'b j d -> b () j d')\n        rel_dist = rel_coors.norm(p = 2, dim = -1)\n\n        # calculate neighborhood indices\n\n        nbhd_indices = None\n        nbhd_masks = None\n        nbhd_ranking = rel_dist.clone()\n\n        if exists(adj_mat):\n            if len(adj_mat.shape) == 2:\n                adj_mat = repeat(adj_mat, 'i j -> b i j', b = b)\n\n            self_mask = torch.eye(n, device = device).bool()\n            self_mask = rearrange(self_mask, 'i j -> () i j')\n            adj_mat.masked_fill_(self_mask, False)\n\n            max_adj_neighbors = adj_mat.long().sum(dim = -1).max().item() + 1\n\n            num_nn = max_adj_neighbors if only_sparse_neighbors else (num_nn + max_adj_neighbors)\n            valid_neighbor_radius = 0 if only_sparse_neighbors else valid_neighbor_radius\n\n            nbhd_ranking = nbhd_ranking.masked_fill(self_mask, -1.)\n            nbhd_ranking = nbhd_ranking.masked_fill(adj_mat, 0.)\n\n        if num_nn > 0:\n            # make sure padding does not end up becoming neighbors\n            if exists(mask):\n                ranking_mask = mask[:, :, None] * mask[:, None, :]\n                nbhd_ranking = nbhd_ranking.masked_fill(~ranking_mask, 1e5)\n\n            nbhd_values, nbhd_indices = nbhd_ranking.topk(num_nn, dim = -1, largest = False)\n            nbhd_masks = nbhd_values <= valid_neighbor_radius\n\n        # derive queries keys and values\n\n        q, k, v = self.to_qkv(feats).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n\n        # calculate nearest neighbors\n\n        i = j = n\n\n        if exists(nbhd_indices):\n            i, j = nbhd_indices.shape[-2:]\n            nbhd_indices_with_heads = repeat(nbhd_indices, 'b n d -> b h n d', h = h)\n            k         = batched_index_select(k, nbhd_indices_with_heads, dim = 2)\n            v         = batched_index_select(v, nbhd_indices_with_heads, dim = 2)\n            rel_dist  = batched_index_select(rel_dist, nbhd_indices, dim = 2)\n            rel_coors = batched_index_select(rel_coors, nbhd_indices, dim = 2)\n        else:\n            k = repeat(k, 'b h j d -> b h n j d', n = n)\n            v = repeat(v, 'b h j d -> b h n j d', n = n)\n\n        # prepare mask\n\n        if exists(mask):\n            q_mask = rearrange(mask, 'b i -> b () i ()')\n            k_mask = repeat(mask, 'b j -> b i j', i = n)\n\n            if exists(nbhd_indices):\n                k_mask = batched_index_select(k_mask, nbhd_indices, dim = 2)\n\n            k_mask = rearrange(k_mask, 'b i j -> b () i j')\n\n            mask = q_mask * k_mask\n\n            if exists(nbhd_masks):\n                mask &= rearrange(nbhd_masks, 'b i j -> b () i j')\n\n        # generate and apply rotary embeddings\n\n        rot_null = torch.zeros_like(rel_dist)\n\n        q_pos_emb_rel_dist = self.rotary_emb(torch.zeros(n, device = device))\n        k_pos_emb_rel_dist = self.rotary_emb(rel_dist * 1e2)\n\n        q_pos_emb = repeat(q_pos_emb_rel_dist, 'i d -> b () i d', b = b)\n        k_pos_emb = repeat(k_pos_emb_rel_dist, 'b i j d -> b () i j d')\n\n        if exists(self.rotary_emb_seq):\n            pos_emb = self.rotary_emb_seq(torch.arange(n, device = device))\n\n            q_pos_emb_seq = repeat(pos_emb, 'n d -> b () n d', b = b)\n            k_pos_emb_seq = repeat(pos_emb, 'n d -> b () n j d', b = b, j = j)\n\n            q_pos_emb = safe_cat(q_pos_emb, q_pos_emb_seq, dim = -1)\n            k_pos_emb = safe_cat(k_pos_emb, k_pos_emb_seq, dim = -1)\n\n        q = apply_rotary_pos_emb(q, q_pos_emb)\n        k = apply_rotary_pos_emb(k, k_pos_emb)\n        v = apply_rotary_pos_emb(v, k_pos_emb)\n\n        # calculate inner product for queries and keys\n\n        qk = einsum('b h i d, b h i j d -> b h i j', q, k) * (self.scale if not exists(edges) else 1)\n\n        # add edge information and pass through edges MLP if needed\n\n        if exists(edges):\n            if exists(nbhd_indices):\n                edges = batched_index_select(edges, nbhd_indices, dim = 2)\n\n            qk = rearrange(qk, 'b h i j -> b i j h')\n            qk = torch.cat((qk, edges), dim = -1)\n            qk = self.edge_mlp(qk)\n            qk = rearrange(qk, 'b i j h -> b h i j')\n\n        # coordinate MLP and calculate coordinate updates\n\n        coors_mlp_input = rearrange(qk, 'b h i j -> b i j h')\n        coor_weights = self.coors_mlp(coors_mlp_input)\n\n        if exists(mask):\n            mask_value = max_neg_value(coor_weights)\n            coor_mask = repeat(mask, 'b () i j -> b i j ()')\n            coor_weights.masked_fill_(~coor_mask, mask_value)\n\n        coor_attn = coor_weights.softmax(dim = -2)\n\n        rel_coors_sign = self.coors_gate(coors_mlp_input)\n        rel_coors_sign = rearrange(rel_coors_sign, 'b i j h -> b i j () h')\n\n        rel_coors = self.norm_rel_coors(rel_coors)\n\n        rel_coors = repeat(rel_coors, 'b i j c -> b i j c h', h = h)\n\n        rel_coors = rel_coors * rel_coors_sign\n\n        coors_out = einsum('b i j h, b i j c h, h -> b i c', coor_attn, rel_coors, self.coors_combine)\n\n        # derive attention\n\n        sim = qk.clone()\n\n        if exists(mask):\n            mask_value = max_neg_value(sim)\n"}