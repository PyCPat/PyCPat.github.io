{"BEFORE":"    def __init__(self,\n                 input_dim,\n                 hidden_dim,\n                 embed_dim,\n                 num_layers: int,\n                 act: str = 'ReLU',\n                 bn: bool = False,\n                 end_up_with_fc=False,\n                 bias=True):\n        super(MLP, self).__init__()\n        self.module_list = []\n        for i in range(num_layers):\n            d_in = input_dim if i == 0 else hidden_dim\n            d_out = embed_dim if i == num_layers - 1 else hidden_dim\n            self.module_list.append(nn.Linear(d_in, d_out, bias=bias))\n            if end_up_with_fc:\n                continue\n            if bn:\n                self.module_list.append(nn.BatchNorm1d(d_out))\n            self.module_list.append(getattr(nn, act)(True))\n        self.module_list = nn.Sequential(*self.module_list)\n","AFTER":"    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3):\n        super(MLP, self).__init__()\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.num_layers = num_layers\n\n        self.lins = nn.ModuleList()\n        self.bns = nn.ModuleList()\n\n        self.lins.append(nn.Linear(input_dim, hidden_dim))\n        self.bns.append(nn.BatchNorm1d(hidden_dim))\n\n        for i in range(self.num_layers - 2):\n            self.lins.append(nn.Linear(hidden_dim, hidden_dim))\n            self.bns.append(nn.BatchNorm1d(hidden_dim))\n\n        self.lins.append(nn.Linear(hidden_dim, output_dim))\n"}