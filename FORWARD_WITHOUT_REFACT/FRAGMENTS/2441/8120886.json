{"BEFORE":"        flatten = input.reshape(-1, self.dim)\n","AFTER":"        input = self.project_in(input)\n\n        if not self.initted:\n            self.init_embed_(input)\n\n        dtype = input.dtype\n        flatten = rearrange(input, '... d -> (...) d')\n        dist = (\n            flatten.pow(2).sum(1, keepdim=True)\n            - 2 * flatten @ self.embed\n            + self.embed.pow(2).sum(0, keepdim=True)\n        )\n\n        _, embed_ind = (-dist).max(1)\n        embed_onehot = F.one_hot(embed_ind, self.n_embed).type(dtype)\n        embed_ind = embed_ind.view(*input.shape[:-1])\n\n        commit_loss = 0.\n        quantize = F.embedding(embed_ind, self.embed.transpose(0, 1))\n\n        if self.training:\n            ema_inplace(self.cluster_size, embed_onehot.sum(0), self.decay)\n            embed_sum = flatten.transpose(0, 1) @ embed_onehot\n            ema_inplace(self.embed_avg, embed_sum, self.decay)\n            cluster_size = laplace_smoothing(self.cluster_size, self.n_embed, self.eps) * self.cluster_size.sum()\n            embed_normalized = self.embed_avg \/ cluster_size.unsqueeze(0)\n            self.embed.data.copy_(embed_normalized)\n\n            commit_loss = F.mse_loss(quantize.detach(), input) * self.commitment\n            quantize = input + (quantize - input).detach()\n\n        quantize = self.project_out(quantize)\n"}