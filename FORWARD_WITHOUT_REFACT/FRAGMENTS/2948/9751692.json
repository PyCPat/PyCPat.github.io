{"BEFORE":"        mem = default(mem, lambda: torch.empty(self.depth, b, 0, d))\n        cmem = default(cmem, lambda: torch.empty(self.depth, b, 0, d))\n\n        total_len = mem.shape[2] + cmem.shape[2] + t\n        pos_emb = self.pos_emb[:, (self.seq_len - t):total_len]\n\n        next_mem = []\n        next_cmem = []\n        aux_loss = torch.zeros(1, requires_grad = True, **to(x))\n\n        for attn, ff, m, c in zip(self.attn_layers, self.ff_layers, mem, cmem):\n","AFTER":"        mem = default(mem, lambda: torch.empty(self.depth, b, 0, d))\n        cmem = default(cmem, lambda: torch.empty(self.depth, b, 0, d))\n\n        total_len = mem.shape[2] + cmem.shape[2] + t\n        pos_emb = self.pos_emb[:, (self.seq_len - t):total_len]\n\n        next_mem = []\n        next_cmem = []\n        aux_loss = torch.zeros(1, requires_grad = True, **to(x))\n\n        for ind, (attn, ff, m, c) in enumerate(zip(self.attn_layers, self.ff_layers, mem, cmem)):\n            layer_num = ind + 1\n            use_memory = layer_num in self.use_memory_layers\n"}