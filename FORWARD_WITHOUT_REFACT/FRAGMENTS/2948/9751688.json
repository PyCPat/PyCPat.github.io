{"BEFORE":"        mask = segment_fn(mask) if mask is not None else ((None,) * num_segments)\n\n        max_batch_size = x.shape[0] if max_batch_size is None else max_batch_size\n        split_batch_fn = lambda x: x.split(max_batch_size, dim=0)\n\n        grad_accumulate_every = math.ceil(x.shape[0] \/ max_batch_size)\n        mems = [None] * grad_accumulate_every\n\n        for xi_seg, xo_seg, mask_seg in zip(xi, xo, mask):\n","AFTER":"    def forward(self, x, max_batch_size = None, return_loss = False, truncate_every = None, **kwargs):\n        pad = partial(pad_sequence, batch_first = True, padding_value = self.pad_value)\n\n        if not return_loss:\n            if not isinstance(x, torch.Tensor):\n                x = pad(x)\n            return self.net(x, **kwargs)\n\n        if isinstance(x, torch.Tensor):\n            xi = x[:, :-1]\n            xo = x[:, 1:]\n        else:\n            xi = pad(list(map(lambda t: t[:-1], x)))\n            xo = pad(list(map(lambda t: t[1:], x)))\n\n        # help auto-solve an area of confusion around input masks in auto-regressive\n        # if user supplies a mask that is only off by one from the source sequence, resolve it for them\n        mask = kwargs.pop('mask', None)\n        if mask is not None and mask.shape[1] == x.shape[1]:\n            mask = mask[:, :-1]\n\n        segment_fn = lambda x: x.split(self.seq_len, dim=1)\n        (xi, xo) = map(segment_fn, (xi, xo))\n\n        num_segments = len(xi)\n        mask = segment_fn(mask) if mask is not None else ((None,) * num_segments)\n\n        max_batch_size = x.shape[0] if max_batch_size is None else max_batch_size\n        split_batch_fn = lambda x: x.split(max_batch_size, dim=0)\n\n        grad_accumulate_every = math.ceil(x.shape[0] \/ max_batch_size)\n        mems = [None] * grad_accumulate_every\n\n        for ind, (xi_seg, xo_seg, mask_seg) in enumerate(zip(xi, xo, mask)):\n            xi_seg, xo_seg = map(split_batch_fn, (xi_seg, xo_seg))\n            mask_seg = split_batch_fn(mask_seg) if mask_seg is not None else ((None,) * grad_accumulate_every)\n            truncate = truncate_every is not None and ((ind + 1) % truncate_every) == 0\n"}