{"BEFORE":"        mol_graph, semiF_features = mol_graph\n        f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope = mol_graph.get_components()\n\n        if next(self.parameters()).is_cuda:\n            f_atoms, f_bonds, a2b, b2a, b2revb = f_atoms.cuda(), f_bonds.cuda(), a2b.cuda(), b2a.cuda(), b2revb.cuda()\n\n        # Input\n        b_input = self.W_i(f_bonds)  # num_bonds x hidden_size\n        b_message = self.act_func(b_input)  # num_bonds x hidden_size\n\n        if self.message_attention:\n            b2b = mol_graph.get_b2b()  # Warning: this is O(n_atoms^3) when using virtual edges\n\n            if next(self.parameters()).is_cuda:\n                b2b = b2b.cuda()\n\n            message_attention_mask = (b2b != 0).float()  # num_bonds x max_num_bonds\n\n        if self.global_attention:\n            global_attention_mask = torch.zeros(mol_graph.n_bonds, mol_graph.n_bonds)  # num_bonds x num_bonds\n\n            for start, length in b_scope:\n                for i in range(start, start + length):\n                    global_attention_mask[i, start:start + length] = 1\n\n            if next(self.parameters()).is_cuda:\n                global_attention_mask = global_attention_mask.cuda()\n\n        # Message passing\n        for depth in range(self.depth - 1):\n            if self.message_attention:\n                # TODO: Parallelize attention heads\n                nei_b_message = index_select_ND(b_message, b2b)\n                b_message = b_message.unsqueeze(1).repeat((1, nei_b_message.size(1), 1))  # num_bonds x maxnb x hidden\n                attention_scores = [(self.W_ma[i](nei_b_message) * b_message).sum(dim=2)\n                                    for i in range(self.num_heads)]  # num_bonds x maxnb\n                attention_scores = [attention_scores[i] * message_attention_mask + (1 - message_attention_mask) * (-1e+20)\n                                    for i in range(self.num_heads)]  # num_bonds x maxnb\n                attention_weights = [F.softmax(attention_scores[i], dim=1)\n                                     for i in range(self.num_heads)]  # num_bonds x maxnb\n                message_components = [nei_b_message * attention_weights[i].unsqueeze(2).repeat((1, 1, self.hidden_size))\n                                      for i in range(self.num_heads)]  # num_bonds x maxnb x hidden\n                message_components = [component.sum(dim=1) for component in message_components]  # num_bonds x hidden\n                b_message = torch.cat(message_components, dim=1)  # num_bonds x num_heads * hidden\n            else:\n                # m(a1 -> a2) = [sum_{a0 \\in nei(a1)} m(a0 -> a1)] - m(a2 -> a1)\n                # b_message      a_message = sum(nei_a_message)      rev_b_message\n                nei_a_message = index_select_ND(b_message, a2b)  # num_atoms x max_num_bonds x hidden\n                a_message = nei_a_message.sum(dim=1)  # num_atoms x hidden\n                rev_b_message = b_message[b2revb]  # num_bonds x hidden\n                b_message = a_message[b2a] - rev_b_message  # num_bonds x hidden\n\n            b_message = self.W_h(b_message)  # num_bonds x hidden\n\n            if self.master_node:\n                # master_state = self.W_master_in(self.act_func(nei_message.sum(dim=0))) #try something like this to preserve invariance for master node\n                # master_state = self.GRU_master(nei_message.unsqueeze(1))\n                # master_state = master_state[-1].squeeze(0) #this actually doesn't preserve order invariance anymore\n                mol_vecs = [self.cached_zero_vector]\n                for start, size in b_scope:\n                    if size == 0:\n                        continue\n                    mol_vec = b_message.narrow(0, start, size)\n                    mol_vec = mol_vec.sum(dim=0) \/ size\n                    mol_vecs += [mol_vec for _ in range(size)]\n                master_state = self.act_func(self.W_master_in(torch.stack(mol_vecs, dim=0)))  # num_bonds x hidden_size\n                b_message = self.act_func(b_input + b_message + self.W_master_out(master_state))  # num_bonds x hidden_size\n            else:\n                b_message = self.act_func(b_input + b_message)  # num_bonds x hidden_size\n\n            if self.global_attention:\n                attention_scores = torch.matmul(self.W_ga1(b_message), b_message.t())  # num_bonds x num_bonds\n                attention_scores = attention_scores * global_attention_mask + (1 - global_attention_mask) * (-1e+20)  # num_bonds x num_bonds\n                attention_weights = F.softmax(attention_scores, dim=1)  # num_bonds x num_bonds\n                attention_hiddens = torch.matmul(attention_weights, b_message)  # num_bonds x hidden_size\n                attention_hiddens = self.act_func(self.W_ga2(attention_hiddens))  # num_bonds x hidden_size\n                attention_hiddens = self.dropout_layer(attention_hiddens)  # num_bonds x hidden_size\n                b_message = b_message + attention_hiddens  # num_bonds x hidden_size\n\n                if viz_dir is not None:\n                    visualize_attention(viz_dir, mol_graph, attention_weights, depth)\n\n            if self.use_layer_norm:\n                b_message = self.layer_norm(b_message)\n\n            b_message = self.dropout_layer(b_message)  # num_bonds x hidden\n\n        if self.master_node and self.use_master_as_output:\n            assert self.hidden_size == self.master_dim\n            mol_vecs = []\n            for start, size in b_scope:\n                if size == 0:\n                    mol_vecs.append(self.cached_zero_vector)\n                else:\n                    mol_vecs.append(master_state[start])\n            return torch.stack(mol_vecs, dim=0)\n\n        # Get atom hidden states from message hidden states\n        nei_a_message = index_select_ND(b_message, a2b)  # num_atoms x max_num_bonds x hidden\n        a_message = nei_a_message.sum(dim=1)  # num_atoms x hidden\n        a_input = torch.cat([f_atoms, a_message], dim=1)  # num_atoms x (atom_fdim + hidden)\n        atom_hiddens = self.act_func(self.W_o(a_input))  # num_atoms x hidden\n        atom_hiddens = self.dropout_layer(atom_hiddens)  # num_atoms x hidden\n\n        # Readout\n        if self.set2set:\n            # Set up sizes\n            batch_size = len(a_scope)\n            lengths = [length for _, length in a_scope]\n            max_num_atoms = max(lengths)\n\n            # Set up memory from atom features\n            memory = torch.zeros(batch_size, max_num_atoms, self.hidden_size)  # (batch_size, max_num_atoms, hidden_size)\n            for i, (start, size) in enumerate(a_scope):\n                memory[i, :size] = atom_hiddens.narrow(0, start, size)\n            memory_transposed = memory.transpose(2, 1)  # (batch_size, hidden_size, max_num_atoms)\n\n            # Create mask (1s for atoms, 0s for not atoms)\n            mask = create_mask(lengths, cuda=next(self.parameters()).is_cuda)  # (max_num_atoms, batch_size)\n            mask = mask.t().unsqueeze(2)  # (batch_size, max_num_atoms, 1)\n\n            # Set up query\n            query = torch.ones(1, batch_size, self.hidden_size)  # (1, batch_size, hidden_size)\n\n            # Move to cuda\n            if next(self.parameters()).is_cuda:\n                memory, memory_transposed, query = memory.cuda(), memory_transposed.cuda(), query.cuda()\n\n            # Run RNN\n            for _ in range(self.set2set_iters):\n                # Compute attention weights over atoms in each molecule\n                query = query.squeeze(0).unsqueeze(2)  # (batch_size,  hidden_size, 1)\n                dot = torch.bmm(memory, query)  # (batch_size, max_num_atoms, 1)\n                dot = dot * mask + (1 - mask) * (-1e+20)  # (batch_size, max_num_atoms, 1)\n                attention = F.softmax(dot, dim=1)  # (batch_size, max_num_atoms, 1)\n\n                # Construct next input as attention over memory\n                attended = torch.bmm(memory_transposed, attention)  # (batch_size, hidden_size, 1)\n                attended = attended.view(1, batch_size, self.hidden_size)  # (1, batch_size, hidden_size)\n\n                # Run RNN for one step\n                query, _ = self.set2set_rnn(attended)  # (1, batch_size, hidden_size)\n\n            # Final RNN output is the molecule encodings\n            mol_vecs = query.squeeze(0)  # (batch_size, hidden_size)\n        else:\n            mol_vecs = []\n            # TODO: Maybe do this in parallel with masking rather than looping\n            for start, size in a_scope:\n                if size == 0:\n                    mol_vecs.append(self.cached_zero_vector)\n                else:\n                    cur_hiddens = atom_hiddens.narrow(0, start, size)\n\n                    if self.attention:\n                        att_w = torch.matmul(self.W_a(cur_hiddens), cur_hiddens.t())\n                        att_w = F.softmax(att_w, dim=1)\n                        att_hiddens = torch.matmul(att_w, cur_hiddens)\n                        att_hiddens = self.act_func(self.W_b(att_hiddens))\n                        att_hiddens = self.dropout_layer(att_hiddens)\n                        mol_vec = (cur_hiddens + att_hiddens)\n                    else:\n                        mol_vec = cur_hiddens  # (num_atoms, hidden_size)\n\n                    if self.deepset:\n                        mol_vec = self.W_s2s_a(mol_vec)\n                        mol_vec = self.act_func(mol_vec)\n                        mol_vec = self.W_s2s_b(mol_vec)\n\n                    mol_vec = mol_vec.sum(dim=0) \/ size\n                    mol_vecs.append(mol_vec)\n\n            mol_vecs = torch.stack(mol_vecs, dim=0)  # (num_molecules, hidden_size)\n        \n        semiF_features = np.stack(semiF_features).todense()\n        semiF_features = torch.from_numpy(semiF_features).cuda()\n        print(semiF_features.size())\n        import pdb; pdb.set_trace()\n        return torch.cat(mol_vecs, semiF_features, dim=1)  # (num_molecules, hidden_size)\n","AFTER":"        if self.args.semiF_path:\n            mol_graph, semiF_features = mol_graph\n            if self.args.semiF_only:\n                semiF_features = np.stack([features.todense() for features in semiF_features])\n                semiF_features = torch.from_numpy(semiF_features).float().cuda()\n                return semiF_features\n\n        f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope = mol_graph.get_components()\n\n        if next(self.parameters()).is_cuda:\n            f_atoms, f_bonds, a2b, b2a, b2revb = f_atoms.cuda(), f_bonds.cuda(), a2b.cuda(), b2a.cuda(), b2revb.cuda()\n\n        # Input\n        b_input = self.W_i(f_bonds)  # num_bonds x hidden_size\n        b_message = self.act_func(b_input)  # num_bonds x hidden_size\n\n        if self.message_attention:\n            b2b = mol_graph.get_b2b()  # Warning: this is O(n_atoms^3) when using virtual edges\n\n            if next(self.parameters()).is_cuda:\n                b2b = b2b.cuda()\n\n            message_attention_mask = (b2b != 0).float()  # num_bonds x max_num_bonds\n\n        if self.global_attention:\n            global_attention_mask = torch.zeros(mol_graph.n_bonds, mol_graph.n_bonds)  # num_bonds x num_bonds\n\n            for start, length in b_scope:\n                for i in range(start, start + length):\n                    global_attention_mask[i, start:start + length] = 1\n\n            if next(self.parameters()).is_cuda:\n                global_attention_mask = global_attention_mask.cuda()\n\n        # Message passing\n        for depth in range(self.depth - 1):\n            if self.message_attention:\n                # TODO: Parallelize attention heads\n                nei_b_message = index_select_ND(b_message, b2b)\n                b_message = b_message.unsqueeze(1).repeat((1, nei_b_message.size(1), 1))  # num_bonds x maxnb x hidden\n                attention_scores = [(self.W_ma[i](nei_b_message) * b_message).sum(dim=2)\n                                    for i in range(self.num_heads)]  # num_bonds x maxnb\n                attention_scores = [attention_scores[i] * message_attention_mask + (1 - message_attention_mask) * (-1e+20)\n                                    for i in range(self.num_heads)]  # num_bonds x maxnb\n                attention_weights = [F.softmax(attention_scores[i], dim=1)\n                                     for i in range(self.num_heads)]  # num_bonds x maxnb\n                message_components = [nei_b_message * attention_weights[i].unsqueeze(2).repeat((1, 1, self.hidden_size))\n                                      for i in range(self.num_heads)]  # num_bonds x maxnb x hidden\n                message_components = [component.sum(dim=1) for component in message_components]  # num_bonds x hidden\n                b_message = torch.cat(message_components, dim=1)  # num_bonds x num_heads * hidden\n            else:\n                # m(a1 -> a2) = [sum_{a0 \\in nei(a1)} m(a0 -> a1)] - m(a2 -> a1)\n                # b_message      a_message = sum(nei_a_message)      rev_b_message\n                nei_a_message = index_select_ND(b_message, a2b)  # num_atoms x max_num_bonds x hidden\n                a_message = nei_a_message.sum(dim=1)  # num_atoms x hidden\n                rev_b_message = b_message[b2revb]  # num_bonds x hidden\n                b_message = a_message[b2a] - rev_b_message  # num_bonds x hidden\n\n            b_message = self.W_h(b_message)  # num_bonds x hidden\n\n            if self.master_node:\n                # master_state = self.W_master_in(self.act_func(nei_message.sum(dim=0))) #try something like this to preserve invariance for master node\n                # master_state = self.GRU_master(nei_message.unsqueeze(1))\n                # master_state = master_state[-1].squeeze(0) #this actually doesn't preserve order invariance anymore\n                mol_vecs = [self.cached_zero_vector]\n                for start, size in b_scope:\n                    if size == 0:\n                        continue\n                    mol_vec = b_message.narrow(0, start, size)\n                    mol_vec = mol_vec.sum(dim=0) \/ size\n                    mol_vecs += [mol_vec for _ in range(size)]\n                master_state = self.act_func(self.W_master_in(torch.stack(mol_vecs, dim=0)))  # num_bonds x hidden_size\n                b_message = self.act_func(b_input + b_message + self.W_master_out(master_state))  # num_bonds x hidden_size\n            else:\n                b_message = self.act_func(b_input + b_message)  # num_bonds x hidden_size\n\n            if self.global_attention:\n                attention_scores = torch.matmul(self.W_ga1(b_message), b_message.t())  # num_bonds x num_bonds\n                attention_scores = attention_scores * global_attention_mask + (1 - global_attention_mask) * (-1e+20)  # num_bonds x num_bonds\n                attention_weights = F.softmax(attention_scores, dim=1)  # num_bonds x num_bonds\n                attention_hiddens = torch.matmul(attention_weights, b_message)  # num_bonds x hidden_size\n                attention_hiddens = self.act_func(self.W_ga2(attention_hiddens))  # num_bonds x hidden_size\n                attention_hiddens = self.dropout_layer(attention_hiddens)  # num_bonds x hidden_size\n                b_message = b_message + attention_hiddens  # num_bonds x hidden_size\n\n                if viz_dir is not None:\n                    visualize_attention(viz_dir, mol_graph, attention_weights, depth)\n\n            if self.use_layer_norm:\n                b_message = self.layer_norm(b_message)\n\n            b_message = self.dropout_layer(b_message)  # num_bonds x hidden\n\n        if self.master_node and self.use_master_as_output:\n            assert self.hidden_size == self.master_dim\n            mol_vecs = []\n            for start, size in b_scope:\n                if size == 0:\n                    mol_vecs.append(self.cached_zero_vector)\n                else:\n                    mol_vecs.append(master_state[start])\n            return torch.stack(mol_vecs, dim=0)\n\n        # Get atom hidden states from message hidden states\n        nei_a_message = index_select_ND(b_message, a2b)  # num_atoms x max_num_bonds x hidden\n        a_message = nei_a_message.sum(dim=1)  # num_atoms x hidden\n        a_input = torch.cat([f_atoms, a_message], dim=1)  # num_atoms x (atom_fdim + hidden)\n        atom_hiddens = self.act_func(self.W_o(a_input))  # num_atoms x hidden\n        atom_hiddens = self.dropout_layer(atom_hiddens)  # num_atoms x hidden\n\n        # Readout\n        if self.set2set:\n            # Set up sizes\n            batch_size = len(a_scope)\n            lengths = [length for _, length in a_scope]\n            max_num_atoms = max(lengths)\n\n            # Set up memory from atom features\n            memory = torch.zeros(batch_size, max_num_atoms, self.hidden_size)  # (batch_size, max_num_atoms, hidden_size)\n            for i, (start, size) in enumerate(a_scope):\n                memory[i, :size] = atom_hiddens.narrow(0, start, size)\n            memory_transposed = memory.transpose(2, 1)  # (batch_size, hidden_size, max_num_atoms)\n\n            # Create mask (1s for atoms, 0s for not atoms)\n            mask = create_mask(lengths, cuda=next(self.parameters()).is_cuda)  # (max_num_atoms, batch_size)\n            mask = mask.t().unsqueeze(2)  # (batch_size, max_num_atoms, 1)\n\n            # Set up query\n            query = torch.ones(1, batch_size, self.hidden_size)  # (1, batch_size, hidden_size)\n\n            # Move to cuda\n            if next(self.parameters()).is_cuda:\n                memory, memory_transposed, query = memory.cuda(), memory_transposed.cuda(), query.cuda()\n\n            # Run RNN\n            for _ in range(self.set2set_iters):\n                # Compute attention weights over atoms in each molecule\n                query = query.squeeze(0).unsqueeze(2)  # (batch_size,  hidden_size, 1)\n                dot = torch.bmm(memory, query)  # (batch_size, max_num_atoms, 1)\n                dot = dot * mask + (1 - mask) * (-1e+20)  # (batch_size, max_num_atoms, 1)\n                attention = F.softmax(dot, dim=1)  # (batch_size, max_num_atoms, 1)\n\n                # Construct next input as attention over memory\n                attended = torch.bmm(memory_transposed, attention)  # (batch_size, hidden_size, 1)\n                attended = attended.view(1, batch_size, self.hidden_size)  # (1, batch_size, hidden_size)\n\n                # Run RNN for one step\n                query, _ = self.set2set_rnn(attended)  # (1, batch_size, hidden_size)\n\n            # Final RNN output is the molecule encodings\n            mol_vecs = query.squeeze(0)  # (batch_size, hidden_size)\n        else:\n            mol_vecs = []\n            # TODO: Maybe do this in parallel with masking rather than looping\n            for start, size in a_scope:\n                if size == 0:\n                    mol_vecs.append(self.cached_zero_vector)\n                else:\n                    cur_hiddens = atom_hiddens.narrow(0, start, size)\n\n                    if self.attention:\n                        att_w = torch.matmul(self.W_a(cur_hiddens), cur_hiddens.t())\n                        att_w = F.softmax(att_w, dim=1)\n                        att_hiddens = torch.matmul(att_w, cur_hiddens)\n                        att_hiddens = self.act_func(self.W_b(att_hiddens))\n                        att_hiddens = self.dropout_layer(att_hiddens)\n                        mol_vec = (cur_hiddens + att_hiddens)\n                    else:\n                        mol_vec = cur_hiddens  # (num_atoms, hidden_size)\n\n                    if self.deepset:\n                        mol_vec = self.W_s2s_a(mol_vec)\n                        mol_vec = self.act_func(mol_vec)\n                        mol_vec = self.W_s2s_b(mol_vec)\n\n                    mol_vec = mol_vec.sum(dim=0) \/ size\n                    mol_vecs.append(mol_vec)\n\n            mol_vecs = torch.stack(mol_vecs, dim=0)  # (num_molecules, hidden_size)\n        \n        if self.args.semiF_path:\n            semiF_features = np.stack([features.todense() for features in semiF_features])\n            semiF_features = torch.from_numpy(semiF_features).float().cuda()\n            return torch.cat([mol_vecs, semiF_features], dim=1)  # (num_molecules, hidden_size)\n        return mol_vecs # num_molecules x hidden\n"}