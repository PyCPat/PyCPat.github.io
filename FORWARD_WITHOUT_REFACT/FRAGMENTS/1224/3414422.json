{"BEFORE":"                 sequential_self_attention: bool = False) -> None:\n        # Call super constructor\n        super(WindowMultiHeadAttention, self).__init__()\n        # Check parameter\n        assert (in_features % number_of_heads) == 0, \\\n            \"The number of input features (in_features) are not divisible by the number of heads (number_of_heads).\"\n        # Save parameters\n        self.in_features: int = in_features\n        self.window_size: int = window_size\n        self.number_of_heads: int = number_of_heads\n        self.sequential_self_attention: bool = sequential_self_attention\n        # Init query, key and value mapping as a single layer\n        self.mapping_qkv: nn.Module = nn.Linear(in_features=in_features, out_features=in_features * 3, bias=True)\n        # Init attention dropout\n        self.attention_dropout: nn.Module = nn.Dropout(dropout_attention)\n        # Init projection mapping\n        self.projection: nn.Module = nn.Linear(in_features=in_features, out_features=in_features, bias=True)\n        # Init projection dropout\n        self.projection_dropout: nn.Module = nn.Dropout(dropout_projection)\n        # Init meta network for positional encodings\n        self.meta_network: nn.Module = nn.Sequential(\n            nn.Linear(in_features=2, out_features=meta_network_hidden_features, bias=True),\n            nn.ReLU(inplace=True),\n            nn.Linear(in_features=meta_network_hidden_features, out_features=number_of_heads, bias=True))\n        # Init tau\n        self.register_parameter(\"tau\", torch.nn.Parameter(torch.ones(1, number_of_heads, 1, 1)))\n","AFTER":"        window_size: Tuple[int, int],\n        drop_attn: float = 0.0,\n        drop_proj: float = 0.0,\n        meta_hidden_dim: int = 384,  # FIXME what's the optimal value?\n        sequential_attn: bool = False,\n    ) -> None:\n        super(WindowMultiHeadAttention, self).__init__()\n        assert dim % num_heads == 0, \\\n            \"The number of input features (in_features) are not divisible by the number of heads (num_heads).\"\n        self.in_features: int = dim\n        self.window_size: Tuple[int, int] = window_size\n        self.num_heads: int = num_heads\n        self.sequential_attn: bool = sequential_attn\n\n        self.qkv = nn.Linear(in_features=dim, out_features=dim * 3, bias=True)\n        self.attn_drop = nn.Dropout(drop_attn)\n        self.proj = nn.Linear(in_features=dim, out_features=dim, bias=True)\n        self.proj_drop = nn.Dropout(drop_proj)\n        # meta network for positional encodings\n        self.meta_mlp = Mlp(\n            2,  # x, y\n            hidden_features=meta_hidden_dim,\n            out_features=num_heads,\n            act_layer=nn.ReLU,\n            drop=0.  # FIXME should we add stochasticity?\n        )\n        self.register_parameter(\"tau\", torch.nn.Parameter(torch.ones(num_heads)))\n"}