{"BEFORE":"        quantizeds = list()\n        codes = list()\n        logits = list()\n        targets = list()\n        for xRaw, prob, squeeze, codebook, k in zip(latents, self._prob, self._squeeze, self._codebook, self._k):\n            targets.append(xRaw)\n            n, c, h, w = xRaw.shape\n            # [c, k] -> [k, c]\n            codewords = codebook.weight.t()\n            # [n, c, h, w] -> [h, w, n, c]\n            encoderIn = xRaw.permute(2, 3, 0, 1)\n            # [h, w, n, c] -> [h*w, n, c]\n            posisted = self._position(encoderIn).reshape(-1, n, c)\n            encoderIn = encoderIn.reshape(-1, n, c)\n            # [h*w, n, c]\n            x = self._encoder(posisted, codewords[:, None, ...].expand(k, n, c))\n            # [h*w, n, k] -> [n, h*w, k]\n            logit = prob(x).permute(1, 0, 2)\n            sample = F.gumbel_softmax(logit, temperature, hard)\n            # [N, h*w, c] <- [N, h*w, k] @ [k, C]\n            # quantized = codebook(sample)\n            # [n, h*w, k] -> [h*w, n, k]\n            quantized = sample.permute(1, 0, 2)\n            quantized \/= (k - 0.5) \/ (2 * k - 2)\n            quantized -= 0.5 \/ (k - 1)\n            # [h*w, n, c]\n            quantized = squeeze(quantized)\n            mixed = (mixin * encoderIn \/ (mixin + 1)) + (quantized \/ (mixin + 1))\n\n            # [h*w, n, c] -> [n, h*w, c] -> [n, h, w, c]\n            deTransformed = self._decoder(mixed).permute(1, 0, 2).reshape(n, h, w, c)\n            # [n, c, h, w]\n            quantizeds.append(deTransformed.permute(0, 3, 1, 2))\n            codes.append(sample)\n            logits.append(logit.reshape(n, h, w, k))\n        return quantizeds, targets, codes, logits\n","AFTER":"        quantizeds = list()\n        codes = list()\n        logits = list()\n        for xRaw, prob, squeeze, codebook, k in zip(latents, self._prob, self._squeeze, self._codebook, self._k):\n            n, c, h, w = xRaw.shape\n            # [c, k] -> [k, c]\n            # codewords = codebook.weight.t()\n            # [n, c, h, w] -> [h, w, n, c]\n            encoderIn = xRaw.permute(2, 3, 0, 1)\n            # [h, w, n, c] -> [h*w, n, c]\n            posisted = self._position(encoderIn).reshape(-1, n, c)\n            # encoderIn = encoderIn.reshape(-1, n, c)\n            # [h*w, n, c]\n            x = self._encoder(posisted)\n            # x = self._encoder(posisted, codewords[:, None, ...].expand(k, n, c))\n            # [h*w, n, k]\n            logit = prob(x, h, w)\n            sample = F.gumbel_softmax(logit, temperature, hard)\n\n            # [N, h*w, c] <- [N, h*w, k] @ [k, C]\n            # quantized = codebook(sample)\n\n            quantized = sample\n\n            # normalize\n            quantized \/= (k - 0.5) \/ (2 * k - 2)\n            quantized -= 0.5 \/ (k - 1)\n            # [h*w, n, c]\n            quantized = squeeze(quantized, h, w)\n            # posistedQuantized = self._position(quantized.reshape(h, w, n, c))\n            # mixed = (mixin * encoderIn \/ (mixin + 1)) + (quantized \/ (mixin + 1))\n\n            # [h*w, n, c] -> [n, c, h*w] -> [n, c, h, w]\n            deTransformed = self._decoder(quantized, quantized).permute(1, 2, 0).reshape(n, c, h, w)\n            # [n, c, h, w]\n            quantizeds.append(deTransformed)\n            codes.append(sample)\n            logits.append(logit.reshape(n, h, w, k))\n        return quantizeds, codes, logits\n"}