{"BEFORE":"            values = fea[i][nonzero_index[:,0], nonzero_index[:,1]] # v in SEFT paper\n            time_index = nonzero_index[:,0]\n            time_sequence = times[:, i]\n            time_points = time_sequence[time_index]  # t in SEFT paper\n            pe_ = self.pos_encoder(time_points.unsqueeze(1)).squeeze(1)\n            variable = nonzero_index[:,1] # the dimensions of variables. The m value in SEFT paper.\n\n            # unit = torch.cat([pe_, values.unsqueeze(1), variable.unsqueeze(1)], dim=1)\n\n            # \"\"\"positional encoding\"\"\"  AUROC ~0.86 Why positional encoding works?\n            # values_ = self.pos_encoder_value(values.unsqueeze(1)).squeeze(1)\n            variable_ = self.pos_encoder_sensor(variable.unsqueeze(1)).squeeze(1)\n\n            \"\"\"linear mapping\"\"\" # AUROC~0.8\n            # values_ =  self.linear_value(values.float().unsqueeze(1)).squeeze(1)\n            # variable_ = self.linear_sensor(variable.float().unsqueeze(1)).squeeze(1)\n\n            \"\"\"Nonlinear transformation\"\"\" # AUROC ~0.8\n            values_ =  F.relu(self.linear_value(values.float().unsqueeze(1))).squeeze(1)\n            # variable_ =  F.relu(self.linear_sensor(variable.float().unsqueeze(1))).squeeze(1)\n\n            unit = torch.cat([pe_, values_, variable_], dim=1)\n            \"\"\"Add Normalization across samples here, to make all 48-dimensions are in similar scale\"\"\"\n            unit = F.normalize(unit, dim=1)\n","AFTER":"            values = fea[i][nonzero_index[:,0], nonzero_index[:,1]] # v in SEFT paper\n            time_index = nonzero_index[:,0]\n            time_sequence = times[:, i]\n            time_points = time_sequence[time_index]  # t in SEFT paper\n            pe_ = self.pos_encoder(time_points.unsqueeze(1)).squeeze(1)\n            variable = nonzero_index[:,1] # the dimensions of variables. The m value in SEFT paper.\n\n            unit = torch.cat([pe_, values.unsqueeze(1), variable.unsqueeze(1)], dim=1)\n"}