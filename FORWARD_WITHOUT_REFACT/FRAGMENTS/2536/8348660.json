{"BEFORE":"        bqk = torch.reshape(sqk, (batch_size, chunk_size, -1, dim))\n        bv = torch.reshape(sv, (batch_size, chunk_size, -1, dim))\n        bq_buckets = bkv_buckets = torch.reshape(sbuckets_and_t \/\/ seqlen, (batch_size, chunk_size, -1))\n\n        # Hashing operates on unit-length vectors. Unnormalized query vectors are\n        # fine because they effectively provide a learnable temperature for the\n        # attention softmax, but normalizing keys is needed so that similarity for\n        # the purposes of attention correctly corresponds to hash locality.\n        bq = bqk\n        bk = F.normalize(bqk, p=2, dim=-1)\n\n        # Allow each chunk to attend within itself, and also one chunk back. Chunk\n        # boundaries might occur in the middle of a sequence of items from the\n        # same bucket, so this increases the chances of attending to relevant items.\n        def look_one_back(x):\n            x_extra = torch.cat([x[:, -1:, ...], x[:, :-1, ...]], dim=1)\n            return torch.cat([x, x_extra], dim=2)\n\n        bk = look_one_back(bk)\n        bv = look_one_back(bv)\n        bkv_t = look_one_back(bkv_t)\n        bkv_buckets = look_one_back(bkv_buckets)\n\n        # Dot-product attention.\n        dots = torch.einsum('bhie,bhje->bhij', bq, bk) * (dim ** -0.5)\n\n        # Causal masking\n        if self.causal:\n            mask = bq_t[:, :, :, None] < bkv_t[:, :, None, :].clamp(max=query_len - 1)\n            dots.masked_fill_(mask, float('-inf'))\n            del mask\n\n        # Mask out attention to self except when no other targets are available.\n        self_mask = bq_t[:, :, :, None] == bkv_t[:, :, None, :]\n        dots.masked_fill_(self_mask, - 1e5)\n        del self_mask\n\n        # Mask out attention to other hash buckets.\n        if not self._attend_across_buckets:\n            bucket_mask = bq_buckets[:, :, :, None] != bkv_buckets[:, :, None, :]\n            dots.masked_fill_(bucket_mask, float('-inf'))\n            del bucket_mask\n\n        # Don't double-count query-key pairs across multiple rounds of hashing.\n        # There are two possible strategies here. (1) The default is to count how\n        # many times a query-key pair is repeated, and to lower its log-prob\n        # correspondingly at each repetition. (2) When hard_k is set, the code\n        # instead masks all but the first occurence of each query-key pair.\n        if not self._allow_duplicate_attention:\n            locs1 = undo_sort \/\/ bq_t.shape[-1]\n            locs2 = (locs1 + 1) % chunk_size\n            if not self._attend_across_buckets:\n                locs1 = buckets * chunk_size + locs1\n                locs2 = buckets * chunk_size + locs2\n            locs = torch.cat([\n                torch.reshape(locs1, (batch_size, self.n_hashes, seqlen)),\n                torch.reshape(locs2, (batch_size, self.n_hashes, seqlen)),\n            ], 1).permute((0, 2, 1))\n\n            slocs = batched_index_select(locs, st)\n            b_locs = torch.reshape(slocs, (batch_size, chunk_size, -1, 2 * self.n_hashes))\n\n            b_locs1 = b_locs[:, :, :, None, :self.n_hashes]\n\n            bq_locs = b_locs1.expand(b_locs.shape[:3] + (2, self.n_hashes))\n            bq_locs = torch.reshape(bq_locs, b_locs.shape)\n            bkv_locs = look_one_back(b_locs)\n\n            dup_counts = (bq_locs[:, :, :, None, :] == bkv_locs[:, :, None, :, :])\n            # for memory considerations, chunk summation of last dimension for counting duplicates\n            dup_counts = chunked_sum(dup_counts, chunks=(self.n_hashes * batch_size))\n            dup_counts = dup_counts.detach()\n            assert dup_counts.shape == dots.shape\n            dots = dots - torch.log(dup_counts + 1e-9)\n            del dup_counts\n\n        # Softmax.\n        dots_logsumexp = torch.logsumexp(dots, dim=-1, keepdim=True)\n        dots = torch.exp(dots - dots_logsumexp)\n","AFTER":"        bqk = torch.reshape(sqk, (batch_size, chunk_size, -1, dim))\n        bv = torch.reshape(sv, (batch_size, chunk_size, -1, dim))\n        bq_buckets = bkv_buckets = torch.reshape(sbuckets_and_t \/\/ seqlen, (batch_size, chunk_size, -1))\n\n        # Hashing operates on unit-length vectors. Unnormalized query vectors are\n        # fine because they effectively provide a learnable temperature for the\n        # attention softmax, but normalizing keys is needed so that similarity for\n        # the purposes of attention correctly corresponds to hash locality.\n        bq = bqk\n        bk = F.normalize(bqk, p=2, dim=-1).type(bq.type())\n\n        # Allow each chunk to attend within itself, and also one chunk back. Chunk\n        # boundaries might occur in the middle of a sequence of items from the\n        # same bucket, so this increases the chances of attending to relevant items.\n        def look_one_back(x):\n            x_extra = torch.cat([x[:, -1:, ...], x[:, :-1, ...]], dim=1)\n            return torch.cat([x, x_extra], dim=2)\n\n        bk = look_one_back(bk)\n        bv = look_one_back(bv)\n        bkv_t = look_one_back(bkv_t)\n        bkv_buckets = look_one_back(bkv_buckets)\n\n        # Dot-product attention.\n        dots = torch.einsum('bhie,bhje->bhij', bq, bk) * (dim ** -0.5)\n\n        # Causal masking\n        if self.causal:\n            mask = bq_t[:, :, :, None] < bkv_t[:, :, None, :].clamp(max=query_len - 1)\n            dots.masked_fill_(mask, float('-inf'))\n            del mask\n\n        # Mask out attention to self except when no other targets are available.\n        self_mask = bq_t[:, :, :, None] == bkv_t[:, :, None, :]\n        dots.masked_fill_(self_mask, TOKEN_SELF_MASK_VALUE)\n        del self_mask\n\n        # Mask out attention to other hash buckets.\n        if not self._attend_across_buckets:\n            bucket_mask = bq_buckets[:, :, :, None] != bkv_buckets[:, :, None, :]\n            dots.masked_fill_(bucket_mask, float('-inf'))\n            del bucket_mask\n\n        # Don't double-count query-key pairs across multiple rounds of hashing.\n        # There are two possible strategies here. (1) The default is to count how\n        # many times a query-key pair is repeated, and to lower its log-prob\n        # correspondingly at each repetition. (2) When hard_k is set, the code\n        # instead masks all but the first occurence of each query-key pair.\n        if not self._allow_duplicate_attention:\n            locs1 = undo_sort \/\/ bq_t.shape[-1]\n            locs2 = (locs1 + 1) % chunk_size\n            if not self._attend_across_buckets:\n                locs1 = buckets * chunk_size + locs1\n                locs2 = buckets * chunk_size + locs2\n            locs = torch.cat([\n                torch.reshape(locs1, (batch_size, self.n_hashes, seqlen)),\n                torch.reshape(locs2, (batch_size, self.n_hashes, seqlen)),\n            ], 1).permute((0, 2, 1))\n\n            slocs = batched_index_select(locs, st)\n            b_locs = torch.reshape(slocs, (batch_size, chunk_size, -1, 2 * self.n_hashes))\n\n            b_locs1 = b_locs[:, :, :, None, :self.n_hashes]\n\n            bq_locs = b_locs1.expand(b_locs.shape[:3] + (2, self.n_hashes))\n            bq_locs = torch.reshape(bq_locs, b_locs.shape)\n            bkv_locs = look_one_back(b_locs)\n\n            dup_counts = (bq_locs[:, :, :, None, :] == bkv_locs[:, :, None, :, :])\n            # for memory considerations, chunk summation of last dimension for counting duplicates\n            dup_counts = chunked_sum(dup_counts, chunks=(self.n_hashes * batch_size))\n            dup_counts = dup_counts.detach()\n            assert dup_counts.shape == dots.shape\n            dots = dots - torch.log(dup_counts + 1e-9)\n            del dup_counts\n\n        # Softmax.\n        dots_logsumexp = torch.logsumexp(dots, dim=-1, keepdim=True)\n        dots = torch.exp(dots - dots_logsumexp).type(dots.type())\n"}