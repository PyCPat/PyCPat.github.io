{"BEFORE":"        full_obs_space = getattr(obs_space, \"original_space\", obs_space)\n\n        # encoder\n        layers = []\n        if \"fc_layer\" in custom_config[\"model_arch_args\"]:\n            self.obs_size = full_obs_space['obs'].shape[0]\n            input_dim = self.obs_size\n            for i in range(custom_config[\"model_arch_args\"][\"fc_layer\"]):\n                out_dim = custom_config[\"model_arch_args\"][\"out_dim_fc_{}\".format(i)]\n                fc_layer = nn.Linear(input_dim, out_dim)\n                layers.append(fc_layer)\n                input_dim = out_dim\n        elif \"conv_layer\" in custom_config[\"model_arch_args\"]:\n            self.obs_size = full_obs_space['obs'].shape\n            input_dim = self.obs_size[2]\n            for i in range(custom_config[\"model_arch_args\"][\"conv_layer\"]):\n                conv_f = nn.Conv2d(\n                    in_channels=input_dim,\n                    out_channels=custom_config[\"model_arch_args\"][\"out_channel_layer_{}\".format(i)],\n                    kernel_size=custom_config[\"model_arch_args\"][\"kernel_size_layer_{}\".format(i)],\n                    stride=custom_config[\"model_arch_args\"][\"stride_layer_{}\".format(i)],\n                    padding=custom_config[\"model_arch_args\"][\"padding_layer_{}\".format(i)],\n                )\n                relu_f = nn.ReLU()\n                pool_f = nn.MaxPool2d(kernel_size=custom_config[\"model_arch_args\"][\"pool_size_layer_{}\".format(i)])\n\n                layers.append(conv_f)\n                layers.append(relu_f)\n                layers.append(pool_f)\n\n                input_dim = custom_config[\"model_arch_args\"][\"out_channel_layer_{}\".format(i)]\n\n        else:\n            raise ValueError()\n\n        self.encoder = nn.Sequential(\n            *layers\n        )\n\n        # encoder for centralized function\n        if \"state\" not in full_obs_space.spaces:\n            self.cc_encoder = copy.deepcopy(self.encoder)\n            cc_input_dim = input_dim * custom_config[\"num_agents\"]\n        else:\n            state_dim = len(full_obs_space[\"state\"].shape)\n            if state_dim > 1:  # TODO not right\n                raise NotImplementedError()\n            else:\n                cc_layers = []\n                cc_input_dim = full_obs_space[\"state\"].shape[0]\n                for i in range(custom_config[\"model_arch_args\"][\"fc_layer\"]):\n                    cc_out_dim = custom_config[\"model_arch_args\"][\"out_dim_fc_{}\".format(i)]\n                    cc_fc_layer = nn.Linear(cc_input_dim, cc_out_dim)\n                    cc_layers.append(cc_fc_layer)\n                    cc_input_dim = cc_out_dim\n\n                self.cc_encoder = nn.Sequential(\n                    *cc_layers\n                )\n\n        # core rnn\n        self.hidden_state_size = custom_config[\"model_arch_args\"][\"hidden_state_size\"]\n\n        if custom_config[\"model_arch_args\"][\"core_arch\"] == \"gru\":\n            self.rnn = nn.GRU(input_dim, self.hidden_state_size, batch_first=True)\n        elif custom_config[\"model_arch_args\"][\"core_arch\"] == \"lstm\":\n            self.rnn = nn.LSTM(input_dim, self.hidden_state_size, batch_first=True)\n        else:\n            raise ValueError()\n        # action branch and value branch\n        self.action_branch = nn.Linear(self.hidden_state_size, num_outputs)\n        self.value_branch = nn.Linear(self.hidden_state_size, 1)\n\n        # Holds the current \"base\" output (before logits layer).\n        self._features = None\n\n        # Central VF\n        input_size = cc_input_dim + num_outputs * (custom_config[\"num_agents\"] - 1)  # obs + opp_obs + opp_act\n","AFTER":"        custom_config = model_config[\"custom_model_config\"]\n        full_obs_space = getattr(obs_space, \"original_space\", obs_space)\n\n        # encoder\n        layers = []\n        if \"fc_layer\" in custom_config[\"model_arch_args\"]:\n            self.obs_size = full_obs_space['obs'].shape[0]\n            input_dim = self.obs_size\n            for i in range(custom_config[\"model_arch_args\"][\"fc_layer\"]):\n                out_dim = custom_config[\"model_arch_args\"][\"out_dim_fc_{}\".format(i)]\n                fc_layer = nn.Linear(input_dim, out_dim)\n                layers.append(fc_layer)\n                input_dim = out_dim\n        elif \"conv_layer\" in custom_config[\"model_arch_args\"]:\n            self.obs_size = full_obs_space['obs'].shape\n            input_dim = self.obs_size[2]\n            for i in range(custom_config[\"model_arch_args\"][\"conv_layer\"]):\n                conv_f = nn.Conv2d(\n                    in_channels=input_dim,\n                    out_channels=custom_config[\"model_arch_args\"][\"out_channel_layer_{}\".format(i)],\n                    kernel_size=custom_config[\"model_arch_args\"][\"kernel_size_layer_{}\".format(i)],\n                    stride=custom_config[\"model_arch_args\"][\"stride_layer_{}\".format(i)],\n                    padding=custom_config[\"model_arch_args\"][\"padding_layer_{}\".format(i)],\n                )\n                relu_f = nn.ReLU()\n                pool_f = nn.MaxPool2d(kernel_size=custom_config[\"model_arch_args\"][\"pool_size_layer_{}\".format(i)])\n\n                layers.append(conv_f)\n                layers.append(relu_f)\n                layers.append(pool_f)\n\n                input_dim = custom_config[\"model_arch_args\"][\"out_channel_layer_{}\".format(i)]\n\n        else:\n            raise ValueError()\n\n        self.encoder = nn.Sequential(\n            *layers\n        )\n\n        # encoder for centralized function\n        if \"state\" not in full_obs_space.spaces:\n            self.state_dim = full_obs_space[\"obs\"].shape\n            self.cc_encoder = copy.deepcopy(self.encoder)\n            cc_input_dim = input_dim * custom_config[\"num_agents\"]\n        else:\n            self.state_dim = full_obs_space[\"state\"].shape\n            if len(self.state_dim) > 1:  # env return a 3D global state\n                raise NotImplementedError()\n            else:\n                cc_layers = []\n                cc_input_dim = full_obs_space[\"state\"].shape[0]\n                for i in range(custom_config[\"model_arch_args\"][\"fc_layer\"]):\n                    cc_out_dim = custom_config[\"model_arch_args\"][\"out_dim_fc_{}\".format(i)]\n                    cc_fc_layer = nn.Linear(cc_input_dim, cc_out_dim)\n                    cc_layers.append(cc_fc_layer)\n                    cc_input_dim = cc_out_dim\n\n                self.cc_encoder = nn.Sequential(\n                    *cc_layers\n                )\n\n        # core rnn\n        self.hidden_state_size = custom_config[\"model_arch_args\"][\"hidden_state_size\"]\n\n        if custom_config[\"model_arch_args\"][\"core_arch\"] == \"gru\":\n            self.rnn = nn.GRU(input_dim, self.hidden_state_size, batch_first=True)\n        elif custom_config[\"model_arch_args\"][\"core_arch\"] == \"lstm\":\n            self.rnn = nn.LSTM(input_dim, self.hidden_state_size, batch_first=True)\n        else:\n            raise ValueError()\n        # action branch and value branch\n        self.action_branch = nn.Linear(self.hidden_state_size, num_outputs)\n        self.value_branch = nn.Linear(self.hidden_state_size, 1)\n\n        # Holds the current \"base\" output (before logits layer).\n        self._features = None\n\n        # Central VF\n        if isinstance(custom_config[\"space_act\"], Box):  # continues\n            input_size = cc_input_dim + 2 * (custom_config[\"num_agents\"] - 1)\n        else:\n            input_size = cc_input_dim + num_outputs * (custom_config[\"num_agents\"] - 1)\n        self.central_vf = nn.Sequential(\n"}