{"BEFORE":"                calibs):\n        \"\"\"Forward function.\n\n        Args:\n            imgs (list[torch.Tensor]): Image features.\n            bboxes_2d_rescaled (list[torch.Tensor]): 2D bboxes.\n            seeds_3d_depth (torch.Tensor): 3D seeds.\n            img_metas (list[dict]): Meta information of images.\n            calibs: Camera calibration information of the images.\n\n        Returns:\n            torch.Tensor: Concatenated cues of each point.\n            torch.Tensor: Validity mask of each feature.\n        \"\"\"\n        img_features = []\n        masks = []\n        for i, data in enumerate(\n                zip(imgs, bboxes_2d_rescaled, seeds_3d_depth, img_metas)):\n            img, bbox_2d_rescaled, seed_3d_depth, img_meta = data\n            bbox_num = bbox_2d_rescaled.shape[0]\n            seed_num = seed_3d_depth.shape[0]\n\n            img_shape = img_meta['img_shape']\n            img_h, img_w, _ = img_shape\n\n            # first reverse the data transformations\n            xyz_depth = apply_3d_transformation(\n                seed_3d_depth, 'DEPTH', img_meta, reverse=True)\n\n            # then convert from depth coords to camera coords\n            xyz_cam = Coord3DMode.convert_point(\n                xyz_depth,\n                Coord3DMode.DEPTH,\n                Coord3DMode.CAM,\n                rt_mat=calibs['Rt'][i])\n\n            # project to 2d to get image coords (uv)\n            uv_origin = points_cam2img(xyz_cam, calibs['K'][i])\n            uv_origin = (uv_origin - 1).round()\n\n            # rescale 2d coordinates and bboxes\n            uv_rescaled = coord_2d_transform(img_meta, uv_origin, True)\n            bbox_2d_origin = bbox_2d_transform(img_meta, bbox_2d_rescaled,\n                                               False)\n\n            if bbox_num == 0:\n                imvote_num = seed_num * self.max_imvote_per_pixel\n\n                # use zero features\n                two_cues = torch.zeros((15, imvote_num),\n                                       device=seed_3d_depth.device)\n                mask_zero = torch.zeros(\n                    imvote_num - seed_num, device=seed_3d_depth.device).bool()\n                mask_one = torch.ones(\n                    seed_num, device=seed_3d_depth.device).bool()\n                mask = torch.cat([mask_one, mask_zero], dim=0)\n            else:\n                # expand bboxes and seeds\n                bbox_expanded = bbox_2d_origin.view(1, bbox_num, -1).expand(\n                    seed_num, -1, -1)\n                seed_2d_expanded = uv_origin.view(seed_num, 1,\n                                                  -1).expand(-1, bbox_num, -1)\n                seed_2d_expanded_x, seed_2d_expanded_y = \\\n                    seed_2d_expanded.split(1, dim=-1)\n\n                bbox_expanded_l, bbox_expanded_t, bbox_expanded_r, \\\n                    bbox_expanded_b, bbox_expanded_conf, bbox_expanded_cls = \\\n                    bbox_expanded.split(1, dim=-1)\n                bbox_expanded_midx = (bbox_expanded_l + bbox_expanded_r) \/ 2\n                bbox_expanded_midy = (bbox_expanded_t + bbox_expanded_b) \/ 2\n\n                seed_2d_in_bbox_x = (seed_2d_expanded_x > bbox_expanded_l) * \\\n                    (seed_2d_expanded_x < bbox_expanded_r)\n                seed_2d_in_bbox_y = (seed_2d_expanded_y > bbox_expanded_t) * \\\n                    (seed_2d_expanded_y < bbox_expanded_b)\n                seed_2d_in_bbox = seed_2d_in_bbox_x * seed_2d_in_bbox_y\n\n                # semantic cues, dim=class_num\n                sem_cue = torch.zeros_like(bbox_expanded_conf).expand(\n                    -1, -1, self.num_classes)\n                sem_cue = sem_cue.scatter(-1, bbox_expanded_cls.long(),\n                                          bbox_expanded_conf)\n\n                # bbox center - uv\n                delta_u = bbox_expanded_midx - seed_2d_expanded_x\n                delta_v = bbox_expanded_midy - seed_2d_expanded_y\n\n                seed_3d_expanded = seed_3d_depth.view(seed_num, 1, -1).expand(\n                    -1, bbox_num, -1)\n\n                z_cam = xyz_cam[..., 2:3].view(seed_num, 1,\n                                               1).expand(-1, bbox_num, -1)\n\n                delta_u = delta_u * z_cam \/ calibs['K'][i, 0, 0]\n                delta_v = delta_v * z_cam \/ calibs['K'][i, 0, 0]\n\n                imvote = torch.cat(\n                    [delta_u, delta_v,\n                     torch.zeros_like(delta_v)], dim=-1).view(-1, 3)\n\n                # convert from camera coords to depth coords\n                imvote = Coord3DMode.convert_point(\n                    imvote.view((-1, 3)),\n                    Coord3DMode.CAM,\n                    Coord3DMode.DEPTH,\n                    rt_mat=calibs['Rt'][i])\n","AFTER":"            bbox_num = bbox_2d_rescaled.shape[0]\n            seed_num = seed_3d_depth.shape[0]\n\n            img_shape = img_meta['img_shape']\n            img_h, img_w, _ = img_shape\n\n            # first reverse the data transformations\n            xyz_depth = apply_3d_transformation(\n                seed_3d_depth, 'DEPTH', img_meta, reverse=True)\n\n            # project points from depth to image\n            depth2img = xyz_depth.new_tensor(img_meta['depth2img'])\n            uvz_origin = points_cam2img(xyz_depth, depth2img, True)\n            z_cam = uvz_origin[..., 2]\n            uv_origin = (uvz_origin[..., :2] - 1).round()\n\n            # rescale 2d coordinates and bboxes\n            uv_rescaled = coord_2d_transform(img_meta, uv_origin, True)\n            bbox_2d_origin = bbox_2d_transform(img_meta, bbox_2d_rescaled,\n                                               False)\n\n            if bbox_num == 0:\n                imvote_num = seed_num * self.max_imvote_per_pixel\n\n                # use zero features\n                two_cues = torch.zeros((15, imvote_num),\n                                       device=seed_3d_depth.device)\n                mask_zero = torch.zeros(\n                    imvote_num - seed_num, device=seed_3d_depth.device).bool()\n                mask_one = torch.ones(\n                    seed_num, device=seed_3d_depth.device).bool()\n                mask = torch.cat([mask_one, mask_zero], dim=0)\n            else:\n                # expand bboxes and seeds\n                bbox_expanded = bbox_2d_origin.view(1, bbox_num, -1).expand(\n                    seed_num, -1, -1)\n                seed_2d_expanded = uv_origin.view(seed_num, 1,\n                                                  -1).expand(-1, bbox_num, -1)\n                seed_2d_expanded_x, seed_2d_expanded_y = \\\n                    seed_2d_expanded.split(1, dim=-1)\n\n                bbox_expanded_l, bbox_expanded_t, bbox_expanded_r, \\\n                    bbox_expanded_b, bbox_expanded_conf, bbox_expanded_cls = \\\n                    bbox_expanded.split(1, dim=-1)\n                bbox_expanded_midx = (bbox_expanded_l + bbox_expanded_r) \/ 2\n                bbox_expanded_midy = (bbox_expanded_t + bbox_expanded_b) \/ 2\n\n                seed_2d_in_bbox_x = (seed_2d_expanded_x > bbox_expanded_l) * \\\n                    (seed_2d_expanded_x < bbox_expanded_r)\n                seed_2d_in_bbox_y = (seed_2d_expanded_y > bbox_expanded_t) * \\\n                    (seed_2d_expanded_y < bbox_expanded_b)\n                seed_2d_in_bbox = seed_2d_in_bbox_x * seed_2d_in_bbox_y\n\n                # semantic cues, dim=class_num\n                sem_cue = torch.zeros_like(bbox_expanded_conf).expand(\n                    -1, -1, self.num_classes)\n                sem_cue = sem_cue.scatter(-1, bbox_expanded_cls.long(),\n                                          bbox_expanded_conf)\n\n                # bbox center - uv\n                delta_u = bbox_expanded_midx - seed_2d_expanded_x\n                delta_v = bbox_expanded_midy - seed_2d_expanded_y\n\n                seed_3d_expanded = seed_3d_depth.view(seed_num, 1, -1).expand(\n                    -1, bbox_num, -1)\n\n                z_cam = z_cam.view(seed_num, 1, 1).expand(-1, bbox_num, -1)\n                imvote = torch.cat(\n                    [delta_u, delta_v,\n                     torch.zeros_like(delta_v)], dim=-1).view(-1, 3)\n                imvote = imvote * z_cam.reshape(-1, 1)\n                imvote = imvote @ torch.inverse(depth2img.t())\n"}