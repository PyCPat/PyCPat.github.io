{"BEFORE":"        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n\n        # calculate local attention\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        sim = self.rel_pos_bias(sim) + sim\n\n        i, j = sim.shape[-2:]\n        mask_value = -torch.finfo(sim.dtype).max\n\n        causal_mask = torch.ones((i, j), dtype = torch.bool).triu(j - i + 1)\n        sim = sim.masked_fill(causal_mask, mask_value)\n\n        sim = sim - sim.amax(dim = -1, keepdim = True).detach()\n        attn = sim.softmax(dim = -1)\n        attn = self.dropout(attn)\n\n        local_values = einsum('b h i j, b h j d -> b h i d', attn, v)\n\n        # calculate knn attention over memory, if index is passed in\n\n        if exists(index):\n            k_mem, v_mem, mem_mask = index.search(q, k = self.num_retrieved_memories)\n\n            # use null key \/ value to protect against empty memory\n\n            null_k, null_v = map(lambda t: rearrange(t, 'h 1 d -> b h 1 d', b = x.shape[0]), (self.null_k, self.null_v))\n\n            k_mem = torch.cat((null_k, k_mem), dim = -2)\n            v_mem = torch.cat((null_v, v_mem), dim = -2)\n            mem_mask = F.pad(mem_mask, (1, 0), value = True)\n\n            sim_mem = einsum('b h i d, b h j d -> b h i j', q, mem_k) * self.scale\n            sim_mem = sim_mem - sim_mem.amax(dim = -1, keepdim = True).detach()\n\n            sim = sim.masked_fill(~mem_mask, mask_value)\n            attn_mem = sim_mem.softmax(dim = -1)\n","AFTER":"        q = rearrange(q, 'b n (h d) -> b h n d', h = h)\n\n        # calculate local attention\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        sim = self.rel_pos_bias(sim) + sim\n\n        i, j = sim.shape[-2:]\n        mask_value = -torch.finfo(sim.dtype).max\n\n        causal_mask = torch.ones((i, j), dtype = torch.bool).triu(j - i + 1)\n        sim = sim.masked_fill(causal_mask, mask_value)\n\n        attn = stable_softmax(sim)\n        attn = self.dropout(attn)\n\n        local_values = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        # calculate knn attention over memory, if index is passed in\n\n        if exists(index):\n            k_mem, v_mem, mem_mask = index.search(q, k = self.num_retrieved_memories)\n\n            # use null key \/ value to protect against empty memory\n\n            null_k, null_v = map(lambda t: rearrange(t, 'd -> b 1 d', b = x.shape[0]), (self.null_k, self.null_v))\n\n            k_mem = torch.cat((null_k, k_mem), dim = -2)\n            v_mem = torch.cat((null_v, v_mem), dim = -2)\n            mem_mask = F.pad(mem_mask, (1, 0), value = True)\n\n            sim_mem = einsum('b h i d, b j d -> b h i j', q, mem_k) * self.scale\n\n            sim = sim.masked_fill(~mem_mask, mask_value)\n            attn_mem = stable_softmax(sim_mem)\n"}