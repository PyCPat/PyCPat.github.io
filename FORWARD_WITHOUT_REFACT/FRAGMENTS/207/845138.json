{"BEFORE":"        dtype = regressions.dtype\n        anchor = anchors[0, :, :].to(dtype)\n        #--------------------------------------------#\n        #   将先验框转换成中心，宽高的形式\n        #--------------------------------------------#\n        anchor_widths = anchor[:, 3] - anchor[:, 1]\n        anchor_heights = anchor[:, 2] - anchor[:, 0]\n        anchor_ctr_x = anchor[:, 1] + 0.5 * anchor_widths\n        anchor_ctr_y = anchor[:, 0] + 0.5 * anchor_heights\n\n        regression_losses = []\n        classification_losses = []\n        for j in range(batch_size):\n            #-------------------------------------------------------#\n            #   取出每张图片对应的真实框、种类预测结果和回归预测结果\n            #-------------------------------------------------------#\n            bbox_annotation = annotations[j]\n            classification = classifications[j, :, :]\n            regression = regressions[j, :, :]\n            \n            classification = torch.clamp(classification, 5e-4, 1.0 - 5e-4)\n            \n            if len(bbox_annotation) == 0:\n                #-------------------------------------------------------#\n                #   当图片中不存在真实框的时候，所有特征点均为负样本\n                #-------------------------------------------------------#\n                alpha_factor = torch.ones_like(classification) * alpha\n                if cuda:\n                    alpha_factor = alpha_factor.cuda()\n                alpha_factor = 1. - alpha_factor\n                focal_weight = classification\n                focal_weight = alpha_factor * torch.pow(focal_weight, gamma)\n                \n                #-------------------------------------------------------#\n                #   计算特征点对应的交叉熵\n                #-------------------------------------------------------#\n                bce = - (torch.log(1.0 - classification))\n                \n                cls_loss = focal_weight * bce\n                \n                classification_losses.append(cls_loss.sum())\n                #-------------------------------------------------------#\n                #   回归损失此时为0\n                #-------------------------------------------------------#\n                if cuda:\n                    regression_losses.append(torch.tensor(0).to(dtype).cuda())\n                else:\n                    regression_losses.append(torch.tensor(0).to(dtype))\n                    \n                continue\n\n            #------------------------------------------------------#\n            #   计算真实框和先验框的交并比\n            #   targets                 num_anchors, num_classes\n            #   num_positive_anchors    正样本的数量\n            #   positive_indices        num_anchors, \n            #   assigned_annotations    num_anchors, 5\n            #------------------------------------------------------#\n            targets, num_positive_anchors, positive_indices, assigned_annotations = get_target(anchor, \n                                                                                        bbox_annotation, classification, cuda)\n            \n            #------------------------------------------------------#\n            #   首先计算交叉熵loss\n            #------------------------------------------------------#\n            alpha_factor = torch.ones_like(targets) * alpha\n            if cuda:\n                alpha_factor = alpha_factor.cuda()\n            #------------------------------------------------------#\n            #   这里使用的是Focal loss的思想，\n            #   易分类样本权值小\n            #   难分类样本权值大\n            #------------------------------------------------------#\n            alpha_factor = torch.where(torch.eq(targets, 1.), alpha_factor, 1. - alpha_factor)\n            focal_weight = torch.where(torch.eq(targets, 1.), 1. - classification, classification)\n            focal_weight = alpha_factor * torch.pow(focal_weight, gamma)\n\n            bce = - (targets * torch.log(classification) + (1.0 - targets) * torch.log(1.0 - classification))\n            cls_loss = focal_weight * bce\n\n            #------------------------------------------------------#\n            #   把忽略的先验框的loss置为0\n            #------------------------------------------------------#\n            zeros = torch.zeros_like(cls_loss)\n            if cuda:\n                zeros = zeros.cuda()\n            cls_loss = torch.where(torch.ne(targets, -1.0), cls_loss, zeros)\n\n            classification_losses.append(cls_loss.sum() \/ torch.clamp(num_positive_anchors.to(dtype), min=1.0))\n            \n            #------------------------------------------------------#\n            #   如果存在先验框为正样本的话\n            #------------------------------------------------------#\n            if positive_indices.sum() > 0:\n                targets = encode_bbox(assigned_annotations, positive_indices, anchor_widths, anchor_heights, anchor_ctr_x, anchor_ctr_y)\n                #---------------------------------------------------#\n                #   将网络应该有的预测结果和实际的预测结果进行比较\n                #   计算smooth l1 loss\n                #---------------------------------------------------#\n                regression_diff = torch.abs(targets - regression[positive_indices, :])\n                regression_loss = torch.where(\n                    torch.le(regression_diff, 1.0 \/ 9.0),\n                    0.5 * 9.0 * torch.pow(regression_diff, 2),\n                    regression_diff - 0.5 \/ 9.0\n                )\n                regression_losses.append(regression_loss.mean())\n            else:\n                if cuda:\n                    regression_losses.append(torch.tensor(0).to(dtype).cuda())\n                else:\n                    regression_losses.append(torch.tensor(0).to(dtype))\n        \n        # 计算平均loss并返回\n        c_loss = torch.stack(classification_losses).mean()\n","AFTER":"            classification = torch.clamp(classification, 5e-4, 1.0 - 5e-4)\n            \n            if len(bbox_annotation) == 0:\n                #-------------------------------------------------------#\n                #   当图片中不存在真实框的时候，所有特征点均为负样本\n                #-------------------------------------------------------#\n                alpha_factor = torch.ones_like(classification) * alpha\n                alpha_factor = alpha_factor.type_as(classification)\n\n                alpha_factor = 1. - alpha_factor\n                focal_weight = classification\n                focal_weight = alpha_factor * torch.pow(focal_weight, gamma)\n                \n                #-------------------------------------------------------#\n                #   计算特征点对应的交叉熵\n                #-------------------------------------------------------#\n                bce = - (torch.log(1.0 - classification))\n                \n                cls_loss = focal_weight * bce\n                \n                classification_losses.append(cls_loss.sum())\n                #-------------------------------------------------------#\n                #   回归损失此时为0\n                #-------------------------------------------------------#\n                regression_losses.append(torch.tensor(0).type_as(classification))\n                    \n                continue\n\n            #------------------------------------------------------#\n            #   计算真实框和先验框的交并比\n            #   targets                 num_anchors, num_classes\n            #   num_positive_anchors    正样本的数量\n            #   positive_indices        num_anchors, \n            #   assigned_annotations    num_anchors, 5\n            #------------------------------------------------------#\n            targets, num_positive_anchors, positive_indices, assigned_annotations = get_target(anchor, \n                                                                                        bbox_annotation, classification, cuda)\n            \n            #------------------------------------------------------#\n            #   首先计算交叉熵loss\n            #------------------------------------------------------#\n            alpha_factor = torch.ones_like(targets) * alpha\n            alpha_factor = alpha_factor.type_as(classification)\n            #------------------------------------------------------#\n            #   这里使用的是Focal loss的思想，\n            #   易分类样本权值小\n            #   难分类样本权值大\n            #------------------------------------------------------#\n            alpha_factor = torch.where(torch.eq(targets, 1.), alpha_factor, 1. - alpha_factor)\n            focal_weight = torch.where(torch.eq(targets, 1.), 1. - classification, classification)\n            focal_weight = alpha_factor * torch.pow(focal_weight, gamma)\n\n            bce = - (targets * torch.log(classification) + (1.0 - targets) * torch.log(1.0 - classification))\n            cls_loss = focal_weight * bce\n\n            #------------------------------------------------------#\n            #   把忽略的先验框的loss置为0\n            #------------------------------------------------------#\n            zeros = torch.zeros_like(cls_loss)\n            zeros = zeros.type_as(cls_loss)\n            cls_loss = torch.where(torch.ne(targets, -1.0), cls_loss, zeros)\n\n            classification_losses.append(cls_loss.sum() \/ torch.clamp(num_positive_anchors.to(dtype), min=1.0))\n            \n            #------------------------------------------------------#\n            #   如果存在先验框为正样本的话\n            #------------------------------------------------------#\n            if positive_indices.sum() > 0:\n                targets = encode_bbox(assigned_annotations, positive_indices, anchor_widths, anchor_heights, anchor_ctr_x, anchor_ctr_y)\n                #---------------------------------------------------#\n                #   将网络应该有的预测结果和实际的预测结果进行比较\n                #   计算smooth l1 loss\n                #---------------------------------------------------#\n                regression_diff = torch.abs(targets - regression[positive_indices, :])\n                regression_loss = torch.where(\n                    torch.le(regression_diff, 1.0 \/ 9.0),\n                    0.5 * 9.0 * torch.pow(regression_diff, 2),\n                    regression_diff - 0.5 \/ 9.0\n                )\n                regression_losses.append(regression_loss.mean())\n            else:\n                regression_losses.append(torch.tensor(0).type_as(classification))\n"}