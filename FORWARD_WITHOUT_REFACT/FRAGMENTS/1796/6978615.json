{"BEFORE":"        nbhd_indices = None\n        if num_nn > 0:\n            ranking = rel_dist\n\n            # make sure padding does not end up becoming neighbors\n            if exists(mask):\n                ranking_mask = mask[:, :, None] * mask[:, None, :]\n                ranking = ranking.masked_fill(~ranking_mask, 1e5)\n\n            nbhd_indices = ranking.topk(num_nn, dim = -1, largest = False).indices\n\n        rel_dist = rearrange(rel_dist, 'b i j -> b i j ()')\n\n        if fourier_features > 0:\n            rel_dist = fourier_encode_dist(rel_dist, num_encodings = fourier_features)\n            rel_dist = rearrange(rel_dist, 'b i j () d -> b i j d')\n\n        rel_dist = repeat(rel_dist, 'b i j d -> b h i j d', h = h)\n\n        # derive queries keys and values\n\n        q, k, v = self.to_qkv(feats).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n\n        # calculate nearest neighbors\n\n        i = j = n\n\n        if exists(nbhd_indices):\n            i, j = nbhd_indices.shape[-2:]\n            nbhd_indices_with_heads = repeat(nbhd_indices, 'b n d -> b h n d', h = h)\n            k         = batched_index_select(k, nbhd_indices_with_heads, dim = 2)\n            v         = batched_index_select(v, nbhd_indices_with_heads, dim = 2)\n            rel_dist  = batched_index_select(rel_dist, nbhd_indices_with_heads, dim = 3)\n            rel_coors = batched_index_select(rel_coors, nbhd_indices, dim = 2)\n        else:\n            k = repeat(k, 'b h j d -> b h n j d', n = n)\n\n        # prepare mask\n\n        if exists(mask):\n            q_mask = rearrange(mask, 'b i -> b () i ()')\n            k_mask = repeat(mask, 'b j -> b i j', i = n)\n\n            if exists(nbhd_indices):\n                k_mask = batched_index_select(k_mask, nbhd_indices, dim = 2)\n\n            k_mask = rearrange(k_mask, 'b i j -> b () i j')\n            mask = q_mask * k_mask\n\n        # expand queries and keys for concatting\n\n        q = repeat(q, 'b h i d -> b h i n d', n = j)\n\n        edge_input = torch.cat((q, k, rel_dist), dim = -1)\n\n        if exists(edges):\n            if exists(nbhd_indices):\n                edges = batched_index_select(edges, nbhd_indices, dim = 2)\n\n            edges = repeat(edges, 'b i j d -> b h i j d', h = h)\n            edge_input = torch.cat((edge_input, edges), dim = -1)\n\n        m_ij = self.edge_mlp(edge_input)\n\n        coor_mlp_input = rearrange(m_ij, 'b h i j d -> b i j (h d)')\n        coor_weights = self.coors_mlp(coor_mlp_input)\n\n        if exists(mask):\n            coor_mask = rearrange(mask, 'b () i j -> b i j')\n            coor_weights.masked_fill_(~coor_mask, 0.)\n\n        if self.norm_rel_coors:\n            rel_coors = F.normalize(rel_coors, dim = -1, p = 2)\n\n        coors_out = einsum('b i j, b i j c -> b i c', coor_weights, rel_coors)\n\n        # derive attention\n\n        sim = self.to_attn_mlp(m_ij)\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            sim.masked_fill_(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1)\n\n        # weighted sum of values and combine heads\n\n        aggregate_einsum_note = 'b h i j, b h j d -> b h i d' if not exists(nbhd_indices) else 'b h i j, b h i j d -> b h i d'\n        out = einsum(aggregate_einsum_note, attn, v)\n","AFTER":"        rel_dist = repeat(rel_dist, 'b i j d -> b h i j d', h = h)\n\n        # derive queries keys and values\n\n        q, k, v = self.to_qkv(feats).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n\n        # calculate nearest neighbors\n\n        i = j = n\n\n        if exists(nbhd_indices):\n            i, j = nbhd_indices.shape[-2:]\n            nbhd_indices_with_heads = repeat(nbhd_indices, 'b n d -> b h n d', h = h)\n            k         = batched_index_select(k, nbhd_indices_with_heads, dim = 2)\n            v         = batched_index_select(v, nbhd_indices_with_heads, dim = 2)\n            rel_dist  = batched_index_select(rel_dist, nbhd_indices_with_heads, dim = 3)\n            rel_coors = batched_index_select(rel_coors, nbhd_indices, dim = 2)\n        else:\n            k = repeat(k, 'b h j d -> b h n j d', n = n)\n            v = repeat(v, 'b h j d -> b h n j d', n = n)\n\n        rel_dist_pos_emb = self.to_pos_emb(rel_dist)\n\n        # inject position into values\n\n        v = v + rel_dist_pos_emb\n\n        # prepare mask\n\n        if exists(mask):\n            q_mask = rearrange(mask, 'b i -> b () i ()')\n            k_mask = repeat(mask, 'b j -> b i j', i = n)\n\n            if exists(nbhd_indices):\n                k_mask = batched_index_select(k_mask, nbhd_indices, dim = 2)\n\n            k_mask = rearrange(k_mask, 'b i j -> b () i j')\n            mask = q_mask * k_mask\n\n        # expand queries and keys for concatting\n\n        q = repeat(q, 'b h i d -> b h i n d', n = j)\n\n        edge_input = (q - k) + rel_dist_pos_emb\n\n        if exists(edges):\n            if exists(nbhd_indices):\n                edges = batched_index_select(edges, nbhd_indices, dim = 2)\n\n            edges = repeat(edges, 'b i j d -> b h i j d', h = h)\n            edge_input = torch.cat((edge_input, edges), dim = -1)\n\n        m_ij = self.edge_mlp(edge_input)\n\n        coor_mlp_input = rearrange(m_ij, 'b h i j d -> b i j (h d)')\n        coor_weights = self.coors_mlp(coor_mlp_input)\n\n        if exists(mask):\n            coor_mask = rearrange(mask, 'b () i j -> b i j')\n            coor_weights.masked_fill_(~coor_mask, 0.)\n\n        if self.norm_rel_coors:\n            rel_coors = F.normalize(rel_coors, dim = -1, p = 2)\n\n        coors_out = einsum('b i j, b i j c -> b i c', coor_weights, rel_coors)\n\n        # derive attention\n\n        sim = self.to_attn_mlp(m_ij)\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            sim.masked_fill_(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1)\n\n        # weighted sum of values and combine heads\n\n        out = einsum('b h i j, b h i j d -> b h i d', attn, v)\n"}