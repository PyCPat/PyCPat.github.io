{"BEFORE":"        src =  src[:, :, :self.d_inp]\n        maxlen, batch_size = src.shape[0], src.shape[1]  # src.shape = [215, 128, 36]\n\n        \"\"\"Question: why 72 features (36 feature + 36 mask)?\"\"\"\n        src = self.encoder(src) * math.sqrt(self.d_model)  # linear layer: 36 --> 36 # Mapping\n\n        # src = src* math.sqrt(self.d_model)\n\n        pe = self.pos_encoder(times)  # times.shape = [215, 128], the values are hours.\n\n\n        \"\"\"Use late concat for static\"\"\"\n        src = self.dropout(src)  # [215, 128, 36]\n        emb = self.emb(static)  # emb.shape = [128, 64]. Linear layer: 9--> 64\n\n        # append context on front\n        \"\"\"215-D for time series and 1-D for static info\"\"\"\n\n\n\n\n        \"\"\"step 3: use transformer to aggregate temporal information; batchsize in the middle position\"\"\"\n        \"\"\"concat with timestamp\"\"\"\n        r_out = torch.cat([src, pe], dim=-1)  # output shape: [215, 128, (36*4+36)]\n\n        # r_out = src\n\n        \"\"\"mask out the all-zero rows. \"\"\"\n        mask = torch.arange(maxlen + 1)[None, :] >= (lengths.cpu()[:, None] + 1)\n        mask = mask.squeeze(1).cuda()  # shape: [128, 216]\n\n        masked_agg = True\n        if masked_agg ==True:\n            \"\"\" masked aggregation across rows\"\"\"\n            # print('masked aggregation across rows')\n            mask2 = mask.permute(1, 0).unsqueeze(2).long()  # [216, 128, 1]\n            mask2 = mask2[1:]\n            if self.aggreg == 'mean':\n                lengths2 = lengths.unsqueeze(1)\n                output = torch.sum(r_out * (1 - mask2), dim=0) \/ (lengths2 + 1)\n        elif masked_agg ==False:\n            \"\"\"Without masked aggregation across rows\"\"\"\n            output = r_out[-1, :, :].squeeze(0) # take the last step's output, shape[128, 36]\n\n        \"\"\"concat static\"\"\"\n\n        output_ = torch.cat([output, emb], dim=1) # [128, 36*5+9] # emb with dim: d_model\n        output = self.mlp_static(output_)  # 45-->45-->2\n\n        return output , 0, output_ # output_ is the learned feature\n","AFTER":"        r_out = torch.cat([src, pe], dim=-1)  # output shape: [215, 128, (36*4+36)]\n\n        # r_out = src\n\n        \"\"\"mask out the all-zero rows. \"\"\"\n        mask = torch.arange(maxlen + 1)[None, :] >= (lengths.cpu()[:, None] + 1)\n        mask = mask.squeeze(1).cuda()  # shape: [128, 216]\n\n        masked_agg = False\n        if masked_agg ==True:\n            \"\"\" masked aggregation across rows\"\"\"\n            # print('masked aggregation across rows')\n            mask2 = mask.permute(1, 0).unsqueeze(2).long()  # [216, 128, 1]\n            mask2 = mask2[1:]\n            if self.aggreg == 'mean':\n                lengths2 = lengths.unsqueeze(1)\n                output = torch.sum(r_out * (1 - mask2), dim=0) \/ (lengths2 + 1)\n        elif masked_agg ==False:\n            \"\"\"Without masked aggregation across rows\"\"\"\n            # output = r_out[-1, :, :].squeeze(0) # take the last step's output, shape[128, 36]\n            output = torch.sum(r_out, dim=0) \/ (lengths.unsqueeze(1) + 1)\n\n        \"\"\"concat static\"\"\"\n\n        output = torch.cat([output, emb], dim=1) # [128, 36*5+9] # emb with dim: d_model\n        output = self.mlp_static(output)  # 45-->45-->2\n\n        return output # , 0, output_ # output_ is the learned feature\n"}