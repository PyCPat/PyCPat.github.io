{"BEFORE":"        b, device, img_size, = image.shape[0], image.device, self.image_size\n        check_shape(image, 'b c h w', h = img_size, w = img_size, c = self.channels)\n\n        times = torch.randint(0, self.num_timesteps, (b,), device = device, dtype = torch.long)\n\n        image_embed = self.get_image_embed(image)\n        text_encodings = self.get_text_encodings(text) if exists(text) else None\n\n        loss = self.p_losses(image, times, image_embed = image_embed, text_encodings = text_encodings)\n        return loss\n","AFTER":"    def forward(self, image, text = None, unet_number = None):\n        assert not (len(self.unets) > 1 and not exists(unet_number)), f'you must specify which unet you want trained, from a range of 1 to {len(self.unets)}, if you are training cascading DDPM (multiple unets)'\n        unet_number = default(unet_number, 1)\n        assert 1 <= unet_number <= len(self.unets)\n\n        index = unet_number - 1\n        unet = self.unets[index]\n        target_image_size = self.image_sizes[index]\n\n        b, c, h, w, device, = *image.shape, image.device\n\n        check_shape(image, 'b c h w', c = self.channels)\n        assert h >= target_image_size and w >= target_image_size\n\n        times = torch.randint(0, self.num_timesteps, (b,), device = device, dtype = torch.long)\n\n        image_embed = self.get_image_embed(image)\n        text_encodings = self.get_text_encodings(text) if exists(text) else None\n\n        lowres_cond_img = image if index > 0 else None\n        ddpm_image = resize_image_to(image, target_image_size)\n        return self.p_losses(unet, ddpm_image, times, image_embed = image_embed, text_encodings = text_encodings, lowres_cond_img = lowres_cond_img)\n"}