{"BEFORE":"            dots.masked_fill_(mask[None, None, ...], -float('-inf'))\n            del mask\n\n        # input masking\n        if input_mask is not None:\n            mask_q = mask_k = input_mask\n            if padding < cf:\n                mask_k = F.pad(mask_k, (padding, 0), value=True)\n            mask_k = mask_k.reshape(b, -1, cf).sum(dim=-1) > 0\n            mask = mask_q[:, None, :, None] < mask_k[:, None, None, :]\n            mask = F.pad(mask, (1, 0), value=True)\n\n            dots.masked_fill_(~mask, -float('-inf'))\n","AFTER":"        dots = torch.einsum('bhid,bhjd->bhij', q, k) * d ** -0.5\n\n        mask_value = -torch.finfo(dots.dtype).max\n\n        # causal masking, if needed\n        if self.causal:\n            mask_q = mask_k = torch.arange(t, device=device)\n\n            if padding < cf:\n                mask_k = F.pad(mask_k, (padding, 0))\n\n            mask_k, _ = mask_k.reshape(-1, cf).max(dim=-1)\n            mask = mask_q[:, None] < mask_k[None, :]\n            mask = F.pad(mask, (1, 0), value=False)\n\n            dots.masked_fill_(mask[None, None, ...], mask_value)\n            del mask\n\n        # input masking\n        if input_mask is not None:\n            mask_q = mask_k = input_mask\n            if padding < cf:\n                mask_k = F.pad(mask_k, (padding, 0), value=True)\n            mask_k = mask_k.reshape(b, -1, cf).sum(dim=-1) > 0\n            mask = mask_q[:, None, :, None] < mask_k[:, None, None, :]\n            mask = F.pad(mask, (1, 0), value=True)\n\n            dots.masked_fill_(~mask, mask_value)\n"}