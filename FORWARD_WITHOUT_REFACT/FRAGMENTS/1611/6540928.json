{"BEFORE":"        batch_size, seq_len = history.shape\n        valid_his = (history > 0).long()\n        his_vectors = self.i_embeddings(history)\n\n        # Self-attention\n        # TODO: 其他模型结构\n        len_range = torch.from_numpy(np.arange(self.max_his)).to(history.device)\n        position = (lengths[:, None] - len_range[None, :seq_len]) * valid_his\n        pos_vectors = self.p_embeddings(position)\n        his_vectors = his_vectors + pos_vectors\n        attn_mask = valid_his.view(batch_size, 1, 1, seq_len)\n        his_vectors = self.transformer(his_vectors, attn_mask)\n        his_vectors = his_vectors * valid_his[:, :, None].float()\n        his_vector = his_vectors.sum(1) \/ lengths[:, None].float()\n        # his_vector = his_vectors[torch.arange(batch_size), lengths - 1, :]\n\n        intent_pred = self.proj(his_vector)  # bsz, K\n        return intent_pred\n","AFTER":"    def forward(self, history, lengths, t_history, user_min_t):\n        valid_his = (history > 0).long()\n        his_vectors = self.i_embeddings(history)\n        his_vector = self.encoder(his_vectors, lengths, valid_his, t_history, user_min_t)\n        intent_pred = self.proj(his_vector)  # bsz, K\n        return his_vector, intent_pred\n"}