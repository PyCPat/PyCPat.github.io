{"BEFORE":"        codes = list()\n        logits = list()\n        for i, (xRaw, k) in enumerate(zip(latents, self._k)):\n            n, c, h, w = xRaw.shape\n            # [n, c, h, w] -> [h, w, n, c]\n            encoderIn = xRaw.permute(2, 3, 0, 1)\n            # [h, w, n, c] -> [h*w, n, c]\n            if True:\n                encoderIn = self._position(encoderIn).reshape(-1, n, c)\n                # encoderIn = encoderIn.reshape(-1, n, c)\n                # [h*w, n, c]\n                x = self._encoder(encoderIn)\n            else:\n                x = encoderIn.reshape(-1, n ,c)\n            # similar to scaled dot-product attention\n            # [h*w, N, c]\n            quantized, sample, logit = self._attention(x, i, False)\n            if True:\n                # [h*w, n, c]\n                posistedQuantized = self._position(quantized.reshape(h, w, n, c)).reshape(-1, n, c)\n                deTransformed = self._decoder(posistedQuantized, posistedQuantized).reshape(h, w, n, c).permute(2, 3, 0, 1)\n            else:\n                # [h*w, n, c] -> [n, c, h*w] -> [n, c, h, w]\n                deTransformed = quantized.reshape(h, w, n, c).permute(2, 3, 0, 1)\n\n            # mask = torch.rand_like(xRaw) > coeff\n            # mixed = mask * xRaw.detach() + torch.logical_not(mask) * deTransformed\n            # [n, c, h, w]\n            quantizeds.append(deTransformed)\n            codes.append(sample.argmax(-1).permute(1, 0).reshape(n, h, w))\n            logits.append(logit.permute(1, 0, 2).reshape(n, h, w, k))\n","AFTER":"            quantized, samples, logits = self._attention(x, temp, False)\n            # quantized = x\n            if True:\n                # [h*w, n, c]\n                posistedQuantized = self._position(quantized.reshape(h, w, n, c)).reshape(-1, n, c)\n                deTransformed = self._decoder(posistedQuantized, posistedQuantized).reshape(h, w, n, c).permute(2, 3, 0, 1)\n            else:\n                # [h*w, n, c] -> [n, c, h*w] -> [n, c, h, w]\n                deTransformed = quantized.reshape(h, w, n, c).permute(2, 3, 0, 1)\n\n            # mask = torch.rand_like(xRaw) > coeff\n            # mixed = mask * xRaw.detach() + torch.logical_not(mask) * deTransformed\n            # [n, c, h, w]\n            quantizeds.append(deTransformed)\n            samples = [s.argmax(-1).permute(1, 0).reshape(n, h, w) for s in samples]\n            logits = [l.permute(1, 0, 2).reshape(n, h, w, k) for l in logits]\n"}