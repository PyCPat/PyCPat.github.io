{"BEFORE":"        prob = mixin \/ (mixin + 1.0)\n        rolloutDistribution = Bernoulli(probs=torch.tensor(prob).to(latents[0].device))\n        for xRaw, prob, squeeze, codebook, k in zip(latents, self._prob, self._squeeze, self._codebook, self._k):\n            n, c, h, w = xRaw.shape\n            # [c, k] -> [k, c]\n            # codewords = codebook.weight.t()\n            # [n, c, h, w] -> [h, w, n, c]\n            encoderIn = xRaw.permute(2, 3, 0, 1)\n            # [h, w, n, c] -> [h*w, n, c]\n            posisted = self._position(encoderIn).reshape(-1, n, c)\n            encoderIn = encoderIn.reshape(-1, n, c)\n            # [h*w, n, c]\n            x = self._encoder(posisted)\n            # x = self._encoder(posisted, codewords[:, None, ...].expand(k, n, c))\n            # [h*w, n, k]\n            logit = prob(x, h, w)\n            sample = F.gumbel_softmax(logit, temperature, hard)\n\n            # [N, h*w, c] <- [N, h*w, k] @ [k, C]\n            # quantized = codebook(sample)\n\n            quantized = sample\n\n            # normalize\n            # quantized \/= (k - 0.5) \/ (2 * k - 2)\n            # quantized -= 0.5 \/ (k - 1)\n            # [h*w, n, c]\n            quantized = squeeze(quantized, h, w)\n            # posistedQuantized = self._position(quantized.reshape(h, w, n, c))\n            # mixed = (mixin * encoderIn \/ (mixin + 1)) + (quantized \/ (mixin + 1))\n\n            mask = rolloutDistribution.sample((h*w, n, 1)).bool()\n\n            mixed = mask * encoderIn.detach() + torch.logical_not(mask) * quantized\n\n            # [h*w, n, c] -> [n, c, h*w] -> [n, c, h, w]\n            deTransformed = self._decoder(mixed, quantized).permute(1, 2, 0).reshape(n, c, h, w)\n            # [n, c, h, w]\n            quantizeds.append(deTransformed)\n            codes.append(sample)\n","AFTER":"        for xRaw, prob, squeeze, codebook, k in zip(latents, self._prob, self._squeeze, self._codebook, self._k):\n            n, c, h, w = xRaw.shape\n            # [c, k]\n            codewords = codebook.weight\n            # [n, c, h, w] -> [h, w, n, c]\n            encoderIn = xRaw.permute(2, 3, 0, 1)\n            # [h, w, n, c] -> [h*w, n, c]\n            # posisted = self._position(encoderIn).reshape(-1, n, c)\n            encoderIn = encoderIn.reshape(-1, n, c)\n            # [h*w, n, c]\n            # x = self._encoder(posisted)\n            x = self._encoder(encoderIn)\n            # [h*w, n, k]\n            # logit = prob(encoderIn, h, w)\n            logit = torch.matmul(x, codewords)\n            sample = F.gumbel_softmax(logit, temperature, hard)\n\n            # [h*w, N, c] <- [h*w, N, k] @ [k, C]\n            quantized = codebook(sample)\n\n            # quantized = sample\n\n            # normalize\n            # quantized \/= (k - 0.5) \/ (2 * k - 2)\n            # quantized -= 0.5 \/ (k - 1)\n            # [h*w, n, c]\n            # quantized = squeeze(quantized, h, w)\n            # posistedQuantized = self._position(quantized.reshape(h, w, n, c)).reshape(-1, n, c)\n\n            # mixed = (mixin * encoderIn \/ (mixin + 1)) + (quantized \/ (mixin + 1))\n\n            # mask = rolloutDistribution.sample((h*w, n, 1)).bool()\n\n            # mixed = mask * encoderIn.detach() + torch.logical_not(mask) * quantized\n            # [h*w, n, c] -> [n, c, h*w] -> [n, c, h, w]\n            deTransformed = self._decoder(quantized, quantized).permute(1, 2, 0).reshape(n, c, h, w)\n            # deTransformed = quantized.permute(1, 2, 0).reshape(n, c, h, w)\n            # deTransformed = self._dePosition(deTransformed).permute(2, 3, 0, 1)\n            # [n, c, h, w]\n            quantizeds.append(deTransformed)\n            codes.append(sample.argmax(-1).permute(1, 0).reshape(n, h, w))\n"}