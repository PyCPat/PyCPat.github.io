{"BEFORE":"        mel_loss = self.mae_loss(mel_predictions, mel_targets)\r\n\r\n        duration_loss = self.mse_loss(\r\n            log_duration_predictions, log_duration_targets)\r\n\r\n        kl_loss = self.kl_loss(*dist_info)\r\n\r\n        z, logdet = postnet_outputs\r\n        postnet_loss = self.mle_loss(z, logdet, mel_masks.unsqueeze(1))\r\n\r\n        total_loss = (\r\n            mel_loss + kl_loss + postnet_loss + duration_loss\r\n        )\r\n\r\n        return (\r\n            total_loss,\r\n            mel_loss,  # L_VG\r\n            kl_loss,  # L_KL\r\n            postnet_loss,  # L_PN\r\n            duration_loss,  # L_dur\r\n        )\r\n","AFTER":"    def forward(self, inputs, predictions, step):\r\n        (\r\n            mel_targets,\r\n            *_,\r\n        ) = inputs[11:]\r\n        (\r\n            mel_predictions,\r\n            postnet_outputs,\r\n            log_duration_predictions,\r\n            duration_roundeds,\r\n            src_masks,\r\n            mel_masks,\r\n            src_lens,\r\n            mel_lens,\r\n            alignments,\r\n            dist_info,\r\n            src_w_masks,\r\n            _,\r\n            alignment_logprobs,\r\n        ) = predictions\r\n        log_duration_targets = torch.log(duration_roundeds.float() + 1)\r\n        mel_targets = mel_targets[:, : mel_masks.shape[1], :]\r\n        mel_masks = mel_masks[:, :mel_masks.shape[1]]\r\n\r\n        log_duration_targets.requires_grad = False\r\n        mel_targets.requires_grad = False\r\n\r\n        log_duration_predictions = log_duration_predictions.masked_select(\r\n            src_w_masks)\r\n        log_duration_targets = log_duration_targets.masked_select(src_w_masks)\r\n\r\n        mel_predictions = mel_predictions.masked_select(\r\n            mel_masks.unsqueeze(-1))\r\n        mel_targets = mel_targets.masked_select(mel_masks.unsqueeze(-1))\r\n\r\n        mel_loss = self.mae_loss(mel_predictions, mel_targets)\r\n\r\n        duration_loss = self.mse_loss(\r\n            log_duration_predictions, log_duration_targets)\r\n\r\n        kl_loss = self.kl_loss(*dist_info)\r\n\r\n        z, logdet = postnet_outputs\r\n        postnet_loss = self.mle_loss(z, logdet, mel_masks.unsqueeze(1))\r\n\r\n        helper_loss = attn_loss = ctc_loss = torch.zeros(1).to(mel_targets.device)\r\n        if self.helper_type == \"dga\":\r\n            for alignment in alignments[1]: # DGA should be applied on attention without mapping mask\r\n                attn_loss += self.guided_attn_loss(alignment, src_lens, mel_lens)\r\n            # attn_loss = self.guided_attn_loss(alignments[1][0], src_lens, mel_lens)\r\n            helper_loss = self.guided_attn_weight * attn_loss\r\n        elif self.helper_type == \"ctc\":\r\n            for alignment_logprob in alignment_logprobs:\r\n                ctc_loss += self.sum_loss(alignment_logprob, src_lens, mel_lens)\r\n            ctc_loss = ctc_loss.mean()\r\n            helper_loss = (self.ctc_weight_start if step <= self.ctc_step else self.ctc_weight_end) * ctc_loss\r\n\r\n        total_loss = (\r\n            mel_loss + kl_loss + postnet_loss + duration_loss + helper_loss\r\n        )\r\n\r\n        return (\r\n            total_loss,\r\n            mel_loss,  # L_VG\r\n            kl_loss,  # L_KL\r\n            postnet_loss,  # L_PN\r\n            duration_loss,  # L_dur\r\n            helper_loss,\r\n        )\r\n"}