{"BEFORE":"        x = self.upsample_combiner(x, up_hiddens)\n\n        # final top-most residual if needed\n\n        if self.init_conv_to_final_conv_residual:\n            x = torch.cat((x, init_conv_residual), dim = 1)\n\n        if exists(self.final_res_block):\n            x = self.final_res_block(x, t, **conv_kwargs)\n\n        if exists(lowres_cond_img):\n            x = torch.cat((x, lowres_cond_img), dim = 1)\n\n        return self.final_conv(x)\n","AFTER":"            x = torch.cat((x, self_cond), dim = 1)\n\n        # add low resolution conditioning, if present\n\n        assert not (self.lowres_cond and not exists(lowres_cond_img)), 'low resolution conditioning image must be present'\n        assert not (self.lowres_cond and not exists(lowres_noise_times)), 'low resolution conditioning noise time must be present'\n\n        if exists(lowres_cond_img):\n            x = torch.cat((x, lowres_cond_img), dim = 1)\n\n            if exists(cond_video_frames):\n                lowres_cond_img = torch.cat((cond_video_frames, lowres_cond_img), dim = 2)\n                cond_video_frames = torch.cat((cond_video_frames, cond_video_frames), dim = 1)\n\n        # conditioning on video frames as a prompt\n        # todo - add post_cond_video_frames as well\n\n        num_preceding_frames = 0\n        if exists(cond_video_frames):\n            cond_video_frames_len = cond_video_frames.shape[2]\n\n            assert divisible_by(cond_video_frames_len, self.total_temporal_divisor)\n\n            cond_video_frames = resize_video_to(cond_video_frames, x.shape[-1])\n            x = torch.cat((cond_video_frames, x), dim = 2)\n\n            num_preceding_frames = cond_video_frames_len\n\n        # condition on input image\n\n        assert not (self.has_cond_image ^ exists(cond_images)), 'you either requested to condition on an image on the unet, but the conditioning image is not supplied, or vice versa'\n\n        if exists(cond_images):\n            assert cond_images.ndim == 4, 'conditioning images must have 4 dimensions only, if you want to condition on frames of video, use `cond_video_frames` instead'\n            assert cond_images.shape[1] == self.cond_images_channels, 'the number of channels on the conditioning image you are passing in does not match what you specified on initialiation of the unet'\n\n            cond_images = repeat(cond_images, 'b c h w -> b c f h w', f = x.shape[2])\n            cond_images = resize_video_to(cond_images, x.shape[-1], mode = self.resize_mode)\n\n            x = torch.cat((cond_images, x), dim = 1)\n\n        # ignoring time in pseudo 3d resnet blocks\n\n        conv_kwargs = dict(\n            ignore_time = ignore_time\n        )\n\n        # initial convolution\n\n        x = self.init_conv(x)\n\n        if not ignore_time:\n            x = self.init_temporal_peg(x)\n            x = self.init_temporal_attn(x)\n\n        # init conv residual\n\n        if self.init_conv_to_final_conv_residual:\n            init_conv_residual = x.clone()\n\n        # time conditioning\n\n        time_hiddens = self.to_time_hiddens(time)\n\n        # derive time tokens\n\n        time_tokens = self.to_time_tokens(time_hiddens)\n        t = self.to_time_cond(time_hiddens)\n\n        # add lowres time conditioning to time hiddens\n        # and add lowres time tokens along sequence dimension for attention\n\n        if self.lowres_cond:\n            lowres_time_hiddens = self.to_lowres_time_hiddens(lowres_noise_times)\n            lowres_time_tokens = self.to_lowres_time_tokens(lowres_time_hiddens)\n            lowres_t = self.to_lowres_time_cond(lowres_time_hiddens)\n\n            t = t + lowres_t\n            time_tokens = torch.cat((time_tokens, lowres_time_tokens), dim = -2)\n\n        # text conditioning\n\n        text_tokens = None\n\n        if exists(text_embeds) and self.cond_on_text:\n\n            # conditional dropout\n\n            text_keep_mask = prob_mask_like((batch_size,), 1 - cond_drop_prob, device = device)\n\n            text_keep_mask_embed = rearrange(text_keep_mask, 'b -> b 1 1')\n            text_keep_mask_hidden = rearrange(text_keep_mask, 'b -> b 1')\n\n            # calculate text embeds\n\n            text_tokens = self.text_to_cond(text_embeds)\n\n            text_tokens = text_tokens[:, :self.max_text_len]\n            \n            if exists(text_mask):\n                text_mask = text_mask[:, :self.max_text_len]\n\n            text_tokens_len = text_tokens.shape[1]\n            remainder = self.max_text_len - text_tokens_len\n\n            if remainder > 0:\n                text_tokens = F.pad(text_tokens, (0, 0, 0, remainder))\n\n            if exists(text_mask):\n                if remainder > 0:\n                    text_mask = F.pad(text_mask, (0, remainder), value = False)\n\n                text_mask = rearrange(text_mask, 'b n -> b n 1')\n                text_keep_mask_embed = text_mask & text_keep_mask_embed\n\n            null_text_embed = self.null_text_embed.to(text_tokens.dtype) # for some reason pytorch AMP not working\n\n            text_tokens = torch.where(\n                text_keep_mask_embed,\n                text_tokens,\n                null_text_embed\n            )\n\n            if exists(self.attn_pool):\n                text_tokens = self.attn_pool(text_tokens)\n\n            # extra non-attention conditioning by projecting and then summing text embeddings to time\n            # termed as text hiddens\n\n            mean_pooled_text_tokens = text_tokens.mean(dim = -2)\n\n            text_hiddens = self.to_text_non_attn_cond(mean_pooled_text_tokens)\n\n            null_text_hidden = self.null_text_hidden.to(t.dtype)\n\n            text_hiddens = torch.where(\n                text_keep_mask_hidden,\n                text_hiddens,\n                null_text_hidden\n            )\n\n            t = t + text_hiddens\n\n        # main conditioning tokens (c)\n\n        c = time_tokens if not exists(text_tokens) else torch.cat((time_tokens, text_tokens), dim = -2)\n\n        # normalize conditioning tokens\n\n        c = self.norm_cond(c)\n\n        # initial resnet block (for memory efficient unet)\n\n        if exists(self.init_resnet_block):\n            x = self.init_resnet_block(x, t, **conv_kwargs)\n\n        # go through the layers of the unet, down and up\n\n        hiddens = []\n\n        for pre_downsample, init_block, resnet_blocks, attn_block, temporal_peg, temporal_attn, temporal_downsample, post_downsample in self.downs:\n            if exists(pre_downsample):\n                x = pre_downsample(x)\n\n            x = init_block(x, t, c, **conv_kwargs)\n\n            for resnet_block in resnet_blocks:\n                x = resnet_block(x, t, **conv_kwargs)\n                hiddens.append(x)\n\n            x = attn_block(x, c)\n\n            if not ignore_time:\n                x = temporal_peg(x)\n                x = temporal_attn(x)\n\n            hiddens.append(x)\n\n            if exists(temporal_downsample) and not ignore_time:\n                x = temporal_downsample(x)\n\n            if exists(post_downsample):\n                x = post_downsample(x)\n\n        x = self.mid_block1(x, t, c, **conv_kwargs)\n\n        if exists(self.mid_attn):\n            x = self.mid_attn(x)\n\n        if not ignore_time:\n            x = self.mid_temporal_peg(x)\n            x = self.mid_temporal_attn(x)\n\n        x = self.mid_block2(x, t, c, **conv_kwargs)\n\n        add_skip_connection = lambda x: torch.cat((x, hiddens.pop() * self.skip_connect_scale), dim = 1)\n\n        up_hiddens = []\n\n        for init_block, resnet_blocks, attn_block, temporal_peg, temporal_attn, temporal_upsample, upsample in self.ups:\n            if exists(temporal_upsample) and not ignore_time:\n                x = temporal_upsample(x)\n\n            x = add_skip_connection(x)\n            x = init_block(x, t, c, **conv_kwargs)\n\n            for resnet_block in resnet_blocks:\n                x = add_skip_connection(x)\n                x = resnet_block(x, t, **conv_kwargs)\n\n            x = attn_block(x, c)\n\n            if not ignore_time:\n                x = temporal_peg(x)\n                x = temporal_attn(x)\n\n            up_hiddens.append(x.contiguous())\n\n            x = upsample(x)\n\n        # whether to combine all feature maps from upsample blocks\n\n        x = self.upsample_combiner(x, up_hiddens)\n\n        # final top-most residual if needed\n\n        if self.init_conv_to_final_conv_residual:\n            x = torch.cat((x, init_conv_residual), dim = 1)\n\n        if exists(self.final_res_block):\n            x = self.final_res_block(x, t, **conv_kwargs)\n\n        if exists(lowres_cond_img):\n            x = torch.cat((x, lowres_cond_img), dim = 1)\n\n        out = self.final_conv(x)\n\n        if num_preceding_frames > 0:\n            out = out[:, :, num_preceding_frames:]\n\n        return out\n"}