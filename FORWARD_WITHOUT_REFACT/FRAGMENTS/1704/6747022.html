<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        if exists(lowres_cond_img):
            x = torch.cat((x, lowres_cond_img), dim = 1)

        <a id="change">return </a><a id="change">self.final_conv(</a>x<a id="change">)</a>
</code></pre><h3>After Change</h3><pre><code class='java'>

            assert divisible_by(cond_video_frames_len, self.total_temporal_divisor)

            cond_video_frames<a id="change"> = </a>resize_video_to(cond_video_frames, x.shape[-1])
            x<a id="change"> = </a><a id="change">torch.cat(</a>(cond_video_frames, x)<a id="change">, dim = 2)</a>

            num_preceding_frames = cond_video_frames_len

        &#47&#47 condition on input image

        assert not (self.has_cond_image ^ exists(cond_images)), &quotyou either requested to condition on an image on the unet, but the conditioning image is not supplied, or vice versa&quot

        if exists(cond_images):
            assert cond_images.ndim == 4, &quotconditioning images must have 4 dimensions only, if you want to condition on frames of video, use `cond_video_frames` instead&quot
            assert cond_images.shape[1] == self.cond_images_channels, &quotthe number of channels on the conditioning image you are passing in does not match what you specified on initialiation of the unet&quot

            cond_images = repeat(cond_images, &quotb c h w -&gt; b c f h w&quot, f = x.shape[2])
            cond_images = resize_video_to(cond_images, x.shape[-1], mode = self.resize_mode)

            x = torch.cat((cond_images, x), dim = 1)

        &#47&#47 ignoring time in pseudo 3d resnet blocks

        conv_kwargs = dict(
            ignore_time = ignore_time
        )

        &#47&#47 initial convolution

        x = self.init_conv(x)

        if not ignore_time:
            x = self.init_temporal_peg(x)
            x = self.init_temporal_attn(x)

        &#47&#47 init conv residual

        if self.init_conv_to_final_conv_residual:
            init_conv_residual = x.clone()

        &#47&#47 time conditioning

        time_hiddens = self.to_time_hiddens(time)

        &#47&#47 derive time tokens

        time_tokens = self.to_time_tokens(time_hiddens)
        t = self.to_time_cond(time_hiddens)

        &#47&#47 add lowres time conditioning to time hiddens
        &#47&#47 and add lowres time tokens along sequence dimension for attention

        if self.lowres_cond:
            lowres_time_hiddens = self.to_lowres_time_hiddens(lowres_noise_times)
            lowres_time_tokens = self.to_lowres_time_tokens(lowres_time_hiddens)
            lowres_t = self.to_lowres_time_cond(lowres_time_hiddens)

            t = t + lowres_t
            time_tokens = torch.cat((time_tokens, lowres_time_tokens), dim = -2)

        &#47&#47 text conditioning

        text_tokens = None

        if exists(text_embeds) and self.cond_on_text:

            &#47&#47 conditional dropout

            text_keep_mask = prob_mask_like((batch_size,), 1 - cond_drop_prob, device = device)

            text_keep_mask_embed = rearrange(text_keep_mask, &quotb -&gt; b 1 1&quot)
            text_keep_mask_hidden = rearrange(text_keep_mask, &quotb -&gt; b 1&quot)

            &#47&#47 calculate text embeds

            text_tokens = self.text_to_cond(text_embeds)

            text_tokens = text_tokens[:, :self.max_text_len]
            
            if exists(text_mask):
                text_mask = text_mask[:, :self.max_text_len]

            text_tokens_len = text_tokens.shape[1]
            remainder = self.max_text_len - text_tokens_len

            if remainder &gt; 0:
                text_tokens = F.pad(text_tokens, (0, 0, 0, remainder))

            if exists(text_mask):
                if remainder &gt; 0:
                    text_mask = F.pad(text_mask, (0, remainder), value = False)

                text_mask = rearrange(text_mask, &quotb n -&gt; b n 1&quot)
                text_keep_mask_embed = text_mask & text_keep_mask_embed

            null_text_embed = self.null_text_embed.to(text_tokens.dtype) &#47&#47 for some reason pytorch AMP not working

            text_tokens = torch.where(
                text_keep_mask_embed,
                text_tokens,
                null_text_embed
            )

            if exists(self.attn_pool):
                text_tokens = self.attn_pool(text_tokens)

            &#47&#47 extra non-attention conditioning by projecting and then summing text embeddings to time
            &#47&#47 termed as text hiddens

            mean_pooled_text_tokens = text_tokens.mean(dim = -2)

            text_hiddens = self.to_text_non_attn_cond(mean_pooled_text_tokens)

            null_text_hidden = self.null_text_hidden.to(t.dtype)

            text_hiddens = torch.where(
                text_keep_mask_hidden,
                text_hiddens,
                null_text_hidden
            )

            t = t + text_hiddens

        &#47&#47 main conditioning tokens (c)

        c = time_tokens if not exists(text_tokens) else torch.cat((time_tokens, text_tokens), dim = -2)

        &#47&#47 normalize conditioning tokens

        c = self.norm_cond(c)

        &#47&#47 initial resnet block (for memory efficient unet)

        if exists(self.init_resnet_block):
            x = self.init_resnet_block(x, t, **conv_kwargs)

        &#47&#47 go through the layers of the unet, down and up

        hiddens = []

        for pre_downsample, init_block, resnet_blocks, attn_block, temporal_peg, temporal_attn, temporal_downsample, post_downsample in self.downs:
            if exists(pre_downsample):
                x = pre_downsample(x)

            x = init_block(x, t, c, **conv_kwargs)

            for resnet_block in resnet_blocks:
                x = resnet_block(x, t, **conv_kwargs)
                hiddens.append(x)

            x = attn_block(x, c)

            if not ignore_time:
                x = temporal_peg(x)
                x = temporal_attn(x)

            hiddens.append(x)

            if exists(temporal_downsample) and not ignore_time:
                x = temporal_downsample(x)

            if exists(post_downsample):
                x = post_downsample(x)

        x = self.mid_block1(x, t, c, **conv_kwargs)

        if exists(self.mid_attn):
            x = self.mid_attn(x)

        if not ignore_time:
            x = self.mid_temporal_peg(x)
            x = self.mid_temporal_attn(x)

        x = self.mid_block2(x, t, c, **conv_kwargs)

        add_skip_connection = lambda x: torch.cat((x, hiddens.pop() * self.skip_connect_scale), dim = 1)

        up_hiddens = []

        for init_block, resnet_blocks, attn_block, temporal_peg, temporal_attn, temporal_upsample, upsample in self.ups:
            if exists(temporal_upsample) and not ignore_time:
                x = temporal_upsample(x)

            x = add_skip_connection(x)
            x = init_block(x, t, c, **conv_kwargs)

            for resnet_block in resnet_blocks:
                x = add_skip_connection(x)
                x = resnet_block(x, t, **conv_kwargs)

            x = attn_block(x, c)

            if not ignore_time:
                x = temporal_peg(x)
                x = temporal_attn(x)

            up_hiddens.append(x.contiguous())

            x = upsample(x)

        &#47&#47 whether to combine all feature maps from upsample blocks

        x = self.upsample_combiner(x, up_hiddens)

        &#47&#47 final top-most residual if needed

        if self.init_conv_to_final_conv_residual:
            x = torch.cat((x, init_conv_residual), dim = 1)

        if exists(self.final_res_block):
            x = self.final_res_block(x, t, **conv_kwargs)

        if exists(lowres_cond_img):
            x = torch.cat((x, lowres_cond_img), dim = 1)

        out<a id="change"> = </a><a id="change">self.final_conv(</a>x<a id="change">)</a>

        if num_preceding_frames &gt; 0:
            out<a id="change"> = </a>out[:, :, num_preceding_frames:]

        <a id="change">return </a>out
</code></pre>