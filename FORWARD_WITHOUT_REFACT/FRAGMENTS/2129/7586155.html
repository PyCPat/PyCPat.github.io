<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        items_embedding = self.item_embedding(torch.tensor([i for i in range(self.items_total)]).to(nodes_output.device))
        alpha = torch.sigmoid(self.alpha)
        embed = (1 - alpha) * items_embedding.clone() + alpha * nodes_output
        <a id="change">return </a>embed


class AggregateTemporalNodeFeatures(nn.Module):</code></pre><h3>After Change</h3><pre><code class='java'>
        :param nodes_output: the output of self-attention model in time dimension, (n_1+n_2+..., F)
        :return:
        
        batch_size = nodes_output.shape[0]<a id="change"> // </a>self.items_total
        id = 0
        num_nodes = self.items_total
        items_embedding = self.item_embedding(torch.tensor([i for i in range(self.items_total)]).to(nodes_output.device))
        batch_embedding = []
        <a id="change">for _</a> in <a id="change">range(</a>batch_size<a id="change">):
            </a>output_node_features<a id="change"> = </a>nodes_output[id:id + num_nodes, :]
            embed = (1 - self.alpha) * items_embedding

            embed<a id="change"> = </a>embed + self.alpha * output_node_features
            batch_embedding.append(embed)
            id += num_nodes
        batch_embedding = torch.stack(batch_embedding)</code></pre>