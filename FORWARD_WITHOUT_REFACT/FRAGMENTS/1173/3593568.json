{"BEFORE":"            mask = q_mask * k_mask\n\n        # expand queries and keys for concatting\n\n        q = repeat(q, 'b h i d -> b h i n d', n = j)\n\n        edge_input = (q - k) + rel_dist_pos_emb\n\n        if exists(edges):\n            if exists(nbhd_indices):\n                edges = batched_index_select(edges, nbhd_indices, dim = 2)\n\n            edges = repeat(edges, 'b i j d -> b h i j d', h = h)\n            edge_input = torch.cat((edge_input, edges), dim = -1)\n\n        m_ij = self.edge_mlp(edge_input)\n\n        coor_mlp_input = rearrange(m_ij, 'b h i j d -> b i j (h d)')\n        coor_weights = self.coors_mlp(coor_mlp_input)\n\n        if exists(mask):\n            coor_mask = rearrange(mask, 'b () i j -> b i j')\n            coor_weights.masked_fill_(~coor_mask, 0.)\n\n        rel_coors = self.rel_coors_norm(rel_coors)\n        coors_out = einsum('b i j, b i j c -> b i c', coor_weights, rel_coors)\n","AFTER":"            mask = q_mask * k_mask\n\n        # expand queries and keys for concatting\n\n        q = repeat(q, 'b h i d -> b h i n d', n = j)\n\n        edge_input = (q - k) + rel_dist_pos_emb\n\n        if exists(edges):\n            if exists(nbhd_indices):\n                edges = batched_index_select(edges, nbhd_indices, dim = 2)\n\n            edges = repeat(edges, 'b i j d -> b h i j d', h = h)\n            edge_input = torch.cat((edge_input, edges), dim = -1)\n\n        m_ij = self.edge_mlp(edge_input)\n\n        coor_weights = self.coors_mlp(m_ij)\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(coor_weights.dtype).max\n            coor_weights.masked_fill_(~mask, max_neg_value)\n            coor_weights = coor_weights.softmax(dim = -1)\n\n        rel_coors = self.rel_coors_norm(rel_coors)\n\n        coors_out = einsum('b h i j, b i j c -> b i c h', coor_weights, rel_coors)\n        coors_out = self.to_coors_out(coors_out)\n"}