{"BEFORE":"        input = rearrange(input, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = p, p2 = p)\n\n        mask = get_mask_subset_with_prob(input, self.mask_prob)\n\n        # mask input with mask patches with probability of `replace_prob` (keep patches the same with probability 1 - replace_prob)\n        masked_input = input.clone().detach()\n\n         # if random token probability > 0 for mpp\n        if self.random_patch_prob > 0:\n            random_patch_sampling_prob = self.random_patch_prob \/ (1 - self.replace_prob)\n            random_patch_prob = prob_mask_like(input, random_patch_sampling_prob)\n            bool_random_patch_prob = mask * random_patch_prob == True\n            random_patches = torch.randint(0, input.shape[1], (input.shape[0], input.shape[1]), device=input.device)\n            randomized_input = masked_input[torch.arange(masked_input.shape[0]).unsqueeze(-1), random_patches]\n            masked_input[bool_random_patch_prob] = randomized_input[bool_random_patch_prob]\n\n        # [mask] input\n        replace_prob = prob_mask_like(input, self.replace_prob)\n        bool_mask_replace = (mask * replace_prob) == True\n        masked_input[bool_mask_replace] = self.mask_token\n\n        # get labels for input patches that were masked\n        bool_mask = mask == True\n        labels = input[bool_mask]\n\n        # get generator output and get mpp loss\n        cls_logits = self.transformer(masked_input, mpp=True, **kwargs)\n","AFTER":"        masked_input[bool_mask_replace] = self.mask_token\n\n        # linear embedding of patches\n        masked_input = self.transformer.patch_to_embedding(masked_input)\n\n        # add cls token to input sequence\n        b, n, _ = masked_input.shape\n        cls_tokens = repeat(self.transformer.cls_token, '() n d -> b n d', b = b)\n        masked_input = torch.cat((cls_tokens, masked_input), dim=1)\n\n        # add positional embeddings to input\n        masked_input += self.transformer.pos_embedding[:, :(n + 1)]\n        masked_input = self.transformer.dropout(masked_input)\n\n        # get generator output and get mpp loss\n        masked_input = self.transformer.transformer(masked_input, **kwargs)\n        cls_logits = self.to_bits(masked_input)\n"}