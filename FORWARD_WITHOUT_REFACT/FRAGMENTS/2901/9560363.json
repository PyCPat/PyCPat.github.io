{"BEFORE":"        sig_t = torch.nn.functional.softplus(b_t) + self.eps\n\n        mu_t = self.mu_prev + torch.nn.functional.softplus(k_t)\n        g_t = torch.softmax(g_t, dim=-1) \/ sig_t + self.eps\n\n        # each B x K x T_in\n        g_t = g_t.unsqueeze(2).expand(g_t.size(0),\n                                      g_t.size(1),\n                                      inputs.size(1))\n        sig_t = sig_t.unsqueeze(2).expand_as(g_t)\n        mu_t_ = mu_t.unsqueeze(2).expand_as(g_t)\n        j = self.J[:g_t.size(0), :, :inputs.size(1)]\n\n        # attention weights\n        phi_t = g_t * torch.exp(-0.5 * (mu_t_ - j)**2 \/ (sig_t**2))\n","AFTER":"        sig_t = torch.nn.functional.softplus(b_t) + self.eps\n\n        mu_t = self.mu_prev + torch.nn.functional.softplus(k_t)\n        g_t = torch.softmax(g_t, dim=-1) \/ sig_t + self.eps\n\n        j = self.J[:inputs.size(1)+1]\n\n        # attention weights\n        phi_t = g_t.unsqueeze(-1) * torch.exp(-0.5 * (mu_t.unsqueeze(-1) - j)**2 \/ (sig_t.unsqueeze(-1)**2))\n\n        # discritize attention weights\n        alpha_t = self.COEF * torch.sum(phi_t, 1)\n        alpha_t = alpha_t[:, 1:] - alpha_t[:, :-1]\n"}