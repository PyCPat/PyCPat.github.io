{"BEFORE":"        zs = torch.split(z, self.chunk_size, 1)\n        z = zs[0]\n        if shared_label is None:\n            shared_label = self.shared(label)\n        else:\n            pass\n        labels = [torch.cat([shared_label, item], 1) for item in zs[1:]]\n\n        act = self.linear0(z)\n        act = act.view(-1, self.in_dims[0], self.bottom, self.bottom)\n        counter = 0\n        for index, blocklist in enumerate(self.blocks):\n            for block in blocklist:\n                if isinstance(block, ops.SelfAttention):\n                    act = block(act)\n                else:\n                    act = block(act, labels[counter])\n                    counter += 1\n\n        act = self.bn4(act)\n        act = self.activation(act)\n        act = self.conv2d5(act)\n        out = self.tanh(act)\n","AFTER":"    def forward(self, z, label, shared_label=None, eval=False):\n        with torch.cuda.amp.autocast() if self.mixed_precision and not eval else misc.dummy_context_mgr() as mp:\n            zs = torch.split(z, self.chunk_size, 1)\n            z = zs[0]\n            if shared_label is None:\n                shared_label = self.shared(label)\n            else:\n                pass\n            labels = [torch.cat([shared_label, item], 1) for item in zs[1:]]\n\n            act = self.linear0(z)\n            act = act.view(-1, self.in_dims[0], self.bottom, self.bottom)\n            counter = 0\n            for index, blocklist in enumerate(self.blocks):\n                for block in blocklist:\n                    if isinstance(block, ops.SelfAttention):\n                        act = block(act)\n                    else:\n                        act = block(act, labels[counter])\n                        counter += 1\n\n            act = self.bn4(act)\n            act = self.activation(act)\n            act = self.conv2d5(act)\n            out = self.tanh(act)\n        return out\n"}