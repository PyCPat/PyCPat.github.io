{"BEFORE":"        T = self.params[1].item()\n        Z_l = self.params[2].item()\n        Z_ab = self.params[3].item()\n\n        momentum = self.params[4].item()\n        batchSize = l.size(0)\n        outputSize = self.memory_l.size(0)\n        inputSize = self.memory_l.size(1)\n\n        # score computation\n        if idx is None:\n            idx = self.multinomial.draw(batchSize * (self.K + 1)).view(batchSize, -1)\n            idx.select(1, 0).copy_(y.data)\n        # sample\n        weight_l = torch.index_select(self.memory_l, 0, idx.view(-1)).detach()\n        weight_l = weight_l.view(batchSize, K + 1, inputSize)\n        out_ab = torch.bmm(weight_l, ab.view(batchSize, inputSize, 1))\n        # sample\n        weight_ab = torch.index_select(self.memory_ab, 0, idx.view(-1)).detach()\n        weight_ab = weight_ab.view(batchSize, K + 1, inputSize)\n        out_l = torch.bmm(weight_ab, l.view(batchSize, inputSize, 1))\n\n        if self.use_softmax:\n            out_ab = torch.div(out_ab, T)\n            out_l = torch.div(out_l, T)\n            out_l = out_l.contiguous()\n            out_ab = out_ab.contiguous()\n        else:\n            out_ab = torch.exp(torch.div(out_ab, T))\n            out_l = torch.exp(torch.div(out_l, T))\n            # set Z_0 if haven't been set yet,\n            # Z_0 is used as a constant approximation of Z, to scale the probs\n            if Z_l < 0:\n                self.params[2] = out_l.mean() * outputSize\n                Z_l = self.params[2].clone().detach().item()\n                print(\"normalization constant Z_l is set to {:.1f}\".format(Z_l))\n            if Z_ab < 0:\n                self.params[3] = out_ab.mean() * outputSize\n                Z_ab = self.params[3].clone().detach().item()\n                print(\"normalization constant Z_ab is set to {:.1f}\".format(Z_ab))\n            # compute out_l, out_ab\n            out_l = torch.div(out_l, Z_l).contiguous()\n            out_ab = torch.div(out_ab, Z_ab).contiguous()\n","AFTER":"        T = self.params[1].item()\n        Z_l = self.params[2].item()\n        Z_ab = self.params[3].item()\n        momentum = self.params[4].item()\n        batchSize = l.size(0)\n        outputSize = self.memory_l.size(0)\n        inputSize = self.memory_l.size(1)\n\n        # score computation\n        if idx is None:\n            idx = self.multinomial.draw(batchSize * (self.K + 1)).view(batchSize, -1)\n            idx.select(1, 0).copy_(y.data)\n        # sample\n        weight_l = torch.index_select(self.memory_l, 0, idx.view(-1)).detach()\n        weight_l = weight_l.view(batchSize, K + 1, inputSize)\n        out_ab = torch.bmm(weight_l, ab.view(batchSize, inputSize, 1))\n        # sample\n        weight_ab = torch.index_select(self.memory_ab, 0, idx.view(-1)).detach()\n        weight_ab = weight_ab.view(batchSize, K + 1, inputSize)\n        out_l = torch.bmm(weight_ab, l.view(batchSize, inputSize, 1))\n        print('l', torch.max(l), torch.max(weight_ab))\n        print('max', torch.max(out_l))\n        if self.use_softmax:\n            out_ab = torch.div(out_ab, T)\n            out_l = torch.div(out_l, T)\n            out_l = out_l.contiguous()\n            out_ab = out_ab.contiguous()\n        else:\n            tmp = torch.div(out_l, T) #EDIT            \n            out_ab = torch.exp(torch.div(torch.div(out_ab, out_ab.mean()*outputSize), T))\n            out_l = torch.exp(torch.div(torch.div(out_l, out_ab.mean()*outputSize), T))\n            print('divide T', torch.max(tmp))\n            print('after', torch.max(out_l))\n\n            # set Z_0 if haven't been set yet,\n            # Z_0 is used as a constant approximation of Z, to scale the probs\n            # if Z_l < 0:\n            self.params[2] = out_l.mean() * outputSize\n            Z_l = self.params[2].clone().detach().item()\n            print(\"normalization constant Z_l is set to {:.1f}\".format(Z_l))\n            \n            # if Z_ab < 0:\n            self.params[3] = out_ab.mean() * outputSize\n            Z_ab = self.params[3].clone().detach().item()\n            print(\"normalization constant Z_ab is set to {:.1f}\".format(Z_ab))\n            # compute out_l, out_ab\n            print(torch.max(out_l), Z_l)\n            # out_l = torch.div(out_l, Z_l).contiguous()\n            # out_ab = torch.div(out_ab, Z_ab).contiguous()\n            # print('after norm', torch.max(out_l))\n\n        # # update memory\n        with torch.no_grad():\n            l_pos = torch.index_select(self.memory_l, 0, y.view(-1))\n            l_pos.mul_(momentum)\n            l_pos.add_(torch.mul(l, 1 - momentum))\n            l_norm = l_pos.pow(2).sum(1, keepdim=True).pow(0.5)\n            updated_l = l_pos.div(l_norm)\n            print('updated_l', torch.max(updated_l))\n"}