{"BEFORE":"        retrieved = self.token_emb(retrieved)\n\n        # get absolute positional embedding\n\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n        pos_emb = rearrange(pos_emb, 'n d -> 1 n d')\n        embed = embed + pos_emb\n\n        # encode\n\n        retrieved = rearrange(retrieved, 'b k n d -> (b k) n d')\n","AFTER":"        if retrieved.ndim == 3:\n            retrieved = rearrange(retrieved, 'b k n -> b k 1 n') # 1 neighbor retrieved\n\n        # if training, derive labels\n\n        if return_loss:\n            seq, labels = seq[:, :-1], seq[:, 1:]\n\n        # variables\n\n        n, chunk_size, num_retrieved, device = seq.shape[-1], retrieved.shape[-1], retrieved.shape[-2], seq.device\n\n        assert divisible_by(n, chunk_size), 'sequence length must be divisible by chunk size'\n\n        # embed both sequence and retrieved chunks\n\n        embed = self.token_emb(seq)\n        retrieved = self.token_emb(retrieved)\n\n        # get absolute positional embedding\n\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n        pos_emb = rearrange(pos_emb, 'n d -> 1 n d')\n        embed = embed + pos_emb\n\n        # encode\n\n        retrieved = rearrange(retrieved, 'b k r n d -> (b k r) n d', r = num_retrieved)\n        embed_as_context = repeat(embed, 'b (k n) d -> (b k r) n d', n = chunk_size, r = num_retrieved)\n\n        retrieved = self.encoder(retrieved, context = embed_as_context)\n        retrieved = rearrange(retrieved, '(b k r) n d -> b k r n d', k = n \/\/ chunk_size, r = num_retrieved)\n\n        # project both sequence embedding and retrieved embedding to decoder dimension if necessary\n\n        embed = self.to_decoder_model_dim(embed)\n        retrieved = self.encoder_output_to_decoder_dim(retrieved)\n"}