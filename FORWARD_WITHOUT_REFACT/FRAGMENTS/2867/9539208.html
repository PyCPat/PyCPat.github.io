<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        shape, dtype = x.shape, x.dtype

        flatten<a id="change"> = </a>rearrange(x, &quot... d -&gt; (...) d&quot)
        flatten = l2norm(flatten)

        self.init_embed_(flatten)

        embed = self.embed if not self.learnable_codebook else self.embed.detach()
        embed = l2norm(embed)

        dist = flatten @ embed.t()
        embed_ind = gumbel_sample(dist, dim = -1, temperature = self.sample_codebook_temp)
        embed_onehot = F.one_hot(embed_ind, self.codebook_size).type(dtype)
        embed_ind = embed_ind.view(*shape[:-1])

        quantize = F.embedding(embed_ind, self.embed)

        if self.training:
            bins = embed_onehot.sum(0)
            self.all_reduce_fn(bins)

            ema_inplace(self.cluster_size, bins, self.decay)

            zero_mask = (bins == 0)
            bins = bins.masked_fill(zero_mask, 1.)

            embed_sum = flatten.t() @ embed_onehot
            self.all_reduce_fn(embed_sum)

            embed_normalized<a id="change"> = </a>(embed_sum<a id="change"> / </a><a id="change">bins.unsqueeze(0</a><a id="change">)</a>).t()
            embed_normalized = l2norm(embed_normalized)
            embed_normalized = torch.where(zero_mask[..., None], embed,
                                           embed_normalized)</code></pre><h3>After Change</h3><pre><code class='java'>
        x = x.float()

        if needs_codebook_dim:
            x<a id="change"> = </a>rearrange(x, &quot... -&gt; 1 ...&quot)

        shape, dtype = x.shape, x.dtype

        flatten = rearrange(x, &quoth ... d -&gt; h (...) d&quot)
        flatten = l2norm(flatten)

        self.init_embed_(flatten)

        embed = self.embed if not self.learnable_codebook else self.embed.detach()
        embed = l2norm(embed)

        dist = einsum(&quoth n d, h c d -&gt; h n c&quot, flatten, embed)
        embed_ind = gumbel_sample(dist, dim = -1, temperature = self.sample_codebook_temp)
        embed_onehot = F.one_hot(embed_ind, self.codebook_size).type(dtype)
        embed_ind = embed_ind.view(*shape[:-1])

        quantize = batched_embedding(embed_ind, self.embed)

        if self.training:
            bins = embed_onehot.sum(dim = 1)
            self.all_reduce_fn(bins)

            ema_inplace(self.cluster_size, bins, self.decay)

            zero_mask = (bins == 0)
            bins = bins.masked_fill(zero_mask, 1.)

            embed_sum<a id="change"> = </a>einsum(&quoth n d, h n c -&gt; h c d&quot, flatten, embed_onehot)
            self.all_reduce_fn(embed_sum)

            embed_normalized = embed_sum / rearrange(bins, &quot... -&gt; ... 1&quot)
            embed_normalized = l2norm(embed_normalized)

            embed_normalized = torch.where(
                rearrange(zero_mask, &quot... -&gt; ... 1&quot),
                embed,
                embed_normalized
            )

            ema_inplace(self.embed, embed_normalized, self.decay)
            self.expire_codes_(x)

        <a id="change">if needs_codebook_dim</a><a id="change">:
            </a>quantize<a id="change">, embed_ind = </a>map(lambda t: rearrange(t, &quot1 ... -&gt; ...&quot), (quantize<a id="change">, embed_ind</a>))

        return quantize, embed_ind
</code></pre>