<html><h3>Pattern ID :1388
</h3><img src='4036669.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        text_mask = None,
        return_loss = False
    ):
        seq_len, device = <a id="change">text.shape[1]</a>, text.device
        assert seq_len &lt;= self.text_max_seq_len, &quotyour input text has a greater length than what was designated on initialization&quot

        tokens = self.text_embedding(text)</code></pre><h3>After Change</h3><pre><code class='java'>
        text_embeds = self.text_transformer(tokens, mask = text_mask)

        frame_indices = self.vae.get_video_indices(video)
        frame_indices<a id="change"> = </a>rearrange(frame_indices, &quotb ... -&gt; b (...)&quot)
        frame_indices_input = frame_indices[:, :-1] if return_loss else frame_indices

        frame_embeddings = self.image_embedding(frame_indices_input)
        frame_embeddings = self.video_pos_emb(frame_embeddings)<a id="change"> + </a>frame_embeddings

        bos = repeat(self.video_bos, &quotd -&gt; b 1 d&quot, b = batch)
        frame_embeddings<a id="change"> = </a>torch.cat((bos, frame_embeddings), dim = 1)
        frame_embeddings = self.video_transformer(frame_embeddings)

        logits = self.to_logits(frame_embeddings)

        if not return_loss:
            return logits

        loss<a id="change"> = </a>F.cross_entropy(rearrange(logits, &quotb n c -&gt; b c n&quot), frame_indices)
        <a id="change">return </a>loss
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/nuwa-pytorch/commit/daeefa2be5de809f9003a5333dad62183cf819f7#diff-47b637f0a46bb7e4f245f382a9b47f0351b6a5fda8ab6476a0476b4677441eddL219' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4036669</div><div id='project'> Project Name: lucidrains/nuwa-pytorch</div><div id='commit'> Commit Name: daeefa2be5de809f9003a5333dad62183cf819f7</div><div id='time'> Time: 2021-12-28</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: nuwa_pytorch/nuwa_pytorch.py</div><div id='m_class'> M Class Name: NUWA</div><div id='n_method'> N Class Name: NUWA</div><div id='m_method'> M Method Name: forward(1)</div><div id='n_method'> N Method Name: forward(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: nuwa_pytorch/nuwa_pytorch.py</div><div id='n_file'> N File Name: nuwa_pytorch/nuwa_pytorch.py</div><div id='m_start'> M Start Line: 219</div><div id='m_end'> M End Line: 227</div><div id='n_start'> N Start Line: 270</div><div id='n_end'> N End Line: 296</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        padding_mask: Optional[torch.Tensor] = None,
    ) -&gt; torch.Tensor:

        seq_len = <a id="change">x.shape[1]</a>  &#47&#47 Batch first = True

        x = self.embedding(x)  &#47&#47 (batch_size, target_seq_len, d_model)
        x *= math.sqrt(self.d_model)</code></pre><h3>After Change</h3><pre><code class='java'>
            output = output + self.dropout(
                self.attention[i](normed_output, normed_output, normed_output, target_mask)
            )
            normed_output<a id="change"> = </a>self.layer_norm(output)
            output = output<a id="change"> + </a>self.dropout(
                self.source_attention[i](normed_output, memory, memory, source_mask)
            )
            normed_output<a id="change"> = </a>self.layer_norm(output)
            output<a id="change"> = </a>output + self.dropout(self.position_feed_forward[i](normed_output))

        <a id="change">return </a>self.layer_norm(output)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/mindee/doctr/commit/fddceba7bee5098b4219b7ba6a0bdf4f4a98adfe#diff-5eb4b6b7a215017ac91351a0c5e665af04fa644ddf1ecd5037545ba096ec1337L69' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4036735</div><div id='project'> Project Name: mindee/doctr</div><div id='commit'> Commit Name: fddceba7bee5098b4219b7ba6a0bdf4f4a98adfe</div><div id='time'> Time: 2022-06-09</div><div id='author'> Author: felixdittrich92@gmail.com</div><div id='file'> File Name: doctr/models/recognition/transformer/pytorch.py</div><div id='m_class'> M Class Name: Decoder</div><div id='n_method'> N Class Name: Decoder</div><div id='m_method'> M Method Name: forward(5)</div><div id='n_method'> N Method Name: forward(5)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: doctr/models/recognition/transformer/pytorch.py</div><div id='n_file'> N File Name: doctr/models/recognition/transformer/pytorch.py</div><div id='m_start'> M Start Line: 74</div><div id='m_end'> M End Line: 91</div><div id='n_start'> N Start Line: 147</div><div id='n_end'> N End Line: 167</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        return loss

    def forward(self, image, text = None):
        b, device, img_size, = <a id="change">image.shape[0]</a>, image.device, self.image_size
        check_shape(image, &quotb c h w&quot, h = img_size, w = img_size, c = self.channels)

        times = torch.randint(0, self.num_timesteps, (b,), device = device, dtype = torch.long)</code></pre><h3>After Change</h3><pre><code class='java'>

    def forward(self, image, text = None, unet_number = None):
        assert not (len(self.unets) &gt; 1 and not exists(unet_number)), f&quotyou must specify which unet you want trained, from a range of 1 to {len(self.unets)}, if you are training cascading DDPM (multiple unets)&quot
        unet_number<a id="change"> = </a>default(unet_number, 1)
        assert 1 &lt;= unet_number &lt;= len(self.unets)

        index<a id="change"> = </a>unet_number<a id="change"> - </a>1
        unet = self.unets[index]
        target_image_size = self.image_sizes[index]

        b, c, h, w, device, = *image.shape, image.device

        check_shape(image, &quotb c h w&quot, c = self.channels)
        assert h &gt;= target_image_size and w &gt;= target_image_size

        times = torch.randint(0, self.num_timesteps, (b,), device = device, dtype = torch.long)

        image_embed = self.get_image_embed(image)
        text_encodings = self.get_text_encodings(text) if exists(text) else None

        lowres_cond_img = image if index &gt; 0 else None
        ddpm_image<a id="change"> = </a>resize_image_to(image, target_image_size)
        <a id="change">return </a>self.p_losses(unet, ddpm_image, times, image_embed = image_embed, text_encodings = text_encodings, lowres_cond_img = lowres_cond_img)

&#47&#47 main class
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/dalle2-pytorch/commit/0332eaa6ff7b5804a42495cbcedad0b31928e51f#diff-038ecede954c29266888cb88e37cc06f61ba2433f8b5142c7b2b2cefde5ed0edL1146' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4036990</div><div id='project'> Project Name: lucidrains/dalle2-pytorch</div><div id='commit'> Commit Name: 0332eaa6ff7b5804a42495cbcedad0b31928e51f</div><div id='time'> Time: 2022-04-18</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: dalle2_pytorch/dalle2_pytorch.py</div><div id='m_class'> M Class Name: Decoder</div><div id='n_method'> N Class Name: Decoder</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: dalle2_pytorch/dalle2_pytorch.py</div><div id='n_file'> N File Name: dalle2_pytorch/dalle2_pytorch.py</div><div id='m_start'> M Start Line: 1147</div><div id='m_end'> M End Line: 1156</div><div id='n_start'> N Start Line: 1186</div><div id='n_end'> N End Line: 1207</div><BR>