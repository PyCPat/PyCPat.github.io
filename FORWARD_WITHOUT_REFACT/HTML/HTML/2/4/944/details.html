<html><h3>Pattern ID :944
</h3><img src='2863931.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        prior1 = torch.distributions.Categorical(logits=torch.zeros_like(l1))
        prior2 = torch.distributions.Categorical(logits=torch.zeros_like(l2))

        reg = <a id="change">torch.distributions.kl_divergence(posterior1, prior1).mean()</a> + torch.distributions.kl_divergence(posterior2, prior2).mean()

        return ssimLoss, contextLoss, reg
</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 l1 = l1.mean((2,3))
        &#47&#47 l2 = l2.mean((2,3))
        regs = list()
        <a id="change">for l</a> in allLogits<a id="change">:
            </a>posterior = torch.distributions.Categorical(logits=l)
            prior = torch.distributions.Categorical(logits=torch.zeros_like(l))
            reg = torch.distributions.kl_divergence(posterior, prior).mean()
            <a id="change">regs.append(</a>reg<a id="change">)</a>

        return ssimLoss, contextLoss, sum(regs)

</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/xiaosu-zhu/mcquic/commit/290ac3b044e8dcee03a63e86c2356c47628be8a6#diff-3011478e20a73863fd2a7b6d06c5f4e8d11dbabe016cd75181b121bff42db884L137' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2863931</div><div id='project'> Project Name: xiaosu-zhu/mcquic</div><div id='commit'> Commit Name: 290ac3b044e8dcee03a63e86c2356c47628be8a6</div><div id='time'> Time: 2021-09-16</div><div id='author'> Author: xiaosu.zhu@outlook.com</div><div id='file'> File Name: src/mcqc/losses/quantization.py</div><div id='m_class'> M Class Name: CompressionLossBig</div><div id='n_method'> N Class Name: CompressionLossBig</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(8)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: src/mcqc/losses/quantization.py</div><div id='n_file'> N File Name: src/mcqc/losses/quantization.py</div><div id='m_start'> M Start Line: 137</div><div id='m_end'> M End Line: 164</div><div id='n_start'> N Start Line: 140</div><div id='n_end'> N End Line: 158</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        cls_loss = ((50 * torch.topk(soft_one_hot_classes, largest = False, dim = 1, k = 999)[0]) ** 2).mean()

        sim_loss = -self.loss_coef * <a id="change">torch.cosine_similarity(text_embed, image_embed, dim = -1).mean()</a>
        return (lat_loss, cls_loss, sim_loss)

class Imagine(nn.Module):
    def __init__(</code></pre><h3>After Change</h3><pre><code class='java'>
        results = []
        for txt_embed in text_embeds:
            results.append(self.sim_txt_to_img(txt_embed, image_embed))
        <a id="change">for txt_min_embed</a> in text_min_embeds<a id="change">:
            results.append(</a>self.sim_txt_to_img(txt_min_embed, image_embed, "min")<a id="change">)</a>
        sim_loss = sum(results).mean()
        return (lat_loss, cls_loss, sim_loss)

class Imagine(nn.Module):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/big-sleep/commit/4e40452960be080128e5cf7fc323d48635dc69b0#diff-a32d425a1d65b549cda9588699a004a9d283f46d0623256309606cc74f8d3dd8L179' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2863930</div><div id='project'> Project Name: lucidrains/big-sleep</div><div id='commit'> Commit Name: 4e40452960be080128e5cf7fc323d48635dc69b0</div><div id='time'> Time: 2021-02-28</div><div id='author'> Author: samsepi0l@fastmail.com</div><div id='file'> File Name: big_sleep/big_sleep.py</div><div id='m_class'> M Class Name: BigSleep</div><div id='n_method'> N Class Name: BigSleep</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: big_sleep/big_sleep.py</div><div id='n_file'> N File Name: big_sleep/big_sleep.py</div><div id='m_start'> M Start Line: 202</div><div id='m_end'> M End Line: 226</div><div id='n_start'> N Start Line: 190</div><div id='n_end'> N End Line: 242</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        )

        if self.combining_operation == "mean":
            e = <a id="change">iid_embeddings.mean(dim=1)</a>
        elif self.combining_operation == "sum":
            e = iid_embeddings.sum(dim=1)
        else:
            raise ValueError("combining_operation must be in [&quotsum&quot, &quotmean&quot].")</code></pre><h3>After Change</h3><pre><code class='java'>
        else:
            combined_embedding = []
            trial_counts = torch.zeros(batch, 1)
            <a id="change">for i</a> in range(batch)<a id="change">:
                &#47&#47 remove NaNs
                </a>valid_x = x[i, ~torch.isnan(x[i, :, 0]), :]
                trial_counts[i] = valid_x.shape[0]
                trial_embeddings = self.trial_net(valid_x)
                &#47&#47 apply combining operation over permutation dimension
                <a id="change">combined_embedding.append(
                    </a>self.combining_function(trial_embeddings, dim=0)<a id="change">
                )</a>

            combined_embedding = torch.stack(combined_embedding, dim=0)

        assert not torch.isnan(combined_embedding).any(), "NaNs in embedding."</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/mackelab/sbi/commit/1352e77bdbc47aa4a4130679903b57672e48218c#diff-672ba10e6c3065a6f5554033b9b173c4fe88f4c05ffc31c70a9198691e6c6659L262' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2863926</div><div id='project'> Project Name: mackelab/sbi</div><div id='commit'> Commit Name: 1352e77bdbc47aa4a4130679903b57672e48218c</div><div id='time'> Time: 2023-03-01</div><div id='author'> Author: jan.boelts@tum.de</div><div id='file'> File Name: sbi/neural_nets/embedding_nets.py</div><div id='m_class'> M Class Name: PermutationInvariantEmbedding</div><div id='n_method'> N Class Name: PermutationInvariantEmbedding</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: sbi/neural_nets/embedding_nets.py</div><div id='n_file'> N File Name: sbi/neural_nets/embedding_nets.py</div><div id='m_start'> M Start Line: 271</div><div id='m_end'> M End Line: 284</div><div id='n_start'> N Start Line: 277</div><div id='n_end'> N End Line: 304</div><BR>