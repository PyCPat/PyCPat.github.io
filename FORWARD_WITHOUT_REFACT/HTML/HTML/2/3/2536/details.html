<html><h3>Pattern ID :2536
</h3><img src='8348660.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 attention softmax, but normalizing keys is needed so that similarity for
        &#47&#47 the purposes of attention correctly corresponds to hash locality.
        bq = bqk
        bk = <a id="change">F.normalize(</a>bqk<a id="change">, p=2, dim=-1)</a>

        &#47&#47 Allow each chunk to attend within itself, and also one chunk back. Chunk
        &#47&#47 boundaries might occur in the middle of a sequence of items from the
        &#47&#47 same bucket, so this increases the chances of attending to relevant items.</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 attention softmax, but normalizing keys is needed so that similarity for
        &#47&#47 the purposes of attention correctly corresponds to hash locality.
        bq = bqk
        bk = <a id="change">F.normalize(bqk, p=2, dim=-1).type(</a>bq.type()<a id="change">)</a>

        &#47&#47 Allow each chunk to attend within itself, and also one chunk back. Chunk
        &#47&#47 boundaries might occur in the middle of a sequence of items from the
        &#47&#47 same bucket, so this increases the chances of attending to relevant items.</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/reformer-pytorch/commit/a362875eefa251bd7f9e58f16558c2cd620281d7#diff-7e740488ff2ce47996cfdd5356981ed3e6ac2bb38739a71b400275ac91d1c3bfL207' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 8348660</div><div id='project'> Project Name: lucidrains/reformer-pytorch</div><div id='commit'> Commit Name: a362875eefa251bd7f9e58f16558c2cd620281d7</div><div id='time'> Time: 2020-01-29</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: reformer_pytorch/reformer_pytorch.py</div><div id='m_class'> M Class Name: LSHAttention</div><div id='n_method'> N Class Name: LSHAttention</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: reformer_pytorch/reformer_pytorch.py</div><div id='n_file'> N File Name: reformer_pytorch/reformer_pytorch.py</div><div id='m_start'> M Start Line: 207</div><div id='m_end'> M End Line: 285</div><div id='n_start'> N Start Line: 211</div><div id='n_end'> N End Line: 289</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        t = query_len

        q = qk[:, 0:query_len]
        qk = <a id="change">F.normalize(</a>qk, 2<a id="change">, dim=-1)</a>

        dot = torch.einsum(&quotbie,bje-&gt;bij&quot, q, qk)

        &#47&#47 qk attention requires tokens not attend to self</code></pre><h3>After Change</h3><pre><code class='java'>
        t = query_len

        q = qk[:, 0:query_len]
        qk = <a id="change">F.normalize(qk, 2, dim=-1).type(</a>q.type()<a id="change">)</a>

        dot = torch.einsum(&quotbie,bje-&gt;bij&quot, q, qk)

        &#47&#47 qk attention requires tokens not attend to self</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/reformer-pytorch/commit/a362875eefa251bd7f9e58f16558c2cd620281d7#diff-7e740488ff2ce47996cfdd5356981ed3e6ac2bb38739a71b400275ac91d1c3bfL330' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 8348658</div><div id='project'> Project Name: lucidrains/reformer-pytorch</div><div id='commit'> Commit Name: a362875eefa251bd7f9e58f16558c2cd620281d7</div><div id='time'> Time: 2020-01-29</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: reformer_pytorch/reformer_pytorch.py</div><div id='m_class'> M Class Name: FullQKAttention</div><div id='n_method'> N Class Name: FullQKAttention</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: reformer_pytorch/reformer_pytorch.py</div><div id='n_file'> N File Name: reformer_pytorch/reformer_pytorch.py</div><div id='m_start'> M Start Line: 335</div><div id='m_end'> M End Line: 341</div><div id='n_start'> N Start Line: 338</div><div id='n_end'> N End Line: 345</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        qk, v = map(lambda x: x.reshape(b, h, num_clusters, wsz, d), (qk, v))

        q = qk
        k = <a id="change">F.normalize(</a>qk, 2<a id="change">, dim=-1)</a>

        dots = torch.einsum(&quotbhnid,bhnjd-&gt;bhnij&quot, q, k) * (d ** -0.5)
        dots = dots + self.rel_pos(q)
</code></pre><h3>After Change</h3><pre><code class='java'>
        qk, v = map(lambda x: x.reshape(b, h, num_clusters, wsz, d), (qk, v))

        q = qk
        k = <a id="change">F.normalize(qk, 2, dim=-1).type(</a>qk.dtype<a id="change">)</a>

        dots = torch.einsum(&quotbhnid,bhnjd-&gt;bhnij&quot, q, k) * (d ** -0.5)
        dots = dots + self.rel_pos(q)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/routing-transformer/commit/06c234c4d94e2defe6d67e81c86d1de55c342d05#diff-1a96ccaa437a86e24c768ea078dd19db87f8694dae807ecbb231a703aba4702dL249' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 8348657</div><div id='project'> Project Name: lucidrains/routing-transformer</div><div id='commit'> Commit Name: 06c234c4d94e2defe6d67e81c86d1de55c342d05</div><div id='time'> Time: 2020-05-23</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: routing_transformer/routing_transformer.py</div><div id='m_class'> M Class Name: KmeansAttention</div><div id='n_method'> N Class Name: KmeansAttention</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: routing_transformer/routing_transformer.py</div><div id='n_file'> N File Name: routing_transformer/routing_transformer.py</div><div id='m_start'> M Start Line: 268</div><div id='m_end'> M End Line: 290</div><div id='n_start'> N Start Line: 268</div><div id='n_end'> N End Line: 290</div><BR>