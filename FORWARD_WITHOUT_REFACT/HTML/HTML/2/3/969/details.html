<html><h3>Pattern ID :969
</h3><img src='2996697.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        k_img, v_img = map(lambda t: F.unfold(t, kernel_size, padding = padding, dilation = dilation), (k_img, v_img))
        k_img, v_img = map(lambda t: rearrange(t, &quotb (d j) i -&gt; b i j d&quot, j = kernel_size ** 2), (k_img, v_img))

        k_text, v_text = <a id="change">map(</a>lambda t: repeat(t, &quotb j d -&gt; b i j d&quot, i = img_seq_len), (k_text<a id="change">, v_text</a>)<a id="change">)</a>

        &#47&#47 let image attend to all of text

        k_img = torch.cat((k_text, k_img), dim = 2)</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 aggregate

        attn_image_to_text, attn_image = <a id="change">attn[..., :text_len]</a>, attn[..., text_len:]

        out_image_to_image = einsum(&quotb i j, b i j d -&gt; b i d&quot, attn_image, v_img)
        out_image_to_text = einsum(&quotb i j, b j d -&gt; b i d&quot, attn_image_to_text, v_text)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/dalle-pytorch/commit/f14a313431e9072bef9a8219ea3d99d7683ada06#diff-58807b9968341c05944b3adf3de1bfcd9d33660121a78f3ef013fd0e9bf16fdfL94' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2996697</div><div id='project'> Project Name: lucidrains/dalle-pytorch</div><div id='commit'> Commit Name: f14a313431e9072bef9a8219ea3d99d7683ada06</div><div id='time'> Time: 2021-04-22</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: dalle_pytorch/attention.py</div><div id='m_class'> M Class Name: SparseConvCausalAttention</div><div id='n_method'> N Class Name: SparseConvCausalAttention</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: dalle_pytorch/attention.py</div><div id='n_file'> N File Name: dalle_pytorch/attention.py</div><div id='m_start'> M Start Line: 116</div><div id='m_end'> M End Line: 168</div><div id='n_start'> N Start Line: 94</div><div id='n_end'> N End Line: 173</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 prepare text key / values for the image tokens to attend to

        k_text, v_text = <a id="change">map(</a>lambda t: repeat(t, &quotb n d -&gt; (b ax) n d&quot, ax = img_size), (k_text<a id="change">, v_text</a>)<a id="change">)</a>
        k_img = torch.cat((k_text, k_img), dim = 1)
        v_img = torch.cat((v_text, v_img), dim = 1)

        &#47&#47 similarity</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 aggregate

        attn_image_to_text, attn_image_to_image = <a id="change">attn[..., :text_len]</a>, attn[..., text_len:]

        out_image_to_image = einsum(&quotb x i j, b x j d -&gt; b x i d&quot, attn_image_to_image, v_img)
        out_image_to_text = einsum(&quotb x i j, b j d -&gt; b x i d&quot, attn_image_to_text, v_text)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/dalle-pytorch/commit/100a110d6fa98b8559a780ae71d9ef1f1c7789c7#diff-58807b9968341c05944b3adf3de1bfcd9d33660121a78f3ef013fd0e9bf16fdfL204' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2996694</div><div id='project'> Project Name: lucidrains/dalle-pytorch</div><div id='commit'> Commit Name: 100a110d6fa98b8559a780ae71d9ef1f1c7789c7</div><div id='time'> Time: 2021-04-22</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: dalle_pytorch/attention.py</div><div id='m_class'> M Class Name: SparseAxialCausalAttention</div><div id='n_method'> N Class Name: SparseAxialCausalAttention</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: dalle_pytorch/attention.py</div><div id='n_file'> N File Name: dalle_pytorch/attention.py</div><div id='m_start'> M Start Line: 230</div><div id='m_end'> M End Line: 279</div><div id='n_start'> N Start Line: 208</div><div id='n_end'> N End Line: 281</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 bucket query, key, values

        bucket_fn = partial(bucket, buckets)
        b_q, b_k, b_v = <a id="change">map(</a>bucket_fn, (q<a id="change">, k, v</a>)<a id="change">)</a>

        &#47&#47 calculate reordering matrix R with simple sort net

        R = self.sort_net(q, k)</code></pre><h3>After Change</h3><pre><code class='java'>
        b_q = bucket(buckets, q)
        b_k, b_v = map(partial(bucket, self.kv_buckets), (k, v))

        bsz = <a id="change">b_k.shape[2]</a>

        &#47&#47 calculate reordering matrix R with simple sort net

        R = self.sort_net(q, k)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/sinkhorn-transformer/commit/5cd330003e7689cd49cecf3eaf9491d462ec131d#diff-2ed19ae7feb552dc52b4c7a7c89a078c0fd371c922789f894af09ff01f35eccbL194' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2996691</div><div id='project'> Project Name: lucidrains/sinkhorn-transformer</div><div id='commit'> Commit Name: 5cd330003e7689cd49cecf3eaf9491d462ec131d</div><div id='time'> Time: 2020-04-09</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: sinkhorn_transformer/sinkhorn_transformer.py</div><div id='m_class'> M Class Name: SinkhornAttention</div><div id='n_method'> N Class Name: SinkhornAttention</div><div id='m_method'> M Method Name: forward(5)</div><div id='n_method'> N Method Name: forward(5)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: sinkhorn_transformer/sinkhorn_transformer.py</div><div id='n_file'> N File Name: sinkhorn_transformer/sinkhorn_transformer.py</div><div id='m_start'> M Start Line: 197</div><div id='m_end'> M End Line: 234</div><div id='n_start'> N Start Line: 198</div><div id='n_end'> N End Line: 237</div><BR>