<html><h3>Pattern ID :119
</h3><img src='518932.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 masked by the missing entries
        &#47&#47 note, different batch may contain different number of real entities
        tensor_list = <a id="change">[]</a>
        for i, batch in enumerate(out):
            mean_entity = 0.
            real_number = real_number_tensor[i]
            real_number = real_number if real_number != 0 else 1
            for j, entity in enumerate(batch):
                if j &gt;= real_number_tensor[i]:
                    break        
                mean_entity = mean_entity + entity
            mean_entity = mean_entity / (real_number)
            <a id="change">tensor_list.append(</a>mean_entity.reshape(1, -1)<a id="change">)</a>
        tensor_mean = torch.cat(tensor_list, dim=0)
        print(&quottensor_mean:&quot, tensor_mean) if debug else None
</code></pre><h3>After Change</h3><pre><code class='java'>
        mask = mask &lt; entity_num.unsqueeze(dim=1)

        print(&quotmask:&quot, mask) if debug else None
        print(&quotmask.shape:&quot, mask.shape)<a id="change"> if </a>debug<a id="change"> else </a>None

        &#47&#47 mask: [batch_size, max_entities]
        device = next(self.parameters()).device</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/liuruoze/mini-alphastar/commit/612d42a7bf5ef827e1e919198d839fce106155cd#diff-71869e70f9cb835282d541d2d3529d25b6ed6795c79bc770e860e9318619f6b1L716' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 518932</div><div id='project'> Project Name: liuruoze/mini-alphastar</div><div id='commit'> Commit Name: 612d42a7bf5ef827e1e919198d839fce106155cd</div><div id='time'> Time: 2021-11-25</div><div id='author'> Author: liuruoze@163.com</div><div id='file'> File Name: alphastarmini/core/arch/entity_encoder.py</div><div id='m_class'> M Class Name: EntityEncoder</div><div id='n_method'> N Class Name: EntityEncoder</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: alphastarmini/core/arch/entity_encoder.py</div><div id='n_file'> N File Name: alphastarmini/core/arch/entity_encoder.py</div><div id='m_start'> M Start Line: 721</div><div id='m_end'> M End Line: 758</div><div id='n_start'> N Start Line: 716</div><div id='n_end'> N End Line: 778</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        total_len = mem.shape[2] + lmem.shape[2] + self.seq_len
        pos_emb = self.pos_emb[:, (self.seq_len - t):total_len]

        next_mem = <a id="change">[]</a>
        next_lmem = []

        mem_iter, lmem_iter = map(iterate_tensor, (mem, lmem))

        for ind, (attn, ff) in enumerate(zip(self.attn_layers, self.ff_layers)):
            layer_num = ind + 1
            use_memory = layer_num in self.memory_layers

            memories = None
            if use_memory:
                memories = (next(mem_iter), next(lmem_iter))

            x, (mem_out, lmem_out) = attn(x, memories = memories, calc_memory = use_memory, input_mask = mask, pos_emb = pos_emb)
            x, = ff(x)

            if use_memory:
                <a id="change">next_mem.append(</a>mem_out<a id="change">)</a>
                next_lmem.append(lmem_out)

        out = self.to_logits(x)
</code></pre><h3>After Change</h3><pre><code class='java'>
        for ind, (attn, ff) in enumerate(zip(self.attn_layers, self.ff_layers)):
            layer_num = ind + 1
            use_memory = layer_num in self.memory_layers
            memories = map(next, (mem_iter, lmem_iter))<a id="change"> if </a>use_memory<a id="change"> else </a>None

            if use_memory:
                hiddens.append(x)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/memory-transformer-xl/commit/cbabe1ae6fa311092a9d0a88116c079a5ad8d790#diff-a0bf60cdef5ad628f9a83f0454c9c66a535268529835e839bb54088805dba051L253' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 518931</div><div id='project'> Project Name: lucidrains/memory-transformer-xl</div><div id='commit'> Commit Name: cbabe1ae6fa311092a9d0a88116c079a5ad8d790</div><div id='time'> Time: 2020-07-22</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: memory_transformer_xl/memory_transformer_xl.py</div><div id='m_class'> M Class Name: MemoryTransformerXL</div><div id='n_method'> N Class Name: MemoryTransformerXL</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: memory_transformer_xl/memory_transformer_xl.py</div><div id='n_file'> N File Name: memory_transformer_xl/memory_transformer_xl.py</div><div id='m_start'> M Start Line: 255</div><div id='m_end'> M End Line: 296</div><div id='n_start'> N Start Line: 306</div><div id='n_end'> N End Line: 345</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    if self.spectrogram_upsampler: &#47&#47 use conditional model
      spectrogram = self.spectrogram_upsampler(spectrogram)

    skip = <a id="change">[]</a>
    for layer in self.residual_layers:
      x, skip_connection = layer(x, diffusion_step, spectrogram)
      <a id="change">skip.append(</a>skip_connection<a id="change">)</a>

    x = torch.sum(torch.stack(skip), dim=0) / sqrt(len(self.residual_layers))
    x = self.skip_projection(x)
    x = F.relu(x)</code></pre><h3>After Change</h3><pre><code class='java'>
    skip = None
    for layer in self.residual_layers:
      x, skip_connection = layer(x, diffusion_step, spectrogram)
      skip = skip_connection<a id="change"> if </a>skip is None<a id="change"> else </a>skip_connection + skip

    x = skip / sqrt(len(self.residual_layers))
    x = self.skip_projection(x)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lmnt-com/diffwave/commit/75ffa505ae4c4bab77bd7b7069226ba5a45f1dbd#diff-4ebc9849865159d36b22a9e2b9aac6025cf9c0e93a7e41550e03aee253aaf05fL145' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 518928</div><div id='project'> Project Name: lmnt-com/diffwave</div><div id='commit'> Commit Name: 75ffa505ae4c4bab77bd7b7069226ba5a45f1dbd</div><div id='time'> Time: 2021-11-09</div><div id='author'> Author: sharvil.nanavati@gmail.com</div><div id='file'> File Name: src/diffwave/model.py</div><div id='m_class'> M Class Name: DiffWave</div><div id='n_method'> N Class Name: DiffWave</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: src/diffwave/model.py</div><div id='n_file'> N File Name: src/diffwave/model.py</div><div id='m_start'> M Start Line: 156</div><div id='m_end'> M End Line: 161</div><div id='n_start'> N Start Line: 156</div><div id='n_end'> N End Line: 161</div><BR>