<html><h3>Pattern ID :1684
</h3><img src='6713799.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 if no NaNs for padding varying trial lengths we can batch the computation
        if not torch.isnan(x).any():
            trial_embeddings = <a id="change">self.trial_net(x.view(batch * permutation_dim, -1)).view(
                </a>batch, permutation_dim, <a id="change">-1</a><a id="change">
            )</a>
            combined_embedding<a id="change"> = </a>self.combining_function(trial_embeddings, dim=1)
            trial_counts = torch.ones(batch, 1, dtype=torch.float32) * permutation_dim

        &#47&#47 otherwise we need to loop over the batch to account for varying trial lengths</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 Take mean over permutation dimension divide by number of trials
        &#47&#47 (instead of just taking torch.mean) to account for masking.
        <a id="change">if self.aggregation_fn == "mean"</a><a id="change">:
            </a>combined_embedding<a id="change"> = </a>(
                trial_embeddings.sum(dim=self.aggregation_dim) / trial_counts
            )
        else:
            combined_embedding<a id="change"> = </a>trial_embeddings.sum(dim=self.aggregation_dim)

        assert not torch.isnan(combined_embedding).any(), "NaNs in embedding."
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/mackelab/sbi/commit/3831fd6d5fda0ca050db8c54868ed30558451042#diff-672ba10e6c3065a6f5554033b9b173c4fe88f4c05ffc31c70a9198691e6c6659L274' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6713799</div><div id='project'> Project Name: mackelab/sbi</div><div id='commit'> Commit Name: 3831fd6d5fda0ca050db8c54868ed30558451042</div><div id='time'> Time: 2023-03-01</div><div id='author'> Author: jan.boelts@tum.de</div><div id='file'> File Name: sbi/neural_nets/embedding_nets.py</div><div id='m_class'> M Class Name: PermutationInvariantEmbedding</div><div id='n_method'> N Class Name: PermutationInvariantEmbedding</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: sbi/neural_nets/embedding_nets.py</div><div id='n_file'> N File Name: sbi/neural_nets/embedding_nets.py</div><div id='m_start'> M Start Line: 274</div><div id='m_end'> M End Line: 300</div><div id='n_start'> N Start Line: 279</div><div id='n_end'> N End Line: 306</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        if len(torch.unique(eos_mask.sum(1))) &gt; 1:
            raise ValueError("All examples must have the same number of &lt;eos&gt; tokens.")
        vec<a id="change"> = </a><a id="change">hidden_states[eos_mask, :].view(</a>hidden_states.size(0), <a id="change">-1</a>,
                                              hidden_states.size(-1)<a id="change">)</a>[:, -1, :]

        logits = self.classifier(vec)
        prob = F.softmax(logits)</code></pre><h3>After Change</h3><pre><code class='java'>
    def forward(self, source_ids=None, labels=None):
        source_ids = source_ids.view(-1, self.args.max_source_length)

        <a id="change">if self.args.model_type == &quotcodet5&quot</a><a id="change">:
            </a>vec = self.get_t5_vec(source_ids)
        elif self.args.model_type == &quotbart&quot:
            vec<a id="change"> = </a>self.get_bart_vec(source_ids)
        elif self.args.model_type == &quotroberta&quot:
            vec<a id="change"> = </a>self.get_roberta_vec(source_ids)

        logits = self.classifier(vec)
        prob = nn.functional.softmax(logits)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/salesforce/codet5/commit/0bf3c0c43e92fcf54d9df68c793ac22f2b60aad4#diff-dcfeaa5ac3dffd54264034760914588d92fc7395c81eaa9274715840dc28100bL110' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6713806</div><div id='project'> Project Name: salesforce/codet5</div><div id='commit'> Commit Name: 0bf3c0c43e92fcf54d9df68c793ac22f2b60aad4</div><div id='time'> Time: 2021-10-29</div><div id='author'> Author: 337111657@qq.com</div><div id='file'> File Name: models.py</div><div id='m_class'> M Class Name: CloneModel</div><div id='n_method'> N Class Name: CloneModel</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: models.py</div><div id='n_file'> N File Name: models.py</div><div id='m_start'> M Start Line: 111</div><div id='m_end'> M End Line: 125</div><div id='n_start'> N Start Line: 103</div><div id='n_end'> N End Line: 116</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        else:
            W = self.encoder.src_embedding.get_embedding().T

        mask = <a id="change">x.view(-1</a><a id="change">)</a>.eq(self.MASK)
        enc_output<a id="change"> = </a>enc_output.view(-1, self.d_model)[mask]
        
        logits = torch.matmul(enc_output, W)
        </code></pre><h3>After Change</h3><pre><code class='java'>

        enc_output = enc_outputs[0]

        <a id="change">if self.activation == "relu"</a><a id="change">:
            </a>enc_output<a id="change"> = </a>F.relu(enc_output)
        elif self.activation == "gelu":
            enc_output<a id="change"> = </a>gelu_new(enc_output)
        enc_output = self.norm(enc_output)
            
        if self.share_emb_out_proj == False:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/yaoxiaoyuan/mimix/commit/59b6082bb310a2a17c2ac30225e300124904cc2f#diff-f4ca477841bd3ad5c0b292cbfff7c3ae403a65fb6c97deb15ad996f85dc87bd8L1162' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6713804</div><div id='project'> Project Name: yaoxiaoyuan/mimix</div><div id='commit'> Commit Name: 59b6082bb310a2a17c2ac30225e300124904cc2f</div><div id='time'> Time: 2022-06-21</div><div id='author'> Author: sbsbsbsbsb945@gmail.com</div><div id='file'> File Name: src/models.py</div><div id='m_class'> M Class Name: TransformerBiLM</div><div id='n_method'> N Class Name: TransformerBiLM</div><div id='m_method'> M Method Name: forward(5)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: src/models.py</div><div id='n_file'> N File Name: src/models.py</div><div id='m_start'> M Start Line: 1165</div><div id='m_end'> M End Line: 1179</div><div id='n_start'> N Start Line: 1399</div><div id='n_end'> N End Line: 1433</div><BR>