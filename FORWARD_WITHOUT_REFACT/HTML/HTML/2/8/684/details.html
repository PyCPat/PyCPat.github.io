<html><h3>Pattern ID :684
</h3><img src='2318755.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        t_0 = self.l_4(t_0, attention_mask=decoder_attention_mask, position_bias=x1, encoder_hidden_states=x0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=x2)
        &#47&#47 returning:
        &#47&#47 T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]
        <a id="change">return </a>(t_0<a id="change"></a>,)

    def state_dict(self,*args,**kwargs):
        &#47&#47 we return the state dict of this part as it should be in the original model</code></pre><h3>After Change</h3><pre><code class='java'>
        t_0 = self.l_1(t_0, attention_mask=attention_mask, position_bias=x0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        t_0 = self.l_2(t_0)
        t_0 = self.l_3(t_0)
        t_1<a id="change"> = </a>self.l_4(x2)
        t_1 = self.l_5(t_1, attention_mask=decoder_attention_mask, position_bias=None, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=None)
        t_2 = <a id="change">t_1[0]</a>
        t_3 = t_1[1]
        t_1 = <a id="change">t_1[2]</a>
        t_2 = self.l_6(t_2, attention_mask=decoder_attention_mask, position_bias=t_3, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=t_1)
        t_2 = self.l_7(t_2, attention_mask=decoder_attention_mask, position_bias=t_3, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=t_1)
        t_2 = self.l_8(t_2, attention_mask=decoder_attention_mask, position_bias=t_3, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=t_1)
        t_2 = self.l_9(t_2, attention_mask=decoder_attention_mask, position_bias=t_3, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=t_1)
        t_2 = self.l_10(t_2, attention_mask=decoder_attention_mask, position_bias=t_3, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=t_1)
        t_2<a id="change"> = </a>self.l_11(t_2, attention_mask=decoder_attention_mask, position_bias=t_3, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=t_1)
        &#47&#47 returning:
        &#47&#47 T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        &#47&#47 T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/e959641a351e7827214f66e1a3a0675b9059392e#diff-d0be6aca0bdc49b9a5cfa74ef6f4d9a78ebbf8a7f6c923df203d594715ce1306L963' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2318755</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: e959641a351e7827214f66e1a3a0675b9059392e</div><div id='time'> Time: 2020-08-17</div><div id='author'> Author: saareliad@campus.technion.ac.il</div><div id='file'> File Name: models/partitioned/t5_large_tied_lmhead_8p_bw12_async_squad1.py</div><div id='m_class'> M Class Name: Partition6</div><div id='n_method'> N Class Name: Partition6</div><div id='m_method'> M Method Name: forward(1)</div><div id='n_method'> N Method Name: forward(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: models/partitioned/t5_large_tied_lmhead_8p_bw12_async_squad1.py</div><div id='n_file'> N File Name: models/partitioned/t5_large_tied_lmhead_8p_bw12_async_squad1.py</div><div id='m_start'> M Start Line: 1079</div><div id='m_end'> M End Line: 1087</div><div id='n_start'> N Start Line: 963</div><div id='n_end'> N End Line: 984</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

                &#47&#47 return early results
                if uncertain &lt; inference_speed:
                    <a id="change">return </a>prob<a id="change">, i, uncertain_infos</a>
            return prob, i, uncertain_infos

        &#47&#47 Training phase: the first phase corresponds to the backbone training
        &#47&#47 the second phase to the branches training (actual distillation)</code></pre><h3>After Change</h3><pre><code class='java'>
                uncertain = torch.sum(prob * log_prob, 1) / (-torch.log(self.num_class))

                &#47&#47 checking if there&quots enough information
                enough_info<a id="change"> = </a>uncertain &lt; inference_speed

                right_pos = positions[enough_info]
                final_probs[right_pos] = prob[enough_info]

                hidden_states = (hidden_states[0][~enough_info],)
                attention_mask<a id="change"> = </a>attention_mask[~enough_info]

                &#47&#47 if we have processed all the samples
                if <a id="change">hidden_states[0].shape[0]</a> == 0:
                    return final_probs, i

                positions = positions[~enough_info]  &#47&#47 updating the positions to fit the new batch</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/julesbelveze/bert-squeeze/commit/3cb14b8f1e742b86fe609843f2779e3bb36de4aa#diff-aac748de92a29499c39463320f41c1eaeb283dae0ed8ead9584db8318265a4acL49' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2318752</div><div id='project'> Project Name: julesbelveze/bert-squeeze</div><div id='commit'> Commit Name: 3cb14b8f1e742b86fe609843f2779e3bb36de4aa</div><div id='time'> Time: 2021-12-11</div><div id='author'> Author: 32683010+JulesBelveze@users.noreply.github.com</div><div id='file'> File Name: bert-squeeze/models/custom_transformers/fastbert.py</div><div id='m_class'> M Class Name: FastBertGraph</div><div id='n_method'> N Class Name: FastBertGraph</div><div id='m_method'> M Method Name: forward(7)</div><div id='n_method'> N Method Name: forward(6)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: bert-squeeze/models/custom_transformers/fastbert.py</div><div id='n_file'> N File Name: bert-squeeze/models/custom_transformers/fastbert.py</div><div id='m_start'> M Start Line: 57</div><div id='m_end'> M End Line: 70</div><div id='n_start'> N Start Line: 49</div><div id='n_end'> N End Line: 87</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 returing:
        &#47&#47 GPT2Model/Block[2]/MLP[mlp]/Dropout[dropout]
        &#47&#47 GPT2Model/Block[2]/aten::add5567
        <a id="change">return </a>(self.l_29(self.l_28(torch.mul(input=torch.mul(input=t_33, other=0.5), other=torch.add(input=Tensor.tanh(torch.mul(input=torch.add(input=t_33, other=torch.mul(input=Tensor.pow(t_33, exponent=3), other=0.044715)), other=0.7978845608028654)), other=1))))<a id="change">, t_32</a>)

    def state_dict(self,device=None):
        &#47&#47 we return the state dict of this part as it should be in the original model</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 calling torch.add with arguments:
        &#47&#47 GPT2LMHeadModel/GPT2Model[transformer]/Block[2]/aten::add5663
        &#47&#47 GPT2LMHeadModel/GPT2Model[transformer]/Block[2]/MLP[mlp]/Dropout[dropout]
        t_34<a id="change"> = </a>torch.add(input=t_32, other=self.l_29(self.l_28(torch.mul(input=torch.mul(input=t_33, other=0.5), other=torch.add(input=Tensor.tanh(torch.mul(input=torch.add(input=t_33, other=torch.mul(input=Tensor.pow(t_33, exponent=3), other=0.044715)), other=0.7978845608028654)), other=1)))))
        &#47&#47 calling torch.split with arguments:
        &#47&#47 GPT2LMHeadModel/GPT2Model[transformer]/Block[3]/Attention[attn]/Conv1D[c_attn]
        &#47&#47 GPT2LMHeadModel/GPT2Model[transformer]/Block[3]/Attention[attn]/prim::Constant5779
        &#47&#47 GPT2LMHeadModel/GPT2Model[transformer]/Block[3]/Attention[attn]/prim::Constant5780
        t_35 = Tensor.split(self.l_31(self.l_30(t_34)), split_size=768, dim=2)
        t_36 = <a id="change">t_35[0]</a>
        t_37 = <a id="change">t_35[1]</a>
        t_38<a id="change"> = </a>t_35[2]
        &#47&#47 calling torch.div with arguments:
        &#47&#47 GPT2LMHeadModel/GPT2Model[transformer]/Block[3]/Attention[attn]/aten::matmul5854
        &#47&#47 GPT2LMHeadModel/GPT2Model[transformer]/Block[3]/Attention[attn]/prim::Constant5855</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/9ddf98456ca57cac3a4a982fc519c5a71642dc25#diff-4d93af50bd9b6dc7752538156bde96fadf361ac3f37eb0019e2bbf01206492abL311' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2318782</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: 9ddf98456ca57cac3a4a982fc519c5a71642dc25</div><div id='time'> Time: 2020-03-10</div><div id='author'> Author: saareliad@campus.technion.ac.il</div><div id='file'> File Name: models/partitioned/gpt2.py</div><div id='m_class'> M Class Name: Partition0</div><div id='n_method'> N Class Name: Partition0</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: models/partitioned/gpt2.py</div><div id='n_file'> N File Name: models/partitioned/gpt2.py</div><div id='m_start'> M Start Line: 460</div><div id='m_end'> M End Line: 467</div><div id='n_start'> N Start Line: 396</div><div id='n_end'> N End Line: 430</div><BR>