<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                         dimension corresponds to the number of negative samples (N_B  or N_B + N_A)
        
        logit_softmax = F.softmax(logit, dim = 1)
        diff = logit.diag().view(-1, 1).expand_as(logit)<a id="change"> - </a>logit
        loss = -torch.log(torch.mean(logit_softmax<a id="change"> * </a><a id="change">torch.sigmoid(</a>diff<a id="change">)</a>))
        &#47&#47 add regularization
        loss += self.lambda_ * torch.mean(logit_softmax * (logit ** 2))
        <a id="change">return </a>loss


class TOP1Loss(nn.Module):</code></pre><h3>After Change</h3><pre><code class='java'>
                                                            + All negative samples (mini-batch + additional samples )
            negative_mask: (&#47&#47pos_target, &#47&#47neg_samples) : specify the negative items for each positive target
        
        positive_mask = <a id="change">~negative_mask</a>
        positives = logits_scores.diag().view(-1, 1).expand_as(logits_scores)
        diff = positives<a id="change"> - logits_scores</a>
        loss = <a id="change">F.sigmoid(</a>diff<a id="change">)</a>
        logit_softmax = torch.softmax(logits_scores, dim = 1)
        loss<a id="change"> = </a>logit_softmax * loss
        reg<a id="change"> = </a>logit_softmax * (<a id="change">logits_scores</a><a id="change">**2</a>)
        &#47&#47 set to zeros the diff scores of positive targets present in the same session
        loss = loss.masked_fill(positive_mask, 0)
        reg<a id="change"> = </a>reg.masked_fill(positive_mask, 0)
        &#47&#47 Average over the nb of negative sample per
        loss = -torch.log(loss.sum(1))<a id="change"> + </a>self.lambda_ * reg.sum(1)
        <a id="change">return </a>torch.mean(loss)


class TOP1(nn.Module):</code></pre>