<html><h3>Pattern ID :2572
</h3><img src='8368127.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        tensor_list = []
        for i, batch in enumerate(out):
            mean_entity = 0.
            real_number = <a id="change">real_number_tensor[i]</a>
            real_number = real_number if real_number != 0 else 1
            for j, entity in enumerate(batch):
                if j &gt;= real_number_tensor[i]:</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 mask: [batch_size, max_entities]
        device = next(self.parameters()).device
        mask = <a id="change">mask.to(</a>device<a id="change">)</a>

        &#47&#47 assert the input shape is : batch_seq_size x entities_size x embeding_size
        &#47&#47 note: because the feature size of entity is not equal to 256, so it can not fed into transformer directly.
        &#47&#47 thus, we add a embedding layer to transfer it to right size.
        &#47&#47 x is batch_entities_tensor (dim = 3). Shape: batch_seq_size x entities_size x embeding_size
        x = self.embedd(x)
        print(&quotx.shape:&quot, x.shape) if debug else None

        &#47&#47 mask for transformer need a special format
        mask_seq_len = mask.shape[-1]
        tran_mask = mask.unsqueeze(1)

        &#47&#47 tran_mask: [batch_seq_size x max_entities x max_entities]
        tran_mask = tran_mask.repeat(1, mask_seq_len, 1)

        &#47&#47 out: [batch_seq_size x entities_size x embeding_size]
        out = self.transformer(x, mask=tran_mask)
        print(&quotout.shape:&quot, out.shape) if debug else None

        &#47&#47 entity_embeddings: [batch_seq_size x entities_size x conv1_output_size]
        entity_embeddings = F.relu(self.conv1(F.relu(out).transpose(1, 2))).transpose(1, 2)
        print(&quotentity_embeddings.shape:&quot, entity_embeddings.shape) if debug else None

        &#47&#47 AlphaStar: The mean of the transformer output across across the units (masked by the missing entries) 
        &#47&#47 is fed through a linear layer of size 256 and a ReLU to yield `embedded_entity`
        masked_out<a id="change"> = </a>out * mask.unsqueeze(2)

        &#47&#47 sum over across the units
        &#47&#47 masked_out: [batch_seq_size x entities_size x embeding_size]
        &#47&#47 z: [batch_size, embeding_size]
        z<a id="change"> = </a>masked_out.sum(dim=1, keepdim=False)

        &#47&#47 here we should dived by the entity_num, not the cls.max_entities
        &#47&#47 z: [batch_size, embeding_size]
        z<a id="change"> = </a>z / entity_num

        &#47&#47 note, dim=1 means the mean is across all entities in one timestep
        &#47&#47 The mean of the transformer output across across the units  </code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/liuruoze/mini-alphastar/commit/612d42a7bf5ef827e1e919198d839fce106155cd#diff-71869e70f9cb835282d541d2d3529d25b6ed6795c79bc770e860e9318619f6b1L716' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 8368127</div><div id='project'> Project Name: liuruoze/mini-alphastar</div><div id='commit'> Commit Name: 612d42a7bf5ef827e1e919198d839fce106155cd</div><div id='time'> Time: 2021-11-25</div><div id='author'> Author: liuruoze@163.com</div><div id='file'> File Name: alphastarmini/core/arch/entity_encoder.py</div><div id='m_class'> M Class Name: EntityEncoder</div><div id='n_method'> N Class Name: EntityEncoder</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: alphastarmini/core/arch/entity_encoder.py</div><div id='n_file'> N File Name: alphastarmini/core/arch/entity_encoder.py</div><div id='m_start'> M Start Line: 721</div><div id='m_end'> M End Line: 758</div><div id='n_start'> N Start Line: 716</div><div id='n_end'> N End Line: 778</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            &#47&#47 masked value
            x, edge_index, _, subset, _ = self.get_subgraph(node_idx, x, edge_index)
            new_node_idx = torch.where(subset == node_idx)[0]
            _, edge_mask = self.explain(x, edge_index, <a id="change">embed[node_idx]</a>, tmp=1.0, training=False)
            data = Data(x=x, edge_index=edge_index)
            selected_nodes = calculate_selected_nodes(data, edge_mask, top_k)
            masked_nodes_list = [node for node in range(data.x.shape[0]) if node not in selected_nodes]</code></pre><h3>After Change</h3><pre><code class='java'>
        top_k = kwargs.get(&quottop_k&quot) if kwargs.get(&quottop_k&quot) is not None else 10
        y = kwargs.get(&quoty&quot)
        x = x.to(self.device)
        edge_index<a id="change"> = </a><a id="change">edge_index.to(</a>self.device<a id="change">)</a>
        y = y.to(self.device)

        self.__clear_masks__()
        logits<a id="change"> = </a>self.model(x, edge_index)
        probs<a id="change"> = </a>F.softmax(logits, dim=-1)
        embed = self.model.get_emb(x, edge_index)

        if self.explain_graph:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/divelab/dig/commit/ff80d071135716b458791b82064d91ffe0454e3e#diff-568d7f47b1c5fd2fa00b587179854781743fd9ce06a1713f6ce1d3ee14c5743fL451' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 8368047</div><div id='project'> Project Name: divelab/dig</div><div id='commit'> Commit Name: ff80d071135716b458791b82064d91ffe0454e3e</div><div id='time'> Time: 2021-04-26</div><div id='author'> Author: 1161283769@qq.com</div><div id='file'> File Name: dig/xgraph/method/pgexplainer.py</div><div id='m_class'> M Class Name: PGExplainer</div><div id='n_method'> N Class Name: PGExplainer</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: dig/xgraph/method/pgexplainer.py</div><div id='n_file'> N File Name: dig/xgraph/method/pgexplainer.py</div><div id='m_start'> M Start Line: 463</div><div id='m_end'> M End Line: 487</div><div id='n_start'> N Start Line: 479</div><div id='n_end'> N End Line: 512</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        u_hat = torch.einsum(&quotijk, jkl -&gt; ijl&quot, x, self.route_weights)

        &#47&#47 Dynamic route
        b = torch.zeros(<a id="change">x.shape[1]</a>, self.num_capsules, requires_grad=True)
        for it in range(self.num_iterations):
            c = b.softmax(dim=-1)
</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 Dynamic route
        &#47&#47 b: (batch_size, num_capsules, num_route_nodes)
        b = <a id="change">torch.zeros(batch_size, self.num_capsules, self.num_route_nodes).to(</a>device<a id="change">)</a>
        for it in range(self.num_iterations - 1):
            c = b.softmax(dim=1)

            &#47&#47 c: (batch_size, num_capsules, num_route_nodes)
            &#47&#47 u_hat: (batch_size, num_capsules, num_route_nodes, out_channels)
            &#47&#47 s: (batch_size, num_capsules, out_channels)
            s = torch.einsum(&quotijk, ijkl -&gt; ijl&quot, c, u_hat_temp)
            v = squash(s)

            &#47&#47 Update b
            &#47&#47 u_hat: (batch_size, num_capsules, num_route_nodes, out_channels)
            &#47&#47 v: (batch_size, num_capsules, out_channels)
            &#47&#47 Shape of b: (batch_size, num_capsules, num_route_nodes)
            uv = torch.einsum(&quotijkl, ijl -&gt; ijk&quot, u_hat_temp, v)
            b += uv

        &#47&#47 Last iteration with original u_hat to pass gradient
        c<a id="change"> = </a>b.softmax(dim=1)
        s<a id="change"> = </a>torch.einsum(&quotijk, ijkl -&gt; ijl&quot, c, u_hat_temp)
        v<a id="change"> = </a>squash(s)

        return v
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/riroaki/capsnet/commit/408b1e77f4e40589def9c313c0b11beaa88f2108#diff-903964c7bdce50a4b995b4520374f243c677f45b2f8c7ed7b15abeeb6f26af12L45' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 8368060</div><div id='project'> Project Name: riroaki/capsnet</div><div id='commit'> Commit Name: 408b1e77f4e40589def9c313c0b11beaa88f2108</div><div id='time'> Time: 2020-03-08</div><div id='author'> Author: aki@akideMacBook-Pro.local</div><div id='file'> File Name: capsnet.py</div><div id='m_class'> M Class Name: DigitCaps</div><div id='n_method'> N Class Name: DigitCaps</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: capsnet.py</div><div id='n_file'> N File Name: capsnet.py</div><div id='m_start'> M Start Line: 47</div><div id='m_end'> M End Line: 65</div><div id='n_start'> N Start Line: 52</div><div id='n_end'> N End Line: 84</div><BR>