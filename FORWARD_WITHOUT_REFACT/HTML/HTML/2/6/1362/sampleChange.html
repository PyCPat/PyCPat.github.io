<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        return softmax

    def forward(self, logits, samples, soft):
        <a id="change">if samples is None</a><a id="change">:
            </a>return self.gumbel_softmax(logits, self._temperature, self._eps, hard=True)
        else:
            return -torch.sum(-samples * F.log_softmax(logits, -1), -1)
</code></pre><h3>After Change</h3><pre><code class='java'>

class GumbelSoftmax(nn.Module):
    def forward(self, logits: torch.Tensor, tau: float = 1, hard: bool = False, dim: int = -1):
        gumbels<a id="change"> = -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log()</a>  &#47&#47 ~Gumbel(0,1)
        gumbels = (logits + gumbels) / tau  &#47&#47 ~Gumbel(logits,tau)
        y_soft = gumbels.softmax(dim)

        if hard:
            &#47&#47 Straight through.
            index = y_soft.max(dim, keepdim=True)[1]
            y_hard<a id="change"> = </a><a id="change">torch.zeros_like(</a>logits<a id="change">, memory_format=torch.legacy_contiguous_format)</a>.scatter_(dim, index, 1.0)
            ret = y_hard - y_soft.detach() + y_soft
        else:
            &#47&#47 Reparametrization trick.</code></pre>