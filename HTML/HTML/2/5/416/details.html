<html><h3>Pattern ID :416
</h3><img src='850456.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

            parallel_net = Chunk(ff_chunks, FeedForward(dim), along_dim = 1) if not use_pkm else PKM(dim)

            layer<a id="change"> = </a>nn.ModuleList([
                PreNorm(dim, SelfAttention(dim, heads, causal, one_kv_head = one_kv_head, blindspot_size = blindspot_size, n_local_attn_heads = local_heads, local_attn_window_size = local_attn_window_size, psi_fn = psi_fn)),
                PreNorm(dim, parallel_net)
            ])
            layers.append(layer)

            if not receives_context:
                continue

            layer = nn.ModuleList([
                PreNorm(dim, SelfAttention(dim, heads, one_kv_head = one_kv_head, psi_fn = psi_fn, receives_context = True)),
                PreNorm(dim, Chunk(ff_chunks, FeedForward(dim), along_dim = 1))
            ])
            <a id="change">layers.append(</a>layer<a id="change">)</a>

        execute_type = ReversibleSequence if reversible else SequentialSequence

        attn_context_layer = ((True, False),) if receives_context else tuple()</code></pre><h3>After Change</h3><pre><code class='java'>
            ]))

            if attend_axially:
                layers.append(nn.ModuleList(<a id="change">[
                    </a>PreNorm(dim, FoldAxially(local_attn_window_size, SelfAttention(dim, heads, causal, one_kv_head = one_kv_head, psi_fn = psi_fn))),
                    <a id="change">PreNorm(</a>dim, Chunk(ff_chunks, <a id="change">FeedForward(</a>dim<a id="change">)</a>, along_dim = 1)<a id="change">)</a>
                ]))

            if receives_context:
                layers.append(nn.ModuleList([</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/linear-attention-transformer/commit/1e04f31292ccbe233d2b9a8bc0b6f21c0f66f071#diff-eee6cf669463e9683bcf9643c4b19a493056f98ded32854287144d47f02a7032L322' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 850456</div><div id='project'> Project Name: lucidrains/linear-attention-transformer</div><div id='commit'> Commit Name: 1e04f31292ccbe233d2b9a8bc0b6f21c0f66f071</div><div id='time'> Time: 2020-06-09</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: linear_attention_transformer/linear_attention_transformer.py</div><div id='class'> Class Name: LinearAttentionTransformer</div><div id='method'> Method Name: __init__</div><div id='parent_class'> Parent Class: nn.Module</div><BR><BR><div id='link'><a href='https://github.com/lucidrains/adjacent-attention-network/commit/fa7a72d7d257d72c159b59572297133ee91905cb#diff-71f2ddd08dfbd4dae492be6a13060eb54c6e79f8b54246b5f612c2e7c269ab4fL103' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 850458</div><div id='project'> Project Name: lucidrains/adjacent-attention-network</div><div id='commit'> Commit Name: fa7a72d7d257d72c159b59572297133ee91905cb</div><div id='time'> Time: 2020-12-13</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: adjacent_attention_network/adjacent_attention_network.py</div><div id='class'> Class Name: AdjacentAttentionNetwork</div><div id='method'> Method Name: __init__</div><div id='parent_class'> Parent Class: nn.Module</div><BR><BR><div id='link'><a href='https://github.com/lucidrains/perceiver-pytorch/commit/dc530de88e6035a2f08d7e35ce23e57abe8371bd#diff-d109c4942e7a2f6d51cd8e55cbab114efc5eb0b0ce37d8e682449385df84ec13L125' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 850454</div><div id='project'> Project Name: lucidrains/perceiver-pytorch</div><div id='commit'> Commit Name: dc530de88e6035a2f08d7e35ce23e57abe8371bd</div><div id='time'> Time: 2021-08-30</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: perceiver_pytorch/perceiver_io.py</div><div id='class'> Class Name: PerceiverIO</div><div id='method'> Method Name: __init__</div><div id='parent_class'> Parent Class: nn.Module</div><BR>