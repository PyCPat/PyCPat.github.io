<html><h3>Pattern ID :692
</h3><img src='1656040.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            nn.LayerNorm(dim) if dual_patchnorm else None,
        )

        self.axial_pos_emb = <a id="change">nn.Parameter(</a><a id="change">torch.randn(</a>2, patch_height_width, dim<a id="change">)</a> * 0.02<a id="change">)</a>

        self.to_pixels = nn.Sequential(
            LayerNorm(dim),
            nn.Linear(dim, pixel_patch_dim),</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 axial positional embeddings, parameterized by an MLP

        pos_emb_dim = dim<a id="change"> // 2</a>

        self.axial_pos_emb_height_mlp = nn.Sequential(
            Rearrange(&quot... -&gt; ... 1&quot),
            nn.Linear(1, pos_emb_dim),
            nn.SiLU(),
            nn.Linear(pos_emb_dim, pos_emb_dim),
            nn.SiLU(),
            nn.Linear(pos_emb_dim, dim)
        )

        self.axial_pos_emb_width_mlp = <a id="change">nn.Sequential(
            </a>Rearrange(&quot... -&gt; ... 1&quot),
            nn.Linear(1, pos_emb_dim),
            nn.SiLU(),
            nn.Linear(pos_emb_dim, pos_emb_dim),
            nn.SiLU(),
            nn.Linear(pos_emb_dim, dim)<a id="change">
        )</a>

        &#47&#47 nn.Parameter(torch.randn(2, patch_height_width, dim) * 0.02)

        self.to_pixels = nn.Sequential(</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/recurrent-interface-network-pytorch/commit/b323532e40464af272a7a4e43275fb70579232ae#diff-c245517a15c6c34e71df6f08ed422324484176189f4795463b7e58278d35b9cdL363' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 1656040</div><div id='project'> Project Name: lucidrains/recurrent-interface-network-pytorch</div><div id='commit'> Commit Name: b323532e40464af272a7a4e43275fb70579232ae</div><div id='time'> Time: 2023-03-05</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: rin_pytorch/rin_pytorch.py</div><div id='class'> Class Name: RIN</div><div id='method'> Method Name: __init__</div><div id='parent_class'> Parent Class: nn.Module</div><BR><BR><div id='link'><a href='https://github.com/lucidrains/etsformer-pytorch/commit/efd13ff72791a8a937a7f61515cb8823d6642c18#diff-b4b8264f25f5dfdaf86bc5e5eff19dd88e54974eb3ebdbb09741da0265b56130L388' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 1656031</div><div id='project'> Project Name: lucidrains/etsformer-pytorch</div><div id='commit'> Commit Name: efd13ff72791a8a937a7f61515cb8823d6642c18</div><div id='time'> Time: 2022-03-20</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: etsformer_pytorch/etsformer_pytorch.py</div><div id='class'> Class Name: ClassificationWrapper</div><div id='method'> Method Name: __init__</div><div id='parent_class'> Parent Class: nn.Module</div><BR><BR><div id='link'><a href='https://github.com/lucidrains/glom-pytorch/commit/570bc6667245f45ef03ad01b42cb335bda11d728#diff-2bce7536af7c0a2fe47054a9c6dd00320e623412cf5c9d7e24a0b0b8c950a574L26' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 1656035</div><div id='project'> Project Name: lucidrains/glom-pytorch</div><div id='commit'> Commit Name: 570bc6667245f45ef03ad01b42cb335bda11d728</div><div id='time'> Time: 2021-03-05</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: glom_pytorch/glom_pytorch.py</div><div id='class'> Class Name: GroupedFeedForward</div><div id='method'> Method Name: __init__</div><div id='parent_class'> Parent Class: nn.Module</div><BR>