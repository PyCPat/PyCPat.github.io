<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 initializing partition buffers
        &#47&#47 GPT2Model/Block[9]/Attention[attn]/Tensor[bias]
        <a id="change">self.register_buffer(&quotb_0&quot</a>,tensors[&quotGPT2Model/Block[9]/Attention[attn]/Tensor[bias]&quot]<a id="change">)</a>
        &#47&#47 GPT2Model/Block[10]/Attention[attn]/Tensor[bias]
        self.register_buffer(&quotb_1&quot,tensors[&quotGPT2Model/Block[10]/Attention[attn]/Tensor[bias]&quot])
        &#47&#47 GPT2Model/Block[11]/Attention[attn]/Tensor[bias]
        self.register_buffer(&quotb_2&quot,tensors[&quotGPT2Model/Block[11]/Attention[attn]/Tensor[bias]&quot])</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 initializing partition parameters

        self.device = torch.device(&quotcuda:3&quot)
        self.lookup = <a id="change">{ </a>&quotl_0&quot: &quottransformer.10.attn.attn_dropout&quot,
                        &quotl_1&quot: &quottransformer.10.attn.c_proj&quot,
                        &quotl_2&quot: &quottransformer.10.attn.resid_dropout&quot,
                        &quotl_3&quot: &quottransformer.10.ln_2&quot,
                        &quotl_4&quot: &quottransformer.10.mlp.c_fc&quot,
                        &quotl_5&quot: &quottransformer.10.mlp.c_proj&quot,
                        &quotl_6&quot: &quottransformer.10.mlp.dropout&quot,
                        &quotl_7&quot: &quottransformer.11.ln_1&quot,
                        &quotl_8&quot: &quottransformer.11.attn.c_attn&quot,
                        &quotl_9&quot: &quottransformer.11.attn.attn_dropout&quot,
                        &quotl_10&quot: &quottransformer.11.attn.c_proj&quot,
                        &quotl_11&quot: &quottransformer.11.attn.resid_dropout&quot,
                        &quotl_12&quot: &quottransformer.11.ln_2&quot,
                        &quotl_13&quot: &quottransformer.11.mlp.c_fc&quot,
                        &quotl_14&quot: &quottransformer.11.mlp.c_proj&quot,
                        &quotl_15&quot: &quottransformer.11.mlp.dropout&quot,
                        &quotl_16&quot: &quottransformer.ln_f&quot,
                        &quotl_17&quot: &quotlm_head&quot,
                        &quotb_0&quot: &quottransformer.10.attn.bias&quot,
                        &quotb_1&quot: &quottransformer.11.attn.bias&quot<a id="change">}</a>

    def forward(self, x0, x1, x2):
        &#47&#47 GPT2LMHeadModel/GPT2Model[transformer]/Block[10]/Attention[attn]/Dropout[attn_dropout] &lt;=&gt; self.l_0
        &#47&#47 GPT2LMHeadModel/GPT2Model[transformer]/Block[10]/Attention[attn]/Conv1D[c_proj] &lt;=&gt; self.l_1</code></pre>