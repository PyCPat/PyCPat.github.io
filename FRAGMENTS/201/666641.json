{"BEFORE":"    def __init__(self, in_dim, action_dim, hidden_dim, num_layers_linear_hidden):\n        super(Critic, self).__init__()\n\n        assert len(in_dim) == 1\n        assert len(action_dim) == 1\n\n        in_dim = np.product(in_dim)\n        action_dim = np.product(action_dim)\n\n        self.operators = nn.ModuleList([\n            Flatten(),\n            nn.Linear(in_dim + action_dim, hidden_dim),\n            nn.ReLU()\n        ])\n\n        for l in range(num_layers_linear_hidden - 1):\n            self.operators.append(nn.Linear(hidden_dim, hidden_dim))\n            self.operators.append(nn.ReLU())\n\n        self.operators.append(nn.Linear(hidden_dim, 1))\n","AFTER":"        assert value_structure[0][1] is not None\n        prev_object = value_structure.pop(0)\n\n        self.operators = nn.ModuleList([\n            Flatten(),\n            nn.Linear(in_dim + action_dim, prev_object[1]),\n        ])\n\n        for layer, argument in value_structure[:-1]:\n            if layer == 'linear':\n                self.operators.append(nn.Linear(prev_object[1], argument))\n                prev_object = (layer, argument)\n            elif layer == 'relu':\n                assert argument is None, 'No argument for ReLU please'\n                self.operators.append(nn.ReLU())\n            elif layer == 'dropout':\n                self.operators.append(nn.Dropout(argument))\n            else:\n                raise NotImplementedError(f'{layer} not known')\n\n        self.operators.append(nn.Linear(prev_object[1], 1))\n"}