{"BEFORE":"    self.noise = global_params.noise\n    self.channels = global_params.channels\n    self.image_size = global_params.image_size\n    self.batch_norm_momentum = global_params.batch_norm_momentum\n    self.relu_negative_slope = global_params.relu_negative_slope\n\n    def block(in_features, out_features, normalize=True):\n      r\"\"\" Define neuron module layer.\n\n      Args:\n        in_features (int): size of each input sample.\n        out_features (int): size of each output sample.\n        normalize (bool): If set to ``False``, the block will not add\n                            an batch normalization method. Default: ``True``.\n\n      Returns:\n        Some neural model layers\n\n      Examples:\n        >>> block(6, 16, normalize=False)\n        [Linear(in_features=6, out_features=16, bias=True),\n        LeakyReLU(negative_slope=0.2, inplace=True)]\n        >>> block(6, 16)\n        [Linear(in_features=6, out_features=16, bias=True),\n        BatchNorm1d(16, eps=0.8, momentum=0.1, affine=True, track_running_stats=True),\n        LeakyReLU(negative_slope=0.2, inplace=True)]\n      \"\"\"\n      layers = [nn.Linear(in_features, out_features)]\n      if normalize:\n        layers.append(nn.BatchNorm1d(out_features, self.batch_norm_momentum))\n      layers.append(nn.LeakyReLU(self.relu_negative_slope, inplace=True))\n      return layers\n\n    self.main = nn.Sequential(\n      *block(self.noise, 128, normalize=False),\n      *block(128, 256),\n      *block(256, 512),\n      *block(512, 1024),\n      nn.Linear(1024, self.channels * self.image_size * self.image_size),\n      nn.Tanh()\n    )\n","AFTER":"    self.main = nn.Sequential(\n      nn.Linear(global_params.noise, 128),\n      nn.LeakyReLU(global_params.negative_slope, inplace=True),\n\n      nn.Linear(128, 256),\n      nn.BatchNorm1d(256, global_params.batch_norm_momentum),\n      nn.LeakyReLU(global_params.negative_slope, inplace=True),\n\n      nn.Linear(256, 512),\n      nn.BatchNorm1d(512, global_params.batch_norm_momentum),\n      nn.LeakyReLU(global_params.negative_slope, inplace=True),\n\n      nn.Linear(512, 1024),\n      nn.BatchNorm1d(1024, global_params.batch_norm_momentum),\n      nn.LeakyReLU(global_params.negative_slope, inplace=True),\n\n      nn.Linear(1024, self.channels * self.image_size * self.image_size),\n      nn.Tanh()\n    )\n"}