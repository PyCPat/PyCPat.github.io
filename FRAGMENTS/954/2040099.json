{"BEFORE":"        learned_variance = False\n    ):\n        super().__init__()\n\n        # determine dimensions\n\n        self.channels = channels\n        self.self_condition = self_condition\n        input_channels = channels * (2 if self_condition else 1)\n\n        init_dim = default(init_dim, dim)\n        self.init_conv = nn.Conv2d(input_channels, init_dim, 7, padding = 3)\n\n        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n        in_out = list(zip(dims[:-1], dims[1:]))\n\n        block_klass = partial(ResnetBlock, groups = resnet_block_groups)\n\n        # time embeddings\n\n        time_dim = dim * 4\n\n        self.time_mlp = nn.Sequential(\n            SinusoidalPosEmb(dim),\n            nn.Linear(dim, time_dim),\n            nn.GELU(),\n            nn.Linear(time_dim, time_dim)\n        )\n\n        # layers\n\n        self.downs = nn.ModuleList([])\n        self.ups = nn.ModuleList([])\n        num_resolutions = len(in_out)\n\n        for ind, (dim_in, dim_out) in enumerate(in_out):\n            is_last = ind >= (num_resolutions - 1)\n\n            self.downs.append(nn.ModuleList([\n                block_klass(dim_in, dim_in, time_emb_dim = time_dim),\n                block_klass(dim_in, dim_in, time_emb_dim = time_dim),\n                Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n                Downsample(dim_in, dim_out) if not is_last else nn.Conv2d(dim_in, dim_out, 3, padding = 1)\n            ]))\n\n        mid_dim = dims[-1]\n        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim = time_dim)\n        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim = time_dim)\n\n        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n            is_last = ind == (len(in_out) - 1)\n\n            self.ups.append(nn.ModuleList([\n                block_klass(dim_out + dim_in, dim_out, time_emb_dim = time_dim),\n                block_klass(dim_out + dim_in, dim_out, time_emb_dim = time_dim),\n                Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n                Upsample(dim_out, dim_in) if not is_last else  nn.Conv2d(dim_out, dim_in, 3, padding = 1)\n            ]))\n\n        default_out_dim = channels * (1 if not learned_variance else 2)\n        self.out_dim = default(out_dim, default_out_dim)\n\n        self.final_res_block = block_klass(dim * 2, dim, time_emb_dim = time_dim)\n        self.final_conv = nn.Conv2d(dim, self.out_dim, 1)\n","AFTER":"        init_dim = default(init_dim, dim)\n\n        self.init_conv = nn.Conv2d(input_channels, init_dim, 7, padding = 3)\n        self.cond_init_conv = nn.Conv2d(channels, init_dim, 7, padding = 3)\n\n        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n        in_out = list(zip(dims[:-1], dims[1:]))\n\n        block_klass = partial(ResnetBlock, groups = resnet_block_groups)\n\n        # time embeddings\n\n        time_dim = dim * 4\n\n        self.time_mlp = nn.Sequential(\n            SinusoidalPosEmb(dim),\n            nn.Linear(dim, time_dim),\n            nn.GELU(),\n            nn.Linear(time_dim, time_dim)\n        )\n\n        # layers\n\n        num_resolutions = len(in_out)\n\n        # downsampling encoding blocks\n\n        self.downs = nn.ModuleList([])\n\n        for ind, (dim_in, dim_out) in enumerate(in_out):\n            is_last = ind >= (num_resolutions - 1)\n\n            self.downs.append(nn.ModuleList([\n                block_klass(dim_in, dim_in, time_emb_dim = time_dim),\n                block_klass(dim_in, dim_in, time_emb_dim = time_dim),\n                Residual(LinearAttention(dim_in)),\n                LayerNorm(dim_in, bias = True),\n                Downsample(dim_in, dim_out) if not is_last else nn.Conv2d(dim_in, dim_out, 3, padding = 1)\n            ]))\n\n        # middle blocks\n\n        mid_dim = dims[-1]\n        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim = time_dim)\n        self.mid_attn = Residual(Attention(mid_dim))\n        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim = time_dim)\n\n        # condition encoding path will be the same as the main encoding path\n\n        self.cond_downs = copy.deepcopy(self.downs)\n        self.cond_mid_block1 = copy.deepcopy(self.mid_block1)\n\n        # upsampling decoding blocks\n\n        self.ups = nn.ModuleList([])\n\n        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n            is_last = ind == (len(in_out) - 1)\n\n            self.ups.append(nn.ModuleList([\n                block_klass(dim_out + dim_in, dim_out, time_emb_dim = time_dim),\n                block_klass(dim_out + dim_in, dim_out, time_emb_dim = time_dim),\n                Residual(LinearAttention(dim_out)),\n                Upsample(dim_out, dim_in) if not is_last else  nn.Conv2d(dim_out, dim_in, 3, padding = 1)\n            ]))\n\n        # projection out to predictions\n\n        self.final_res_block = block_klass(dim * 2, dim, time_emb_dim = time_dim)\n        self.final_conv = nn.Conv2d(dim, channels, 1)\n"}