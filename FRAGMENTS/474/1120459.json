{"BEFORE":"        self.num_patches = (image_height \/\/ patch_height) * (image_width \/\/ patch_width)\n        self.patch_dim = channels * patch_height * patch_width\n        \n        self.to_patch_embedding = nn.Sequential(\n            nn.Conv2d(channels, dim, kernel_size=patch_size, stride=patch_size),\n            Rearrange('b c h w -> b (h w) c'),\n        )\n        self.en_pos_embedding = nn.Parameter(torch.randn(1, self.num_patches, dim))\n","AFTER":"        en_pos_embedding = get_2d_sincos_pos_embed(dim, (image_height \/\/ patch_height, image_width \/\/ patch_width))\n\n        self.num_patches = (image_height \/\/ patch_height) * (image_width \/\/ patch_width)\n        self.patch_dim = channels * patch_height * patch_width\n\n        self.to_patch_embedding = nn.Sequential(\n            nn.Conv2d(channels, dim, kernel_size=patch_size, stride=patch_size),\n            Rearrange('b c h w -> b (h w) c'),\n        )\n        self.en_pos_embedding = nn.Parameter(torch.from_numpy(en_pos_embedding).float().unsqueeze(0), requires_grad=False)\n        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim)\n        self.norm = nn.LayerNorm(dim)\n"}