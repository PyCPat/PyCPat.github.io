{"BEFORE":"        channels = [self.in_shape[0]]\n        channels += [self.hid_dim * (2 ** i) for i in range(0, n_layers)]\n        # by how much will increase after flattening\n        factor = (self.in_shape[1] * self.in_shape[2]) \/\/ ((2 ** n_layers) ** 2)\n\n        if self.is_transpose:\n            channels.reverse()\n\n        layers = []\n        in_chan = channels[0]\n        for out_chan in channels[1:]:\n","AFTER":"        self.norm_layer = norm_layer\n        self.activation = activation\n\n        Norm = get_Normalization(self.norm_layer, 2)\n        # don't use bias with batch_norm https:\/\/twitter.com\/karpathy\/status\/1013245864570073090?l...\n        is_bias = Norm == nn.Identity\n\n        assert is_pow2(self.in_shape[1]) and is_pow2(self.in_shape[2])\n\n        n_layers = 4\n        # for size 32 will go 32,16,8,4,2\n        # channels for hid_dim=32: 3,32,64,128,256\n        channels = [self.in_shape[0]]\n        channels += [self.hid_dim * (2 ** i) for i in range(0, n_layers)]\n        # by how much will increase after flattening\n        factor = (self.in_shape[1] * self.in_shape[2]) \/\/ ((2 ** n_layers) ** 2)\n\n        if self.is_transpose:\n            channels.reverse()\n\n        layers = []\n        in_chan = channels[0]\n        for i, out_chan in enumerate(channels[1:]):\n            is_last = i == len(channels[1:]) - 1\n            layers += self.make_block(\n                in_chan, out_chan, Norm, is_bias, is_last, **kwargs\n"}