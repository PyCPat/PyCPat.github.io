{"BEFORE":"        in_out = list(zip(dims[:-1], dims[1:]))\n\n        # time, image embeddings, and optional text encoding\n\n        cond_dim = default(cond_dim, dim)\n        time_cond_dim = dim * 4\n\n        # embedding time for discrete gaussian diffusion or log(snr) noise for continuous version\n\n        self.fourier_embed_time_or_noise = fourier_embed_time_or_noise\n\n        if fourier_embed_time_or_noise:\n            self.to_time_hiddens = nn.Sequential(\n                SinusoidalPosEmb(dim),\n                nn.Linear(dim, time_cond_dim),\n                nn.SiLU()\n            )\n        else:\n            self.to_time_hiddens = nn.Sequential(\n                Rearrange('... -> ... 1'),\n                nn.Linear(1, time_cond_dim),\n                nn.SiLU(),\n                nn.LayerNorm(time_cond_dim),\n                nn.Linear(time_cond_dim, time_cond_dim),\n                nn.SiLU(),\n                nn.LayerNorm(time_cond_dim)\n            )\n\n        self.to_time_cond = nn.Sequential(\n            nn.Linear(time_cond_dim, time_cond_dim)\n        )\n\n        # project to time tokens as well as time hiddens\n\n        self.to_time_tokens = nn.Sequential(\n            nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n            Rearrange('b (r d) -> b r d', r = num_time_tokens)\n        )\n\n        # low res aug noise conditioning\n\n        self.lowres_cond = lowres_cond\n\n        if lowres_cond:\n            self.to_lowres_time_hiddens = nn.Sequential(\n                Rearrange('... -> ... 1'),\n                nn.Linear(1, time_cond_dim),\n                nn.SiLU(),\n                nn.LayerNorm(time_cond_dim),\n                nn.Linear(time_cond_dim, time_cond_dim),\n                nn.SiLU(),\n                nn.LayerNorm(time_cond_dim)\n            )\n\n            self.to_lowres_time_cond = nn.Sequential(\n                nn.Linear(time_cond_dim, time_cond_dim)\n            )\n\n            self.to_lowres_time_tokens = nn.Sequential(\n                nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n                Rearrange('b (r d) -> b r d', r = num_time_tokens)\n            )\n\n        # normalizations\n\n        self.norm_cond = nn.LayerNorm(cond_dim)\n        self.norm_mid_cond = nn.LayerNorm(cond_dim)\n\n        # text encoding conditioning (optional)\n\n        self.text_to_cond = None\n\n        if cond_on_text:\n            assert exists(text_embed_dim), 'text_embed_dim must be given to the unet if cond_on_text is True'\n            self.text_to_cond = nn.Linear(text_embed_dim, cond_dim)\n\n        # finer control over whether to condition on text encodings\n\n        self.cond_on_text = cond_on_text\n\n        # attention pooling\n\n        self.attn_pool = PerceiverResampler(dim = cond_dim, depth = 2, dim_head = attn_dim_head, heads = attn_heads, num_latents = attn_pool_num_latents) if attn_pool_text else None\n\n        # for classifier free guidance\n\n        self.max_text_len = max_text_len\n\n        self.null_text_embed = nn.Parameter(torch.randn(1, max_text_len, cond_dim))\n        self.null_text_hidden = nn.Parameter(torch.randn(1, time_cond_dim))\n\n        # for non-attention based text conditioning at all points in the network where time is also conditioned\n\n        self.to_text_non_attn_cond = None\n\n        if cond_on_text:\n            self.to_text_non_attn_cond = nn.Sequential(\n                nn.LayerNorm(cond_dim),\n                nn.Linear(cond_dim, time_cond_dim),\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, time_cond_dim)\n            )\n\n        # attention related params\n\n        attn_kwargs = dict(heads = attn_heads, dim_head = attn_dim_head)\n\n        num_layers = len(in_out)\n\n        # resnet block klass\n\n        num_resnet_blocks = cast_tuple(num_resnet_blocks, num_layers)\n        resnet_groups = cast_tuple(resnet_groups, num_layers)\n\n        layer_attns = cast_tuple(layer_attns, num_layers)\n        layer_cross_attns = cast_tuple(layer_cross_attns, num_layers)\n\n        assert all([layers == num_layers for layers in list(map(len, (resnet_groups, layer_attns, layer_cross_attns)))])\n\n        # downsample klass\n\n        downsample_klass = Downsample\n        if cross_embed_downsample:\n            downsample_klass = partial(CrossEmbedLayer, kernel_sizes = cross_embed_downsample_kernel_sizes)\n\n        # layers\n\n        self.downs = nn.ModuleList([])\n        self.ups = nn.ModuleList([])\n        num_resolutions = len(in_out)\n\n        layer_params = [num_resnet_blocks, resnet_groups, layer_attns, layer_cross_attns]\n        reversed_layer_params = list(map(reversed, layer_params))\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_cross_attn) in enumerate(zip(in_out, *layer_params)):\n            is_last = ind >= (num_resolutions - 1)\n\n            layer_use_linear_cross_attn = not layer_cross_attn and use_linear_cross_attn\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            transformer_block_klass = TransformerBlock if layer_attn else (LinearAttentionTransformerBlock if use_linear_attn else nn.Identity)\n\n            self.downs.append(nn.ModuleList([\n                downsample_klass(dim_in, dim_out = dim_out) if memory_efficient else None,\n                ResnetBlock(dim_out if memory_efficient else dim_in, dim_out, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(dim_out, dim_out, groups = groups) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = dim_out, heads = attn_heads, dim_head = attn_dim_head, ff_mult = ff_mult),\n                downsample_klass(dim_out) if not memory_efficient and not is_last else None,\n            ]))\n\n        mid_dim = dims[-1]\n\n        self.mid_block1 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n        self.mid_attn = EinopsToAndFrom('b c h w', 'b (h w) c', Residual(Attention(mid_dim, **attn_kwargs))) if attend_at_middle else None\n        self.mid_block2 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n\n        up_in_out_slice = slice(1 if not memory_efficient else None, None)\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_cross_attn) in enumerate(zip(reversed(in_out[up_in_out_slice]), *reversed_layer_params)):\n            layer_use_linear_cross_attn = not layer_cross_attn and use_linear_cross_attn\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            transformer_block_klass = TransformerBlock if layer_attn else (LinearAttentionTransformerBlock if use_linear_attn else nn.Identity)\n\n            # todo: fix the ordering of the upsampling block in memory inefficient version\n\n            self.ups.append(nn.ModuleList([\n                ResnetBlock(dim_out * 2, dim_in, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(dim_in, dim_in, groups = groups) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = dim_in, heads = attn_heads, dim_head = attn_dim_head, ff_mult = ff_mult),\n                Upsample(dim_in)\n            ]))\n\n        final_conv_dim = dim * (2 if not memory_efficient else 1)\n\n        self.final_conv = nn.Sequential(\n            ResnetBlock(final_conv_dim, dim, groups = resnet_groups[0]),\n","AFTER":"        in_out = list(zip(dims[:-1], dims[1:]))\n\n        # time, image embeddings, and optional text encoding\n\n        cond_dim = default(cond_dim, dim)\n        time_cond_dim = dim * 4\n\n        # embedding time for discrete gaussian diffusion or log(snr) noise for continuous version\n\n        self.fourier_embed_time_or_noise = fourier_embed_time_or_noise\n\n        if fourier_embed_time_or_noise:\n            self.to_time_hiddens = nn.Sequential(\n                SinusoidalPosEmb(dim),\n                nn.Linear(dim, time_cond_dim),\n                nn.SiLU()\n            )\n        else:\n            self.to_time_hiddens = nn.Sequential(\n                Rearrange('... -> ... 1'),\n                nn.Linear(1, time_cond_dim),\n                nn.SiLU(),\n                nn.LayerNorm(time_cond_dim),\n                nn.Linear(time_cond_dim, time_cond_dim),\n                nn.SiLU(),\n                nn.LayerNorm(time_cond_dim)\n            )\n\n        self.to_time_cond = nn.Sequential(\n            nn.Linear(time_cond_dim, time_cond_dim)\n        )\n\n        # project to time tokens as well as time hiddens\n\n        self.to_time_tokens = nn.Sequential(\n            nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n            Rearrange('b (r d) -> b r d', r = num_time_tokens)\n        )\n\n        # low res aug noise conditioning\n\n        self.lowres_cond = lowres_cond\n\n        if lowres_cond:\n            self.to_lowres_time_hiddens = nn.Sequential(\n                Rearrange('... -> ... 1'),\n                nn.Linear(1, time_cond_dim),\n                nn.SiLU(),\n                nn.LayerNorm(time_cond_dim),\n                nn.Linear(time_cond_dim, time_cond_dim),\n                nn.SiLU(),\n                nn.LayerNorm(time_cond_dim)\n            )\n\n            self.to_lowres_time_cond = nn.Sequential(\n                nn.Linear(time_cond_dim, time_cond_dim)\n            )\n\n            self.to_lowres_time_tokens = nn.Sequential(\n                nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n                Rearrange('b (r d) -> b r d', r = num_time_tokens)\n            )\n\n        # normalizations\n\n        self.norm_cond = nn.LayerNorm(cond_dim)\n        self.norm_mid_cond = nn.LayerNorm(cond_dim)\n\n        # text encoding conditioning (optional)\n\n        self.text_to_cond = None\n\n        if cond_on_text:\n            assert exists(text_embed_dim), 'text_embed_dim must be given to the unet if cond_on_text is True'\n            self.text_to_cond = nn.Linear(text_embed_dim, cond_dim)\n\n        # finer control over whether to condition on text encodings\n\n        self.cond_on_text = cond_on_text\n\n        # attention pooling\n\n        self.attn_pool = PerceiverResampler(dim = cond_dim, depth = 2, dim_head = attn_dim_head, heads = attn_heads, num_latents = attn_pool_num_latents) if attn_pool_text else None\n\n        # for classifier free guidance\n\n        self.max_text_len = max_text_len\n\n        self.null_text_embed = nn.Parameter(torch.randn(1, max_text_len, cond_dim))\n        self.null_text_hidden = nn.Parameter(torch.randn(1, time_cond_dim))\n\n        # for non-attention based text conditioning at all points in the network where time is also conditioned\n\n        self.to_text_non_attn_cond = None\n\n        if cond_on_text:\n            self.to_text_non_attn_cond = nn.Sequential(\n                nn.LayerNorm(cond_dim),\n                nn.Linear(cond_dim, time_cond_dim),\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, time_cond_dim)\n            )\n\n        # attention related params\n\n        attn_kwargs = dict(heads = attn_heads, dim_head = attn_dim_head)\n\n        num_layers = len(in_out)\n\n        # resnet block klass\n\n        num_resnet_blocks = cast_tuple(num_resnet_blocks, num_layers)\n        resnet_groups = cast_tuple(resnet_groups, num_layers)\n\n        layer_attns = cast_tuple(layer_attns, num_layers)\n        layer_cross_attns = cast_tuple(layer_cross_attns, num_layers)\n\n        assert all([layers == num_layers for layers in list(map(len, (resnet_groups, layer_attns, layer_cross_attns)))])\n\n        # downsample klass\n\n        downsample_klass = Downsample\n        if cross_embed_downsample:\n            downsample_klass = partial(CrossEmbedLayer, kernel_sizes = cross_embed_downsample_kernel_sizes)\n\n        # layers\n\n        self.downs = nn.ModuleList([])\n        self.ups = nn.ModuleList([])\n        num_resolutions = len(in_out)\n\n        layer_params = [num_resnet_blocks, resnet_groups, layer_attns, layer_cross_attns]\n        reversed_layer_params = list(map(reversed, layer_params))\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_cross_attn) in enumerate(zip(in_out, *layer_params)):\n            is_last = ind >= (num_resolutions - 1)\n\n            layer_use_linear_cross_attn = not layer_cross_attn and use_linear_cross_attn\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            transformer_block_klass = TransformerBlock if layer_attn else (LinearAttentionTransformerBlock if use_linear_attn else nn.Identity)\n\n            self.downs.append(nn.ModuleList([\n                downsample_klass(dim_in, dim_out = dim_out) if memory_efficient else None,\n                ResnetBlock(dim_out if memory_efficient else dim_in, dim_out, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(dim_out, dim_out, groups = groups) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = dim_out, heads = attn_heads, dim_head = attn_dim_head, ff_mult = ff_mult),\n                downsample_klass(dim_out) if not memory_efficient and not is_last else None,\n            ]))\n\n        mid_dim = dims[-1]\n\n        self.mid_block1 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n        self.mid_attn = EinopsToAndFrom('b c h w', 'b (h w) c', Residual(Attention(mid_dim, **attn_kwargs))) if attend_at_middle else None\n        self.mid_block2 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_cross_attn) in enumerate(zip(reversed(in_out), *reversed_layer_params)):\n            is_last = ind == (len(in_out) - 1)\n            layer_use_linear_cross_attn = not layer_cross_attn and use_linear_cross_attn\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n            transformer_block_klass = TransformerBlock if layer_attn else (LinearAttentionTransformerBlock if use_linear_attn else nn.Identity)\n\n            self.ups.append(nn.ModuleList([\n                ResnetBlock(dim_out * 2, dim_in, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(dim_in, dim_in, groups = groups) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = dim_in, heads = attn_heads, dim_head = attn_dim_head, ff_mult = ff_mult),\n                Upsample(dim_in) if not is_last or memory_efficient else nn.Identity()\n            ]))\n\n        self.final_conv = nn.Sequential(\n            ResnetBlock(dim, dim, groups = resnet_groups[0]),\n"}