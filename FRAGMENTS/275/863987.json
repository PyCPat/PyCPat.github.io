{"BEFORE":"        english_words = set()\n        foreign_words = set()\n        for aligned_sent in self.aligned_sents:\n            english_words.update(aligned_sent.words)\n            foreign_words.update(aligned_sent.mots)\n        # add the NULL token to the foreign word set.\n        foreign_words.add(None)\n        num_probs = len(english_words)*len(foreign_words)\n\n        # Initialise t(e|f) uniformly\n        t = defaultdict(lambda: float(1)\/len(english_words))\n        s_total = defaultdict(float)\n        for e in english_words:\n            for f in foreign_words:\n                z = t[e,f]\n\n        globally_converged = False\n        iteration_count = 0\n        while not globally_converged:\n            # count(e|f)\n            count = defaultdict(float)\n            # total(f)\n            total = defaultdict(float)\n\n            for aligned_sent in self.aligned_sents:\n                # Compute normalization\n                for e_w in aligned_sent.words:\n                    s_total[e_w] = 0.0\n                    for f_w in aligned_sent.mots+[None]:\n                        s_total[e_w] += t[e_w, f_w]\n\n                # Collect counts\n                for e_w in aligned_sent.words:\n                    for f_w in aligned_sent.mots+[None]:\n                        cnt = t[e_w, f_w] \/ s_total[e_w]\n                        count[e_w, f_w] += cnt\n                        total[f_w] += cnt\n\n            # Estimate probabilities\n            num_converged = 0\n            for f_w in foreign_words:\n                for e_w in english_words:\n                    new_prob = count[e_w, f_w] \/ total[f_w]\n                    delta = abs(t[e_w, f_w] - new_prob)\n                    if delta < self.convergent_threshold:\n","AFTER":"        logging.debug(\"Starting training\")\n\n        # Collect up sets of all English and foreign words\n        english_words = set()\n        foreign_words = set()\n        for aligned_sent in self.aligned_sents:\n            english_words.update(aligned_sent.words)\n            foreign_words.update(aligned_sent.mots)\n        # add the NULL token to the foreign word set.\n        foreign_words.add(None)\n        num_probs = len(english_words) * len(foreign_words)\n\n        # Initialise t(e|f) uniformly\n        default_prob = 1.0 \/ len(english_words)\n        t = defaultdict(lambda: default_prob)\n\n        convergent_threshold = self.convergent_threshold\n        globally_converged = False\n        iteration_count = 0\n        while not globally_converged:\n            # count(e|f)\n            count = defaultdict(float)\n            # total(f)\n            total = defaultdict(float)\n\n            for aligned_sent in self.aligned_sents:\n                s_total = {}\n                # Compute normalization\n                for e_w in aligned_sent.words:\n                    s_total[e_w] = 0.0\n                    for f_w in aligned_sent.mots+[None]:\n                        s_total[e_w] += t[e_w, f_w]\n\n                # Collect counts\n                for e_w in aligned_sent.words:\n                    for f_w in aligned_sent.mots+[None]:\n                        cnt = t[e_w, f_w] \/ s_total[e_w]\n                        count[e_w, f_w] += cnt\n                        total[f_w] += cnt\n\n            # Estimate probabilities\n            num_converged = 0\n            for f_w in foreign_words:\n                for e_w in english_words:\n                    new_prob = count[e_w, f_w] \/ total[f_w]\n                    delta = abs(t[e_w, f_w] - new_prob)\n                    if delta < convergent_threshold:\n"}