{"BEFORE":"        self.l_18 = layers['GPT2Model\/Block[11]\/LayerNorm[ln_1]']\n        assert isinstance(self.l_18,LayerNorm) ,f'layers[GPT2Model\/Block[11]\/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_18)}'\n        self.l_19 = layers['GPT2Model\/Block[11]\/Attention[attn]\/Conv1D[c_attn]']\n        assert isinstance(self.l_19,Conv1D) ,f'layers[GPT2Model\/Block[11]\/Attention[attn]\/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_19)}'\n        self.l_20 = layers['GPT2Model\/Block[11]\/Attention[attn]\/Dropout[attn_dropout]']\n        assert isinstance(self.l_20,Dropout) ,f'layers[GPT2Model\/Block[11]\/Attention[attn]\/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_20)}'\n        self.l_21 = layers['GPT2Model\/Block[11]\/Attention[attn]\/Conv1D[c_proj]']\n        assert isinstance(self.l_21,Conv1D) ,f'layers[GPT2Model\/Block[11]\/Attention[attn]\/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_21)}'\n        self.l_22 = layers['GPT2Model\/Block[11]\/Attention[attn]\/Dropout[resid_dropout]']\n        assert isinstance(self.l_22,Dropout) ,f'layers[GPT2Model\/Block[11]\/Attention[attn]\/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_22)}'\n        self.l_23 = layers['GPT2Model\/Block[11]\/LayerNorm[ln_2]']\n        assert isinstance(self.l_23,LayerNorm) ,f'layers[GPT2Model\/Block[11]\/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_23)}'\n        self.l_24 = layers['GPT2Model\/Block[11]\/MLP[mlp]\/Conv1D[c_fc]']\n        assert isinstance(self.l_24,Conv1D) ,f'layers[GPT2Model\/Block[11]\/MLP[mlp]\/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_24)}'\n        self.l_25 = layers['GPT2Model\/Block[11]\/MLP[mlp]\/Conv1D[c_proj]']\n        assert isinstance(self.l_25,Conv1D) ,f'layers[GPT2Model\/Block[11]\/MLP[mlp]\/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_25)}'\n        self.l_26 = layers['GPT2Model\/Block[11]\/MLP[mlp]\/Dropout[dropout]']\n        assert isinstance(self.l_26,Dropout) ,f'layers[GPT2Model\/Block[11]\/MLP[mlp]\/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_26)}'\n        self.l_27 = layers['GPT2Model\/LayerNorm[ln_f]']\n        assert isinstance(self.l_27,LayerNorm) ,f'layers[GPT2Model\/LayerNorm[ln_f]] is expected to be of type LayerNorm but was of type {type(self.l_27)}'\n\n        # initializing partition buffers\n        # GPT2Model\/Block[9]\/Attention[attn]\/Tensor[bias]\n        self.register_buffer('b_0',tensors['GPT2Model\/Block[9]\/Attention[attn]\/Tensor[bias]'])\n        # GPT2Model\/Block[10]\/Attention[attn]\/Tensor[bias]\n        self.register_buffer('b_1',tensors['GPT2Model\/Block[10]\/Attention[attn]\/Tensor[bias]'])\n        # GPT2Model\/Block[11]\/Attention[attn]\/Tensor[bias]\n        self.register_buffer('b_2',tensors['GPT2Model\/Block[11]\/Attention[attn]\/Tensor[bias]'])\n        \n        # initializing partition parameters\n\n        self.device = torch.device('cuda:3')\n        self.lookup = { 'l_0': '9.ln_1',\n                        'l_1': '9.attn.c_attn',\n                        'l_2': '9.attn.attn_dropout',\n                        'l_3': '9.attn.c_proj',\n                        'l_4': '9.attn.resid_dropout',\n                        'l_5': '9.ln_2',\n                        'l_6': '9.mlp.c_fc',\n                        'l_7': '9.mlp.c_proj',\n                        'l_8': '9.mlp.dropout',\n                        'l_9': '10.ln_1',\n                        'l_10': '10.attn.c_attn',\n                        'l_11': '10.attn.attn_dropout',\n                        'l_12': '10.attn.c_proj',\n                        'l_13': '10.attn.resid_dropout',\n                        'l_14': '10.ln_2',\n                        'l_15': '10.mlp.c_fc',\n                        'l_16': '10.mlp.c_proj',\n                        'l_17': '10.mlp.dropout',\n                        'l_18': '11.ln_1',\n                        'l_19': '11.attn.c_attn',\n                        'l_20': '11.attn.attn_dropout',\n                        'l_21': '11.attn.c_proj',\n                        'l_22': '11.attn.resid_dropout',\n                        'l_23': '11.ln_2',\n                        'l_24': '11.mlp.c_fc',\n                        'l_25': '11.mlp.c_proj',\n                        'l_26': '11.mlp.dropout',\n                        'l_27': 'ln_f',\n                        'b_0': '9.attn.bias',\n                        'b_1': '10.attn.bias',\n                        'b_2': '11.attn.bias'}\n","AFTER":"        self.lookup = { 'l_0': 'transformer.10.attn.attn_dropout',\n                        'l_1': 'transformer.10.attn.c_proj',\n                        'l_2': 'transformer.10.attn.resid_dropout',\n                        'l_3': 'transformer.10.ln_2',\n                        'l_4': 'transformer.10.mlp.c_fc',\n                        'l_5': 'transformer.10.mlp.c_proj',\n                        'l_6': 'transformer.10.mlp.dropout',\n                        'l_7': 'transformer.11.ln_1',\n                        'l_8': 'transformer.11.attn.c_attn',\n                        'l_9': 'transformer.11.attn.attn_dropout',\n                        'l_10': 'transformer.11.attn.c_proj',\n                        'l_11': 'transformer.11.attn.resid_dropout',\n                        'l_12': 'transformer.11.ln_2',\n                        'l_13': 'transformer.11.mlp.c_fc',\n                        'l_14': 'transformer.11.mlp.c_proj',\n                        'l_15': 'transformer.11.mlp.dropout',\n                        'l_16': 'transformer.ln_f',\n                        'l_17': 'lm_head',\n                        'b_0': 'transformer.10.attn.bias',\n                        'b_1': 'transformer.11.attn.bias'}\n"}