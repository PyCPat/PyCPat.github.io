{"BEFORE":"                 embed_dim: int = 96,\n                 num_classes: int = 1000,\n                 window_size: int = 7,\n                 patch_size: int = 4,\n                 mlp_ratio: int = 4,\n                 drop_rate: float = 0.0,\n                 attn_drop_rate: float = 0.0,\n                 drop_path_rate: float = 0.0,\n                 norm_layer: Type[nn.Module] = nn.LayerNorm,\n                 use_checkpoint: bool = False,\n                 sequential_self_attention: bool = False,\n                 use_deformable_block: bool = False,\n                 **kwargs: Any) -> None:\n        # Call super constructor\n        super(SwinTransformerV2CR, self).__init__()\n        # Save parameters\n        self.num_classes: int = num_classes\n        self.patch_size: int = patch_size\n        self.input_resolution: Tuple[int, int] = img_size\n        self.window_size: int = window_size\n        self.num_features: int = int(embed_dim * 2 ** (len(depths) - 1))\n        # Init patch embedding\n        self.patch_embedding: nn.Module = PatchEmbedding(in_channels=in_chans, out_channels=embed_dim,\n                                                         patch_size=patch_size, norm_layer=norm_layer)\n        # Compute patch resolution\n        patch_resolution: Tuple[int, int] = (img_size[0] \/\/ patch_size, img_size[1] \/\/ patch_size)\n        # Path dropout dependent on depth\n        drop_path_rate = torch.linspace(0., drop_path_rate, sum(depths)).tolist()\n        # Init stages\n        self.stages: nn.ModuleList = nn.ModuleList()\n        for index, (depth, number_of_head) in enumerate(zip(depths, num_heads)):\n            self.stages.append(\n                SwinTransformerStage(\n                    in_channels=embed_dim * (2 ** max(index - 1, 0)),\n                    depth=depth,\n                    downscale=index != 0,\n                    input_resolution=(patch_resolution[0] \/\/ (2 ** max(index - 1, 0)),\n                                      patch_resolution[1] \/\/ (2 ** max(index - 1, 0))),\n                    number_of_heads=number_of_head,\n                    window_size=window_size,\n                    ff_feature_ratio=mlp_ratio,\n                    dropout=drop_rate,\n                    dropout_attention=attn_drop_rate,\n                    dropout_path=drop_path_rate[sum(depths[:index]):sum(depths[:index + 1])],\n                    use_checkpoint=use_checkpoint,\n                    sequential_self_attention=sequential_self_attention,\n                    use_deformable_block=use_deformable_block and (index > 0),\n                    norm_layer=norm_layer\n                ))\n        # Init final adaptive average pooling, and classification head\n        self.average_pool: nn.Module = nn.AdaptiveAvgPool2d(1)\n        self.head: nn.Module = nn.Linear(in_features=self.num_features,\n                                         out_features=num_classes)\n","AFTER":"        patch_size: int = 4,\n        window_size: Optional[int] = None,\n        img_window_ratio: int = 32,\n        in_chans: int = 3,\n        num_classes: int = 1000,\n        embed_dim: int = 96,\n        depths: Tuple[int, ...] = (2, 2, 6, 2),\n        num_heads: Tuple[int, ...] = (3, 6, 12, 24),\n        mlp_ratio: float = 4.0,\n        drop_rate: float = 0.0,\n        attn_drop_rate: float = 0.0,\n        drop_path_rate: float = 0.0,\n        norm_layer: Type[nn.Module] = nn.LayerNorm,\n        grad_checkpointing: bool = False,\n        extra_norm_period: int = 0,\n        sequential_attn: bool = False,\n        global_pool: str = 'avg',\n        **kwargs: Any\n    ) -> None:\n        super(SwinTransformerV2Cr, self).__init__()\n        img_size = to_2tuple(img_size)\n        window_size = tuple([\n            s \/\/ img_window_ratio for s in img_size]) if window_size is None else to_2tuple(window_size)\n\n        self.num_classes: int = num_classes\n        self.patch_size: int = patch_size\n        self.img_size: Tuple[int, int] = img_size\n        self.window_size: int = window_size\n        self.num_features: int = int(embed_dim * 2 ** (len(depths) - 1))\n\n        self.patch_embed: nn.Module = PatchEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=in_chans,\n            embed_dim=embed_dim, norm_layer=norm_layer)\n        patch_grid_size: Tuple[int, int] = self.patch_embed.grid_size\n\n        drop_path_rate = torch.linspace(0.0, drop_path_rate, sum(depths)).tolist()\n        stages = []\n        for index, (depth, num_heads) in enumerate(zip(depths, num_heads)):\n            stage_scale = 2 ** max(index - 1, 0)\n            stages.append(\n                SwinTransformerStage(\n                    embed_dim=embed_dim * stage_scale,\n                    depth=depth,\n                    downscale=index != 0,\n                    feat_size=(patch_grid_size[0] \/\/ stage_scale, patch_grid_size[1] \/\/ stage_scale),\n                    num_heads=num_heads,\n                    window_size=window_size,\n                    mlp_ratio=mlp_ratio,\n                    drop=drop_rate,\n                    drop_attn=attn_drop_rate,\n                    drop_path=drop_path_rate[sum(depths[:index]):sum(depths[:index + 1])],\n                    grad_checkpointing=grad_checkpointing,\n                    extra_norm_period=extra_norm_period,\n                    sequential_attn=sequential_attn,\n                    norm_layer=norm_layer,\n                )\n            )\n        self.stages = nn.Sequential(*stages)\n\n        self.global_pool: str = global_pool\n        self.head: nn.Module = nn.Linear(\n            in_features=self.num_features, out_features=num_classes) if num_classes else nn.Identity()\n"}