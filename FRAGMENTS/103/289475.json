{"BEFORE":"        self.num_attention_heads = num_attention_heads\r\n        self.layer_id = layer_id  # 第几层\r\n        self.is_decoder = kwargs.get('is_decoder', False)\r\n\r\n        # assert hidden_size % num_attention_heads == 0  # 旧逻辑，t5_pegasus_small中不可以整除\r\n        # 兼容t5_pegasus_small\r\n        if kwargs.get('attention_head_size'):\r\n            self.attention_head_size = kwargs.get('attention_head_size')\r\n        else:\r\n            self.attention_head_size = int(hidden_size \/ num_attention_heads)\r\n\r\n        self.inner_dim = self.num_attention_heads * self.attention_head_size  # 新逻辑\r\n        self.attention_scale = attention_scale\r\n        self.output_attentions = output_attentions\r\n\r\n        self.bias = bias\r\n\r\n        if kwargs.get('attention_key_size'):\r\n            self.attention_key_size = kwargs.get('attention_key_size')\r\n            self.q = nn.Linear(hidden_size, self.attention_key_size * self.num_attention_heads, bias=bias)\r\n            self.k = nn.Linear(hidden_size, self.attention_key_size * self.num_attention_heads, bias=bias)\r\n\r\n        else:\r\n            self.q = nn.Linear(hidden_size, self.inner_dim, bias=bias)\r\n            self.k = nn.Linear(hidden_size, self.inner_dim, bias=bias)\r\n\r\n        self.v = nn.Linear(hidden_size, self.inner_dim, bias=bias)\r\n","AFTER":"        self.num_attention_heads = num_attention_heads\r\n        self.layer_id = layer_id  # 第几层\r\n        self.is_decoder = kwargs.get('is_decoder', False)\r\n        self.attention_scale = attention_scale\r\n        self.output_attentions = output_attentions\r\n        self.bias = bias\r\n\r\n        # t5_pegasus_small中hidden_size\/num_attention_heads != 0\r\n        # 苏神的roberta small中qk的维度和v不同\r\n        self.attention_head_size = kwargs.get('attention_head_size', int(hidden_size\/num_attention_heads))\r\n        self.attention_key_size = kwargs.get('attention_key_size', self.attention_head_size)\r\n        self.qk_inner_dim = self.attention_key_size * self.num_attention_heads\r\n        self.v_inner_dim = self.attention_head_size * self.num_attention_heads\r\n\r\n        self.q = nn.Linear(hidden_size, self.qk_inner_dim, bias=bias)\r\n        self.k = nn.Linear(hidden_size, self.qk_inner_dim, bias=bias)\r\n"}