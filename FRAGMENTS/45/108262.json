{"BEFORE":"            hidden_features = [hidden_features]\n        self.layers = nn.ModuleList()\n\n        if layer_norm:\n            self.layers.append(nn.LayerNorm(in_features))\n\n        self.layers.append(GINConv(in_features=in_features,\n                                   out_features=hidden_features[0],\n                                   batch_norm=batch_norm,\n                                   eps=eps,\n                                   activation=activation,\n                                   dropout=dropout))\n        for i in range(len(hidden_features) - 1):\n            if layer_norm:\n                self.layers.append(nn.LayerNorm(hidden_features[i]))\n            self.layers.append(GINConv(in_features=hidden_features[i],\n                                       out_features=hidden_features[i + 1],\n                                       batch_norm=batch_norm,\n                                       eps=eps,\n                                       activation=activation,\n                                       dropout=dropout))\n        self.linear1 = nn.Linear(hidden_features[-2], hidden_features[-1])\n        self.linear2 = nn.Linear(hidden_features[-1], out_features)\n","AFTER":"                 n_layers,\n                 n_mlp_layers=2,\n                 activation=F.relu,\n                 layer_norm=False,\n                 batch_norm=True,\n                 eps=0.0,\n                 feat_norm=None,\n                 adj_norm_func=None,\n                 dropout=0.0):\n        super(GIN, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.feat_norm = feat_norm\n        self.adj_norm_func = adj_norm_func\n        self.activation = activation\n        if type(hidden_features) is int:\n            hidden_features = [hidden_features] * (n_layers - 1)\n        elif type(hidden_features) is list or type(hidden_features) is tuple:\n            assert len(hidden_features) == (n_layers - 1), \"Incompatible sizes between hidden_features and n_layers.\"\n        n_features = [in_features] + hidden_features + [out_features]\n\n        self.layers = nn.ModuleList()\n        for i in range(n_layers - 1):\n            if layer_norm:\n                self.layers.append(nn.LayerNorm(n_features[i]))\n            self.layers.append(GINConv(in_features=n_features[i],\n                                       out_features=n_features[i + 1],\n                                       batch_norm=batch_norm,\n                                       eps=eps,\n                                       activation=activation,\n                                       dropout=dropout))\n        self.mlp_layers = nn.ModuleList()\n        for i in range(n_mlp_layers):\n            if i == n_mlp_layers - 1:\n                self.mlp_layers.append(nn.Linear(hidden_features[-1], out_features))\n            else:\n                self.mlp_layers.append(nn.Linear(hidden_features[-1], hidden_features[-1]))\n        if dropout > 0.0:\n"}