{"BEFORE":"        self.activation = activation\r\n        self.use_bn = bn\r\n        self.init_method = init_method\r\n        self.logger = getLogger()\r\n\r\n        mlp_modules = []\r\n        for idx, (input_size, output_size) in enumerate(zip(self.layers[:-1], self.layers[1:])):\r\n            mlp_modules.append(nn.Dropout(p=self.dropout))\r\n            mlp_modules.append(nn.Linear(input_size, output_size))\r\n            if self.use_bn:\r\n                mlp_modules.append(nn.BatchNorm1d(num_features=output_size))\r\n            if self.activation.lower() == 'sigmoid':\r\n                mlp_modules.append(nn.Sigmoid())\r\n            elif self.activation.lower() == 'tanh':\r\n                mlp_modules.append(nn.Tanh())\r\n            elif self.activation.lower() == 'relu':\r\n                mlp_modules.append(nn.ReLU())\r\n            elif self.activation.lower() == 'leakyrelu':\r\n                mlp_modules.append(nn.LeakyReLU())\r\n            elif self.activation.lower() == 'dice':\r\n                mlp_modules.append(Dice(output_size))\r\n            elif self.activation.lower() == 'none':\r\n                pass\r\n            else:\r\n                self.logger.warning('Received unrecognized activation function, set default activation function')\r\n        self.mlp_layers = nn.Sequential(*mlp_modules)\r\n","AFTER":"        self.activation = activation\r\n        self.use_bn = bn\r\n        self.init_method = init_method\r\n\r\n        mlp_modules = []\r\n        for idx, (input_size, output_size) in enumerate(zip(self.layers[:-1], self.layers[1:])):\r\n            mlp_modules.append(nn.Dropout(p=self.dropout))\r\n            mlp_modules.append(nn.Linear(input_size, output_size))\r\n            if self.use_bn:\r\n                mlp_modules.append(nn.BatchNorm1d(num_features=output_size))\r\n            activation_fun = activation_layer(self.activation, output_size)\r\n            if activation_fun is not None:\r\n                mlp_modules.append(activation_fun)\r\n\r\n        self.mlp_layers = nn.Sequential(*mlp_modules)\r\n"}