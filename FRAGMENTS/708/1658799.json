{"BEFORE":"                 bert_embedding=True, bert_type=\"bert-base-uncased\", vocab_size=30522,\n                 out_drop=0.3, max_out_len=30, checkpoint_pth=None, device=torch.device(\"cpu\")):\n        super().__init__()\n        # save info\n        self.vocab_size = vocab_size\n        self.max_out_len = max_out_len\n        self.device = device\n\n        # encoder\n        self.encoder = SwinTransformer3D(pretrained=pretrained, pretrained2d=pretrained2d,\n                                         patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n                                         depths=depths, num_heads=num_heads, window_size=window_size,\n                                         mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                                         drop_rate=drop_rate, attn_drop_rate=attn_drop_rate,\n                                         drop_path_rate=drop_path_rate, norm_layer=norm_layer,\n                                         patch_norm=patch_norm, frozen_stages=frozen_stages,\n                                         use_checkpoint=use_checkpoint).to(device)\n        load_checkpoint(self.encoder, checkpoint_pth, map_location=str(device), revise_keys=[(r'^backbone\\.', '')])\n        self.avg_pool = torch.nn.AdaptiveAvgPool2d((1, 1)).to(device)\n\n        # decoder\n        decoder_layer = nn.TransformerDecoderLayer(d_model=encoder_dim, nhead=decoder_head)\n        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=decoder_layers).to(device)\n\n        # BERT\n        # forward(input_ids=None, attention_mask=None)\n        self.bert_embedding = bert_embedding\n        if bert_embedding is True:\n            self.embedding = BertModel.from_pretrained(bert_type).to(device)\n            self.embedding.eval()\n        else:\n            self.embedding = nn.Embedding(vocab_size, 768).to(device)\n        self.tokenizer = AutoTokenizer.from_pretrained(bert_type)\n\n        # out MLP\n        self.out_drop = nn.Dropout(p=out_drop).to(device)\n        self.out_linear = nn.Linear(encoder_dim, vocab_size).to(device)\n","AFTER":"                 tokenizer_type=\"bert-base-uncased\", bert=\"bert-base-uncased\"):\n        super().__init__()\n        # save info\n        if num_heads is None:\n            num_heads = [3, 6, 12, 24]\n        if depths is None:\n            depths = [2, 2, 6, 2]\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_type)\n        self.vocab_size = tokenizer.vocab_size\n        self.pad_id = tokenizer.convert_tokens_to_ids(\"[PAD]\")\n        del tokenizer\n        self.device = device\n        self.bert = bert\n\n        # encoder\n        self.encoder = SwinTransformer3D(pretrained=pretrained, pretrained2d=pretrained2d,\n                                         patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n                                         depths=depths, num_heads=num_heads, window_size=window_size,\n                                         mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                                         drop_rate=drop_rate, attn_drop_rate=attn_drop_rate,\n                                         drop_path_rate=drop_path_rate, norm_layer=norm_layer,\n                                         patch_norm=patch_norm, frozen_stages=frozen_stages,\n                                         use_checkpoint=use_checkpoint).to(device)\n        load_checkpoint(self.encoder, checkpoint_pth, map_location=str(device), revise_keys=[(r'^backbone\\.', '')])\n        self.avg_pool = torch.nn.AdaptiveAvgPool2d((1, 1)).to(device)\n\n        # decoder\n        decoder_layer = nn.TransformerDecoderLayer(d_model=encoder_dim, nhead=decoder_head, batch_first=True)\n        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=decoder_layers).to(device)\n\n        # BERT\n        if bert is not None:\n            self.embedding = BertModel.from_pretrained(bert).to(device)\n            self.embedding.eval()\n        else:\n            self.embedding = nn.Embedding(self.vocab_size, encoder_dim, padding_idx=self.pad_id).to(device)\n\n        # out MLP\n        self.out_drop = nn.Dropout(p=out_drop).to(device)\n        self.out_linear = nn.Linear(encoder_dim, self.vocab_size).to(device)\n"}