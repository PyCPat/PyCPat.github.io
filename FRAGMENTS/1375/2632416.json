{"BEFORE":"    def __init__(self):\n        super().__init__()\n\n        self.conv_1 = nn.Conv1d(1, 16, (3, 1))\n        self.conv_2 = nn.Conv2d(16, 32, (3, 1))\n        self.conv_3 = nn.Conv2d(32, 32, (3, 1))\n        self.conv_4 = nn.Conv2d(32, 64, (3, 1))\n        self.conv_5 = nn.Conv2d(64, 64, (3, 1))\n","AFTER":"    def __init__(self, num_patches, projection_dim, position=True, modalities=6, learnable=False):\n        super().__init__()\n\n        self.conv_1 = nn.Conv2d(1, 8, (3, 1))\n        self.bn_1 = nn.BatchNorm2d(8)\n        self.conv_2 = nn.Conv2d(8, 16, (3, 1))\n        self.bn_2 = nn.BatchNorm2d(16)\n        self.conv_3 = nn.Conv2d(16, 64, (3, 1))\n        self.bn_3 = nn.BatchNorm2d(64)\n        # self.conv_4 = nn.Conv2d(32, 128, (3, 1))\n        # self.bn_4 = nn.BatchNorm2d(128)\n\n        self.max = nn.AdaptiveMaxPool2d(4)\n\n        if position:\n            if learnable:\n                self.position_embedding = LearnablePositionalEncoding(dict_size=num_patches, num_pos_feats=projection_dim)\n            else:\n                self.position_embedding = PositionalEncoding(projection_dim)\n\n\n    def forward(self, inputs):\n"}