{"BEFORE":"            self.layers.append(nn.ModuleList([\n                PreNorm(latent_dim, Attention(latent_dim, input_dim, dropout = attn_dropout), context_dim = input_dim),\n                PreNorm(latent_dim, FeedForward(latent_dim, dropout = ff_dropout)),\n                PreNorm(latent_dim, Attention(latent_dim, dropout = attn_dropout)),\n                PreNorm(latent_dim, FeedForward(latent_dim, dropout = ff_dropout))\n            ]))\n","AFTER":"        get_cross_attn = lambda: PreNorm(latent_dim, Attention(latent_dim, input_dim, dropout = attn_dropout), context_dim = input_dim)\n        get_cross_ff = lambda: PreNorm(latent_dim, FeedForward(latent_dim, dropout = ff_dropout))\n        get_latent_attn = lambda: PreNorm(latent_dim, Attention(latent_dim, dropout = attn_dropout))\n        get_latent_ff = lambda: PreNorm(latent_dim, FeedForward(latent_dim, dropout = ff_dropout))\n\n        if weight_tie_layers:\n            get_cross_attn, get_cross_ff, get_latent_attn, get_latent_ff = map(cache_fn, (get_cross_attn, get_cross_ff, get_latent_attn, get_latent_ff))\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                get_cross_attn(),\n                get_cross_ff(),\n                get_latent_attn(),\n                get_latent_ff()\n            ]))\n"}