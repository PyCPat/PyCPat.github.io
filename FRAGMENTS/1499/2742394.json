{"BEFORE":"        hidden_dim = inp \/\/ expand_ratio\n        if hidden_dim < oup \/6.:\n            hidden_dim = math.ceil(oup \/ 6.)\n            hidden_dim = _make_divisible(hidden_dim, 16)\n\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        layers = []\n        # dw\n        layers.append(ConvBNReLU(inp, inp, kernel_size=3, stride=1, groups=inp, norm_layer=norm_layer))\n        if expand_ratio != 1:\n            # pw-linear\n            layers.extend([\n                nn.Conv2d(inp, hidden_dim, kernel_size=1, stride=1, padding=0, groups=1, bias=False),\n                norm_layer(hidden_dim),\n            ])\n        layers.extend([\n            # pw\n            ConvBNReLU(hidden_dim, oup, kernel_size=1, stride=1, groups=1, norm_layer=norm_layer),\n            # dw-linear\n            nn.Conv2d(oup, oup, kernel_size=3, stride=stride, groups=oup, padding=1, bias=False),\n            norm_layer(oup),\n        ])\n","AFTER":"    def __init__(self, inp, oup, stride, expand_ratio, identity_tensor_multiplier=1.0, norm_layer=None, keep_3x3=False):\n        super(SandGlass, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n        self.use_identity = False if identity_tensor_multiplier==1.0 else True\n        self.identity_tensor_channels = int(round(inp*identity_tensor_multiplier))\n\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n\n        hidden_dim = inp \/\/ expand_ratio\n        if hidden_dim < oup \/6.:\n            hidden_dim = math.ceil(oup \/ 6.)\n            hidden_dim = _make_divisible(hidden_dim, 16)\n\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        layers = []\n        # dw\n        if expand_ratio == 2 or inp==oup or keep_3x3:\n            layers.append(ConvBNReLU(inp, inp, kernel_size=3, stride=1, groups=inp, norm_layer=norm_layer))\n        if expand_ratio != 1:\n            # pw-linear\n            layers.extend([\n                nn.Conv2d(inp, hidden_dim, kernel_size=1, stride=1, padding=0, groups=1, bias=False),\n                norm_layer(hidden_dim),\n            ])\n        layers.extend([\n            # pw\n            ConvBNReLU(hidden_dim, oup, kernel_size=1, stride=1, groups=1, norm_layer=norm_layer),\n        ])\n        if expand_ratio == 2 or inp==oup or keep_3x3 or stride==2:\n            layers.extend([\n            # dw-linear\n            nn.Conv2d(oup, oup, kernel_size=3, stride=stride, groups=oup, padding=1, bias=False),\n            norm_layer(oup),\n        ])\n        self.conv = nn.Sequential(*layers)\n"}