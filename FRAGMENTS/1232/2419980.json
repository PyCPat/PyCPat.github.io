{"BEFORE":"        layers = []\n        for _ in range(depth):\n            layers.extend([\n                Residual(PreNorm(dim, SelfAttention(dim, heads))),\n                Residual(PreNorm(dim, FeedForward(dim)))\n            ])\n        self.layers = nn.Sequential(*layers)\n","AFTER":"    def __init__(self, num_tokens, dim, seq_len, depth, mem_len = None, heads = 8):\n        super().__init__()\n        self.mem_len = default(mem_len, seq_len)\n        self.seq_len = seq_len\n        self.depth = depth\n        self.token_emb = nn.Embedding(num_tokens, dim)\n        self.pos_emb = RelativePositionalEmbedding\n        self.to_logits = nn.Linear(dim, num_tokens)\n\n        self.attn_layers = nn.ModuleList([Residual(PreNorm(dim, SelfAttention(dim, heads))) for _ in range(depth)])\n        self.ff_layers = nn.ModuleList([Residual(PreNorm(dim, FeedForward(dim))) for _ in range(depth)])\n"}