{"BEFORE":"        H = 0.0  # entropy is conventionally denoted by \"H\"\n        for ngram in text_ngrams:\n            if len(ngram) == 1:\n                context, word = None, ngram[0]\n            else:\n                context, word = ngram[:-1], ngram[-1]\n            score = self.score(word, context)\n            H -= score * log_base2(score)\n        return H\n","AFTER":"        return -1 * np.mean([self.logscore(ngram[-1], ngram[:-1]) for ngram in text_ngrams])\n"}