{"BEFORE":"        if act == 'relu':\n            act = nn.ReLU(True)\n        elif act == 'gaussian':\n            act = GaussianActivation(a=kwargs['a'])\n        elif act == 'quadratic':\n            act = QuadraticActivation(a=kwargs['a'])\n        elif act == 'multi-quadratic':\n            act = MultiQuadraticActivation(a=kwargs['a'])\n        elif act == 'laplacian':\n            act = LaplacianActivation(a=kwargs['a'])\n        elif act == 'super-gaussian':\n            act = SuperGaussianActivation(a=kwargs['a'], b=kwargs['b'])\n        elif act == 'expsin':\n            act = ExpSinActivation(a=kwargs['a'])\n\n        layers = [nn.Linear(n_in, n_hidden_units), act]\n        for i in range(n_layers-1):\n            if i != n_layers-2:\n                layers += [nn.Linear(n_hidden_units, n_hidden_units), act]\n            else:\n                layers += [nn.Linear(n_hidden_units, 3), nn.Sigmoid()]\n\n        self.net = nn.Sequential(*layers)\n","AFTER":"                 act='relu', act_trainable=False,\n                 **kwargs):\n        super().__init__()\n\n        layers = []\n        for i in range(n_layers):\n\n            if i == 0:\n                l = nn.Linear(n_in, n_hidden_units)\n                a = l.weight.std().item()\n            elif 0 < i < n_layers-1:\n                l = nn.Linear(n_hidden_units, n_hidden_units)\n                a = l.weight.std().item()\n\n            if act == 'relu':\n                act_ = nn.ReLU(True)\n            elif act == 'gaussian':\n                act_ = GaussianActivation(a=kwargs['a'], trainable=act_trainable)\n            elif act == 'quadratic':\n                act_ = QuadraticActivation(a=kwargs['a'], trainable=act_trainable)\n            elif act == 'multi-quadratic':\n                act_ = MultiQuadraticActivation(a=kwargs['a'], trainable=act_trainable)\n            elif act == 'laplacian':\n                act_ = LaplacianActivation(a=kwargs['a'], trainable=act_trainable)\n            elif act == 'super-gaussian':\n                act_ = SuperGaussianActivation(a=kwargs['a'], b=kwargs['b'],\n                                               trainable=act_trainable)\n            elif act == 'expsin':\n                act_ = ExpSinActivation(a=kwargs['a'], trainable=act_trainable)\n\n            if i < n_layers-1:\n                layers += [l, act_]\n            else:\n                layers += [nn.Linear(n_hidden_units, 3), nn.Sigmoid()]\n\n        self.net = nn.Sequential(*layers)\n"}