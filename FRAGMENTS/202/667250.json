{"BEFORE":"        assert (not use_attention) or (\n            exists(attention_heads)\n            and exists(attention_features)\n            and exists(attention_multiplier)\n        )\n\n        self.use_pre_downsample = use_pre_downsample\n        self.use_attention = use_attention\n\n        channels = out_channels if use_pre_downsample else in_channels\n\n        self.blocks = nn.ModuleList(\n            [\n                ResnetBlock1d(\n                    in_channels=channels,\n                    out_channels=channels,\n                    dilation=dilation,\n                    num_groups=num_groups,\n                    time_context_features=time_context_features,\n                )\n                for dilation in dilations\n            ]\n        )\n\n        self.transformer = (\n            TransformerBlock1d(\n                channels=channels,\n                num_heads=attention_heads,\n                head_features=attention_features,\n                multiplier=attention_multiplier,\n            )\n            if use_attention\n            else nn.Identity()\n        )\n","AFTER":"        if use_attention:\n            assert (\n                exists(attention_heads)\n                and exists(attention_features)\n                and exists(attention_multiplier)\n            )\n            self.transformer = TransformerBlock1d(\n                channels=channels,\n                num_heads=attention_heads,\n                head_features=attention_features,\n                multiplier=attention_multiplier,\n            )\n\n        self.downsample = Downsample1d(\n"}