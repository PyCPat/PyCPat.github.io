{"BEFORE":"        for aligned_sentence in parallel_corpus:\n            trg_sentence = aligned_sentence.words\n            src_sentence = [None] + aligned_sentence.mots\n            l = len(src_sentence) - 1  # exclude NULL token\n            m = len(trg_sentence)\n            initial_value = 1 \/ (l + 1)\n            for i in range(0, l + 1):\n                for j in range(1, m + 1):\n                    alignment_table[i][j][l][m] = initial_value\n\n        for i in range(0, iterations):\n            count_t_given_s = defaultdict(lambda: defaultdict(float))\n            count_any_t_given_s = defaultdict(float)\n\n            # count of i given j, l, m\n            alignment_count = defaultdict(\n                lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(\n                    lambda: 0.0))))\n            alignment_count_for_any_i = defaultdict(\n                lambda: defaultdict(lambda: defaultdict(\n                    lambda: 0.0)))\n\n            total_count = defaultdict(float)\n\n            for aligned_sentence in parallel_corpus:\n                trg_sentence = aligned_sentence.words\n                src_sentence = [None] + aligned_sentence.mots\n                l = len(src_sentence) - 1\n                m = len(trg_sentence)\n\n                # E step (a): Compute normalization factors to weigh counts\n                for j in range(1, m + 1):\n                    t = trg_sentence[j - 1]\n                    total_count[t] = 0\n                    for i in range(0, l + 1):\n                        s = src_sentence[i]\n                        count = (translation_table[t][s] *\n                                 alignment_table[i][j][l][m])\n                        total_count[t] += count\n\n                # E step (b): Collect counts\n                for j in range(1, m + 1):\n                    t = trg_sentence[j - 1]\n                    for i in range(0, l + 1):\n                        s = src_sentence[i]\n                        count = (translation_table[t][s] *\n                                 alignment_table[i][j][l][m])\n                        normalized_count = count \/ total_count[t]\n\n                        count_t_given_s[t][s] += normalized_count\n                        count_any_t_given_s[s] += normalized_count\n                        alignment_count[i][j][l][m] += normalized_count\n                        alignment_count_for_any_i[j][l][m] += normalized_count\n\n            translation_table = defaultdict(lambda: defaultdict(lambda: 0.0))\n            alignment_table = defaultdict(lambda: defaultdict(\n                lambda: defaultdict(lambda: defaultdict(lambda: 0.0))))\n\n            # Perform Laplace smoothing of alignment counts.\n            # Note that smoothing is not in the original IBM Model 2 algorithm.\n            for aligned_sentence in parallel_corpus:\n                trg_sentence = aligned_sentence.words\n                src_sentence = [None] + aligned_sentence.mots\n                l = len(src_sentence) - 1\n                m = len(trg_sentence)\n\n                laplace = 1.0\n                for i in range(0, l + 1):\n                    for j in range(1, m + 1):\n                        value = alignment_count[i][j][l][m]\n                        if 0 < value < laplace:\n                            laplace = value\n\n                laplace *= 0.5\n                for i in range(0, l + 1):\n                    for j in range(1, m + 1):\n                        alignment_count[i][j][l][m] += laplace\n\n                initial_value = laplace * m\n                for j in range(1, m + 1):\n                    alignment_count_for_any_i[j][l][m] += initial_value\n\n            # M step: Update probabilities with maximum likelihood estimates\n            for s in src_vocab:\n                for t in trg_vocab:\n                    translation_table[t][s] = (count_t_given_s[t][s] \/\n                                               count_any_t_given_s[s])\n\n            for aligned_sentence in parallel_corpus:\n                trg_sentence = aligned_sentence.words\n                src_sentence = [None] + aligned_sentence.mots\n                l = len(src_sentence) - 1\n                m = len(trg_sentence)\n","AFTER":"        for aligned_sentence in parallel_corpus:\n            l = len(aligned_sentence.mots)\n            m = len(aligned_sentence.words)\n            initial_value = 1 \/ (l + 1)\n            for i in range(0, l + 1):\n                for j in range(1, m + 1):\n                    alignment_table[i][j][l][m] = initial_value\n\n        for i in range(0, iterations):\n            count_t_given_s = defaultdict(lambda: defaultdict(float))\n            count_any_t_given_s = defaultdict(float)\n\n            # count of i given j, l, m\n            alignment_count = defaultdict(\n                lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(\n                    lambda: 0.0))))\n            alignment_count_for_any_i = defaultdict(\n                lambda: defaultdict(lambda: defaultdict(\n                    lambda: 0.0)))\n\n            total_count = defaultdict(float)\n\n            for aligned_sentence in parallel_corpus:\n                src_sentence = [None] + aligned_sentence.mots\n                trg_sentence = aligned_sentence.words\n                l = len(aligned_sentence.mots)\n                m = len(trg_sentence)\n\n                # E step (a): Compute normalization factors to weigh counts\n                for j in range(1, m + 1):\n                    t = trg_sentence[j - 1]\n                    total_count[t] = 0\n                    for i in range(0, l + 1):\n                        s = src_sentence[i]\n                        count = (translation_table[t][s] *\n                                 alignment_table[i][j][l][m])\n                        total_count[t] += count\n\n                # E step (b): Collect counts\n                for j in range(1, m + 1):\n                    t = trg_sentence[j - 1]\n                    for i in range(0, l + 1):\n                        s = src_sentence[i]\n                        count = (translation_table[t][s] *\n                                 alignment_table[i][j][l][m])\n                        normalized_count = count \/ total_count[t]\n\n                        count_t_given_s[t][s] += normalized_count\n                        count_any_t_given_s[s] += normalized_count\n                        alignment_count[i][j][l][m] += normalized_count\n                        alignment_count_for_any_i[j][l][m] += normalized_count\n\n            translation_table = defaultdict(lambda: defaultdict(lambda: 0.0))\n            alignment_table = defaultdict(lambda: defaultdict(\n                lambda: defaultdict(lambda: defaultdict(lambda: 0.0))))\n\n            # Perform Laplace smoothing of alignment counts.\n            # Note that smoothing is not in the original IBM Model 2 algorithm.\n            for aligned_sentence in parallel_corpus:\n                l = len(aligned_sentence.mots)\n                m = len(aligned_sentence.words)\n\n                laplace = 1.0\n                for i in range(0, l + 1):\n                    for j in range(1, m + 1):\n                        value = alignment_count[i][j][l][m]\n                        if 0 < value < laplace:\n                            laplace = value\n\n                laplace *= 0.5\n                for i in range(0, l + 1):\n                    for j in range(1, m + 1):\n                        alignment_count[i][j][l][m] += laplace\n\n                initial_value = laplace * m\n                for j in range(1, m + 1):\n                    alignment_count_for_any_i[j][l][m] += initial_value\n\n            # M step: Update probabilities with maximum likelihood estimates\n            for s in src_vocab:\n                for t in trg_vocab:\n                    translation_table[t][s] = (count_t_given_s[t][s] \/\n                                               count_any_t_given_s[s])\n\n            for aligned_sentence in parallel_corpus:\n                l = len(aligned_sentence.mots)\n                m = len(aligned_sentence.words)\n"}