{"BEFORE":"    def __init__(self, node_dim, edge_dim, hidden_dim, target_dim, n_layers: int = 4, **kwargs):\n        super(MPNN3D, self).__init__()\n        self.input = nn.Linear(node_dim, hidden_dim)\n        self.edge_input = nn.Linear(edge_dim, hidden_dim)\n        self.mp_layers = nn.ModuleList()\n        for _ in range(n_layers):\n            self.mp_layers.append(MPLayer(\n                message_network=nn.Sequential(nn.Linear(3 * hidden_dim + 1, hidden_dim),\n                                              nn.ReLU(),\n                                              nn.Linear(hidden_dim, hidden_dim),\n                                              nn.ReLU()),\n                update_network=nn.Sequential(nn.Linear(hidden_dim, hidden_dim),\n                                             nn.ReLU(),\n                                             nn.Linear(hidden_dim, hidden_dim),\n                                             nn.ReLU()\n                                             )))\n\n        self.output = nn.Linear(2 * hidden_dim, target_dim)\n","AFTER":"    def __init__(self,\n                 node_dim,\n                 edge_dim,\n                 hidden_dim,\n                 target_dim,\n                 readout_batchnorm: bool = True,\n                 readout_hidden_dim=None,\n                 readout_layers: int = 2,\n                 residual: bool = True,\n                 pairwise_distances: bool = False,\n                 activation: Union[Callable, str] = \"relu\",\n                 last_activation: Union[Callable, str] = \"none\",\n                 mid_batch_norm: bool = False,\n                 last_batch_norm: bool = False,\n                 propagation_depth: int = 5,\n                 dropout: float = 0.0,\n                 posttrans_layers: int = 1,\n                 pretrans_layers: int = 1,\n                 batch_norm_momentum=0.1,\n                 **kwargs):\n        super(MPNN3D, self).__init__()\n        self.node_input_net = MLP(\n            in_dim=node_dim,\n            hidden_size=hidden_dim,\n            out_dim=hidden_dim,\n            mid_batch_norm=mid_batch_norm,\n            last_batch_norm=last_batch_norm,\n            layers=1,\n            mid_activation='relu',\n            dropout=dropout,\n            last_activation=last_activation,\n            batch_norm_momentum=batch_norm_momentum\n\n        )\n\n        self.mp_layers = nn.ModuleList()\n        for _ in range(propagation_depth):\n            self.mp_layers.append(MPLayer(in_dim=hidden_dim,\n                                           out_dim=int(hidden_dim),\n                                           in_dim_edges=edge_dim,\n                                           residual=residual,\n                                           dropout=dropout,\n                                           activation=activation,\n                                           last_activation=last_activation,\n                                           mid_batch_norm=mid_batch_norm,\n                                           last_batch_norm=last_batch_norm,\n                                           posttrans_layers=posttrans_layers,\n                                           pretrans_layers=pretrans_layers,\n                                           batch_norm_momentum=batch_norm_momentum\n                                           ))\n\n        self.output = MLP(in_dim=hidden_dim * 2, hidden_size=readout_hidden_dim,\n                          mid_batch_norm=readout_batchnorm, out_dim=target_dim,\n                          layers=readout_layers, batch_norm_momentum=batch_norm_momentum)\n"}