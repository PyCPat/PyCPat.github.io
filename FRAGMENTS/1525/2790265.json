{"BEFORE":"    def __init__(self, in_channels, out_channels, stride, expand_ratio, outp_size=None):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        self.use_res_connect = self.stride == 1 and in_channels == out_channels\n\n        self.inv_block = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels * expand_ratio, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(in_channels * expand_ratio),\n            nn.PReLU(),\n\n            nn.Conv2d(in_channels * expand_ratio, in_channels * expand_ratio, 3, stride, 1,\n                      groups=in_channels * expand_ratio, bias=False),\n            nn.BatchNorm2d(in_channels * expand_ratio),\n            nn.PReLU(),\n\n            nn.Conv2d(in_channels * expand_ratio, out_channels, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(out_channels),\n            SELayer(out_channels, 8, nn.PReLU, outp_size)\n        )\n","AFTER":"        hidden_dim = int(round(inp * expand_ratio))\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        layers = []\n        if expand_ratio != 1:\n            # pw\n            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n        layers.extend([\n            # dw\n            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim),\n            # pw-linear\n            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(oup),\n        ])\n        self.conv = nn.Sequential(*layers)\n"}