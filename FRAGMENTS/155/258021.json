{"BEFORE":"        src_vocab = set()\n        trg_vocab = set()\n        for aligned_sentence in parallel_corpus:\n            trg_vocab.update(aligned_sentence.words)\n            src_vocab.update(aligned_sentence.mots)\n        # Add the NULL token\n        src_vocab.add(None)\n\n        initial_prob = 1 \/ len(trg_vocab)\n\n        # Create the translation model with initial probability\n        translation_table = defaultdict(\n            lambda: defaultdict(lambda: initial_prob))\n\n        for i in range(0, iterations):\n            count_t_given_s = defaultdict(lambda: defaultdict(lambda: 0.0))\n            count_any_t_given_s = defaultdict(lambda: 0.0)\n\n            for aligned_sentence in parallel_corpus:\n                trg_sentence = aligned_sentence.words\n                src_sentence = [None] + aligned_sentence.mots\n                total_count = defaultdict(lambda: 0.0)\n\n                # E step (a): Compute normalization factors to weigh counts\n                for t in trg_sentence:\n                    if total_count[t] == 0.0:\n                        for s in src_sentence:\n                            total_count[t] += translation_table[t][s]\n\n                # E step (b): Collect counts\n                for t in trg_sentence:\n                    for s in src_sentence:\n                        count = translation_table[t][s]\n                        normalized_count = count \/ total_count[t]\n                        count_t_given_s[t][s] += normalized_count\n                        count_any_t_given_s[s] += normalized_count\n\n            # M step: Update probabilities with maximum likelihood estimate\n            for s in src_vocab:\n                for t in trg_vocab:\n                    translation_table[t][s] = (count_t_given_s[t][s] \/\n                                               count_any_t_given_s[s])\n\n        self.translation_table = translation_table\n","AFTER":"                            total_count[t] += self.translation_table[t][s]\n\n                # E step (b): Collect counts\n                for t in trg_sentence:\n                    for s in src_sentence:\n                        count = self.translation_table[t][s]\n                        normalized_count = count \/ total_count[t]\n                        count_t_given_s[t][s] += normalized_count\n                        count_any_t_given_s[s] += normalized_count\n\n            # M step: Update probabilities with maximum likelihood estimate\n            for s in self.src_vocab:\n                for t in self.trg_vocab:\n                    self.translation_table[t][s] = (count_t_given_s[t][s] \/\n"}