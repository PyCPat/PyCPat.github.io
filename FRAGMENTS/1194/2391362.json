{"BEFORE":"    def __init__(self, nc=80, anchors=(), ch=(), inference=False):  # detection layer\n        super(HEADS, self).__init__()\n        self.nc = nc  # number of classes\n        self.no = nc + 5  # number of outputs per anchor\n        self.nl = len(anchors)  # number of detection layers\n        self.na = len(anchors[0]) * self.nl  # number of anchors\n        self.naxs = len(anchors[0])\n        self.grid = [torch.empty(1)] * self.nl  # init grid\n        self.anchor_grid = [torch.empty(1)] * self.nl  # init anchor grid\n\n        # https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.Module.html command+f register_buffer\n        # has the same result as self.anchors = anchors but, it's a way to register a buffer (make\n        # a variable available in runtime) that should not be considered a model parameter\n        self.stride = [8, 16, 32]\n        anchors_ = torch.tensor(anchors).float().view(self.nl, -1, 2) \/ torch.tensor(self.stride).repeat(6, 1).T.reshape(3, 3, 2)\n        self.register_buffer('anchors', anchors_)  # shape(nl,na,2)\n\n        self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.naxs, 1) for x in ch)  # output conv\n        self.inference = inference\n","AFTER":"        self.out_convs = nn.ModuleList()\n        for in_channels in ch:\n            self.out_convs += [\n                nn.Conv2d(in_channels=in_channels, out_channels=(5+self.nc) * self.naxs, kernel_size=1)\n            ]\n\n    def forward(self, x):\n"}