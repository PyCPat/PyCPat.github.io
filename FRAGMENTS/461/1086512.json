{"BEFORE":"        module_list = [nn.Linear(n_in, n_hidden_units), nn.ReLU(True)]\n        for i in range(n_layers-1):\n            if i != n_layers-2:\n                module_list += [nn.Linear(n_hidden_units, n_hidden_units),\n                                nn.ReLU(True)]\n","AFTER":"                 act='relu', **kwargs):\n        super().__init__()\n\n        if act == 'relu':\n            act = nn.ReLU(True)\n        elif act == 'gaussian':\n            act = GaussianActivation(a=kwargs['a'])\n        elif act == 'quadratic':\n            act = QuadraticActivation(a=kwargs['a'])\n        elif act == 'multi-quadratic':\n            act = MultiQuadraticActivation(a=kwargs['a'])\n        elif act == 'laplacian':\n            act = LaplacianActivation(a=kwargs['a'])\n        elif act == 'super-gaussian':\n            act = SuperGaussianActivation(a=kwargs['a'], b=kwargs['b'])\n        elif act == 'expsin':\n            act = ExpSinActivation(a=kwargs['a'])\n\n        layers = [nn.Linear(n_in, n_hidden_units), act]\n        for i in range(n_layers-1):\n            if i != n_layers-2:\n                layers += [nn.Linear(n_hidden_units, n_hidden_units), act]\n"}