<html><h3>Pattern ID :2147
</h3><img src='5005845.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)

        self.to_out = nn.Sequential(
            <a id="change">nn.Linear(</a>inner_dim, dim<a id="change">)</a>,
            <a id="change">nn.Dropout(</a>dropout<a id="change">)</a>
        ) if project_out else nn.Identity()

    def forward(self, x):
        </code></pre><h3>After Change</h3><pre><code class='java'>
            channel_mlp = token_channels
        self.AddPosEmb2Value = AddPosEmb2Value

        self.norm1 = <a id="change">nn.LayerNorm(</a>token_channels<a id="change">)</a>
        self.Attention = MultiHeadAttention(token_channels, heads, dim_head, dropout)
        self.norm2 = nn.LayerNorm(token_channels)
        self.FFN = FeedForward(token_channels, channel_mlp, dropout=dropout)
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/wamawama/wama_modules/commit/f6865257f4bea4ed62e0170644d99dcdb1d20c94#diff-f5d4e504fb1ef29f758d7f77376318f372efbfb4498293f8d8797c46106d1d7aL53' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 5005845</div><div id='project'> Project Name: wamawama/wama_modules</div><div id='commit'> Commit Name: f6865257f4bea4ed62e0170644d99dcdb1d20c94</div><div id='time'> Time: 2022-10-28</div><div id='author'> Author: wmy19970215@gmail.com</div><div id='file'> File Name: wama_modules/Transformer.py</div><div id='m_class'> M Class Name: SelfAttention</div><div id='n_method'> N Class Name: TransformerEncoderLayer</div><div id='m_method'> M Method Name: __init__(7)</div><div id='n_method'> N Method Name: __init__(5)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: wama_modules/Transformer.py</div><div id='n_file'> N File Name: wama_modules/Transformer.py</div><div id='m_start'> M Start Line: 53</div><div id='m_end'> M End Line: 65</div><div id='n_start'> N Start Line: 114</div><div id='n_end'> N End Line: 133</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.scale = dim_head ** -0.5

        self.attend = nn.Softmax(dim = -1)
        self.dropout = <a id="change">nn.Dropout(</a>dropout<a id="change">)</a>

        self.to_q = <a id="change">nn.Linear(</a>dim, inner_dim<a id="change">, bias = False)</a>
        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)

        self.to_out = nn.Sequential(
            nn.Linear(inner_dim, dim),</code></pre><h3>After Change</h3><pre><code class='java'>

        self.to_latent = nn.Identity()
        self.linear_head = nn.Sequential(
            <a id="change">nn.LayerNorm(</a>dim<a id="change">)</a>,
            nn.Linear(dim, num_classes)
        )
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/vit-pytorch/commit/9a8e509b27da763eed0645183501f7582eb4132d#diff-7fc903f86af6cba7e277fc3f2211799faf6cf56ef0788be5caa0bd4e953cebbcL39' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 5005847</div><div id='project'> Project Name: lucidrains/vit-pytorch</div><div id='commit'> Commit Name: 9a8e509b27da763eed0645183501f7582eb4132d</div><div id='time'> Time: 2023-03-07</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: vit_pytorch/mp3.py</div><div id='m_class'> M Class Name: CrossAttention</div><div id='n_method'> N Class Name: ViT</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(5)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: vit_pytorch/mp3.py</div><div id='n_file'> N File Name: vit_pytorch/mp3.py</div><div id='m_start'> M Start Line: 41</div><div id='m_end'> M End Line: 54</div><div id='n_start'> N Start Line: 109</div><div id='n_end'> N End Line: 133</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            nn.BatchNorm1d(mlp_head_dims[0]),
            nn.GELU(),
            nn.Dropout(dropout),
            <a id="change">nn.Linear(</a>mlp_head_dims[0], mlp_head_dims[1]<a id="change">)</a>,
            nn.BatchNorm1d(mlp_head_dims[1]),
            nn.GELU(),
            <a id="change">nn.Dropout(</a>dropout<a id="change">)</a>
        )

        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(mlp_head_dims[1], num_classes)</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 self.signal_encoder = SignalEncoderConv(hidden_dim, projection_dim, learnable=False)

        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nheads, dropout=dropout, dim_feedforward=1024, activation=&quotgelu&quot)
        encoder_norm = <a id="change">nn.LayerNorm(</a>hidden_dim<a id="change">)</a>
        self.transformer = nn.TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)

        self.mlp_head = nn.Sequential(
            nn.Dropout(dropout),</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/kolaszko/haptic_transformer/commit/480c8a6c89740a357ff79f97d11b8fde6f6e09be#diff-9a02f4e72f98b173cff993e361b8c108ffdd07649fe9a77c6611a0c5af9ecb4aL7' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 5005843</div><div id='project'> Project Name: kolaszko/haptic_transformer</div><div id='commit'> Commit Name: 480c8a6c89740a357ff79f97d11b8fde6f6e09be</div><div id='time'> Time: 2021-05-05</div><div id='author'> Author: mikolaj.lysakowski.bk@gmail.com</div><div id='file'> File Name: models/haptr.py</div><div id='m_class'> M Class Name: HAPTR</div><div id='n_method'> N Class Name: HAPTR</div><div id='m_method'> M Method Name: __init__(8)</div><div id='n_method'> N Method Name: __init__(8)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: models/haptr.py</div><div id='n_file'> N File Name: models/haptr.py</div><div id='m_start'> M Start Line: 8</div><div id='m_end'> M End Line: 28</div><div id='n_start'> N Start Line: 9</div><div id='n_end'> N End Line: 29</div><BR>