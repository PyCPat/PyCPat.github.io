<html><h3>Pattern ID :2316
</h3><img src='5745671.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>


        &#47&#47 One Propagation layer for each meta path
        self.propagation_layers<a id="change"> = </a><a id="change">nn.ModuleList()</a>
        for i in range(len(meta_paths)):
            self.propagation_layers.append(APPNPConv(k_layer, alpha, edge_drop))
        self.semantic_fusion = SemanticFusion()
        self.meta_paths = list(tuple(meta_path) for meta_path in meta_paths)</code></pre><h3>After Change</h3><pre><code class='java'>
        semantic_attention = SemanticAttention(in_size=in_size)
        self.model = MetapathConv(
            meta_paths,
            <a id="change">[APPNPConv(k_layer, alpha, edge_drop)
             for _ in meta_paths]</a>,
            semantic_attention
        )
        self._cached_graph = None</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/bupt-gamma/openhgnn/commit/bd15d668053389800a158b521ef97860b18e3719#diff-dcb3ada8373d1a7302711fab0b7c9bdcd8be5e440bde471c0babc84098fd2d37L106' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 5745671</div><div id='project'> Project Name: bupt-gamma/openhgnn</div><div id='commit'> Commit Name: bd15d668053389800a158b521ef97860b18e3719</div><div id='time'> Time: 2021-09-07</div><div id='author'> Author: 34649403+Theheavens@users.noreply.github.com</div><div id='file'> File Name: openhgnn/models/HPN.py</div><div id='m_class'> M Class Name: HPNLayer</div><div id='n_method'> N Class Name: HPNLayer</div><div id='m_method'> M Method Name: __init__(8)</div><div id='n_method'> N Method Name: __init__(8)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: openhgnn/models/HPN.py</div><div id='n_file'> N File Name: openhgnn/models/HPN.py</div><div id='m_start'> M Start Line: 129</div><div id='m_end'> M End Line: 133</div><div id='n_start'> N Start Line: 106</div><div id='n_end'> N End Line: 117</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        super().__init__()
        &#47&#47 self._squeeze = nn.ModuleList([Transit(numCodewords, cin) for numCodewords in k])
        &#47&#47 self._prob = nn.ModuleList([Transit(cin, numCodewords) for numCodewords in k])
        self._squeeze<a id="change"> = </a><a id="change">nn.ModuleList(</a>[Transit(numCodewords, cin, order="last") for numCodewords in k]<a id="change">)</a>
        self._prob = nn.ModuleList([Transit(cin, numCodewords, order="first") for numCodewords in k])
        &#47&#47 self._encoder = nn.Transformer(cin, )
        self._encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(cin, 8, cin, rate, "gelu"), 12, None)
        self._decoder = nn.TransformerDecoder(nn.TransformerDecoderLayer(cin, 8, cin, rate, "gelu"), 12, None)</code></pre><h3>After Change</h3><pre><code class='java'>
        self._codebookAsValue = nn.ModuleList([nn.Linear(cin, cin) for numCodewords in k])
        self._xAsQuery = nn.ModuleList([nn.Linear(cin, cin) for numCodewords in k])
        self._k = k
        self._scaling = <a id="change">[sqrt(kk) for kk in k]</a>
        self._d = float(cin) ** 0.5
        self._c = cin
        self._position = PositionalEncoding2D(cin, 120, 120)
        self._dePosition = DePositionalEncoding2D(cin, 120, 120)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/xiaosu-zhu/mcquic/commit/594ba7cd5c9d147e95ba5bd0b842270302dea2a2#diff-6b290d83a36e7951ea6e9dbd33da6b8e0b6f40c45926540ce74b74ea4ab48a24L209' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 5745670</div><div id='project'> Project Name: xiaosu-zhu/mcquic</div><div id='commit'> Commit Name: 594ba7cd5c9d147e95ba5bd0b842270302dea2a2</div><div id='time'> Time: 2021-03-02</div><div id='author'> Author: xiaosu.zhu@outlook.com</div><div id='file'> File Name: src/mcqc/models/quantizer.py</div><div id='m_class'> M Class Name: TransformerQuantizer</div><div id='n_method'> N Class Name: TransformerQuantizer</div><div id='m_method'> M Method Name: __init__(4)</div><div id='n_method'> N Method Name: __init__(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: src/mcqc/models/quantizer.py</div><div id='n_file'> N File Name: src/mcqc/models/quantizer.py</div><div id='m_start'> M Start Line: 213</div><div id='m_end'> M End Line: 218</div><div id='n_start'> N Start Line: 214</div><div id='n_end'> N End Line: 220</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        self.latents_attend_to_patches = Attention(dim, norm = True, norm_context = True, **attn_kwargs)

        self.latent_self_attns<a id="change"> = </a><a id="change">nn.ModuleList(</a>[]<a id="change">)</a>
        for _ in range(latent_self_attn_depth):
            self.latent_self_attns.append(nn.ModuleList([
                Attention(dim, norm = True, **attn_kwargs),
                FeedForward(dim)</code></pre><h3>After Change</h3><pre><code class='java'>

        attn_kwargs = {**attn_kwargs, &quottime_cond_dim&quot: time_dim}

        self.blocks = nn.ModuleList(<a id="change">[RINBlock(dim, latent_self_attn_depth = latent_self_attn_depth, **attn_kwargs) for _ in range(depth)]</a>)

    def forward(
        self,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/recurrent-interface-network-pytorch/commit/a81dbfaf9e61843bc0da154cf547099de9ffc093#diff-c245517a15c6c34e71df6f08ed422324484176189f4795463b7e58278d35b9cdL271' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 5745668</div><div id='project'> Project Name: lucidrains/recurrent-interface-network-pytorch</div><div id='commit'> Commit Name: a81dbfaf9e61843bc0da154cf547099de9ffc093</div><div id='time'> Time: 2022-12-26</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: rin_pytorch/rin_pytorch.py</div><div id='m_class'> M Class Name: RIN</div><div id='n_method'> N Class Name: RIN</div><div id='m_method'> M Method Name: __init__(9)</div><div id='n_method'> N Method Name: __init__(9)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: rin_pytorch/rin_pytorch.py</div><div id='n_file'> N File Name: rin_pytorch/rin_pytorch.py</div><div id='m_start'> M Start Line: 332</div><div id='m_end'> M End Line: 350</div><div id='n_start'> N Start Line: 385</div><div id='n_end'> N End Line: 385</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        self.tok_emb = nn.Embedding(codebook_size, dim)
        self.pos_emb = nn.Parameter(torch.zeros(1, 512, dim))
        self.EncoderLayers<a id="change"> = </a><a id="change">nn.ModuleList(</a>[Encoder(dim) for _ in range(N)]<a id="change">)</a>
        self.Token_Prediction = nn.Linear(in_features=dim, out_features=codebook_size)
        self.apply(weights_init)
</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 self.pos_emb = PositionalEmbedding(args.dim, self.num_image_tokens + 1)
        self.pos_emb = nn.init.trunc_normal_(nn.Parameter(torch.zeros(self.num_image_tokens + 1, args.dim)), 0., 0.02)
        &#47&#47 self.register_buffer("pos_emb", nn.init.trunc_normal_(nn.Parameter(torch.zeros(1024, args.dim)), 0., 0.02))
        self.blocks = nn.Sequential(*<a id="change">[Encoder(args.dim, args.hidden_dim) for _ in range(args.n_layers)]</a>)
        self.Token_Prediction = nn.Sequential(*[
            nn.Linear(in_features=args.dim, out_features=args.dim),
            nn.GELU(),</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/dome272/maskgit-pytorch/commit/5042389656c05da20be7632a6169a7567a65891a#diff-41c56779d9ab1a289bece438090e25fb814c36631ac5af1aff4ee936c166c37eL78' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 5745667</div><div id='project'> Project Name: dome272/maskgit-pytorch</div><div id='commit'> Commit Name: 5042389656c05da20be7632a6169a7567a65891a</div><div id='time'> Time: 2022-04-22</div><div id='author'> Author: 61938694+dome272@users.noreply.github.com</div><div id='file'> File Name: bidirectional_transformer.py</div><div id='m_class'> M Class Name: BidirectionalTransformer</div><div id='n_method'> N Class Name: BidirectionalTransformer</div><div id='m_method'> M Method Name: __init__(2)</div><div id='n_method'> N Method Name: __init__(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: bidirectional_transformer.py</div><div id='n_file'> N File Name: bidirectional_transformer.py</div><div id='m_start'> M Start Line: 78</div><div id='m_end'> M End Line: 84</div><div id='n_start'> N Start Line: 107</div><div id='n_end'> N End Line: 122</div><BR>