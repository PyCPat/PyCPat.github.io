<html><h3>Pattern ID :1489
</h3><img src='4072255.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        skip_dims = []

        down_stage_parameters<a id="change"> = </a><a id="change">[
            </a>in_out,
            nested_unet_depths,
            num_blocks_per_stage<a id="change"></a>
        ]

        up_stage_parameters = [reversed(params[:-1]) for params in down_stage_parameters]
</code></pre><h3>After Change</h3><pre><code class='java'>
            self.downs.append(nn.ModuleList([
                blocks(dim_in, dim_in, nested_unet_depth = nested_unet_depth, nested_unet_dim = nested_unet_dim),
                nn.ModuleList([blocks(dim_in, dim_in, nested_unet_depth = nested_unet_depth, nested_unet_dim = nested_unet_dim) for _ in range(num_blocks - 1)]),
                nn.ModuleList(<a id="change">[TransformerBlock(dim_in, depth = self_attn_blocks, **attn_kwargs) for _ in range(self_attn_blocks)]</a>),
                Downsample(dim_in, dim_out)
            ]))
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/x-unet/commit/f95f833eade55222c51aa137f446b0fdc9669ed2#diff-6e9e7a6d8d7a8c116b8bdae64f3e845f516adfec59d0379fc01849d294784966L283' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4072255</div><div id='project'> Project Name: lucidrains/x-unet</div><div id='commit'> Commit Name: f95f833eade55222c51aa137f446b0fdc9669ed2</div><div id='time'> Time: 2022-08-19</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: x_unet/x_unet.py</div><div id='m_class'> M Class Name: XUnet</div><div id='n_method'> N Class Name: XUnet</div><div id='m_method'> M Method Name: __init__(18)</div><div id='n_method'> N Method Name: __init__(15)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: x_unet/x_unet.py</div><div id='n_file'> N File Name: x_unet/x_unet.py</div><div id='m_start'> M Start Line: 283</div><div id='m_end'> M End Line: 311</div><div id='n_start'> N Start Line: 300</div><div id='n_end'> N End Line: 400</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.pos_emb = RelativePositionalEmbedding
        self.to_logits = nn.Linear(dim, num_tokens)

        layers<a id="change"> = </a><a id="change">[]</a>
        for _ in range(depth):
            layers.extend([
                Residual(PreNorm(dim, SelfAttention(dim, heads))),
                Residual(PreNorm(dim, FeedForward(dim)))</code></pre><h3>After Change</h3><pre><code class='java'>
        self.to_logits = nn.Linear(dim, num_tokens)

        self.attn_layers = nn.ModuleList([Residual(PreNorm(dim, SelfAttention(dim, heads))) for _ in range(depth)])
        self.ff_layers = nn.ModuleList(<a id="change">[Residual(PreNorm(dim, FeedForward(dim))) for _ in range(depth)]</a>)

    def forward(self, x, mem = None):
        x = self.token_emb(x)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/compressive-transformer-pytorch/commit/47a5b8448090fce7ca1f7356001fb2ac381c2239#diff-493cd95e4b724ba15b0c327254df365435ee9204e2a14c51200a07a55d030b8dL87' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4072264</div><div id='project'> Project Name: lucidrains/compressive-transformer-pytorch</div><div id='commit'> Commit Name: 47a5b8448090fce7ca1f7356001fb2ac381c2239</div><div id='time'> Time: 2020-06-30</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: compressive_transformer_pytorch/compressive_transformer_pytorch.py</div><div id='m_class'> M Class Name: CompressiveTransformer</div><div id='n_method'> N Class Name: CompressiveTransformer</div><div id='m_method'> M Method Name: __init__(7)</div><div id='n_method'> N Method Name: __init__(6)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: compressive_transformer_pytorch/compressive_transformer_pytorch.py</div><div id='n_file'> N File Name: compressive_transformer_pytorch/compressive_transformer_pytorch.py</div><div id='m_start'> M Start Line: 93</div><div id='m_end'> M End Line: 99</div><div id='n_start'> N Start Line: 100</div><div id='n_end'> N End Line: 110</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.num_dilations = len(dilations)

        net = []
        stacked_in_channels<a id="change"> = </a><a id="change">[]</a>

        for idx in range(self.num_dilations):
            if len(stacked_in_channels) == 0:
                stacked_in_channels.append(in_channels)</code></pre><h3>After Change</h3><pre><code class='java'>

        if type(out_channels) is int:
            assert depth is not None, "Specify `depth`"
            out_channels = <a id="change">[
                out_channels for _ in range(depth)
            ]</a>
        elif type(out_channels) is list:
            depth = len(out_channels)
        else:
            raise ValueError("Not support out_channels={}".format(out_channels))</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tky823/dnn-based_source_separation/commit/2532067abd8ebb53668444d81029dd25ea61795b#diff-ead896e273fdbcf5c8f3a2cfbd8c95af50b268919d59e26568c480f7c48682c7L57' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4072261</div><div id='project'> Project Name: tky823/dnn-based_source_separation</div><div id='commit'> Commit Name: 2532067abd8ebb53668444d81029dd25ea61795b</div><div id='time'> Time: 2021-03-11</div><div id='author'> Author: 40362510+tky823@users.noreply.github.com</div><div id='file'> File Name: src/models/d3net.py</div><div id='m_class'> M Class Name: D2Block</div><div id='n_method'> N Class Name: D2Block</div><div id='m_method'> M Method Name: __init__(6)</div><div id='n_method'> N Method Name: __init__(6)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: src/models/d3net.py</div><div id='n_file'> N File Name: src/models/d3net.py</div><div id='m_start'> M Start Line: 60</div><div id='m_end'> M End Line: 72</div><div id='n_start'> N Start Line: 80</div><div id='n_end'> N End Line: 99</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        self.latents_attend_to_patches = Attention(dim, norm = True, norm_context = True, **attn_kwargs)

        self.latent_self_attns<a id="change"> = </a>nn.ModuleList(<a id="change">[]</a>)
        for _ in range(latent_self_attn_depth):
            self.latent_self_attns.append(nn.ModuleList([
                Attention(dim, norm = True, **attn_kwargs),</code></pre><h3>After Change</h3><pre><code class='java'>

        attn_kwargs = {**attn_kwargs, &quottime_cond_dim&quot: time_dim}

        self.blocks = nn.ModuleList(<a id="change">[RINBlock(dim, latent_self_attn_depth = latent_self_attn_depth, **attn_kwargs) for _ in range(depth)]</a>)

    def forward(
        self,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/recurrent-interface-network-pytorch/commit/a81dbfaf9e61843bc0da154cf547099de9ffc093#diff-c245517a15c6c34e71df6f08ed422324484176189f4795463b7e58278d35b9cdL271' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4072257</div><div id='project'> Project Name: lucidrains/recurrent-interface-network-pytorch</div><div id='commit'> Commit Name: a81dbfaf9e61843bc0da154cf547099de9ffc093</div><div id='time'> Time: 2022-12-26</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: rin_pytorch/rin_pytorch.py</div><div id='m_class'> M Class Name: RIN</div><div id='n_method'> N Class Name: RIN</div><div id='m_method'> M Method Name: __init__(9)</div><div id='n_method'> N Method Name: __init__(9)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: rin_pytorch/rin_pytorch.py</div><div id='n_file'> N File Name: rin_pytorch/rin_pytorch.py</div><div id='m_start'> M Start Line: 332</div><div id='m_end'> M End Line: 350</div><div id='n_start'> N Start Line: 385</div><div id='n_end'> N End Line: 385</div><BR>