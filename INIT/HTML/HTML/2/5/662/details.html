<html><h3>Pattern ID :662
</h3><img src='2365540.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        self.to_q = nn.Parameter(torch.randn(dim, dim))
        self.to_kv = nn.Parameter(torch.randn(dim, 2 * dim))
        self.to_out = <a id="change">nn.Parameter(</a>torch.randn(dim, dim)<a id="change">)</a>

        self.rezero_g<a id="change"> = nn</a><a id="change">.Parameter(</a>torch.tensor(0.)<a id="change">)</a>

    def forward(self, lmem, smem, hiddens):
        hiddens = hiddens.detach()
        batch, dim_head, mem_depth = lmem.shape[1], self.dim_head, self.num_memory_depth</code></pre><h3>After Change</h3><pre><code class='java'>

        self.norm = nn.LayerNorm(dim, elementwise_affine = False)

        self.to_q = init_parameter((num_memory_depth<a id="change">, dim, dim</a>), dim)
        self.to_kv = init_parameter((num_memory_depth, dim, 2 * dim), dim)
        self.to_out = init_parameter((num_memory_depth, dim, dim * 2), dim)
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/memory-transformer-xl/commit/ac8692c07e7327c16cef521fbdbdeb7a8ed7c69a#diff-a0bf60cdef5ad628f9a83f0454c9c66a535268529835e839bb54088805dba051L220' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2365540</div><div id='project'> Project Name: lucidrains/memory-transformer-xl</div><div id='commit'> Commit Name: ac8692c07e7327c16cef521fbdbdeb7a8ed7c69a</div><div id='time'> Time: 2020-07-22</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: memory_transformer_xl/memory_transformer_xl.py</div><div id='m_class'> M Class Name: MemoryAttentionNetwork</div><div id='n_method'> N Class Name: MemoryAttentionNetwork</div><div id='m_method'> M Method Name: __init__(6)</div><div id='n_method'> N Method Name: __init__(6)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: memory_transformer_xl/memory_transformer_xl.py</div><div id='n_file'> N File Name: memory_transformer_xl/memory_transformer_xl.py</div><div id='m_start'> M Start Line: 220</div><div id='m_end'> M End Line: 228</div><div id='n_start'> N Start Line: 227</div><div id='n_end'> N End Line: 233</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.norm = nn.LayerNorm(dim, elementwise_affine = False)

        self.to_q = nn.Parameter(torch.randn(dim, dim))
        self.to_kv = <a id="change">nn.Parameter(</a>torch.randn(dim, 2 * dim)<a id="change">)</a>
        self.to_out = nn.Parameter(torch.randn(dim, dim))

        self.rezero_g<a id="change"> = </a><a id="change">nn.Parameter(</a>torch.tensor(0.)<a id="change">)</a>

    def forward(self, lmem, smem, hiddens):
        hiddens = hiddens.detach()
        batch, dim_head, mem_depth = lmem.shape[1], self.dim_head, self.num_memory_depth</code></pre><h3>After Change</h3><pre><code class='java'>

        self.norm = nn.LayerNorm(dim, elementwise_affine = False)

        self.to_q = init_parameter((num_memory_depth<a id="change">, dim, dim</a>), dim)
        self.to_kv = init_parameter((num_memory_depth, dim, 2 * dim), dim)
        self.to_out = init_parameter((num_memory_depth, dim, dim * 2), dim)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/memory-transformer-xl/commit/ac8692c07e7327c16cef521fbdbdeb7a8ed7c69a#diff-a0bf60cdef5ad628f9a83f0454c9c66a535268529835e839bb54088805dba051L211' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2365541</div><div id='project'> Project Name: lucidrains/memory-transformer-xl</div><div id='commit'> Commit Name: ac8692c07e7327c16cef521fbdbdeb7a8ed7c69a</div><div id='time'> Time: 2020-07-22</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: memory_transformer_xl/memory_transformer_xl.py</div><div id='m_class'> M Class Name: MemoryAttentionNetwork</div><div id='n_method'> N Class Name: MemoryAttentionNetwork</div><div id='m_method'> M Method Name: __init__(6)</div><div id='n_method'> N Method Name: __init__(6)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: memory_transformer_xl/memory_transformer_xl.py</div><div id='n_file'> N File Name: memory_transformer_xl/memory_transformer_xl.py</div><div id='m_start'> M Start Line: 220</div><div id='m_end'> M End Line: 228</div><div id='n_start'> N Start Line: 227</div><div id='n_end'> N End Line: 233</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self._k = k

        if weight:
            self.weight = <a id="change">nn.Parameter(</a>th.Tensor(in_feats, out_feats)<a id="change">)</a>
        else:
            self.register_parameter(&quotweight&quot, None)

        if bias:
            self.bias<a id="change"> = </a><a id="change">nn.Parameter(</a>th.Tensor(out_feats)<a id="change">)</a>
        else:
            self.register_parameter(&quotbias&quot, None)
</code></pre><h3>After Change</h3><pre><code class='java'>
                 bias=True):
        
        super().__init__()
        if norm not in (&quotnone&quot<a id="change">, &quotboth&quot, &quotright&quot, &quotleft&quot</a>):
            raise DGLError(&quotInvalid norm value. Must be either "none", "both", "right" or "left".&quot
                           &quot But got "{}".&quot.format(norm))     
        self._in_feats = in_feats</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/edisonleeeee/greatx/commit/c43665fd30401c63acbd50175da1880509a52d21#diff-4e82f29fd5c78eabd4fe1d5867020e786dc0deb6b867e435c11f46885eaadad4L63' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2365543</div><div id='project'> Project Name: edisonleeeee/greatx</div><div id='commit'> Commit Name: c43665fd30401c63acbd50175da1880509a52d21</div><div id='time'> Time: 2021-12-06</div><div id='author'> Author: cnljt@outlook.com</div><div id='file'> File Name: graphwar/nn/sgconv.py</div><div id='m_class'> M Class Name: SGConv</div><div id='n_method'> N Class Name: SGConv</div><div id='m_method'> M Method Name: __init__(9)</div><div id='n_method'> N Method Name: __init__(7)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: graphwar/nn/sgconv.py</div><div id='n_file'> N File Name: graphwar/nn/sgconv.py</div><div id='m_start'> M Start Line: 70</div><div id='m_end'> M End Line: 85</div><div id='n_start'> N Start Line: 90</div><div id='n_end'> N End Line: 108</div><BR>