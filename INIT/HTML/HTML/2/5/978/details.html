<html><h3>Pattern ID :978
</h3><img src='3183960.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        self.to_embed = nn.Embedding(num_tokens, dim) if exists(num_tokens) else nn.Identity()

        self.layers = nn.ModuleList(<a id="change">[Residual(PreNorm(dim, gMLPBlock(dim = dim, dim_ff = dim_ff, seq_len = seq_len, heads = heads, window = window))) for i in range(depth)]</a>)

        self.to_logits = nn.Sequential(
            nn.LayerNorm(dim),</code></pre><h3>After Change</h3><pre><code class='java'>
        self.to_embed = nn.Embedding(num_tokens, dim)

        window = cast_tuple(window, depth)
        layers = <a id="change">nn.ModuleList(</a>[]<a id="change">)</a>

        for ind, w in zip(range(depth), window):
            layer_blocks = nn.ModuleList([
                PreNorm(dim, gMLPBlock(dim = dim, dim_ff = dim_ff, seq_len = seq_len, heads = heads, window = w))
            ])

            if reversible:
                layer_blocks.append(PreNorm(dim, gMLPBlock(dim = dim, dim_ff = dim_ff, seq_len = seq_len, heads = heads, window = w)))

            layers.append(layer_blocks)

        execute_klass = SequentialSequence if not reversible else ReversibleSequence
        self.net<a id="change"> = </a>execute_klass(layers)

        self.to_logits = nn.Sequential(
            nn.LayerNorm(dim),</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/g-mlp-gpt/commit/7642e36ff19c6b299a77e5c1ace038e9e6726202#diff-9e1c762f97dc7e52cf231d6bec8dbb8412bc9383faaf89eadd9b1c02e46fea80L188' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 3183960</div><div id='project'> Project Name: lucidrains/g-mlp-gpt</div><div id='commit'> Commit Name: 7642e36ff19c6b299a77e5c1ace038e9e6726202</div><div id='time'> Time: 2021-05-20</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: g_mlp_gpt/g_mlp_gpt.py</div><div id='class'> Class Name: gMLPGPT</div><div id='method'> Method Name: __init__</div><div id='parent_class'> Parent Class: nn.Module</div><BR><BR><div id='link'><a href='https://github.com/ryantd/veloce/commit/74675f3569dcd22861151eae9245540bd891c826#diff-1fae5f29f685e022e7bee571c33f34a6d639ca10ff29fed7d4d103e75251f34cL20' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 3183961</div><div id='project'> Project Name: ryantd/veloce</div><div id='commit'> Commit Name: 74675f3569dcd22861151eae9245540bd891c826</div><div id='time'> Time: 2022-01-04</div><div id='author'> Author: xiaoyu.zhai@hotmail.com</div><div id='file'> File Name: phetware/layer/core.py</div><div id='class'> Class Name: DNN</div><div id='method'> Method Name: __init__</div><div id='parent_class'> Parent Class: nn.Module</div><BR><BR><div id='link'><a href='https://github.com/lucidrains/linear-attention-transformer/commit/a0a35e7e9727b6428c7527ec34f5192529ab5e82#diff-eee6cf669463e9683bcf9643c4b19a493056f98ded32854287144d47f02a7032L191' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 3183963</div><div id='project'> Project Name: lucidrains/linear-attention-transformer</div><div id='commit'> Commit Name: a0a35e7e9727b6428c7527ec34f5192529ab5e82</div><div id='time'> Time: 2020-06-04</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: linear_attention_transformer/linear_attention_transformer.py</div><div id='class'> Class Name: LinearAttentionTransformer</div><div id='method'> Method Name: __init__</div><div id='parent_class'> Parent Class: nn.Module</div><BR>