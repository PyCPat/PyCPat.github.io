<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
class LinearAttentionTransformer(nn.Module):
    def __init__(self, dim, depth, max_seq_len, heads = 8, bucket_size = 64, causal = False, one_kv_head = False):
        super().__init__()
        self.attn_layers = nn.ModuleList(<a id="change">[PreNorm(dim, SelfAttention(dim, heads, causal, one_kv_head = one_kv_head)) for _ in range(depth)]</a>)
        self.ff_layers = nn.ModuleList([PreNorm(dim, FeedForward(dim)) for _ in range(depth)])

    def forward(self, x, **kwargs):</code></pre><h3>After Change</h3><pre><code class='java'>
class LinearAttentionTransformer(nn.Module):
    def __init__(self, dim, depth, max_seq_len, heads = 8, bucket_size = 64, causal = False, one_kv_head = False):
        super().__init__()
        layers<a id="change"> = </a><a id="change">nn.ModuleList(</a>[]<a id="change">)</a>

        for _ in range(depth):
            layer = nn.ModuleList([
                PreNorm(dim, SelfAttention(dim, heads, causal, one_kv_head = one_kv_head)),</code></pre>