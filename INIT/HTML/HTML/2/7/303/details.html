<html><h3>Pattern ID :303
</h3><img src='1171137.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        self.q_out = nn.Linear(dim_head, dim_head, bias = False)
        self.attn_out = nn.Linear(dim_head, dim_head, bias = False)
        self.out_bias = <a id="change">nn.Parameter(</a>torch.zeros(1, 1, dim_head)<a id="change">)</a>

        self.q_gate = nn.Linear(dim_head, dim_head, bias = False)
        self.attn_gate = nn.Linear(dim_head, dim_head, bias = False)
        self.gate_bias<a id="change"> = nn</a><a id="change">.Parameter(</a>torch.zeros(1, 1, dim_head)<a id="change">)</a>

    def forward(self, x, context = None):
        h = self.heads
</code></pre><h3>After Change</h3><pre><code class='java'>

        self.dropout = nn.Dropout(dropout)

        self.aoa = <a id="change">nn.Sequential(
            </a>nn.Linear(<a id="change">2</a><a id="change"> * </a>inner_dim, 2 * dim),
            nn.GLU(),
            nn.Dropout(aoa_dropout)<a id="change">
        )</a>

    def forward(self, x, context = None):
        h = self.heads
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/aoa-pytorch/commit/97d99d0fce4683fdba7b8fc05ff64aa69cdcf37a#diff-c18fe4708e2a88fadace8b9d34204ab559fa61cd6bffa9b7839d96720fc85f5bL24' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 1171137</div><div id='project'> Project Name: lucidrains/aoa-pytorch</div><div id='commit'> Commit Name: 97d99d0fce4683fdba7b8fc05ff64aa69cdcf37a</div><div id='time'> Time: 2020-11-07</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: aoa_pytorch/aoa_pytorch.py</div><div id='m_class'> M Class Name: AttentionOnAttention</div><div id='n_method'> N Class Name: AttentionOnAttention</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: aoa_pytorch/aoa_pytorch.py</div><div id='n_file'> N File Name: aoa_pytorch/aoa_pytorch.py</div><div id='m_start'> M Start Line: 29</div><div id='m_end'> M End Line: 35</div><div id='n_start'> N Start Line: 24</div><div id='n_end'> N End Line: 37</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        self.q_out = nn.Linear(dim_head, dim_head, bias = False)
        self.attn_out = nn.Linear(dim_head, dim_head, bias = False)
        self.out_bias<a id="change"> = </a><a id="change">nn.Parameter(</a>torch.zeros(1, 1, dim_head)<a id="change">)</a>

        self.q_gate = nn.Linear(dim_head, dim_head, bias = False)
        self.attn_gate = nn.Linear(dim_head, dim_head, bias = False)
        self.gate_bias = <a id="change">nn.Parameter(</a>torch.zeros(1, 1, dim_head)<a id="change">)</a>

    def forward(self, x, context = None):
        h = self.heads
</code></pre><h3>After Change</h3><pre><code class='java'>

        self.dropout = nn.Dropout(dropout)

        self.aoa = <a id="change">nn.Sequential(
            </a>nn.Linear(<a id="change">2</a><a id="change"> * </a>inner_dim, 2 * dim),
            nn.GLU(),
            nn.Dropout(aoa_dropout)<a id="change">
        )</a>

    def forward(self, x, context = None):
        h = self.heads
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/aoa-pytorch/commit/97d99d0fce4683fdba7b8fc05ff64aa69cdcf37a#diff-c18fe4708e2a88fadace8b9d34204ab559fa61cd6bffa9b7839d96720fc85f5bL14' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 1171136</div><div id='project'> Project Name: lucidrains/aoa-pytorch</div><div id='commit'> Commit Name: 97d99d0fce4683fdba7b8fc05ff64aa69cdcf37a</div><div id='time'> Time: 2020-11-07</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: aoa_pytorch/aoa_pytorch.py</div><div id='m_class'> M Class Name: AttentionOnAttention</div><div id='n_method'> N Class Name: AttentionOnAttention</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: aoa_pytorch/aoa_pytorch.py</div><div id='n_file'> N File Name: aoa_pytorch/aoa_pytorch.py</div><div id='m_start'> M Start Line: 29</div><div id='m_end'> M End Line: 35</div><div id='n_start'> N Start Line: 24</div><div id='n_end'> N End Line: 37</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
class GroupedFeedForward(nn.Module):
    def __init__(self, *, dim, groups, mult = 4):
        super().__init__()
        self.project_in  = <a id="change">nn.Parameter(</a>torch.randn(groups, dim, dim * 4)<a id="change">)</a>
        self.nonlin      = nn.GELU()
        self.project_out<a id="change"> = </a><a id="change">nn.Parameter(</a>torch.randn(groups, dim * 4, dim)<a id="change">)</a>

    def forward(self, levels):
        x = einsum(&quotb n l d, l d e -&gt; b n l e&quot, levels, self.project_in)
        x = self.nonlin(x)</code></pre><h3>After Change</h3><pre><code class='java'>
    def __init__(self, *, dim, groups, mult = 4):
        super().__init__()
        total_dim = dim * groups &#47&#47 levels * dim
        self.net = <a id="change">nn.Sequential(
            </a>nn.Conv1d(total_dim, total_dim<a id="change"> * 4</a>, 1, groups = groups),
            nn.GELU(),
            nn.Conv1d(total_dim * 4, total_dim, 1, groups = groups)<a id="change">
        )</a>

    def forward(self, levels):
        b, n, l, d = levels.shape
        levels = rearrange(levels, &quotb n l d -&gt; b (l d) n&quot)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/glom-pytorch/commit/570bc6667245f45ef03ad01b42cb335bda11d728#diff-2bce7536af7c0a2fe47054a9c6dd00320e623412cf5c9d7e24a0b0b8c950a574L24' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 1171139</div><div id='project'> Project Name: lucidrains/glom-pytorch</div><div id='commit'> Commit Name: 570bc6667245f45ef03ad01b42cb335bda11d728</div><div id='time'> Time: 2021-03-05</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: glom_pytorch/glom_pytorch.py</div><div id='m_class'> M Class Name: GroupedFeedForward</div><div id='n_method'> N Class Name: GroupedFeedForward</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: glom_pytorch/glom_pytorch.py</div><div id='n_file'> N File Name: glom_pytorch/glom_pytorch.py</div><div id='m_start'> M Start Line: 26</div><div id='m_end'> M End Line: 28</div><div id='n_start'> N Start Line: 26</div><div id='n_end'> N End Line: 31</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.scale = dim_head ** -0.5
        self.dropout = nn.Dropout(dropout)

        self.type_growth = <a id="change">nn.Parameter(</a>torch.randn(model_dim) * 1e-5<a id="change">)</a>
        self.type_seasonal<a id="change"> = </a><a id="change">nn.Parameter(</a>torch.randn(model_dim) * 1e-5<a id="change">)</a>

        self.queries = nn.Parameter(torch.randn(heads, dim_head))

        self.growth_and_seasonal_to_kv = nn.Sequential(</code></pre><h3>After Change</h3><pre><code class='java'>
            Rearrange(&quot... (kv h d) n -&gt; kv ... h n d&quot, kv = 2, h = heads)
        )

        self.seasonal_to_kv = <a id="change">nn.Sequential(
            </a>Rearrange(&quotb n d -&gt; b d n&quot),
            nn.Conv1d(model_dim, inner_dim<a id="change"> * 2</a>, seasonal_kernel_size, bias = False, padding = seasonal_kernel_size // 2),
            Rearrange(&quot... (kv h d) n -&gt; kv ... h n d&quot, kv = 2, h = heads)<a id="change">
        )</a>

        self.level_to_kv = nn.Sequential(
            Rearrange(&quotb n t -&gt; b t n&quot),
            nn.Conv1d(time_features, inner_dim * 2, level_kernel_size, bias = False, padding = level_kernel_size // 2),</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/etsformer-pytorch/commit/efd13ff72791a8a937a7f61515cb8823d6642c18#diff-b4b8264f25f5dfdaf86bc5e5eff19dd88e54974eb3ebdbb09741da0265b56130L375' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 1171138</div><div id='project'> Project Name: lucidrains/etsformer-pytorch</div><div id='commit'> Commit Name: efd13ff72791a8a937a7f61515cb8823d6642c18</div><div id='time'> Time: 2022-03-20</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: etsformer_pytorch/etsformer_pytorch.py</div><div id='m_class'> M Class Name: ClassificationWrapper</div><div id='n_method'> N Class Name: ClassificationWrapper</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: etsformer_pytorch/etsformer_pytorch.py</div><div id='n_file'> N File Name: etsformer_pytorch/etsformer_pytorch.py</div><div id='m_start'> M Start Line: 388</div><div id='m_end'> M End Line: 396</div><div id='n_start'> N Start Line: 390</div><div id='n_end'> N End Line: 409</div><BR>