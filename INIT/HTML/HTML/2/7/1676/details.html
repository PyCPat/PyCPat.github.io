<html><h3>Pattern ID :1676
</h3><img src='4334912.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        if bilinear_upsample:
            upsample_klass = partial(InterpolateUpsample, mode = &quotbilinear&quot)
        elif <a id="change"></a>nearest_neighbor_upsample<a id="change">:
            </a>upsample_klass<a id="change"> = </a>partial(InterpolateUpsample, mode = &quotnearest&quot)
        else:
            upsample_klass<a id="change"> = </a>Upsample

        &#47&#47 upsampling layers
</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 downsampling layers

        skip_connect_dims<a id="change"> = []</a> &#47&#47 keep track of skip connection dimensions

        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_cross_attn) in enumerate(zip(in_out, *layer_params)):
            is_last = ind &gt;= (num_resolutions - 1)

            layer_use_linear_cross_attn = not layer_cross_attn and use_linear_cross_attn
            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None

            transformer_block_klass = TransformerBlock if layer_attn else (LinearAttentionTransformerBlock if use_linear_attn else nn.Identity)

            current_dim = dim_in

            &#47&#47 whether to pre-downsample, from memory efficient unet

            pre_downsample = None

            if memory_efficient:
                pre_downsample = downsample_klass(dim_in, dim_out)
                current_dim = dim_out

            <a id="change">skip_connect_dims.append(</a>current_dim<a id="change">)</a>

            self.downs.append(nn.ModuleList([
                pre_downsample,
                ResnetBlock(current_dim, current_dim, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),
                nn.ModuleList([ResnetBlock(current_dim, current_dim, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),
                transformer_block_klass(dim = current_dim, heads = attn_heads, dim_head = attn_dim_head, ff_mult = ff_mult),
                downsample_klass(current_dim, dim_out) if not memory_efficient and not is_last else None,
            ]))

        &#47&#47 middle layers

        mid_dim = dims[-1]

        self.mid_block1 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])
        self.mid_attn = EinopsToAndFrom(&quotb c h w&quot, &quotb (h w) c&quot, Residual(Attention(mid_dim, **attn_kwargs))) if attend_at_middle else None
        self.mid_block2 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])

        &#47&#47 upsampling layers

        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_cross_attn) in enumerate(zip(reversed(in_out), *reversed_layer_params)):
            is_last = ind == (len(in_out) - 1)
            layer_use_linear_cross_attn = not layer_cross_attn and use_linear_cross_attn
            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None
            transformer_block_klass = TransformerBlock if layer_attn else (LinearAttentionTransformerBlock if use_linear_attn else nn.Identity)

            skip_connect_dim<a id="change"> = </a>skip_connect_dims.pop()

            self.ups.append(nn.ModuleList([
                ResnetBlock(dim_out + skip_connect_dim, dim_out, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/imagen-pytorch/commit/36bdefca0e8670ca42b39236315121b703b9533f#diff-edef3c5fe92797a22c0b8fc6cca1d57b4b84ef03dfdfe802ed9147e21fc88109L1148' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4334912</div><div id='project'> Project Name: lucidrains/imagen-pytorch</div><div id='commit'> Commit Name: 36bdefca0e8670ca42b39236315121b703b9533f</div><div id='time'> Time: 2022-06-27</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: imagen_pytorch/imagen_pytorch.py</div><div id='m_class'> M Class Name: Unet</div><div id='n_method'> N Class Name: Unet</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: imagen_pytorch/imagen_pytorch.py</div><div id='n_file'> N File Name: imagen_pytorch/imagen_pytorch.py</div><div id='m_start'> M Start Line: 1178</div><div id='m_end'> M End Line: 1250</div><div id='n_start'> N Start Line: 1148</div><div id='n_end'> N End Line: 1221</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        super().__init__()

        inner_dim = int(dim * ff_mult)
        <a id="change">if </a>ffn_type == "GEGLU"<a id="change">:
            </a>self.net = nn.Sequential(
                GEGLU(dim, inner_dim),
                nn.Dropout(dropout),
                nn.Linear(inner_dim, dim)
            )
        elif ffn_type == "standard":
            self.net<a id="change"> = </a>nn.Sequential(
                nn.Linear(dim, inner_dim),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(inner_dim, dim)
            )
        elif ffn_type == "bilinear":
            self.net<a id="change"> = </a>nn.Sequential(
                Bilinear(dim, inner_dim),
                nn.Dropout(dropout),
                nn.Linear(inner_dim, dim)</code></pre><h3>After Change</h3><pre><code class='java'>
        upper_tri_rows = rearrange(upper_tri_rows, &quoti j -&gt; () i j&quot)
        slopes = self._get_slopes(heads=int(heads / 2))

        all_rows<a id="change"> = []</a>
        for h_ in range(int(heads / 2)):
            <a id="change">all_rows.append(</a>lower_tri_rows * slopes[h_]<a id="change">)</a>
            all_rows.append(upper_tri_rows * slopes[h_])

        &#47&#47 The resultant bias applies the Alibi position bias looking forward to half of the heads, and backwards to the
        &#47&#47 other half. Since for each head, only 1 direction contains positional information, you should probably use
        &#47&#47 RoPE along with Alibi, to give the opposite direction some positional information.

        self.bias<a id="change"> = </a>torch.cat(all_rows, dim=0).cuda()  &#47&#47 shape (heads, max_length, max_length)

    @staticmethod
    def _get_slopes(heads):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/antofuller/configaformers/commit/f2fa8c59ce1537b400a3288f9c556e84ca993807#diff-fcf59ce11a888c8312fa1547129217a783407a793109b95d67b265e47d08375bL58' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4334922</div><div id='project'> Project Name: antofuller/configaformers</div><div id='commit'> Commit Name: f2fa8c59ce1537b400a3288f9c556e84ca993807</div><div id='time'> Time: 2021-09-04</div><div id='author'> Author: afuller187187@gmail.com</div><div id='file'> File Name: building_blocks.py</div><div id='m_class'> M Class Name: FeedForward</div><div id='n_method'> N Class Name: Alibi</div><div id='m_method'> M Method Name: __init__(3)</div><div id='n_method'> N Method Name: __init__(5)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: building_blocks.py</div><div id='n_file'> N File Name: building_blocks.py</div><div id='m_start'> M Start Line: 58</div><div id='m_end'> M End Line: 103</div><div id='n_start'> N Start Line: 17</div><div id='n_end'> N End Line: 57</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                raise TypeError("Unknown Type:\t{}".format(norm))
        self.norm0 = norm(out_channels)

        <a id="change">if </a>self.concat<a id="change">:
            </a>if self.mode == &quotres_mask&quot:
                k= 2*out_channels + 1
            else:
                k = 2*out_channels
            self.conv1<a id="change"> = </a>conv3x3(k, out_channels)
            self.norm1<a id="change"> = </a>norm(out_channels)
        else:
            self.conv1 = conv3x3(out_channels, out_channels)
            self.norm1 = norm(out_channels)</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 residual structure
        self.conv2 = []
        self.conv3<a id="change"> = []</a>
        for i in range(blocks):
            self.conv2.append(
                nn.Sequential(*[
                    nn.Conv2d(out_channels // 2 + 1, out_channels // 4, 5, 1, 2),
                    nn.ReLU(True),
                    nn.Conv2d(out_channels // 4, 1, 5, 1, 2),
                    nn.Sigmoid()
                ])
            )
            <a id="change">self.conv3.append(</a>conv3x3(out_channels // 2, out_channels)<a id="change">)</a>
        
        self.bn = []
        for _ in range(blocks):
            self.bn.append(norm(out_channels))
        self.bn = nn.ModuleList(self.bn)
        self.conv2 = nn.ModuleList(self.conv2)
        self.conv3<a id="change"> = </a>nn.ModuleList(self.conv3)
        self.act = act

    def forward(self, from_up, from_down, mask=None):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/bcmi/slbr-visible-watermark-removal/commit/43e84b70895d28955496122816e50857863e5bfd#diff-21665f19cd9acd8db0bae320b79f986ec6a79eabb60d74c4c2e813314554df4aL205' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4334936</div><div id='project'> Project Name: bcmi/slbr-visible-watermark-removal</div><div id='commit'> Commit Name: 43e84b70895d28955496122816e50857863e5bfd</div><div id='time'> Time: 2022-01-04</div><div id='author'> Author: lj200820082007@163.com</div><div id='file'> File Name: src/networks/blocks.py</div><div id='m_class'> M Class Name: MBEBlock</div><div id='n_method'> N Class Name: MBEBlock</div><div id='m_method'> M Method Name: __init__(10)</div><div id='n_method'> N Method Name: __init__(10)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: src/networks/blocks.py</div><div id='n_file'> N File Name: src/networks/blocks.py</div><div id='m_start'> M Start Line: 208</div><div id='m_end'> M End Line: 235</div><div id='n_start'> N Start Line: 208</div><div id='n_end'> N End Line: 248</div><BR>