<html><h3>Pattern ID :153
</h3><img src='658536.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                 final_process=None,
                 device="cuda:0"):
        super(TCDNNet, self).__init__()
        <a id="change">if </a>len(att_layers) != len(tc_layers) + 1<a id="change">:
            </a>raise RuntimeError("There must be one more attention layer than that of temporal convolution layers.")
        num_filters = int(m.ceil(m.log(seq_length, 2)))

        self.layers = nn.ModuleDict()
        channels = in_channels

        for i in range(len(tc_layers)):
            self.layers.add_module("attention_{}".format(i),
                                   AttentionBlock(channels, att_layers[i][0], att_layers[i][1], device))
            channels<a id="change"> += </a>att_layers[i][1]
            self.layers.add_module("tconv_{}".format(i),
                                   TCBlock(channels, seq_length, tc_layers[i], device))
            channels += num_filters * tc_layers[i]</code></pre><h3>After Change</h3><pre><code class='java'>
        channels += additional_length
        fc_layers = list(fc_layers) + [out_channels]

        self.layers.add_module("fc_amalgamate", <a id="change">nn.Linear(seq_length, 1).to(</a>device<a id="change">)</a>)
        for i in range(len(fc_layers)):
            self.layers.add_module("fc_{}".format(i), nn.Linear(channels, fc_layers[i]).to(device))
            channels = fc_layers[i]</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/iffix/machin/commit/5b8d4a14723b753d9ee9d6dece151e40b3531c98#diff-b99a43ebb8bdea7f7a9ff2be74dc2b460878eec11cb5e5374efa147eedc92cf6L109' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 658536</div><div id='project'> Project Name: iffix/machin</div><div id='commit'> Commit Name: 5b8d4a14723b753d9ee9d6dece151e40b3531c98</div><div id='time'> Time: 2020-04-25</div><div id='author'> Author: hanhanmumuqq@163.com</div><div id='file'> File Name: models/base/tcdnnet.py</div><div id='m_class'> M Class Name: TCDNNet</div><div id='n_method'> N Class Name: TCDNNet</div><div id='m_method'> M Method Name: __init__(11)</div><div id='n_method'> N Method Name: __init__(11)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: models/base/tcdnnet.py</div><div id='n_file'> N File Name: models/base/tcdnnet.py</div><div id='m_start'> M Start Line: 109</div><div id='m_end'> M End Line: 139</div><div id='n_start'> N Start Line: 195</div><div id='n_end'> N End Line: 218</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        if init_file is not None:
            &#47&#47 handle numpy or torch
            <a id="change">if </a>".pt" in init_file<a id="change">:
                </a>self.W.data = torch.load(
                    init_file, map_location=torch.device(device)
                )
            else:
                temp<a id="change"> = </a>np.load(init_file)
                self.W.data = torch.as_tensor(temp).float()

    def forward(self, inp):</code></pre><h3>After Change</h3><pre><code class='java'>
    def __init__(self, n_comp=100, n_freq=513, init_file=None, device="cuda"):
        super(NMFDecoder, self).__init__()

        self.W = <a id="change">nn.Parameter(
            0.1 * torch.rand(n_freq, n_comp), requires_grad=True
        ).to(</a>device<a id="change">)</a>
        self.activ = nn.ReLU()

        &#47&#47 we need to fix this! we should download from HF
        if init_file is not None:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/speechbrain/speechbrain/commit/b32e61713801c840a551126e7c20c6529b1ebd28#diff-3dd193442407cad3e2b7166c4888b1739f4139037d4022ea3f1fa26afb9fb8e0L102' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 658530</div><div id='project'> Project Name: speechbrain/speechbrain</div><div id='commit'> Commit Name: b32e61713801c840a551126e7c20c6529b1ebd28</div><div id='time'> Time: 2022-11-29</div><div id='author'> Author: csubakan@gmail.com</div><div id='file'> File Name: speechbrain/lobes/models/L2I.py</div><div id='m_class'> M Class Name: NMFDecoder</div><div id='n_method'> N Class Name: NMFDecoder</div><div id='m_method'> M Method Name: __init__(5)</div><div id='n_method'> N Method Name: __init__(5)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: speechbrain/lobes/models/L2I.py</div><div id='n_file'> N File Name: speechbrain/lobes/models/L2I.py</div><div id='m_start'> M Start Line: 105</div><div id='m_end'> M End Line: 120</div><div id='n_start'> N Start Line: 126</div><div id='n_end'> N End Line: 135</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 setting the device

        <a id="change">if </a>not exists(accelerator) and not exists(device)<a id="change">:
            </a>diffusion_prior_device<a id="change"> = </a>next(diffusion_prior.parameters()).device
            self.print(f&quotaccelerator not given, and device not specified: defaulting to device of diffusion prior parameters - {diffusion_prior_device}&quot)
            self.device = diffusion_prior_device
        else:</code></pre><h3>After Change</h3><pre><code class='java'>
            }
            precision_type = cast_type_map[accelerator.mixed_precision]
            assert precision_type == torch.float, "DeepSpeed currently only supports float32 precision when using on the fly embedding generation from clip"
            <a id="change">self.diffusion_prior.clip.to(</a>precision_type<a id="change">)</a>

        &#47&#47 optimizer stuff

        self.optim_kwargs = dict(lr=lr, wd=wd, eps=eps, group_wd_params=group_wd_params)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/dalle2-pytorch/commit/f9423d308b6f36e51152c2c45045ff4ebb308287#diff-617450527162fa367141dbf45e8b201673573479820af0ffc56ba93b7f70947fL174' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 658533</div><div id='project'> Project Name: lucidrains/dalle2-pytorch</div><div id='commit'> Commit Name: f9423d308b6f36e51152c2c45045ff4ebb308287</div><div id='time'> Time: 2022-07-20</div><div id='author'> Author: 51308183+nousr@users.noreply.github.com</div><div id='file'> File Name: dalle2_pytorch/trainer.py</div><div id='m_class'> M Class Name: DiffusionPriorTrainer</div><div id='n_method'> N Class Name: DiffusionPriorTrainer</div><div id='m_method'> M Method Name: __init__(10)</div><div id='n_method'> N Method Name: __init__(12)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: dalle2_pytorch/trainer.py</div><div id='n_file'> N File Name: dalle2_pytorch/trainer.py</div><div id='m_start'> M Start Line: 182</div><div id='m_end'> M End Line: 240</div><div id='n_start'> N Start Line: 177</div><div id='n_end'> N End Line: 246</div><BR>