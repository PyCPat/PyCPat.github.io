<html><h3>Pattern ID :1570
</h3><img src='4067324.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        self.norm = norm_layer(self.num_features)
        self.avgpool = nn.AdaptiveAvgPool1d(1)
        self.head = nn.Linear(self.num_features, num_classes)<a id="change"> if </a>num_classes &gt; 0<a id="change"> else </a>nn.Identity()

        self.apply(self._init_weights)
</code></pre><h3>After Change</h3><pre><code class='java'>
                use_checkpoint=use_checkpoint)
            self.layers.append(layer)

        num_features = [<a id="change">int(</a>embed_dim * 2<a id="change"> ** </a>i<a id="change">)</a> for i in range(self.num_layers)]
        self.num_features = num_features

        &#47&#47 add a norm layer for each output</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 6</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/plemeri/inspyrenet/commit/12b05eaf235665fc6d1f89a9055b84d7cdfec923#diff-bfa9144b84aa2908523fe6e28d0f355d5701af250801acefca0ffbeed05bc3dfL483' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4067324</div><div id='project'> Project Name: plemeri/inspyrenet</div><div id='commit'> Commit Name: 12b05eaf235665fc6d1f89a9055b84d7cdfec923</div><div id='time'> Time: 2021-10-14</div><div id='author'> Author: taehoon1018@postech.ac.kr</div><div id='file'> File Name: lib/backbones/SwinTransformer.py</div><div id='m_class'> M Class Name: SwinTransformer</div><div id='n_method'> N Class Name: SwinTransformer</div><div id='m_method'> M Method Name: __init__(20)</div><div id='n_method'> N Method Name: __init__(19)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: lib/backbones/SwinTransformer.py</div><div id='n_file'> N File Name: lib/backbones/SwinTransformer.py</div><div id='m_start'> M Start Line: 483</div><div id='m_end'> M End Line: 539</div><div id='n_start'> N Start Line: 489</div><div id='n_end'> N End Line: 549</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        self.to_q = nn.Linear(dim, dim, bias = False)

        kv_heads = 1<a id="change"> if </a>one_kv_head<a id="change"> else </a>heads
        self.one_kv_head = one_kv_head
        self.kv_heads = kv_heads
        self.to_k = nn.Linear(dim, d_heads * kv_heads, bias = False)</code></pre><h3>After Change</h3><pre><code class='java'>
        self.psi_fn = psi_fn
        self.receives_context = receives_context

        self.global_attn_heads = heads<a id="change"> - </a>n_local_attn_heads
        self.global_attn_fn = linear_attn if not causal else partial(causal_linear_attn, psi=psi_fn, bucket_size = blindspot_size)

        self.local_attn_heads = n_local_attn_heads
        self.local_attn  = LocalAttention(local_attn_window_size, n_local_attn_heads, d_heads, causal = causal, dropout = attn_dropout)

        self.to_q = nn.Linear(dim, dim, bias = False)

        kv_heads = (int(self.local_attn_heads &gt; 0) + <a id="change">int(</a>self.global_attn_heads &gt; 0<a id="change">)</a>) if one_kv_head else heads

        self.one_kv_head = one_kv_head
        self.kv_heads = kv_heads</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/linear-attention-transformer/commit/75a6cefd9d7facce1ff162dc70138a6e32358f3c#diff-eee6cf669463e9683bcf9643c4b19a493056f98ded32854287144d47f02a7032L269' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4067340</div><div id='project'> Project Name: lucidrains/linear-attention-transformer</div><div id='commit'> Commit Name: 75a6cefd9d7facce1ff162dc70138a6e32358f3c</div><div id='time'> Time: 2020-06-29</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: linear_attention_transformer/linear_attention_transformer.py</div><div id='m_class'> M Class Name: SelfAttention</div><div id='n_method'> N Class Name: SelfAttention</div><div id='m_method'> M Method Name: __init__(12)</div><div id='n_method'> N Method Name: __init__(12)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: linear_attention_transformer/linear_attention_transformer.py</div><div id='n_file'> N File Name: linear_attention_transformer/linear_attention_transformer.py</div><div id='m_start'> M Start Line: 286</div><div id='m_end'> M End Line: 286</div><div id='n_start'> N Start Line: 280</div><div id='n_end'> N End Line: 288</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.na = len(anchors)  &#47&#47 number of anchors (3)
        self.nc = nc  &#47&#47 number of classes (80)
        self.img_size = 0
        device = torch.device(&quotcuda&quot<a id="change"> if </a>torch.cuda.is_available()<a id="change"> else </a>&quotcpu&quot)
        create_grids(self, 32, 1, device=device)

        if ONNX_EXPORT:  &#47&#47 grids must be computed in __init__</code></pre><h3>After Change</h3><pre><code class='java'>
            if cfg.endswith(&quotyolov3-tiny.cfg&quot):
                stride *= 2

            ng = (int(img_size[0] / stride), <a id="change">int(</a>img_size[1]<a id="change"> / </a>stride<a id="change">)</a>)  &#47&#47 number grid points
            create_grids(self, max(img_size), ng)

    def forward(self, p, img_size, var=None):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/mikel-brostrom/yolov3_deepsort_pytorch/commit/cfe354064cd599ae6ea902ffaa4dbd106d3a40fb#diff-dcfeaa5ac3dffd54264034760914588d92fc7395c81eaa9274715840dc28100bL103' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4067306</div><div id='project'> Project Name: mikel-brostrom/yolov3_deepsort_pytorch</div><div id='commit'> Commit Name: cfe354064cd599ae6ea902ffaa4dbd106d3a40fb</div><div id='time'> Time: 2019-04-21</div><div id='author'> Author: glenn.jocher@ultralytics.com</div><div id='file'> File Name: models.py</div><div id='m_class'> M Class Name: YOLOLayer</div><div id='n_method'> N Class Name: YOLOLayer</div><div id='m_method'> M Method Name: __init__(6)</div><div id='n_method'> N Method Name: __init__(6)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: models.py</div><div id='n_file'> N File Name: models.py</div><div id='m_start'> M Start Line: 110</div><div id='m_end'> M End Line: 119</div><div id='n_start'> N Start Line: 112</div><div id='n_end'> N End Line: 117</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)

        self.base = 32<a id="change"> if </a>is_half<a id="change"> else </a>64

        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, self.base, layers[0])</code></pre><h3>After Change</h3><pre><code class='java'>
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)

        self.base = <a id="change">int(</a>64<a id="change"> * </a>width<a id="change">)</a>

        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, self.base, layers[0])
        self.layer2 = self._make_layer(block, self.base * 2, layers[1], stride=2)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/szq0214/cmc_with_image_mixture/commit/c3b36d304fce8787925aa2d9c2415849c9dd0390#diff-da62ce430302dcb6718b9a6a36a17d97e7ef64d1ecba8920e816a058c59c6816L109' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4067302</div><div id='project'> Project Name: szq0214/cmc_with_image_mixture</div><div id='commit'> Commit Name: c3b36d304fce8787925aa2d9c2415849c9dd0390</div><div id='time'> Time: 2019-11-25</div><div id='author'> Author: yonglong@mit.edu</div><div id='file'> File Name: models/resnet.py</div><div id='m_class'> M Class Name: ResNet</div><div id='n_method'> N Class Name: ResNet</div><div id='m_method'> M Method Name: __init__(6)</div><div id='n_method'> N Method Name: __init__(6)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: models/resnet.py</div><div id='n_file'> N File Name: models/resnet.py</div><div id='m_start'> M Start Line: 117</div><div id='m_end'> M End Line: 117</div><div id='n_start'> N Start Line: 118</div><div id='n_end'> N End Line: 118</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop,
            pretrained_window_size=to_2tuple(pretrained_window_size))
        self.norm1 = norm_layer(dim)
        self.drop_path1 = DropPath(drop_path)<a id="change"> if </a>drop_path &gt; 0.<a id="change"> else </a>nn.Identity()

        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)
        self.norm2 = norm_layer(dim)</code></pre><h3>After Change</h3><pre><code class='java'>

        self.drop_path = DropPath(drop_path) if drop_path &gt; 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = <a id="change">int(</a>dim<a id="change"> * </a>mlp_ratio<a id="change">)</a>
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

        if self.shift_size &gt; 0:
            &#47&#47 calculate attention mask for SW-MSA</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/eora-ai/torchok/commit/ab2534f05b48a529d03f8c28af2579245772f4e0#diff-8341451f0cc24d6c15a35a4d4f9dfb66429235a10c33fa87d8ecf5664e38a1c2L67' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4067330</div><div id='project'> Project Name: eora-ai/torchok</div><div id='commit'> Commit Name: ab2534f05b48a529d03f8c28af2579245772f4e0</div><div id='time'> Time: 2022-07-11</div><div id='author'> Author: rashit.bayazitov.1995@gmail.com</div><div id='file'> File Name: src/models/modules/blocks/swin_block.py</div><div id='m_class'> M Class Name: SwinTransformerBlock</div><div id='n_method'> N Class Name: SwinTransformerBlock</div><div id='m_method'> M Method Name: __init__(14)</div><div id='n_method'> N Method Name: __init__(14)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: src/models/modules/blocks/swin_block.py</div><div id='n_file'> N File Name: src/models/modules/blocks/swin_block.py</div><div id='m_start'> M Start Line: 73</div><div id='m_end'> M End Line: 108</div><div id='n_start'> N Start Line: 75</div><div id='n_end'> N End Line: 114</div><BR>