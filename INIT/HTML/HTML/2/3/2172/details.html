<html><h3>Pattern ID :2172
</h3><img src='5009964.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion*planes:
            self.shortcut = <a id="change">nn.Sequential(
                </a>nn.Conv2d(in_planes, self.expansion<a id="change">*</a>planes, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(self.expansion*planes)<a id="change">
            )</a>

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = F.relu(self.bn2(self.conv2(out)))</code></pre><h3>After Change</h3><pre><code class='java'>
        self.to_cls_token = nn.Identity()

        self.mlp_head = nn.Sequential(
            <a id="change">nn.LayerNorm(</a>dim<a id="change">)</a>,
            nn.Linear(dim, mlp_dim),
            nn.GELU(),
            nn.Dropout(dropout),</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/cydia2018/vit-cifar10-pruning/commit/670fb581b519a0875681d2fbf4c4ec824e7fd9a3#diff-dfcdfdb3cc72798869ac0cd03f988cc3a003b313176165f458bb2dee8e033743L42' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 5009964</div><div id='project'> Project Name: cydia2018/vit-cifar10-pruning</div><div id='commit'> Commit Name: 670fb581b519a0875681d2fbf4c4ec824e7fd9a3</div><div id='time'> Time: 2020-10-27</div><div id='author'> Author: meathouse47@gmail.com</div><div id='file'> File Name: models/vit.py</div><div id='m_class'> M Class Name: Bottleneck</div><div id='n_method'> N Class Name: ViT</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: models/vit.py</div><div id='n_file'> N File Name: models/vit.py</div><div id='m_start'> M Start Line: 42</div><div id='m_end'> M End Line: 58</div><div id='n_start'> N Start Line: 87</div><div id='n_end'> N End Line: 111</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):
        super().__init__()
        inner_dim = dim_head<a id="change"> * </a>heads
        project_out = not (heads == 1 and dim_head == dim)

        self.heads = heads
        self.scale = dim_head ** -0.5

        self.attend = nn.Softmax(dim=-1)
        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)

        self.to_out = <a id="change">nn.Sequential(
            </a>nn.Linear(inner_dim, dim),
            nn.Dropout(dropout)<a id="change">
        )</a> if project_out else nn.Identity()

    def forward(self, x):
        </code></pre><h3>After Change</h3><pre><code class='java'>
            channel_mlp = token_channels
        self.AddPosEmb2Value = AddPosEmb2Value

        self.norm1 = <a id="change">nn.LayerNorm(</a>token_channels<a id="change">)</a>
        self.Attention = MultiHeadAttention(token_channels, heads, dim_head, dropout)
        self.norm2 = nn.LayerNorm(token_channels)
        self.FFN = FeedForward(token_channels, channel_mlp, dropout=dropout)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/wamawama/wama_modules/commit/f6865257f4bea4ed62e0170644d99dcdb1d20c94#diff-f5d4e504fb1ef29f758d7f77376318f372efbfb4498293f8d8797c46106d1d7aL51' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 5009966</div><div id='project'> Project Name: wamawama/wama_modules</div><div id='commit'> Commit Name: f6865257f4bea4ed62e0170644d99dcdb1d20c94</div><div id='time'> Time: 2022-10-28</div><div id='author'> Author: wmy19970215@gmail.com</div><div id='file'> File Name: wama_modules/Transformer.py</div><div id='m_class'> M Class Name: SelfAttention</div><div id='n_method'> N Class Name: TransformerEncoderLayer</div><div id='m_method'> M Method Name: __init__(7)</div><div id='n_method'> N Method Name: __init__(5)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: wama_modules/Transformer.py</div><div id='n_file'> N File Name: wama_modules/Transformer.py</div><div id='m_start'> M Start Line: 53</div><div id='m_end'> M End Line: 65</div><div id='n_start'> N Start Line: 114</div><div id='n_end'> N End Line: 133</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nheads)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_encoder_layers)

        self.mlp_head = <a id="change">nn.Sequential(
            </a>nn.Linear(hidden_dim<a id="change"> * </a>projection_dim, mlp_head_dims[0]),
            nn.BatchNorm1d(mlp_head_dims[0]),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(mlp_head_dims[0], mlp_head_dims[1]),
            nn.BatchNorm1d(mlp_head_dims[1]),
            nn.GELU(),
            nn.Dropout(dropout)<a id="change">
        )</a>

        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(mlp_head_dims[1], num_classes)
</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 self.signal_encoder = SignalEncoderConv(hidden_dim, projection_dim, learnable=False)

        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nheads, dropout=dropout, dim_feedforward=1024, activation=&quotgelu&quot)
        encoder_norm = <a id="change">nn.LayerNorm(</a>hidden_dim<a id="change">)</a>
        self.transformer = nn.TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)

        self.mlp_head = nn.Sequential(
            nn.Dropout(dropout),</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/kolaszko/haptic_transformer/commit/480c8a6c89740a357ff79f97d11b8fde6f6e09be#diff-9a02f4e72f98b173cff993e361b8c108ffdd07649fe9a77c6611a0c5af9ecb4aL7' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 5009967</div><div id='project'> Project Name: kolaszko/haptic_transformer</div><div id='commit'> Commit Name: 480c8a6c89740a357ff79f97d11b8fde6f6e09be</div><div id='time'> Time: 2021-05-05</div><div id='author'> Author: mikolaj.lysakowski.bk@gmail.com</div><div id='file'> File Name: models/haptr.py</div><div id='m_class'> M Class Name: HAPTR</div><div id='n_method'> N Class Name: HAPTR</div><div id='m_method'> M Method Name: __init__(8)</div><div id='n_method'> N Method Name: __init__(8)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: models/haptr.py</div><div id='n_file'> N File Name: models/haptr.py</div><div id='m_start'> M Start Line: 8</div><div id='m_end'> M End Line: 28</div><div id='n_start'> N Start Line: 9</div><div id='n_end'> N End Line: 29</div><BR>