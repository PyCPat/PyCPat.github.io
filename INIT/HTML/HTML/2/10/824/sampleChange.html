<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.l_6 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[3]/LayerNorm[ln_1]&quot]
        assert isinstance(self.l_6,LayerNorm) ,f&quotlayers[GPT2LMHeadModel/GPT2Model[transformer]/Block[3]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_6)}&quot
        self.l_7 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[3]/Attention[attn]/Conv1D[c_attn]&quot]
        <a id="change">assert </a>isinstance(self.l_7,Conv1D) ,f&quotlayers[GPT2LMHeadModel/GPT2Model[transformer]/Block[3]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_7)}&quot
        self.l_8 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[3]/Attention[attn]/Dropout[attn_dropout]&quot]
        assert isinstance(self.l_8,Dropout) ,f&quotlayers[GPT2LMHeadModel/GPT2Model[transformer]/Block[3]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_8)}&quot
        self.l_9 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[3]/Attention[attn]/Conv1D[c_proj]&quot]</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 initializing partition layers
        self.l_0 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Dropout[drop]&quot]
        self.l_1 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[0]/LayerNorm[ln_1]&quot]
        self.l_2 = <a id="change">layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[0]/Attention[attn]/Conv1D[c_attn]&quot]</a>
        self.l_3 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[0]/Attention[attn]/Dropout[attn_dropout]&quot]
        self.l_4 = <a id="change">layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[0]/Attention[attn]/Conv1D[c_proj]&quot]</a>
        self.l_5 = <a id="change">layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[0]/Attention[attn]/Dropout[resid_dropout]&quot]</a>
        self.l_6 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[0]/LayerNorm[ln_2]&quot]
        self.l_7 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[0]/MLP[mlp]/Conv1D[c_fc]&quot]
        self.l_8 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[0]/MLP[mlp]/Conv1D[c_proj]&quot]
        self.l_9 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[0]/MLP[mlp]/Dropout[dropout]&quot]
        self.l_10 = <a id="change">layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[1]/LayerNorm[ln_1]&quot]</a>
        self.l_11 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[1]/Attention[attn]/Conv1D[c_attn]&quot]
        self.l_12 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[1]/Attention[attn]/Dropout[attn_dropout]&quot]
        self.l_13 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[1]/Attention[attn]/Conv1D[c_proj]&quot]
        self.l_14 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[1]/Attention[attn]/Dropout[resid_dropout]&quot]</code></pre>