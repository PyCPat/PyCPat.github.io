<html><h3>Pattern ID :817
</h3><img src='2773034.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.l_14 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[3]/MLP[mlp]/Dropout[dropout]&quot]
        assert isinstance(self.l_14,Dropout) ,f&quotlayers[GPT2LMHeadModel/GPT2Model[transformer]/Block[3]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_14)}&quot
        self.l_15 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[4]/LayerNorm[ln_1]&quot]
        <a id="change">assert </a>isinstance(self.l_15,LayerNorm) ,f&quotlayers[GPT2LMHeadModel/GPT2Model[transformer]/Block[4]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_15)}&quot
        self.l_16 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[4]/Attention[attn]/Conv1D[c_attn]&quot]
        assert isinstance(self.l_16,Conv1D) ,f&quotlayers[GPT2LMHeadModel/GPT2Model[transformer]/Block[4]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_16)}&quot
        self.l_17 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[4]/Attention[attn]/Dropout[attn_dropout]&quot]</code></pre><h3>After Change</h3><pre><code class='java'>
        self.l_0 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Dropout[drop]&quot]
        self.l_1 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[0]/LayerNorm[ln_1]&quot]
        self.l_2 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[0]/Attention[attn]/Conv1D[c_attn]&quot]
        self.l_3 = <a id="change">layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[0]/Attention[attn]/Dropout[attn_dropout]&quot]</a>
        self.l_4 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[0]/Attention[attn]/Conv1D[c_proj]&quot]
        self.l_5 = <a id="change">layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[0]/Attention[attn]/Dropout[resid_dropout]&quot]</a>
        self.l_6 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[0]/LayerNorm[ln_2]&quot]
        self.l_7 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[0]/MLP[mlp]/Conv1D[c_fc]&quot]
        self.l_8 = <a id="change">layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[0]/MLP[mlp]/Conv1D[c_proj]&quot]</a>
        self.l_9 = <a id="change">layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[0]/MLP[mlp]/Dropout[dropout]&quot]</a>
        self.l_10 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[1]/LayerNorm[ln_1]&quot]
        self.l_11 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[1]/Attention[attn]/Conv1D[c_attn]&quot]
        self.l_12 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[1]/Attention[attn]/Dropout[attn_dropout]&quot]
        self.l_13 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[1]/Attention[attn]/Conv1D[c_proj]&quot]</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/e0f6d62f2607fe288ebc9345a8a31607d5f796fd#diff-767d38aa90d5514ca1245a5648a485325ea46df26210d355e1dc45cc15317eeaL209' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2773034</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: e0f6d62f2607fe288ebc9345a8a31607d5f796fd</div><div id='time'> Time: 2020-04-18</div><div id='author'> Author: saareliad@campus.technion.ac.il</div><div id='file'> File Name: models/partitioned/gpt2_tied_lm_5p.py</div><div id='m_class'> M Class Name: Partition1</div><div id='n_method'> N Class Name: Partition1</div><div id='m_method'> M Method Name: __init__(3)</div><div id='n_method'> N Method Name: __init__(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: models/partitioned/gpt2_tied_lm_5p.py</div><div id='n_file'> N File Name: models/partitioned/gpt2_tied_lm_5p.py</div><div id='m_start'> M Start Line: 1011</div><div id='m_end'> M End Line: 1074</div><div id='n_start'> N Start Line: 209</div><div id='n_end'> N End Line: 250</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.l_14 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[3]/MLP[mlp]/Dropout[dropout]&quot]
        assert isinstance(self.l_14,Dropout) ,f&quotlayers[GPT2LMHeadModel/GPT2Model[transformer]/Block[3]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_14)}&quot
        self.l_15 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[4]/LayerNorm[ln_1]&quot]
        <a id="change">assert </a>isinstance(self.l_15,LayerNorm) ,f&quotlayers[GPT2LMHeadModel/GPT2Model[transformer]/Block[4]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_15)}&quot
        self.l_16 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[4]/Attention[attn]/Conv1D[c_attn]&quot]
        assert isinstance(self.l_16,Conv1D) ,f&quotlayers[GPT2LMHeadModel/GPT2Model[transformer]/Block[4]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_16)}&quot
        self.l_17 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[4]/Attention[attn]/Dropout[attn_dropout]&quot]</code></pre><h3>After Change</h3><pre><code class='java'>
        self.l_0 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Dropout[drop]&quot]
        self.l_1 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[0]/LayerNorm[ln_1]&quot]
        self.l_2 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[0]/Attention[attn]/Conv1D[c_attn]&quot]
        self.l_3 = <a id="change">layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[0]/Attention[attn]/Dropout[attn_dropout]&quot]</a>
        self.l_4 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[0]/Attention[attn]/Conv1D[c_proj]&quot]
        self.l_5 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[0]/Attention[attn]/Dropout[resid_dropout]&quot]
        self.l_6 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[0]/LayerNorm[ln_2]&quot]
        self.l_7 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[0]/MLP[mlp]/Conv1D[c_fc]&quot]
        self.l_8 = <a id="change">layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[0]/MLP[mlp]/Conv1D[c_proj]&quot]</a>
        self.l_9 = <a id="change">layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[0]/MLP[mlp]/Dropout[dropout]&quot]</a>
        self.l_10 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[1]/LayerNorm[ln_1]&quot]
        self.l_11 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[1]/Attention[attn]/Conv1D[c_attn]&quot]
        self.l_12 = <a id="change">layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[1]/Attention[attn]/Dropout[attn_dropout]&quot]</a>
        self.l_13 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[1]/Attention[attn]/Conv1D[c_proj]&quot]
        self.l_14 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[1]/Attention[attn]/Dropout[resid_dropout]&quot]
        self.l_15 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[1]/LayerNorm[ln_2]&quot]
        self.l_16 = layers[&quotGPT2LMHeadModel/GPT2Model[transformer]/Block[1]/MLP[mlp]/Conv1D[c_fc]&quot]</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/e0f6d62f2607fe288ebc9345a8a31607d5f796fd#diff-767d38aa90d5514ca1245a5648a485325ea46df26210d355e1dc45cc15317eeaL1008' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2773035</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: e0f6d62f2607fe288ebc9345a8a31607d5f796fd</div><div id='time'> Time: 2020-04-18</div><div id='author'> Author: saareliad@campus.technion.ac.il</div><div id='file'> File Name: models/partitioned/gpt2_tied_lm_5p.py</div><div id='m_class'> M Class Name: Partition1</div><div id='n_method'> N Class Name: Partition1</div><div id='m_method'> M Method Name: __init__(3)</div><div id='n_method'> N Method Name: __init__(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: models/partitioned/gpt2_tied_lm_5p.py</div><div id='n_file'> N File Name: models/partitioned/gpt2_tied_lm_5p.py</div><div id='m_start'> M Start Line: 1011</div><div id='m_end'> M End Line: 1074</div><div id='n_start'> N Start Line: 209</div><div id='n_end'> N End Line: 250</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.preactivation = preactivation
        self._incremental_n_modes = incremental_n_modes

        <a id="change">assert </a>len(self.layer_configs) == n_layers
        

        if domain_padding is not None and domain_padding &gt; 0:</code></pre><h3>After Change</h3><pre><code class='java'>
        assert len(uno_layers[&quotn_modes&quot]) == n_layers, "number of modes for all layers are not given"
        assert len(uno_layers[&quotres_scaling&quot]) == n_layers, "Scaling factor for all layers are not given"

        self.n_dim = len(<a id="change">uno_layers[&quotn_modes&quot]</a>[0])
        self.hidden_channels = hidden_channels
        self.lifting_channels = lifting_channels
        self.projection_channels = projection_channels
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.horizontal_skips_map = horizontal_skips_map
        self.joint_factorization = joint_factorization
        self.non_linearity = non_linearity
        self.rank = rank
        self.factorization = factorization
        self.fixed_rank_modes = fixed_rank_modes
        self.decomposition_kwargs = decomposition_kwargs
        self.fno_skip = fno_skip,
        self.mlp_skip = mlp_skip,
        self.fft_norm = fft_norm
        self.implementation = implementation
        self.separable = separable
        self.preactivation = preactivation
        self._incremental_n_modes = incremental_n_modes

        

        self.layer_configs = []
        for l in range(n_layers):
            l_config = {}
            l_config[&quotout_channels&quot] = <a id="change">uno_layers[&quotout_channels&quot]</a>[l]
            l_config[&quotn_modes&quot] = <a id="change">uno_layers[&quotn_modes&quot]</a>[l]
            l_config[&quotres_scaling&quot] = <a id="change">uno_layers[&quotres_scaling&quot]</a>[l]
            self.layer_configs.append(l_config)
        
        if self.horizontal_skips_map is None:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/zongyi-li/fourier_neural_operator/commit/7a8f1452af3fa7f1d287d503e996c402f693cfb6#diff-ac4b85e380a4812b342c118065527678fa9aef5163e07cc6c0bec8e731055599L91' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2773079</div><div id='project'> Project Name: zongyi-li/fourier_neural_operator</div><div id='commit'> Commit Name: 7a8f1452af3fa7f1d287d503e996c402f693cfb6</div><div id='time'> Time: 2023-04-15</div><div id='author'> Author: ashiqbuet14@gmail.com</div><div id='file'> File Name: neuralop/models/uno.py</div><div id='m_class'> M Class Name: UNO</div><div id='n_method'> N Class Name: UNO</div><div id='m_method'> M Method Name: __init__(28)</div><div id='n_method'> N Method Name: __init__(28)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: neuralop/models/uno.py</div><div id='n_file'> N File Name: neuralop/models/uno.py</div><div id='m_start'> M Start Line: 119</div><div id='m_end'> M End Line: 142</div><div id='n_start'> N Start Line: 123</div><div id='n_end'> N End Line: 167</div><BR>