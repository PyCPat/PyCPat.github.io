<html><h3>Pattern ID :242
</h3><img src='975182.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.patch_embedding: nn.Module = PatchEmbedding(in_channels=in_chans, out_channels=embed_dim,
                                                         patch_size=patch_size, norm_layer=norm_layer)
        &#47&#47 Compute patch resolution
        patch_resolution: Tuple[int, int] = (<a id="change">img_size[0]</a> // patch_size, <a id="change">img_size[1]</a> // patch_size)
        &#47&#47 Path dropout dependent on depth
        drop_path_rate = torch.linspace(0., drop_path_rate, sum(depths)).tolist()
        &#47&#47 Init stages</code></pre><h3>After Change</h3><pre><code class='java'>

        self.global_pool: str = global_pool
        self.head: nn.Module = nn.Linear(
            in_features=self.num_features, out_features=num_classes) if num_classes else <a id="change">nn.Identity()</a>

        &#47&#47 FIXME weight init TBD, PyTorch default init appears to be working well,
        &#47&#47 but differs from usual ViT or Swin init.
        &#47&#47 named_apply(init_weights, self)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 5</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/feng-lab/pytorch-image-models/commit/c6e4b7895a7dbcd9b98396cbef383dd1c72b0ad3#diff-ed4e73b2f003b1bf594949d63e8066b203ff134a32665c27fe72e45e3a6607afL611' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 975182</div><div id='project'> Project Name: feng-lab/pytorch-image-models</div><div id='commit'> Commit Name: c6e4b7895a7dbcd9b98396cbef383dd1c72b0ad3</div><div id='time'> Time: 2022-02-23</div><div id='author'> Author: rwightman@gmail.com</div><div id='file'> File Name: timm/models/swin_transformer_v2_cr.py</div><div id='m_class'> M Class Name: SwinTransformerV2CR</div><div id='n_method'> N Class Name: SwinTransformerV2Cr</div><div id='m_method'> M Method Name: __init__(19)</div><div id='n_method'> N Method Name: __init__(17)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: timm/models/swin_transformer_v2_cr.py</div><div id='n_file'> N File Name: timm/models/swin_transformer_v2_cr.py</div><div id='m_start'> M Start Line: 901</div><div id='m_end'> M End Line: 953</div><div id='n_start'> N Start Line: 611</div><div id='n_end'> N End Line: 672</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                &#47&#47 CoaT series: Aggregate features of last three scales for classification.
                assert embed_dims[1] == embed_dims[2] == embed_dims[3]
                self.aggregate = torch.nn.Conv1d(in_channels=3, out_channels=1, kernel_size=1)
                self.head = nn.Linear(<a id="change">embed_dims[3]</a>, num_classes)
            else:
                &#47&#47 CoaT-Lite series: Use feature of last scale for classification.
                self.head = nn.Linear(<a id="change">embed_dims[3]</a>, num_classes)

        &#47&#47 Initialize weights.
        trunc_normal_(self.cls_token1, std=.02)</code></pre><h3>After Change</h3><pre><code class='java'>
                &#47&#47 CoaT series: Aggregate features of last three scales for classification.
                assert embed_dims[1] == embed_dims[2] == embed_dims[3]
                self.aggregate = torch.nn.Conv1d(in_channels=3, out_channels=1, kernel_size=1)
                self.head = nn.Linear(self.num_features, num_classes) if num_classes &gt; 0 else <a id="change">nn.Identity()</a>
            else:
                &#47&#47 CoaT-Lite series: Use feature of last scale for classification.
                self.head = nn.Linear(self.num_features, num_classes) if num_classes &gt; 0 else nn.Identity()
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/feng-lab/pytorch-image-models/commit/8880f696b6b8368a76296126476ea020fc7c814c#diff-515038d09601bfcc6d6f3a526f98449b70f4b62595ddd47708355d5fa9891311L329' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 975183</div><div id='project'> Project Name: feng-lab/pytorch-image-models</div><div id='commit'> Commit Name: 8880f696b6b8368a76296126476ea020fc7c814c</div><div id='time'> Time: 2021-06-12</div><div id='author'> Author: rwightman@gmail.com</div><div id='file'> File Name: timm/models/coat.py</div><div id='m_class'> M Class Name: CoaT</div><div id='n_method'> N Class Name: CoaT</div><div id='m_method'> M Method Name: __init__(18)</div><div id='n_method'> N Method Name: __init__(18)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: timm/models/coat.py</div><div id='n_file'> N File Name: timm/models/coat.py</div><div id='m_start'> M Start Line: 444</div><div id='m_end'> M End Line: 447</div><div id='n_start'> N Start Line: 338</div><div id='n_end'> N End Line: 449</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            last_layer = num_neurons
        
        &#47&#47the last layer without activation
        sequential_layers.append(nn.Linear(last_layer, <a id="change">head_layers[-1]</a>))
        last_layer = <a id="change">head_layers[-1]</a>

        head = nn.Sequential(
            *sequential_layers</code></pre><h3>After Change</h3><pre><code class='java'>
        head = nn.Sequential(
            *sequential_layers
          )
        self.resnet18.fc = <a id="change">nn.Identity()</a>
        self.head = head
        self.out = nn.Linear(last_layer, 2)
    </code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/runinho/pytorch-cutpaste/commit/cf518cadf2892ffd9aac8ed414147d33e4ae7dce#diff-fada037ad086638e65c7ae77e3d223963e9afaa26326aab0ea718f4013176e43L8' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 975180</div><div id='project'> Project Name: runinho/pytorch-cutpaste</div><div id='commit'> Commit Name: cf518cadf2892ffd9aac8ed414147d33e4ae7dce</div><div id='time'> Time: 2021-05-10</div><div id='author'> Author: rune@frohn.cologne</div><div id='file'> File Name: model.py</div><div id='m_class'> M Class Name: ProjectionNet</div><div id='n_method'> N Class Name: ProjectionNet</div><div id='m_method'> M Method Name: __init__(3)</div><div id='n_method'> N Method Name: __init__(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: model.py</div><div id='n_file'> N File Name: model.py</div><div id='m_start'> M Start Line: 17</div><div id='m_end'> M End Line: 30</div><div id='n_start'> N Start Line: 17</div><div id='n_end'> N End Line: 32</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        super().__init__()
        self.output_tokens = output_tokens
        self.image_size = to_2tuple(image_size)
        <a id="change">self.patch_size</a> = to_2tuple(patch_size)
        self.grid_size = (self.image_size[0] // <a id="change">self.patch_size[0]</a>, self.image_size[1] // <a id="change">self.patch_size[1]</a>)
        self.output_dim = output_dim
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)
</code></pre><h3>After Change</h3><pre><code class='java'>
            self.patchnorm_pre_ln = LayerNorm(patch_input_dim)
            self.conv1 = nn.Linear(patch_input_dim, width)
        else:
            self.patchnorm_pre_ln = <a id="change">nn.Identity()</a>
            self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)

        &#47&#47 class embeddings and positional embeddings
        scale = width ** -0.5</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/mlfoundations/open_clip/commit/bd82c5e61542a9e160df383f96611896528eb9b7#diff-6ab3a5f5da1d07042589cbaeae3ff6e10b05897bf1684f2f9b2b688ae7ecee16L326' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 975181</div><div id='project'> Project Name: mlfoundations/open_clip</div><div id='commit'> Commit Name: bd82c5e61542a9e160df383f96611896528eb9b7</div><div id='time'> Time: 2023-02-06</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: src/open_clip/transformer.py</div><div id='m_class'> M Class Name: VisionTransformer</div><div id='n_method'> N Class Name: VisionTransformer</div><div id='m_method'> M Method Name: __init__(18)</div><div id='n_method'> N Method Name: __init__(17)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: src/open_clip/transformer.py</div><div id='n_file'> N File Name: src/open_clip/transformer.py</div><div id='m_start'> M Start Line: 347</div><div id='m_end'> M End Line: 351</div><div id='n_start'> N Start Line: 341</div><div id='n_end'> N End Line: 365</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.patch_embedding: nn.Module = PatchEmbedding(in_channels=in_chans, out_channels=embed_dim,
                                                         patch_size=patch_size, norm_layer=norm_layer)
        &#47&#47 Compute patch resolution
        patch_resolution: Tuple[int, int] = (<a id="change">img_size[0]</a> // patch_size, <a id="change">img_size[1]</a> // patch_size)
        &#47&#47 Path dropout dependent on depth
        drop_path_rate = torch.linspace(0., drop_path_rate, sum(depths)).tolist()
        &#47&#47 Init stages</code></pre><h3>After Change</h3><pre><code class='java'>

        self.global_pool: str = global_pool
        self.head: nn.Module = nn.Linear(
            in_features=self.num_features, out_features=num_classes) if num_classes else <a id="change">nn.Identity()</a>

        &#47&#47 FIXME weight init TBD, PyTorch default init appears to be working well,
        &#47&#47 but differs from usual ViT or Swin init.
        &#47&#47 named_apply(init_weights, self)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/feng-lab/pytorch-image-models/commit/c6e4b7895a7dbcd9b98396cbef383dd1c72b0ad3#diff-ed4e73b2f003b1bf594949d63e8066b203ff134a32665c27fe72e45e3a6607afL896' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 975173</div><div id='project'> Project Name: feng-lab/pytorch-image-models</div><div id='commit'> Commit Name: c6e4b7895a7dbcd9b98396cbef383dd1c72b0ad3</div><div id='time'> Time: 2022-02-23</div><div id='author'> Author: rwightman@gmail.com</div><div id='file'> File Name: timm/models/swin_transformer_v2_cr.py</div><div id='m_class'> M Class Name: SwinTransformerV2CR</div><div id='n_method'> N Class Name: SwinTransformerV2Cr</div><div id='m_method'> M Method Name: __init__(19)</div><div id='n_method'> N Method Name: __init__(17)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: timm/models/swin_transformer_v2_cr.py</div><div id='n_file'> N File Name: timm/models/swin_transformer_v2_cr.py</div><div id='m_start'> M Start Line: 901</div><div id='m_end'> M End Line: 953</div><div id='n_start'> N Start Line: 611</div><div id='n_end'> N End Line: 672</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            last_layer = num_neurons
        
        &#47&#47the last layer without activation
        sequential_layers.append(nn.Linear(last_layer, <a id="change">head_layers[-1]</a>))
        last_layer = <a id="change">head_layers[-1]</a>

        head = nn.Sequential(
            *sequential_layers</code></pre><h3>After Change</h3><pre><code class='java'>
        head = nn.Sequential(
            *sequential_layers
          )
        self.resnet18.fc = <a id="change">nn.Identity()</a>
        self.head = head
        self.out = nn.Linear(last_layer, 2)
    </code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/runinho/pytorch-cutpaste/commit/ef86853105065b720feb847fcfba99a875145fb2#diff-fada037ad086638e65c7ae77e3d223963e9afaa26326aab0ea718f4013176e43L8' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 975186</div><div id='project'> Project Name: runinho/pytorch-cutpaste</div><div id='commit'> Commit Name: ef86853105065b720feb847fcfba99a875145fb2</div><div id='time'> Time: 2021-05-10</div><div id='author'> Author: rune@frohn.cologne</div><div id='file'> File Name: model.py</div><div id='m_class'> M Class Name: ProjectionNet</div><div id='n_method'> N Class Name: ProjectionNet</div><div id='m_method'> M Method Name: __init__(3)</div><div id='n_method'> N Method Name: __init__(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: model.py</div><div id='n_file'> N File Name: model.py</div><div id='m_start'> M Start Line: 17</div><div id='m_end'> M End Line: 30</div><div id='n_start'> N Start Line: 17</div><div id='n_end'> N End Line: 32</div><BR>