{"BEFORE":"        self.fc1 = nn.Linear(z_dim, hidden_dim, bias = False)\n        self.bn1 = nn.BatchNorm1d(hidden_dim, affine = False, eps=1e-6, momentum = 0.5)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim, bias = False)\n        self.bn2 = nn.BatchNorm1d(hidden_dim, affine = False, eps=1e-6, momentum = 0.5)\n        self.fc3 = LinearWeightNorm(hidden_dim, input_dim, weight_scale = 1)\n        self.bn1_b = Parameter(torch.zeros(hidden_dim))\n        self.bn2_b = Parameter(torch.zeros(hidden_dim))\n        nn.init.xavier_uniform(self.fc1.weight)\n        nn.init.xavier_uniform(self.fc2.weight)\n","AFTER":"    def __init__(self, input_dim = 28 ** 2,hidden_dim=[500,500],activations=[nn.Softplus(),nn.Softplus(),nn.Softplus()],z_dim=100,device='cpu'):\n        super(Generator, self).__init__()\n        self.z_dim = z_dim\n        self.device=device\n        self.hidden_dim=hidden_dim\n        self.layers = torch.nn.ModuleList()\n        self.bn_layers=torch.nn.ModuleList()\n        self.bn_b = torch.nn.ParameterList()\n        self.num_hidden=len(hidden_dim)\n        self.activations=activations\n        for _ in range(self.num_hidden):\n            if _==0:\n                in_dim=z_dim\n            else:\n                in_dim=hidden_dim[_-1]\n            out_dim=hidden_dim[_]\n            fc=nn.Linear(in_dim, out_dim, bias=False)\n            nn.init.xavier_uniform(fc.weight)\n            self.layers.append(fc)\n            self.bn_layers.append(nn.BatchNorm1d(out_dim, affine = False, eps=1e-6, momentum = 0.5))\n            self.bn_b.append(Parameter(torch.zeros(out_dim)))\n        self.fc = LinearWeightNorm(hidden_dim[self.num_hidden-1], input_dim, weight_scale = 1)\n"}