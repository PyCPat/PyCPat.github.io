{"BEFORE":"                count = 0\n                for param in self.encoderModel.encoder.parameters():\n\n                    if count < frozen_layer_count:\n                        param.requires_grad = False\n                    count += 1\n\n            else:\n\n                print(\"Number of Layers: \" + str(len(list(self.encoderModel.encoder.parameters()))))\n","AFTER":"            self.encoderModel = model_encoding\n\n          elif model_choice == \"SEBIS\/code_trans_t5_large_source_code_summarization_python_multitask_finetune\":\n\n            model_encoding = AutoModel.from_pretrained(model_choice)\n            embedding_size = 1024\n            self.encoderModel = model_encoding\n\n          elif model_choice == \"roberta-large\":\n\n            model_encoding = AutoModel.from_pretrained(model_choice)\n            embedding_size = 1024\n            self.encoderModel = model_encoding\n\n          else:\n\n            model_encoding = AutoModel.from_pretrained(model_choice)\n            embedding_size = 768\n            self.encoderModel = model_encoding\n\n\n\n          if frozen == True:\n            print(\"Freezing the model parameters\")\n            for param in self.encoderModel.parameters():\n                param.requires_grad = False\n\n\n\n          if frozen_layer_count > 0:\n\n            if model_choice == \"t5-3b\":\n\n                print(\"Freezing T5-3b\")\n                print(\"Number of Layers: \" + str(len(self.encoderModel.encoder.block)))\n\n                for parameter in self.encoderModel.parameters():\n                    parameter.requires_grad = False\n\n                for i, m in enumerate(self.encoderModel.encoder.block):        \n                    #Only un-freeze the last n transformer blocks\n                    if i+1 > 24 - frozen_layer_count:\n                        print(str(i) + \" Layer\")\n                        for parameter in m.parameters():\n                            parameter.requires_grad = True\n\n            else:\n\n                print(\"Number of Layers: \" + str(len(list(self.encoderModel.encoder.layer))))\n"}