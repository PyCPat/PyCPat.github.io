{"BEFORE":"        self.quant_num = quant_num\r\n\r\n        self.feature_layer = nn.Sequential(\r\n            nn.Linear(self.observation_dim, 128),\r\n            nn.ReLU(),\r\n            nn.Linear(128, 128),\r\n            nn.ReLU()\r\n        )\r\n\r\n        self.phi = nn.Linear(1, 128, bias=False)\r\n        self.phi_bias = nn.Parameter(torch.zeros(128), requires_grad=True)\r\n\r\n        self.psi_layer = nn.Sequential(\r\n            nn.Linear(128, 128),\r\n            nn.ReLU(),\r\n            nn.Linear(128, self.action_dim)\r\n        )\r\n\r\n        self.quantile_fraction_layer = nn.Sequential(\r\n            nn.Linear(128, 128),\r\n            nn.ReLU(),\r\n            nn.Linear(128, self.quant_num),\r\n            nn.Softmax(dim=-1)\r\n        )\r\n","AFTER":"    def __init__(self, observation_dim, action_dim, quant_num, cosine_num):\r\n        super(fqf_net, self).__init__()\r\n        self.observation_dim = observation_dim\r\n        self.action_dim = action_dim\r\n        self.quant_num = quant_num\r\n        self.cosine_num = cosine_num\r\n\r\n        self.feature_layer = nn.Sequential(\r\n            nn.Linear(self.observation_dim, 128),\r\n            nn.ReLU(),\r\n            nn.Linear(128, 128)\r\n        )\r\n\r\n        self.cosine_layer = nn.Sequential(\r\n            nn.Linear(self.cosine_num, 128),\r\n            nn.ReLU()\r\n        )\r\n\r\n        self.psi_layer = nn.Sequential(\r\n            nn.Linear(128, 128),\r\n            nn.ReLU(),\r\n            nn.Linear(128, self.action_dim)\r\n        )\r\n\r\n        self.quantile_fraction_layer = nn.Sequential(\r\n            nn.Linear(128, self.quant_num),\r\n            nn.Softmax(dim=-1)\r\n        )\r\n"}