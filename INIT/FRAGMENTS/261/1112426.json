{"BEFORE":"    def __init__(self, in_dim, action_dim, hidden_dim, num_layers_linear_hidden, log_std_min=-20,\n                 log_std_max=2):\n        super(Policy, self).__init__()\n\n        assert len(in_dim) == 1\n        assert len(action_dim) == 1\n\n        in_dim = np.product(in_dim)\n        action_dim = np.product(action_dim)\n\n        # device is initialized by agents Class\n        self.operators = nn.ModuleList([\n            Flatten(),\n            nn.Linear(in_dim, hidden_dim),\n            nn.ReLU()\n        ])\n\n        for l in range(num_layers_linear_hidden - 1):\n            self.operators.append(nn.Linear(hidden_dim, hidden_dim))\n            self.operators.append(nn.ReLU())\n            # self.operators.append(nn.Dropout(0.2))\n\n        self.operators.append(nn.Linear(hidden_dim, 2 * action_dim))\n","AFTER":"        assert policy_structure[0][1] is not None\n        prev_object = policy_structure.pop(0)\n\n        self.operators = nn.ModuleList([\n            Flatten(),\n            nn.Linear(in_dim, prev_object[1]),\n        ])\n\n        for layer, argument in policy_structure[:-1]:\n            if layer == 'linear':\n                self.operators.append(nn.Linear(prev_object[1], argument))\n                prev_object = (layer, argument)\n            elif layer == 'relu':\n                assert argument is None, 'No argument for ReLU please'\n                self.operators.append(nn.ReLU())\n            elif layer == 'dropout':\n                self.operators.append(nn.Dropout(argument))\n            else:\n                raise NotImplementedError(f'{layer} not known')\n\n        self.operators.append(nn.Linear(prev_object[1], 2 * action_dim))\n"}