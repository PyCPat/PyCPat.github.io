{"BEFORE":"        super(conv, self).__init__()\n\n        # Logger setup\n        self.logger = logger\n\n        # Here are summarized the expected options for this class\n        self.expected_options = {\n            \"class_name\": (\"str\", \"mandatory\"),\n            \"recovery\": (\"bool\", \"optional\", \"True\"),\n            \"initialize_with\": (\"str\", \"optional\", \"None\"),\n            \"out_channels\": (\"int(1,inf)\", \"mandatory\"),\n            \"kernel_size\": (\"int_list(1,inf)\", \"mandatory\"),\n            \"stride\": (\"int_list(1,inf)\", \"optional\", \"1,1\"),\n            \"dilation\": (\"int_list(1,inf)\", \"optional\", \"1,1\"),\n            \"padding\": (\"int_list(0,inf)\", \"optional\", \"None\"),\n            \"groups\": (\"int(1,inf)\", \"optional\", \"1\"),\n            \"bias\": (\"bool\", \"optional\", \"True\"),\n            \"padding_mode\": (\"one_of(zeros,circular)\", \"optional\", \"zeros\"),\n        }\n\n        # Check, cast, and expand the options\n        self.conf = check_opts(\n            self, self.expected_options, config, self.logger\n        )\n\n        # Definition of the expected input\n        self.expected_inputs = [\"torch.Tensor\"]\n\n        # Check the first input\n        check_inputs(\n            self.conf, self.expected_inputs, first_input, logger=self.logger\n        )\n\n        # Output folder (useful for parameter saving)\n        if global_config is not None:\n            self.output_folder = global_config[\"output_folder\"]\n        self.funct_name = funct_name\n        self.reshape_conv1d = False\n        self.reshape_conv2d = False\n        self.squeeze_conv2d = False\n        self.transp_conv2d = False\n\n        # Making sure that the kernel size is odd (if the kernel is not\n        # symmetric there could a problem with the padding function)\n        for size in self.kernel_size:\n            if size % 2 == 0:\n                raise ValueError(\n                    \"The field kernel size must be an odd number. Got %s.\"\n                    % (self.kernel_size)\n                )\n\n        # Checking if 1d or 2d is specified\n        self.conv1d = False\n        self.conv2d = False\n\n        if len(self.kernel_size) == 1:\n            self.conv1d = True\n\n        if len(self.kernel_size) == 2:\n            self.conv2d = True\n\n        # Additional check on the input shapes\n        if first_input is not None:\n\n            # Shape check\n            if len(first_input[0].shape) > 5 or len(first_input[0].shape) < 2:\n\n                err_msg = (\n                    'The input of \"linear\" must be a tensor with one of the  '\n                    \"following dimensions: [time] or [batch,time] or \"\n                    \"[batch,channels,time]. Got %s \"\n                    % (str(first_input[0].shape))\n                )\n\n                logger_write(err_msg, logfile=logger)\n\n            # Manage reshaping flags\n            if len(first_input[0].shape) > 3:\n                if self.conv1d:\n                    self.reshape_conv1d = True\n\n            if len(first_input[0].shape) > 4:\n                if self.conv2d:\n                    self.reshape_conv2d = True\n\n            if len(first_input[0].shape) == 3 and self.conv2d:\n                self.squeeze_conv2d = True\n\n            if len(first_input[0].shape) >= 4 and self.conv2d:\n                self.transp_conv2d = True\n\n            # Detecting the number of input channels\n            if self.conv1d:\n                self.in_channels = first_input[0].shape[1]\n\n            if self.conv2d:\n                if len(first_input[0].shape) == 3:\n                    self.in_channels = 1\n\n                elif len(first_input[0].shape) == 4:\n\n                    self.in_channels = first_input[0].shape[2]\n                elif len(first_input[0].shape) == 5:\n                    self.in_channels = (\n                        first_input[0].shape[2] * first_input[0].shape[3]\n                    )\n\n        # Managing 1d convolutions\n        if self.conv1d:\n\n            if self.padding is not None:\n                self.padding = self.padding[0]\n\n            # Initialization of the parameters\n            self.conv = nn.Conv1d(\n                self.in_channels,\n                self.out_channels,\n                self.kernel_size[0],\n                stride=self.stride[0],\n                dilation=self.dilation[0],\n                padding=0,\n                groups=self.groups,\n                bias=self.bias,\n                padding_mode=self.padding_mode,\n            )\n\n        # Managing 2d convolutions\n        if self.conv2d:\n\n            if self.padding is not None:\n                self.padding = self.padding[0:-1]\n\n            # Initialization of the parameters\n            self.conv = nn.Conv2d(\n                self.in_channels,\n                self.out_channels,\n                tuple(self.kernel_size),\n                stride=tuple(self.stride),\n                padding=0,\n                dilation=tuple(self.dilation),\n                groups=self.groups,\n                bias=self.bias,\n                padding_mode=self.padding_mode,\n            )\n\n        # Managing initialization with an external model\n        # (useful for pre-training)\n        initialize_with(self)\n\n        # Automatic recovery\n        if global_config is not None:\n            recovery(self)\n\n    def forward(self, input_lst):\n","AFTER":"        bias=True,\n        padding_mode='zeros',\n        output_folder=None,\n        do_recovery=True,\n        initialize_from=None,\n    ):\n        super().__init__()\n\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.dilation = dilation\n        self.padding = padding\n        self.groups = groups\n        self.bias = bias\n        self.padding_mode = padding_mode\n        self.output_folder = output_folder\n        self.recovery = do_recovery\n        self.initialize_with = initialize_from\n\n        self.reshape_conv1d = False\n        self.reshape_conv2d = False\n        self.squeeze_conv2d = False\n        self.transp_conv2d = False\n\n        # Ensure kernel_size and padding are lists\n        if not isinstance(self.kernel_size, list):\n            self.kernel_size = [self.kernel_size]\n        if self.padding is not None and not isinstance(self.padding, list):\n            self.padding = [self.padding]\n\n        # Making sure that the kernel size is odd (if the kernel is not\n        # symmetric there could a problem with the padding function)\n        for size in self.kernel_size:\n            if size % 2 == 0:\n                raise ValueError(\n                    \"The field kernel size must be an odd number. Got %s.\"\n                    % (self.kernel_size)\n                )\n\n        # Checking if 1d or 2d is specified\n        self.conv1d = False\n        self.conv2d = False\n\n        if len(self.kernel_size) == 1:\n            self.conv1d = True\n\n        if len(self.kernel_size) == 2:\n            self.conv2d = True\n\n        def hook(self, first_input):\n\n            # Manage reshaping flags\n            if len(first_input[0].shape) > 3:\n                if self.conv1d:\n                    self.reshape_conv1d = True\n\n            if len(first_input[0].shape) > 4:\n                if self.conv2d:\n                    self.reshape_conv2d = True\n\n            if len(first_input[0].shape) == 3 and self.conv2d:\n                self.squeeze_conv2d = True\n\n            if len(first_input[0].shape) >= 4 and self.conv2d:\n                self.transp_conv2d = True\n\n            # Detecting the number of input channels\n            if self.conv1d:\n                self.in_channels = first_input[0].shape[1]\n\n            if self.conv2d:\n                if len(first_input[0].shape) == 3:\n                    self.in_channels = 1\n\n                elif len(first_input[0].shape) == 4:\n\n                    self.in_channels = first_input[0].shape[2]\n                elif len(first_input[0].shape) == 5:\n                    self.in_channels = (\n                        first_input[0].shape[2] * first_input[0].shape[3]\n                    )\n\n            # Managing 1d convolutions\n            if self.conv1d:\n\n                if self.padding is not None:\n                    self.padding = self.padding[0]\n\n                # Initialization of the parameters\n                self.conv = nn.Conv1d(\n                    self.in_channels,\n                    self.out_channels,\n                    self.kernel_size[0],\n                    stride=self.stride[0],\n                    dilation=self.dilation[0],\n                    padding=0,\n                    groups=self.groups,\n                    bias=self.bias,\n                    padding_mode=self.padding_mode,\n                ).to(first_input[0].device)\n\n            # Managing 2d convolutions\n            if self.conv2d:\n\n                if self.padding is not None:\n                    self.padding = self.padding[0:-1]\n\n                # Initialization of the parameters\n                self.conv = nn.Conv2d(\n                    self.in_channels,\n                    self.out_channels,\n                    tuple(self.kernel_size),\n                    stride=tuple(self.stride),\n                    padding=0,\n                    dilation=tuple(self.dilation),\n                    groups=self.groups,\n                    bias=self.bias,\n                    padding_mode=self.padding_mode,\n                ).to(first_input[0].device)\n\n            # Managing initialization with an external model\n            # (useful for pre-training)\n            initialize_with(self)\n\n            # Automatic recovery\n            # recovery(self)\n\n            self.hook.remove()\n        self.hook = self.register_forward_pre_hook(hook)\n"}