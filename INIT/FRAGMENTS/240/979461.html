<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        betas = beta_schedule_fn(timesteps, **schedule_fn_kwargs)

        alphas = 1. - betas
        <a id="change">alphas_cumprod</a> = torch.cumprod(alphas, dim=0)
        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)

        timesteps, = betas.shape
        self.num_timesteps = int(timesteps)
        self.loss_type = loss_type

        &#47&#47 sampling related parameters

        self.sampling_timesteps = default(sampling_timesteps, timesteps) &#47&#47 default num sampling timesteps to number of timesteps at training

        assert self.sampling_timesteps &lt;= timesteps
        self.is_ddim_sampling = self.sampling_timesteps &lt; timesteps
        self.ddim_sampling_eta = ddim_sampling_eta

        &#47&#47 helper function to register buffer from float64 to float32

        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))

        register_buffer(&quotbetas&quot, betas)
        register_buffer(&quotalphas_cumprod&quot, alphas_cumprod)
        register_buffer(&quotalphas_cumprod_prev&quot, alphas_cumprod_prev)

        &#47&#47 calculations for diffusion q(x_t | x_{t-1}) and others

        register_buffer(&quotsqrt_alphas_cumprod&quot, torch.sqrt(alphas_cumprod))
        register_buffer(&quotsqrt_one_minus_alphas_cumprod&quot, torch.sqrt(1. - alphas_cumprod))
        register_buffer(&quotlog_one_minus_alphas_cumprod&quot, torch.log(1. - alphas_cumprod))
        register_buffer(&quotsqrt_recip_alphas_cumprod&quot, torch.sqrt(1. / alphas_cumprod))
        register_buffer(&quotsqrt_recipm1_alphas_cumprod&quot, torch.sqrt(1. / alphas_cumprod - 1))

        &#47&#47 calculations for posterior q(x_{t-1} | x_t, x_0)

        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)

        &#47&#47 above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)

        register_buffer(&quotposterior_variance&quot, posterior_variance)

        &#47&#47 below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain

        register_buffer(&quotposterior_log_variance_clipped&quot, torch.log(posterior_variance.clamp(min =1e-20)))
        register_buffer(&quotposterior_mean_coef1&quot, betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))
        register_buffer(&quotposterior_mean_coef2&quot, (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))

        &#47&#47 calculate p2 reweighting

        <a id="change">register_buffer(&quotp2_loss_weight&quot</a>, (p2_loss_weight_k<a id="change"> + </a>alphas_cumprod<a id="change"> / </a>(1<a id="change"> - </a>alphas_cumprod))<a id="change"> ** -</a>p2_loss_weight_gamma<a id="change">)</a>

        &#47&#47 auto-normalization of data [0, 1] -&gt; [-1, 1] - can turn off by setting it to be False

        self.normalize = normalize_to_neg_one_to_one if auto_normalize else identity</code></pre><h3>After Change</h3><pre><code class='java'>
        betas = beta_schedule_fn(timesteps, **schedule_fn_kwargs)

        alphas = 1. - betas
        <a id="change">alphas_cumprod</a> = torch.cumprod(alphas, dim=0)
        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)

        timesteps, = betas.shape
        self.num_timesteps = int(timesteps)
        self.loss_type = loss_type

        &#47&#47 sampling related parameters

        self.sampling_timesteps = default(sampling_timesteps, timesteps) &#47&#47 default num sampling timesteps to number of timesteps at training

        assert self.sampling_timesteps &lt;= timesteps
        self.is_ddim_sampling = self.sampling_timesteps &lt; timesteps
        self.ddim_sampling_eta = ddim_sampling_eta

        &#47&#47 helper function to register buffer from float64 to float32

        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))

        register_buffer(&quotbetas&quot, betas)
        register_buffer(&quotalphas_cumprod&quot, alphas_cumprod)
        register_buffer(&quotalphas_cumprod_prev&quot, alphas_cumprod_prev)

        &#47&#47 calculations for diffusion q(x_t | x_{t-1}) and others

        register_buffer(&quotsqrt_alphas_cumprod&quot, torch.sqrt(alphas_cumprod))
        register_buffer(&quotsqrt_one_minus_alphas_cumprod&quot, torch.sqrt(1. - alphas_cumprod))
        register_buffer(&quotlog_one_minus_alphas_cumprod&quot, torch.log(1. - alphas_cumprod))
        register_buffer(&quotsqrt_recip_alphas_cumprod&quot, torch.sqrt(1. / alphas_cumprod))
        register_buffer(&quotsqrt_recipm1_alphas_cumprod&quot, torch.sqrt(1. / alphas_cumprod - 1))

        &#47&#47 calculations for posterior q(x_{t-1} | x_t, x_0)

        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)

        &#47&#47 above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)

        register_buffer(&quotposterior_variance&quot, posterior_variance)

        &#47&#47 below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain

        register_buffer(&quotposterior_log_variance_clipped&quot, torch.log(posterior_variance.clamp(min =1e-20)))
        register_buffer(&quotposterior_mean_coef1&quot, betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))
        register_buffer(&quotposterior_mean_coef2&quot, (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))

        &#47&#47 derive loss weight
        &#47&#47 snr - signal noise ratio

        <a id="change">snr</a><a id="change"> = </a>alphas_cumprod<a id="change"> / </a>(1<a id="change"> - </a>alphas_cumprod)

        &#47&#47 https://arxiv.org/abs/2303.09556

        <a id="change">maybe_clipped_snr = </a><a id="change">snr.clone()</a>
        <a id="change">if min_snr_loss_weight</a><a id="change">:
            </a><a id="change">maybe_clipped_snr.clamp_(min = min_snr_gamma)</a>

        <a id="change">if objective == &quotpred_noise&quot</a><a id="change">:
            </a>register_buffer(&quotloss_weight&quot, maybe_clipped_snr<a id="change"> / </a>snr)
        elif <a id="change">objective == &quotpred_x0&quot</a><a id="change">:
            </a>register_buffer(&quotloss_weight&quot, maybe_clipped_snr)
        elif <a id="change">objective == &quotpred_v&quot</a><a id="change">:
            </a>register_buffer(&quotloss_weight&quot, maybe_clipped_snr<a id="change"> / </a>(snr<a id="change"> + 1</a>))

        &#47&#47 auto-normalization of data [0, 1] -&gt; [-1, 1] - can turn off by setting it to be False
</code></pre>