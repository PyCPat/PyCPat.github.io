{"BEFORE":"        num_patches = (image_height \/\/ patch_size) * (image_width \/\/ patch_size)\n        patch_dim = channels * patch_size * patch_size\n        self.to_patch_embedding = nn.Sequential(\n            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n            nn.Linear(patch_dim, emb_dim),\n        )\n        #Embedding\n        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_dim))\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, emb_dim))\n\n        #Transformer\n        # encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=nhead, dropout=transformer_dropout, dim_feedforward=emb_dim*4)\n        # self.transformer_encoders = nn.TransformerEncoder(encoder_layer, num_layers=num_layers_encoder)\n        self.transformer_encoders = timm.create_model(\"vit_large_patch16_384\", pretrained=True)\n\n        #Register hooks\n        self.activation = {}\n        self.hooks = hooks\n        self._get_layers_from_hooks(self.hooks)\n\n        #Reassembles Fusion\n        self.reassembles = []\n        self.fusions = []\n        for s in reassemble_s:\n            self.reassembles.append(Reassemble(image_size, read, patch_size, s, emb_dim, resample_dim))\n            self.fusions.append(Fusion(resample_dim))\n        self.reassembles = nn.ModuleList(self.reassembles)\n        self.fusions = nn.ModuleList(self.fusions)\n\n        #Head\n        self.head_depth = HeadDepth(resample_dim)\n        self.head_segmentation = HeadSeg(resample_dim, nclasses=nclasses)\n","AFTER":"                 type               = \"full\",\n                 model_timm         = \"vit_large_patch16_384\"):\n        \"\"\"\n        Focus on Depth\n        type : {\"full\", \"depth\", \"segmentation\"}\n        image_size : (c, h, w)\n        patch_size : *a square*\n        emb_dim <=> D (in the paper)\n        resample_dim <=> ^D (in the paper)\n        read : {\"ignore\", \"add\", \"projection\"}\n        \"\"\"\n        super().__init__()\n\n        #Splitting img into patches\n        channels, image_height, image_width = image_size\n        assert image_height % patch_size == 0 and image_width % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n        num_patches = (image_height \/\/ patch_size) * (image_width \/\/ patch_size)\n        patch_dim = channels * patch_size * patch_size\n        # self.to_patch_embedding = nn.Sequential(\n        #     Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n        #     nn.Linear(patch_dim, emb_dim),\n        # )\n        # #Embedding\n        # self.cls_token = nn.Parameter(torch.randn(1, 1, emb_dim))\n        # self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, emb_dim))\n\n        #Transformer\n        # encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=nhead, dropout=transformer_dropout, dim_feedforward=emb_dim*4)\n        # self.transformer_encoders = nn.TransformerEncoder(encoder_layer, num_layers=num_layers_encoder)\n        self.transformer_encoders = timm.create_model(model_timm, pretrained=True)\n\n        #Register hooks\n        self.activation = {}\n        self.hooks = hooks\n        self._get_layers_from_hooks(self.hooks)\n\n        #Reassembles Fusion\n        self.reassembles = []\n        self.fusions = []\n        for s in reassemble_s:\n            self.reassembles.append(Reassemble(image_size, read, patch_size, s, emb_dim, resample_dim))\n            self.fusions.append(Fusion(resample_dim))\n        self.reassembles = nn.ModuleList(self.reassembles)\n        self.fusions = nn.ModuleList(self.fusions)\n\n        #Head\n        if type == \"full\":\n            self.head_depth = HeadDepth(resample_dim)\n            self.head_segmentation = HeadSeg(resample_dim, nclasses=nclasses)\n        elif type == \"depth\":\n            self.head_depth = HeadDepth(resample_dim)\n            self.head_segmentation = None\n        else:\n            self.head_depth = None\n            self.head_segmentation = HeadSeg(resample_dim, nclasses=nclasses)\n\n    def forward(self, img):\n"}