{"BEFORE":"        self.embed_size = embed_size\n        self.h_dict = h_dict\n        self.need_trans = need_trans\n        self.embed_dict = nn.ParameterDict()\n        linear_dict = {}\n        for ntype, n_nodes in self.n_nodes_dict.items():\n            h = h_dict.get(ntype)\n            if h is None:\n                if all_feats:\n                    embed = nn.Parameter(torch.FloatTensor(n_nodes, self.embed_size))\n                    # initrange = 1.0 \/ self.embed_size\n                    # nn.init.uniform_(embed, -initrange, initrange)\n                    nn.init.xavier_uniform_(embed, gain=nn.init.calculate_gain('relu'))\n                    self.embed_dict[ntype] = embed\n            else:\n                linear_dict[ntype] = [h.shape[1], self.embed_size]\n        if need_trans:\n            self.hetero_linear = HeteroLinearLayer(linear_dict, act=act)\n\n    def forward(self):\n","AFTER":"        self.type_node_num_sum = [0]\n        self.all_type = []\n        for ntype, type_num in n_nodes_dict.items():\n            num_now = self.type_node_num_sum[-1]\n            num_now += type_num\n            self.type_node_num_sum.append(num_now)\n            self.all_type.append(ntype)\n        self.type_node_num_sum = torch.tensor(self.type_node_num_sum)\n\n        linear_dict = {}\n        embed_dict = {}\n        for ntype, n_nodes in self.n_nodes_dict.items():\n            h = h_dict.get(ntype)\n            if h is None:\n                if all_feats:\n                    embed_dict[ntype] = n_nodes\n            else:\n                linear_dict[ntype] = h.shape[1]\n        self.embes = HeteroEmbedding(embed_dict, embed_size)\n        if need_trans:\n            self.linear = HeteroLinear(linear_dict, embed_size)\n        self.act = act  # activate\n"}