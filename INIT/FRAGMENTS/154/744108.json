{"BEFORE":"        if len(att_layers) != len(tc_layers) + 1:\n            raise RuntimeError(\"There must be one more attention layer than that of temporal convolution layers.\")\n        num_filters = int(m.ceil(m.log(seq_length, 2)))\n\n        self.layers = nn.ModuleDict()\n        channels = in_channels\n\n        for i in range(len(tc_layers)):\n            self.layers.add_module(\"attention_{}\".format(i),\n                                   AttentionBlock(channels, att_layers[i][0], att_layers[i][1], device))\n            channels += att_layers[i][1]\n            self.layers.add_module(\"tconv_{}\".format(i),\n                                   TCBlock(channels, seq_length, tc_layers[i], device))\n            channels += num_filters * tc_layers[i]\n\n        self.layers.add_module(\"attention_{}\".format(len(att_layers)-1),\n                               AttentionBlock(channels, att_layers[-1][0], att_layers[-1][1], device))\n        channels += att_layers[-1][1]\n\n        channels += additional_length\n        fc_layers = list(fc_layers) + [out_channels]\n        for i in range(len(fc_layers)):\n            self.layers.add_module(\"fc_{}\".format(i), nn.Linear(channels, fc_layers[i]).to(device))\n            channels = fc_layers[i]\n\n        self.activation = activation if activation is not None else lambda x: x\n        self.device = device\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.seq_length = seq_length\n        self.att_num = len(att_layers)\n","AFTER":"        channels = in_channels\n\n        for i in range(len(tc_layers)):\n            self.layers.add_module(\"tconv_{}\".format(i),\n                                   TCBlock(channels, seq_length, tc_layers[i], device))\n            channels += num_filters * tc_layers[i]\n\n        self.layers.add_module(\"attention\", AttentionBlock(channels, att_layer[0], att_layer[1], device))\n        channels += att_layer[1]\n\n        channels += additional_length\n        fc_layers = list(fc_layers) + [out_channels]\n\n        self.layers.add_module(\"fc_amalgamate\", nn.Linear(seq_length, 1).to(device))\n        for i in range(len(fc_layers)):\n            self.layers.add_module(\"fc_{}\".format(i), nn.Linear(channels, fc_layers[i]).to(device))\n            channels = fc_layers[i]\n\n        self.activation = activation if activation is not None else lambda x: x\n        self.device = device\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.seq_length = seq_length\n        self.tc_num = len(tc_layers)\n"}