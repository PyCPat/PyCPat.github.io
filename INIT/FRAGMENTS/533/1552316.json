{"BEFORE":"        self.activation = activation\n        self.activate_last = activate_last\n\n        if self.with_norm and self.with_bias:\n            warnings.warn('ConvModule has norm and bias at the same time')\n\n        self.conv = build_conv_layer(\n            conv_cfg,\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            dilation,\n            groups,\n            bias=bias)\n\n        self.in_channels = self.conv.in_channels\n        self.out_channels = self.conv.out_channels\n        self.kernel_size = self.conv.kernel_size\n        self.stride = self.conv.stride\n        self.padding = self.conv.padding\n        self.dilation = self.conv.dilation\n        self.transposed = self.conv.transposed\n        self.output_padding = self.conv.output_padding\n        self.groups = self.conv.groups\n\n        if self.with_norm:\n            norm_channels = out_channels if self.activate_last else in_channels\n            self.norm_name, norm = build_norm_layer(normalize, norm_channels)\n            self.add_module(self.norm_name, norm)\n\n        if self.with_activatation:\n            assert activation in ['relu'], 'Only ReLU supported.'\n","AFTER":"        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        self.activation = activation\n        self.inplace = inplace\n        self.activate_last = activate_last\n\n        self.with_norm = norm_cfg is not None\n        self.with_activatation = activation is not None\n        # if the conv layer is before a norm layer, bias is unnecessary.\n        if bias == 'auto':\n            bias = False if self.with_norm else True\n        self.with_bias = bias\n\n        if self.with_norm and self.with_bias:\n            warnings.warn('ConvModule has norm and bias at the same time')\n\n        # build convolution layer\n        self.conv = build_conv_layer(conv_cfg,\n                                     in_channels,\n                                     out_channels,\n                                     kernel_size,\n                                     stride=stride,\n                                     padding=padding,\n                                     dilation=dilation,\n                                     groups=groups,\n                                     bias=bias)\n        # export the attributes of self.conv to a higher level for convenience\n        self.in_channels = self.conv.in_channels\n        self.out_channels = self.conv.out_channels\n        self.kernel_size = self.conv.kernel_size\n        self.stride = self.conv.stride\n        self.padding = self.conv.padding\n        self.dilation = self.conv.dilation\n        self.transposed = self.conv.transposed\n        self.output_padding = self.conv.output_padding\n        self.groups = self.conv.groups\n\n        # build normalization layers\n        if self.with_norm:\n            norm_channels = out_channels if self.activate_last else in_channels\n            self.norm_name, norm = build_norm_layer(norm_cfg, norm_channels)\n            self.add_module(self.norm_name, norm)\n\n        # build activation layer\n        if self.with_activatation:\n            if self.activation not in ['relu']:\n                raise ValueError('{} is currently not supported.'.format(\n                    self.activation))\n            if self.activation == 'relu':\n"}