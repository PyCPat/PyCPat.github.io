{"BEFORE":"        super(DeltaNetAtomic, self).__init__()\n\n        self.pos_dim = 3\n        self.m_dim = m_dim\n        self.embedding_dim = embedding_dim\n        self.n_kernels = n_kernels\n        self.n_mlp = n_mlp\n        self.mlp_dim = mlp_dim\n        self.n_outputs = n_outputs\n        self.initialize_weights = initialize_weights\n        self.fourier_features = fourier_features\n        self.aggr = aggr\n\n        # Embedding\n        self.embedding = nn.Embedding(\n            num_embeddings=11, embedding_dim=self.embedding_dim\n        )\n\n        # Kernel\n        self.kernel_dim = self.embedding_dim\n        self.kernels = nn.ModuleList()\n        for _ in range(self.n_kernels):\n            self.kernels.append(\n                EGNN_sparse(\n                    feats_dim=self.kernel_dim,\n                    pos_dim=self.pos_dim,\n                    m_dim=self.m_dim,\n                    fourier_features=self.fourier_features,\n                    aggr=self.aggr,\n                )\n            )\n\n        # MLP\n        self.fnn = nn.ModuleList()\n        input_fnn = self.kernel_dim * (self.n_kernels + 1)\n        self.fnn.append(nn.Linear(input_fnn, mlp_dim))\n        for _ in range(self.n_mlp):\n            self.fnn.append(nn.Linear(self.mlp_dim, self.mlp_dim))\n        self.fnn.append(nn.Linear(self.mlp_dim, self.n_outputs))\n","AFTER":"        global_prop=True):\n        \"\"\"Main Equivariant Graph Neural Network class.\n\n        Parameters\n        ----------\n        embedding_dim : int, optional\n            Embedding dimension, by default 128\n        n_kernels : int, optional\n            Number of message-passing rounds, by default 5\n        n_mlp : int, optional\n            Number of node-level and global-level MLPs, by default 3\n        mlp_dim : int, optional\n            Hidden size of the node-level and global-level MLPs, by default 256\n        n_outputs : int, optional\n            Number of endpoints to predict, by default 1\n        m_dim : int, optional\n            Node-level hidden size, by default 32\n        initialize_weights : bool, optional\n            Whether to use Xavier init. for learnable weights, by default True\n        fourier_features : int, optional\n            Number of Fourier features to use, by default 32\n        aggr : str, optional\n            Aggregation strategy for global tasks, by default \"mean\"\n        global_prop : bool, optional\n            Whether to predict a molecule-level property or an atomic one, by default True\n        \"\"\"\n        super(EGNN, self).__init__()\n\n        self.pos_dim = 3\n        self.m_dim = m_dim\n        self.embedding_dim = embedding_dim\n        self.n_kernels = n_kernels\n        self.n_mlp = n_mlp\n        self.mlp_dim = mlp_dim\n        self.n_outputs = n_outputs\n        self.initialize_weights = initialize_weights\n        self.fourier_features = fourier_features\n        self.aggr = aggr\n        self.global_prop = global_prop\n\n        # Embedding\n        self.embedding = nn.Embedding(\n            num_embeddings=11, embedding_dim=self.embedding_dim\n        )\n\n        # Kernel\n        self.kernel_dim = self.embedding_dim\n        self.kernels = nn.ModuleList()\n        for _ in range(self.n_kernels):\n            self.kernels.append(\n                EGNN_sparse(\n                    feats_dim=self.kernel_dim,\n                    pos_dim=self.pos_dim,\n                    m_dim=self.m_dim,\n                    fourier_features=self.fourier_features,\n                    aggr=self.aggr,\n                )\n            )\n\n        # MLP 1\n        self.fnn = nn.ModuleList()\n        input_fnn = self.kernel_dim * (self.n_kernels + 1)\n        self.fnn.append(nn.Linear(input_fnn, mlp_dim))\n        for _ in range(self.n_mlp - 1):\n            self.fnn.append(nn.Linear(self.mlp_dim, self.mlp_dim))\n\n        if self.global_prop:\n            # MLP 2\n            self.fnn2 = nn.ModuleList()\n            for _ in range(self.n_mlp - 1):\n                self.fnn2.append(nn.Linear(self.mlp_dim, self.mlp_dim))\n            self.fnn2.append(nn.Linear(self.mlp_dim, self.n_outputs))\n\n        else:\n            self.fnn.append(nn.Linear(self.mlp_dim, self.mlp_dim))  ## This line differs in both implementations!!!! TODO @kenatz\n            self.fnn.append(nn.Linear(self.mlp_dim, self.n_outputs))\n\n        # Initialize weights\n        if self.initialize_weights:\n            self.kernels.apply(weights_init)\n            self.fnn.apply(weights_init)\n            if self.global_prop:\n                self.fnn2.apply(weights_init)\n            nn.init.xavier_uniform_(self.embedding.weight)\n"}