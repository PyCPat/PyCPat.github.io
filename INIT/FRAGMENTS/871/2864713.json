{"BEFORE":"        obj_self_attn=False\n    ):\n        super().__init__()\n        self.attn_masks = attn_masks\n        self.obj_self_attn = obj_self_attn\n\n        # cross attention\n        self.cross_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n        self.dropout1 = nn.Dropout(dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n\n        # self attention\n        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.norm2 = nn.LayerNorm(d_model)\n\n        # obj self attention\n        if obj_self_attn:\n            self.self_attn_obj = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n            self.dropout_obj = nn.Dropout(dropout)\n            self.norm_obj = nn.LayerNorm(d_model)\n            self.obj_attn_mask = self.generate_obj_attn_mask()\n\n        # ffn\n        self.linear1 = nn.Linear(d_model, d_ffn)\n","AFTER":"        obj_self_attn=False,\n        config=None,\n        bbox_props=None,\n        anchors=None\n    ):\n        super().__init__()\n        self.config = config\n        self.obj_self_attn = obj_self_attn\n        self.bbox_props= bbox_props\n        self.anchors = anchors\n\n        self.shapes = {\n            'P0': [160, 160, 256],\n            'P1': [80, 80, 128],\n            'P2': [40, 40, 64],\n            'P3': [20, 20, 32],\n            'P4': [10, 10, 16],\n            'P5': [5, 5, 8]\n        }\n"}