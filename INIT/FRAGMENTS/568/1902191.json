{"BEFORE":"    def __init__(self, vocab_size, input_dim, cutoff, dropout, half_size=False):\n        super().__init__()\n\n        if vocab_size > cutoff[-1]:\n            cutoff = cutoff + [vocab_size]\n        else:\n            assert vocab_size == cutoff[\n                -1], 'cannot specify cutoff smaller than vocab size'\n\n        output_dim = cutoff[0] + len(cutoff) - 1\n\n        self.vocab_size = vocab_size\n        self.cutoff = cutoff\n        self.dropout = dropout\n\n        self.lsm = nn.LogSoftmax(dim=1)\n        self.head = nn.Linear(input_dim, output_dim, bias=False)\n        self.tail = nn.ModuleList()\n\n        extra_denom = 1 if half_size else 0\n\n        for i in range(len(cutoff) - 1):\n            self.tail.append(\n                nn.Sequential(\n                    nn.Linear(input_dim, input_dim \/\/ 4 ** (i + extra_denom), bias=False),\n                    nn.Dropout(dropout),\n                    nn.Linear(input_dim \/\/ 4 ** (i + extra_denom), cutoff[i + 1] - cutoff[i], bias=False)\n                )\n            )\n\n        def init_weights(m):\n","AFTER":"        self.input_dim = input_dim\n\n        self.lsm = nn.LogSoftmax(dim=1)\n        self.head = nn.Linear(input_dim, output_dim, bias=False)\n        self._make_tail(True)\n"}