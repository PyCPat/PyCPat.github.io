{"BEFORE":"    def __init__(self,\n                 in_features,\n                 out_features,\n                 hidden_features,\n                 num_heads,\n                 activation=F.leaky_relu,\n                 layer_norm=False):\n\n        super(GAT, self).__init__()\n        self.layers = nn.ModuleList()\n        if layer_norm:\n            self.layers.append(nn.LayerNorm(in_features))\n        self.layers.append(GATConv(in_features, hidden_features[0], num_heads, activation=activation))\n        for i in range(len(hidden_features) - 1):\n            if layer_norm:\n                self.layers.append(nn.LayerNorm(hidden_features[i] * num_heads))\n            self.layers.append(\n                GATConv(hidden_features[i] * num_heads, hidden_features[i + 1], num_heads, activation=activation))\n        self.layers.append(GATConv(hidden_features[-1] * num_heads, num_heads, out_features))\n","AFTER":"    def __init__(self,\n                 in_features,\n                 out_features,\n                 hidden_features,\n                 num_heads,\n                 activation=F.leaky_relu,\n                 layer_norm=False,\n                 feat_norm=None,\n                 adj_norm_func=GCNAdjNorm,\n                 feat_dropout=0.0,\n                 attn_dropout=0.0,\n                 residual=False,\n                 dropout=0.0):\n        super(GAT, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.feat_norm = feat_norm\n        self.adj_norm_func = adj_norm_func\n        if type(hidden_features) is int:\n            hidden_features = [hidden_features]\n\n        self.layers = nn.ModuleList()\n        if layer_norm:\n            self.layers.append(nn.LayerNorm(in_features))\n        self.layers.append(GATConv(in_feats=in_features,\n                                   out_feats=hidden_features[0],\n                                   num_heads=num_heads,\n                                   feat_drop=feat_dropout,\n                                   attn_drop=attn_dropout,\n                                   residual=residual,\n                                   activation=activation))\n        for i in range(len(hidden_features) - 1):\n            if layer_norm:\n                self.layers.append(nn.LayerNorm(hidden_features[i] * num_heads))\n            self.layers.append(GATConv(in_feats=hidden_features[i] * num_heads,\n                                       out_feats=hidden_features[i + 1],\n                                       num_heads=num_heads,\n                                       feat_drop=feat_dropout,\n                                       attn_drop=attn_dropout,\n                                       residual=residual,\n                                       activation=activation))\n        self.layers.append(GATConv(in_feats=hidden_features[-1] * num_heads,\n                                   out_feats=out_features,\n                                   num_heads=num_heads,\n                                   feat_drop=0.0,\n                                   attn_drop=0.0,\n                                   residual=False,\n                                   activation=None))\n        if dropout > 0.0:\n            self.dropout = nn.Dropout(dropout)\n        else:\n            self.dropout = None\n\n    @property\n"}