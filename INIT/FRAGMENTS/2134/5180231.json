{"BEFORE":"        self.in_dim = np.product(in_dim)\n        self.action_dim = np.product(action_dim)\n        self.hidden_dim = hidden_dim\n\n        # first position should be a layer, linear layer for now\n        assert policy_structure[0][1] is not None\n        prev_object = policy_structure.pop(0)\n\n        self.operators = nn.ModuleList([\n            Flatten(),\n            nn.Linear(in_dim, prev_object[1]),\n        ])\n\n        for layer, argument in policy_structure[:-1]:\n            if layer == 'linear':\n                self.operators.append(nn.Linear(prev_object[1], argument))\n                prev_object = (layer, argument)\n            elif layer == 'relu':\n                assert argument is None, 'No argument for ReLU please'\n                self.operators.append(nn.ReLU())\n            elif layer == 'dropout':\n                self.operators.append(nn.Dropout(argument))\n            else:\n                raise NotImplementedError(f'{layer} not known')\n\n        self.operators.append(nn.Linear(prev_object[1], 2 * action_dim))\n","AFTER":"        in_dim = np.product(in_dim)\n        action_dim = np.product(action_dim)\n\n        self.operators = nn.ModuleList([\n            Flatten()\n        ])\n\n        current_layer_size = in_dim\n\n        for layer, params in network_structure:\n            if layer == 'linear':\n                self.operators.append(nn.Linear(current_layer_size, params))\n                current_layer_size = params\n            elif layer == 'relu':\n                assert params is None, 'No argument for ReLU please'\n                self.operators.append(nn.ReLU())\n            elif layer == 'dropout':\n                self.operators.append(nn.Dropout(params))\n            else:\n                raise NotImplementedError(f'{layer} not known')\n\n        self.operators.append(nn.Linear(current_layer_size, 2 * action_dim))\n"}