{"BEFORE":"                 last_channels=(2048, 256),\n                 ):\n        super().__init__()\n        assert num_layers % num_stacks == 0\n        num_layers_per_stack = num_layers \/\/ num_stacks\n        self.l_diff = num_stacks * (2**num_layers_per_stack - 1)\n\n        self.first_conv = nn.Conv1d(in_channels, residual_channels, 3, padding=1, bias=bias)\n\n        self.conv_layers = nn.ModuleList()\n        for n_layer in range(num_layers):\n            dilation = 2**(n_layer % num_layers_per_stack)\n            conv = ResidualConv1dGLU(\n                residual_channels, gate_channels,\n                skip_out_channels=skip_out_channels,\n                kernel_size=kernel_size,\n                bias=bias,\n                dilation=dilation,\n                dropout=1 - 0.95,\n            )\n            self.conv_layers.append(conv)\n\n        self.last_conv_layers = nn.Sequential(\n            nn.ReLU(True),\n            nn.Conv1d(skip_out_channels, last_channels[0], 3, padding=1, bias=bias),\n            nn.ReLU(True),\n            nn.Conv1d(last_channels[0], last_channels[1], 3, padding=1, bias=bias),\n            nn.Conv1d(last_channels[1], out_channels, 1, bias=True)\n","AFTER":"            nn.Conv1d(skip_out_channels, skip_out_channels, 1, bias=True),\n            nn.ReLU(True),\n            nn.Conv1d(skip_out_channels, out_channels, 1, bias=True),\n"}