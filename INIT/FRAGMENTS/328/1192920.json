{"BEFORE":"                PreNorm(dim, attn_class(dim, causal = causal, seq_len = seq_len, heads = heads, dim_head = dim_head, dropout = attn_dropout)),\n                PreNorm(dim, FeedForward(dim, mult = ff_mult, dropout = ff_dropout))\n","AFTER":"        for ind, sparse_attn, attn_type in zip(range(depth), sparse_layer, attn_type_layer):\n            if attn_type == 'full':\n                attn_class = Attention\n            elif attn_type == 'sparse':\n                attn_class = SparseAttention\n            elif attn_type == 'axial_row':\n                attn_class = partial(SparseAxialCausalAttention, seq_len = seq_len, axis = 0, image_size = image_fmap_size)\n            elif attn_type == 'axial_col':\n                attn_class = partial(SparseAxialCausalAttention, seq_len = seq_len, axis = 1, image_size = image_fmap_size)\n            elif attn_type == 'conv_like':\n                attn_class = partial(SparseConvCausalAttention, seq_len = seq_len, image_size = image_fmap_size)\n            else:\n                raise ValueError(f'attention type \"{attn_type}\" is not valid')\n\n            layers.append(nn.ModuleList([\n                LayerScale(dim, ind + 1, PreNorm(dim, attn_class(dim, causal = causal, seq_len = seq_len, heads = heads, dim_head = dim_head, dropout = attn_dropout))),\n                LayerScale(dim, ind + 1, PreNorm(dim, FeedForward(dim, mult = ff_mult, dropout = ff_dropout)))\n"}