{"BEFORE":"        self.drop = nn.Dropout(dropout)\n        position = torch.arange(0., max_len).unsqueeze(1)\n        div_term = 1 \/ (10000 ** (torch.arange(0., n_hid * 2, 2.)) \/ n_hid \/ 2)\n        self.emb = nn.Embedding(max_len, n_hid * 2)\n        self.emb.weight.data[:, 0::2] = torch.sin(position * div_term) \/ math.sqrt(n_hid)\n        self.emb.weight.data[:, 1::2] = torch.cos(position * div_term) \/ math.sqrt(n_hid)\n        self.emb.requires_grad = False\n        self.lin = nn.Linear(n_hid * 2, n_hid)\n","AFTER":"        div_term = torch.exp(torch.arange(0, n_hid, 2) *\n                             -(math.log(10000.0) \/ n_hid))\n        emb = nn.Embedding(max_len, n_hid)\n        emb.weight.data[:, 0::2] = torch.sin(position * div_term) \/ math.sqrt(n_hid)\n        emb.weight.data[:, 1::2] = torch.cos(position * div_term) \/ math.sqrt(n_hid)\n        emb.requires_grad = False\n        self.emb = emb\n        self.lin = nn.Linear(n_hid, n_hid)\n"}