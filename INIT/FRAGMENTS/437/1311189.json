{"BEFORE":"        input_channels: int,\n        hidden_channels: int,\n        kernel_size: int,\n        stride: int = 1,\n        padding: int = 0,\n        recurrent_kernel_size: int = 3,\n    ):\n        \"\"\"\n        One-Dimensional Convolutional Gated Recurrent Unit (ConvGRU1D) cell.\n\n        The input-to-hidden convolution kernel can be defined arbitrarily using\n        the kernel_size, stride and padding parameters. The hidden-to-hidden\n        convolution kernel is forced to be unit-stride, with a padding assuming\n        an odd kernel size, in order to keep the number of features the same.\n\n        The hidden state is initialized by default to a zero tensor of the\n        appropriate shape.\n\n        Arguments:\n            input_channels {int} -- [Number of channels of the input tensor]\n            hidden_channels {int} -- [Number of channels of the hidden state]\n            kernel_size {int} -- [Size of the input-to-hidden convolving kernel]\n\n        Keyword Arguments:\n            stride {int} -- [Stride of the input-to-hidden convolution]\n                             (default: {1})\n            padding {int} -- [Zero-padding added to both sides of the input]\n                              (default: {0})\n            recurrent_kernel_size {int} -- [Size of the hidden-to-hidden\n                                            convolving kernel] (default: {3})\n        \"\"\"\n        super(ConvGRU1DCell, self).__init__()\n        # ----------------------------------------------------------------------\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.h_channels = hidden_channels\n        self.padding_ih = padding\n        self.padding_hh = recurrent_kernel_size \/\/ 2\n        # ----------------------------------------------------------------------\n        self.weight_ih = nn.Parameter(\n            torch.ones(hidden_channels * 3, input_channels, kernel_size),\n            requires_grad=True,\n        )\n        self.weight_hh = nn.Parameter(\n            torch.ones(hidden_channels * 3, input_channels, recurrent_kernel_size),\n            requires_grad=True,\n        )\n        self.bias_ih = nn.Parameter(torch.zeros(hidden_channels * 3), requires_grad=True)\n        self.bias_hh = nn.Parameter(torch.zeros(hidden_channels * 3), requires_grad=True)\n","AFTER":"            input_dim,\n            hidden_dim,\n            kernel_size=(3, 3),\n            bias=True,\n            activation=F.tanh,\n            batchnorm=False,\n    ):\n        \"\"\"\n        Initialize ConvGRU cell.\n        Parameters\n        ----------\n        input_dim: int\n            Number of channels of input tensor.\n        hidden_dim: int\n            Number of channels of hidden state.\n        kernel_size: (int, int)\n            Size of the convolutional kernel.\n        bias: bool\n            Whether or not to add the bias.\n        \"\"\"\n        super().__init__()\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n\n        self.kernel_size = (\n            kernel_size if isinstance(kernel_size, (tuple, list)) else [kernel_size] * 2\n        )\n        self.padding = self.kernel_size[0] \/\/ 2, self.kernel_size[1] \/\/ 2\n        self.bias = bias\n        self.activation = activation\n        self.batchnorm = batchnorm\n\n        self.conv_zr = nn.Conv2d(\n            in_channels=self.input_dim + self.hidden_dim,\n            out_channels=2 * self.hidden_dim,\n            kernel_size=self.kernel_size,\n            padding=self.padding,\n            bias=self.bias,\n        )\n\n        self.conv_h1 = nn.Conv2d(\n            in_channels=self.input_dim,\n            out_channels=self.hidden_dim,\n            kernel_size=self.kernel_size,\n            padding=self.padding,\n            bias=self.bias,\n        )\n\n        self.conv_h2 = nn.Conv2d(\n            in_channels=self.hidden_dim,\n            out_channels=self.hidden_dim,\n            kernel_size=self.kernel_size,\n            padding=self.padding,\n            bias=self.bias,\n        )\n"}