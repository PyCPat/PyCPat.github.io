{"BEFORE":"    def __init__(self, trainargs):\n        super(FeedforwardEBM, self).__init__()\n        self.output_size = trainargs['output_size']\n        self.num_layers = len(trainargs['hidden_sizes']) + 1\n\n        if trainargs['fp_type'] == 'sep':\n            self.input_dim = trainargs['rctfp_size'] + trainargs['prodfp_size'] # will be rctfp_size + prodfp_size for FF_sep\n        elif trainargs['fp_type'] == 'diff':\n            self.input_dim = trainargs['rctfp_size']\n            assert trainargs['rctfp_size'] == trainargs['prodfp_size'], 'rctfp_size != prodfp_size, unable to make difference FPs!!!'\n\n        self.create_ffn(trainargs)\n","AFTER":"    def __init__(self, hidden_sizes: List[int],\n                dropout: int, activation: str, \n                output_size: Optional[int]=1,\n                rctfp_size: Optional[int]=4096, prodfp_size: Optional[int]=4096, \n                rxn_type: Optional[str]='diff', **kwargs):\n        super(FeedforwardEBM, self).__init__()\n        if rxn_type == 'sep':\n            input_dim = rctfp_size + prodfp_size  \n        elif rxn_type == 'diff':\n            input_dim = rctfp_size\n            assert rctfp_size == prodfp_size, 'rctfp_size must equal prodfp_size for difference fingerprints!'\n\n        num_layers = len(hidden_sizes) + 1\n        dropout = nn.Dropout(dropout)\n        activation = get_activation_function(activation)\n        self.build(dropout, activation, hidden_sizes, input_dim, output_size, num_layers)\n"}