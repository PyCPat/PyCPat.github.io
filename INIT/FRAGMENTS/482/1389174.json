{"BEFORE":"                 num_layers_linear_hidden, init_w=3e-3):\n        assert len(in_dim) == 1\n        assert len(action_dim) == 1\n\n        in_dim = np.product(in_dim)\n        action_dim = np.product(action_dim)\n\n        super(SoftQNetwork, self).__init__()\n\n        layers = [nn.Linear(in_dim + action_dim, hidden_dim)]\n\n        for l in range(num_layers_linear_hidden):\n            layers.append(nn.ReLU())\n            layers.append(nn.Linear(hidden_dim, hidden_dim))\n\n        layers.append(nn.Linear(hidden_dim, 1))\n\n        # init\n        layers[-1].weight.data.uniform_(-init_w, init_w)\n        layers[-1].bias.data.uniform_(-init_w, init_w)\n\n        self.net = nn.Sequential(*layers)\n","AFTER":"        super(Critic, self).__init__()\n\n        assert len(in_dim) == 1\n        assert len(action_dim) == 1\n\n        in_dim = np.product(in_dim)\n        action_dim = np.product(action_dim)\n\n        self.operators = nn.ModuleList([\n            Flatten(),\n            nn.Linear(in_dim + action_dim, hidden_dim)\n        ])\n\n        for l in range(num_layers_linear_hidden):\n            self.operators.append(nn.ReLU())\n            self.operators.append(nn.Linear(hidden_dim, hidden_dim))\n\n        self.operators.append(nn.Linear(hidden_dim, 1))\n\n        self.operators.apply(init_xavier_uniform)\n"}