{"BEFORE":"        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) \/ d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n","AFTER":"                 conditioning_channels, mu_embedding_dim, rnn_channels,\n                 fc_channels, bits, hop_length):\n        super(Vocoder, self).__init__()\n        self.rnn_channels = rnn_channels\n        self.quantization_channels = 2**bits\n        self.hop_length = hop_length\n\n        self.code_embedding = nn.Embedding(512, 64)\n        self.speaker_embedding = nn.Embedding(n_speakers, speaker_embedding_dim)\n        self.rnn1 = nn.GRU(in_channels + speaker_embedding_dim, conditioning_channels,\n                           num_layers=2, batch_first=True, bidirectional=True)\n        self.mu_embedding = nn.Embedding(self.quantization_channels, mu_embedding_dim)\n        self.rnn2 = nn.GRU(mu_embedding_dim + 2*conditioning_channels, rnn_channels, batch_first=True)\n        self.fc1 = nn.Linear(rnn_channels, fc_channels)\n        self.fc2 = nn.Linear(fc_channels, self.quantization_channels)\n"}