{"BEFORE":"        for _, sparse_attn in zip(range(depth), sparse_layer):\n            attn_class = Attention if not sparse_attn else partial(SparseAttention, sparse_attn_global_indices = sparse_attn_global_indices)\n","AFTER":"        attn_types = default(attn_types, ('full',))\n        attn_type_layer = islice(cycle(attn_types), depth)\n\n        for _, sparse_attn, attn_type in zip(range(depth), sparse_layer, attn_type_layer):\n            if attn_type == 'full':\n                attn_class = partial(Attention, noncausal_attn_len = noncausal_attn_len)\n            elif attn_type == 'sparse':\n                attn_class = partial(SparseAttention, sparse_attn_global_indices = sparse_attn_global_indices)\n            elif attn_type == 'axial_row':\n                attn_class = partial(SparseAxialCausalAttention, seq_len = seq_len, axis = 0, image_size = image_fmap_size)\n            elif attn_type == 'axial_col':\n                attn_class = partial(SparseAxialCausalAttention, seq_len = seq_len, axis = 1, image_size = image_fmap_size)\n            elif attn_type == 'conv_like':\n                attn_class = partial(SparseConvCausalAttention, seq_len = seq_len, image_size = image_fmap_size)\n            else:\n                raise ValueError(f'attention type \"{attn_type}\" is not valid')\n\n            layers.append(nn.ModuleList([\n"}