{"BEFORE":"        self.num_branches = len(patch_size)\n\n        self.patch_embed = nn.ModuleList()\n        self.pos_embed = nn.ParameterList([nn.Parameter(torch.zeros(1, 1 + num_patches[i], embed_dim[i])) for i in range(self.num_branches)])\n        for im_s, p, d in zip(img_size, patch_size, embed_dim):\n            self.patch_embed.append(PatchEmbed(img_size=im_s, patch_size=p, in_chans=in_chans, embed_dim=d, multi_conv=multi_conv))\n\n        self.cls_token = nn.ParameterList([nn.Parameter(torch.zeros(1, 1, embed_dim[i])) for i in range(self.num_branches)])\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        total_depth = sum([sum(x[-2:]) for x in depth])\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, total_depth)]  # stochastic depth decay rule\n        dpr_ptr = 0\n        self.blocks = nn.ModuleList()\n        for idx, block_cfg in enumerate(depth):\n            curr_depth = max(block_cfg[:-1]) + block_cfg[-1]\n            dpr_ = dpr[dpr_ptr:dpr_ptr + curr_depth]\n            blk = MultiScaleBlock(embed_dim, num_patches, block_cfg, num_heads=num_heads, mlp_ratio=mlp_ratio,\n                                  qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr_,\n                                  norm_layer=norm_layer)\n            dpr_ptr += curr_depth\n            self.blocks.append(blk)\n\n        self.norm = nn.ModuleList([norm_layer(embed_dim[i]) for i in range(self.num_branches)])\n        self.head = nn.ModuleList([nn.Linear(embed_dim[i], num_classes) if num_classes > 0 else nn.Identity() for i in range(self.num_branches)])\n\n        for i in range(self.num_branches):\n            if self.pos_embed[i].requires_grad:\n                trunc_normal_(self.pos_embed[i], std=.02)\n            trunc_normal_(self.cls_token[i], std=.02)\n\n        self.apply(self._init_weights)\n","AFTER":"        self.num_branches = len(patch_size)\n\n        self.patch_embed = nn.ModuleList()\n\n        # hard-coded for torch jit script\n        for i in range(self.num_branches):\n            setattr(self, f'pos_embed_{i}', nn.Parameter(torch.zeros(1, 1 + num_patches[i], embed_dim[i])))\n            setattr(self, f'cls_token_{i}', nn.Parameter(torch.zeros(1, 1, embed_dim[i])))\n\n        for im_s, p, d in zip(img_size, patch_size, embed_dim):\n            self.patch_embed.append(PatchEmbed(img_size=im_s, patch_size=p, in_chans=in_chans, embed_dim=d, multi_conv=multi_conv))\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        total_depth = sum([sum(x[-2:]) for x in depth])\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, total_depth)]  # stochastic depth decay rule\n        dpr_ptr = 0\n        self.blocks = nn.ModuleList()\n        for idx, block_cfg in enumerate(depth):\n            curr_depth = max(block_cfg[:-1]) + block_cfg[-1]\n            dpr_ = dpr[dpr_ptr:dpr_ptr + curr_depth]\n            blk = MultiScaleBlock(embed_dim, num_patches, block_cfg, num_heads=num_heads, mlp_ratio=mlp_ratio,\n                                  qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr_,\n                                  norm_layer=norm_layer)\n            dpr_ptr += curr_depth\n            self.blocks.append(blk)\n\n        self.norm = nn.ModuleList([norm_layer(embed_dim[i]) for i in range(self.num_branches)])\n        self.head = nn.ModuleList([nn.Linear(embed_dim[i], num_classes) if num_classes > 0 else nn.Identity() for i in range(self.num_branches)])\n\n        for i in range(self.num_branches):\n            if hasattr(self, f'pos_embed_{i}'):\n            # if self.pos_embed[i].requires_grad:\n                trunc_normal_(getattr(self, f'pos_embed_{i}'), std=.02)\n            trunc_normal_(getattr(self, f'cls_token_{i}'), std=.02)\n\n        self.apply(self._init_weights)\n"}