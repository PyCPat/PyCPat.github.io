{"BEFORE":"        context_time_features = channels * 4\n\n        num_layers = len(multipliers) - 1\n        self.num_layers = num_layers\n\n        use_context_channels = len(context_channels) > 0\n        self.use_context_channels = use_context_channels\n\n        context_channels_pad_length = num_layers + 1 - len(context_channels)\n        context_channels = context_channels + [0] * context_channels_pad_length\n        self.context_channels = context_channels\n\n        if use_context_channels:\n            has_context = [c > 0 for c in context_channels]\n            self.has_context = has_context\n            self.channels_ids = [sum(has_context[:i]) for i in range(len(has_context))]\n\n        assert (\n            len(factors) == num_layers\n            and len(attentions) == num_layers\n            and len(num_blocks) == num_layers\n        )\n\n        self.to_in = nn.Sequential(\n            Rearrange(\"b c (l p) -> b (c p) l\", p=patch_size),\n            CrossEmbed1d(\n                in_channels=(in_channels + context_channels[0]) * patch_size,\n                out_channels=channels,\n                kernel_sizes=kernel_sizes_init,\n                stride=1,\n            ),\n        )\n\n        self.to_time = nn.Sequential(\n            TimePositionalEmbedding(dim=channels, out_features=context_time_features),\n            nn.SiLU(),\n            nn.Linear(\n                in_features=context_time_features, out_features=context_time_features\n            ),\n        )\n\n        self.downsamples = nn.ModuleList(\n            [\n                DownsampleBlock1d(\n                    in_channels=channels * multipliers[i],\n                    out_channels=channels * multipliers[i + 1],\n                    context_time_features=context_time_features,\n                    context_channels=context_channels[i + 1],\n                    context_embedding_features=context_embedding_features,\n                    num_layers=num_blocks[i],\n                    factor=factors[i],\n                    kernel_multiplier=kernel_multiplier_downsample,\n                    num_groups=resnet_groups,\n                    use_pre_downsample=True,\n                    use_skip=True,\n                    use_attention=attentions[i],\n                    attention_heads=attention_heads,\n                    attention_features=attention_features,\n                    attention_multiplier=attention_multiplier,\n                )\n                for i in range(num_layers)\n            ]\n        )\n\n        self.bottleneck = BottleneckBlock1d(\n            channels=channels * multipliers[-1],\n            context_time_features=context_time_features,\n            context_embedding_features=context_embedding_features,\n            num_groups=resnet_groups,\n            use_attention=use_attention_bottleneck,\n            attention_heads=attention_heads,\n            attention_features=attention_features,\n        )\n\n        self.upsamples = nn.ModuleList(\n            [\n                UpsampleBlock1d(\n                    in_channels=channels * multipliers[i + 1],\n                    out_channels=channels * multipliers[i],\n                    context_time_features=context_time_features,\n                    context_embedding_features=context_embedding_features,\n                    num_layers=num_blocks[i] + (1 if attentions[i] else 0),\n                    factor=factors[i],\n                    use_nearest=use_nearest_upsample,\n                    num_groups=resnet_groups,\n                    use_skip_scale=use_skip_scale,\n                    use_pre_upsample=False,\n                    use_skip=True,\n                    skip_channels=channels * multipliers[i + 1],\n                    use_attention=attentions[i],\n                    attention_heads=attention_heads,\n                    attention_features=attention_features,\n                    attention_multiplier=attention_multiplier,\n                )\n                for i in reversed(range(num_layers))\n            ]\n        )\n\n        self.to_out = nn.Sequential(\n            ResnetBlock1d(\n                in_channels=channels,\n                out_channels=channels,\n                num_groups=resnet_groups,\n            ),\n            Conv1d(\n                in_channels=channels,\n                out_channels=out_channels * patch_size,\n                kernel_size=1,\n            ),\n            Rearrange(\"b (c p) l -> b c (l p)\", p=patch_size),\n            ConvOut1d(\n                channels=out_channels,\n                kernel_sizes=kernel_sizes_out,\n            )\n            if exists(kernel_sizes_out)\n            else nn.Identity(),\n","AFTER":"        use_context_time: bool,\n        out_channels: Optional[int] = None,\n        context_features: Optional[int] = None,\n        context_channels: Optional[Sequence[int]] = None,\n        context_embedding_features: Optional[int] = None,\n        kernel_sizes_out: Optional[Sequence[int]] = None,\n    ):\n        super().__init__()\n\n        out_channels = default(out_channels, in_channels)\n        context_channels = list(default(context_channels, []))\n        num_layers = len(multipliers) - 1\n        use_context_features = exists(context_features)\n        use_context_channels = len(context_channels) > 0\n        context_mapping_features = None\n\n        self.num_layers = num_layers\n        self.use_context_time = use_context_time\n        self.use_context_features = use_context_features\n        self.use_context_channels = use_context_channels\n        self.use_post_out_block = exists(kernel_sizes_out)\n\n        context_channels_pad_length = num_layers + 1 - len(context_channels)\n        context_channels = context_channels + [0] * context_channels_pad_length\n        self.context_channels = context_channels\n\n        if use_context_channels:\n            has_context = [c > 0 for c in context_channels]\n            self.has_context = has_context\n            self.channels_ids = [sum(has_context[:i]) for i in range(len(has_context))]\n\n        assert (\n            len(factors) == num_layers\n            and len(attentions) == num_layers\n            and len(num_blocks) == num_layers\n        )\n\n        self.to_in = nn.Sequential(\n            Rearrange(\"b c (l p) -> b (c p) l\", p=patch_size),\n            CrossEmbed1d(\n                in_channels=(in_channels + context_channels[0]) * patch_size,\n                out_channels=channels,\n                kernel_sizes=kernel_sizes_init,\n                stride=1,\n            ),\n        )\n\n        if use_context_time or use_context_features:\n            context_mapping_features = channels * 4\n\n            self.to_mapping = nn.Sequential(\n                nn.Linear(context_mapping_features, context_mapping_features),\n                nn.GELU(),\n                nn.Linear(context_mapping_features, context_mapping_features),\n                nn.GELU(),\n            )\n\n        if use_context_time:\n            assert exists(context_mapping_features)\n            self.to_time = nn.Sequential(\n                TimePositionalEmbedding(\n                    dim=channels, out_features=context_mapping_features\n                ),\n                nn.GELU(),\n            )\n\n        if use_context_features:\n            assert exists(context_features) and exists(context_mapping_features)\n            self.to_features = nn.Sequential(\n                nn.Linear(\n                    in_features=context_features, out_features=context_mapping_features\n                ),\n                nn.GELU(),\n            )\n\n        self.downsamples = nn.ModuleList(\n            [\n                DownsampleBlock1d(\n                    in_channels=channels * multipliers[i],\n                    out_channels=channels * multipliers[i + 1],\n                    context_mapping_features=context_mapping_features,\n                    context_channels=context_channels[i + 1],\n                    context_embedding_features=context_embedding_features,\n                    num_layers=num_blocks[i],\n                    factor=factors[i],\n                    kernel_multiplier=kernel_multiplier_downsample,\n                    num_groups=resnet_groups,\n                    use_pre_downsample=True,\n                    use_skip=True,\n                    use_attention=attentions[i],\n                    attention_heads=attention_heads,\n                    attention_features=attention_features,\n                    attention_multiplier=attention_multiplier,\n                )\n                for i in range(num_layers)\n            ]\n        )\n\n        self.bottleneck = BottleneckBlock1d(\n            channels=channels * multipliers[-1],\n            context_mapping_features=context_mapping_features,\n            context_embedding_features=context_embedding_features,\n            num_groups=resnet_groups,\n            use_attention=use_attention_bottleneck,\n            attention_heads=attention_heads,\n            attention_features=attention_features,\n        )\n\n        self.upsamples = nn.ModuleList(\n            [\n                UpsampleBlock1d(\n                    in_channels=channels * multipliers[i + 1],\n                    out_channels=channels * multipliers[i],\n                    context_mapping_features=context_mapping_features,\n                    context_embedding_features=context_embedding_features,\n                    num_layers=num_blocks[i] + (1 if attentions[i] else 0),\n                    factor=factors[i],\n                    use_nearest=use_nearest_upsample,\n                    num_groups=resnet_groups,\n                    use_skip_scale=use_skip_scale,\n                    use_pre_upsample=False,\n                    use_skip=True,\n                    skip_channels=channels * multipliers[i + 1],\n                    use_attention=attentions[i],\n                    attention_heads=attention_heads,\n                    attention_features=attention_features,\n                    attention_multiplier=attention_multiplier,\n                )\n                for i in reversed(range(num_layers))\n            ]\n        )\n\n        self.to_pre_out = ResnetBlock1d(\n            in_channels=channels,\n            out_channels=channels,\n            num_groups=resnet_groups,\n            context_mapping_features=context_mapping_features,\n        )\n\n        self.to_out = nn.Sequential(\n            Conv1d(\n                in_channels=channels,\n                out_channels=out_channels * patch_size,\n                kernel_size=1,\n            ),\n            Rearrange(\"b (c p) l -> b c (l p)\", p=patch_size),\n        )\n\n        if self.use_post_out_block:\n            assert exists(kernel_sizes_out)\n            self.to_post_out = ConvOut1d(\n                channels=out_channels,\n                kernel_sizes=kernel_sizes_out,\n            )\n\n    def get_channels(\n"}