{"BEFORE":"        self.global_to_local = nn.Sequential(\n            nn.Linear(dim_global, dim),\n            nn.GELU(),\n            Rearrange('b d -> b () d')\n        )\n","AFTER":"        self.global_linear_attn = GlobalLinearAttention(dim = dim, dim_head = attn_dim_head, heads = attn_heads) if global_local_linear_attn else None\n\n        self.narrow_conv = nn.Sequential(\n            nn.Conv1d(dim, dim, narrow_conv_kernel, padding = narrow_conv_kernel \/\/ 2),\n            nn.GELU()\n        )\n\n        wide_conv_padding = (wide_conv_kernel + (wide_conv_kernel - 1) * (wide_conv_dilation - 1)) \/\/ 2\n\n        self.wide_conv = nn.Sequential(\n            nn.Conv1d(dim, dim, wide_conv_kernel, dilation = wide_conv_dilation, padding = wide_conv_padding),\n            nn.GELU()\n        )\n\n        self.local_to_global_attn = local_to_global_attn\n\n        if local_to_global_attn:\n            self.extract_global_info = Attention(\n                dim = dim,\n                dim_keys = dim_global,\n                dim_out = dim,\n                heads = attn_heads,\n                dim_head = attn_dim_head\n            )\n        else:\n            self.extract_global_info = nn.Sequential(\n                nn.Linear(dim_global, dim),\n                nn.GELU()\n            )\n\n        self.local_norm = nn.LayerNorm(dim)\n\n        self.local_feedforward = nn.Sequential(\n            Residual(nn.Sequential(\n                nn.Linear(dim, dim),\n                nn.GELU(),\n            )),\n            nn.LayerNorm(dim)\n        )\n\n        self.global_attend_local = Attention(dim = dim_global, dim_out = dim_global, dim_keys = dim, heads = attn_heads, dim_head = attn_dim_head, qk_activation = attn_qk_activation)\n\n        self.global_dense = nn.Sequential(\n            nn.Linear(dim_global, dim_global),\n            nn.GELU()\n        )\n\n        self.global_norm = nn.LayerNorm(dim_global)\n\n        self.global_feedforward = nn.Sequential(\n            Rearrange('b () d -> b d'),\n"}