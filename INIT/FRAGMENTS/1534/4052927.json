{"BEFORE":"        net = nn.ModuleList([])\n        for i in range(len(kernel_size)):\n            net.append(nn.Conv2d(channels[i], channels[i + 1], kernel_size[i],\n                                 padding=kernel_size[i] \/\/ 2))\n            net.append(nn.LeakyReLU(leaky))\n        net = net[:-1]  # remove last ReLU\n","AFTER":"                 scale_output=True, logscale_factor=3., actnorm=True):\n        \"\"\"\n        Constructor\n        :param channels: List of channels of conv layers, first entry is in_channels\n        :param kernel_size: List of kernel sizes, same for height and width\n        :param leaky: Leaky part of ReLU\n        :param init_zeros: Flag whether last layer shall be initialized with zeros\n        :param scale_output: Flag whether to scale output with a log scale parameter\n        :param logscale_factor: Constant factor to be multiplied to log scaling\n        :param actnorm: Flag whether activation normalization shall be done after\n        each conv layer except output\n        \"\"\"\n        super().__init__()\n        # Build network\n        net = nn.ModuleList([])\n        for i in range(len(kernel_size) - 1):\n            net.append(nn.Conv2d(channels[i], channels[i + 1], kernel_size[i],\n                                 padding=kernel_size[i] \/\/ 2))\n            if actnorm:\n                net.append(utils.ActNorm((channels[i + 1],) + (1, 1)))\n            net.append(nn.LeakyReLU(leaky))\n        i = len(kernel_size)\n        net.append(nn.Conv2d(channels[i - 1], channels[i], kernel_size[i - 1],\n                             padding=kernel_size[i - 1] \/\/ 2))\n"}