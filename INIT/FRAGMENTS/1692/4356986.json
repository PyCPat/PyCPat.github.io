{"BEFORE":"                comm_func = None,\n                *args,\n                **kwargs):\n        super().__init__()\n        self.field_dims = field_dims\n        self.embedding_dim = embedding_dim\n        self.mode = mode\n\n        # Decide number of nodes\n        self.parallel_mode = ParallelMode.DEFAULT if parallel_mode is None else parallel_mode\n        self.world_size = DISTMGR.get_world_size(self.parallel_mode)\n        self.rank = DISTMGR.get_rank(self.parallel_mode)\n        self.num_groups = self.world_size # default setting\n\n        if lbmgr is not None:\n            self.lbmgr = lbmgr\n            self.field_dims = lbmgr.get_field_dims()\n            self.embedding_dim = lbmgr.get_base_dim()\n        else:\n            self.lbmgr = LoadBalanceManager(field_dims, self.num_groups, embedding_dim)\n\n        self.group = self.lbmgr.get_group(self.rank)\n        self.block_dim = self.lbmgr.get_block_dim(self.rank)\n\n        if comm_func is not None:\n            self.comm_func = comm_func\n        else:\n            self.comm_func = reduce_forward\n\n        if blk_embed is not None:\n            weights = blk_embed.get_weights()\n            assert weights[0].size() == (sum([self.field_dims[i] for i in self.group]), self.block_dim), \\\n                'passed embedding layer dimensions are wrong: {x1} vs {x2} \\\n                    '.format(x1=weights[0].size(), x2=(sum([self.field_dims[i] for i in self.group]), self.block_dim))\n            if self.block_dim != self.embedding_dim:\n                assert weights[1].size() == (self.embedding_dim, self.block_dim), \\\n                    'passed linear layer dimensions are wrong: {x1} vs {x2} \\\n                    '.format(x1=weights[1].size(), x2=(self.embedding_dim, self.block_dim))\n            assert blk_embed.base_embedding_dim == self.embedding_dim\n","AFTER":"        self.embedding_dim = embedding_dim\n        self.mode = mode\n\n        # Decide number of nodes\n        self.parallel_mode = ParallelMode.DEFAULT if parallel_mode is None else parallel_mode\n        self.world_size = DISTMGR.get_world_size(self.parallel_mode)\n        self.rank = DISTMGR.get_rank(self.parallel_mode)\n        self.num_groups = self.world_size # default setting\n\n        if lbmgr is not None:\n            self.lbmgr = lbmgr\n            self.field_dims = lbmgr.get_field_dims()\n            self.embedding_dim = lbmgr.get_base_dim()\n        else:\n            self.lbmgr = LoadBalanceManager(field_dims, self.num_groups, embedding_dim)\n\n        self.group = self.lbmgr.get_group(self.rank)\n        self.block_dim = self.lbmgr.get_block_dim(self.rank)\n        self.comm_func = reduce_forward\n\n        if blk_embed is not None:\n            weights = blk_embed.get_weights()\n            base_embedding_dim = blk_embed.get_base_embedding_dim()\n            assert weights[0].size() == (sum([self.field_dims[i] for i in self.group]), self.block_dim), \\\n                'passed embedding layer dimensions are wrong: {x1} vs {x2} \\\n                    '.format(x1=weights[0].size(), x2=(sum([self.field_dims[i] for i in self.group]), self.block_dim))\n            if self.block_dim != self.embedding_dim:\n                assert weights[1].size() == (self.embedding_dim, self.block_dim), \\\n                    'passed linear layer dimensions are wrong: {x1} vs {x2} \\\n                    '.format(x1=weights[1].size(), x2=(self.embedding_dim, self.block_dim))\n            if base_embedding_dim != self.embedding_dim:\n                DISTLogger.warning('Base embedding dimension provided by blk_embed is different from \\\n                    default or manually passed. Will overwrite by blk_embed.base_embedding_dim')\n                self.embedding_dim = base_embedding_dim\n            self.embed = BlockEmbeddingBag.from_pretrained(\n"}