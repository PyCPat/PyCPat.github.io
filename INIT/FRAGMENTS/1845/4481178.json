{"BEFORE":"        causal = False,\n        dim_head = 64,\n        dim_context = None,\n        heads = 8,\n        norm_context = False,\n        dropout = 0.1\n    ):\n        super().__init__()\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n        self.causal = causal\n        inner_dim = dim_head * heads\n\n        dim_context = default(dim_context, dim)\n\n        self.norm = LayerNorm(dim)\n        self.context_norm = LayerNorm(dim_context) if norm_context else nn.Identity()\n\n        self.attn_dropout = nn.Dropout(dropout)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim_context, dim_head * 2, bias = False)\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            nn.Dropout(dropout)\n        )\n","AFTER":"        window_size = 7\n    ):\n        super().__init__()\n        assert (dim % dim_head) == 0, 'dimension should be divisible by dimension per head'\n\n        self.norm = nn.LayerNorm(dim)\n\n        self.heads = dim \/\/ dim_head\n        self.scale = dim_head ** -0.5\n\n        self.to_qkv = nn.Linear(dim, dim * 3, bias = False)\n\n        self.attend = nn.Sequential(\n            nn.Softmax(dim = -1),\n            nn.Dropout(dropout)\n        )\n\n        self.to_out = nn.Sequential(\n            nn.Linear(dim, dim, bias = False),\n            nn.Dropout(dropout)\n        )\n\n        # relative positional bias\n\n        self.rel_pos_bias = nn.Embedding((2 * window_size - 1) ** 2, self.heads)\n\n        pos = torch.arange(window_size)\n        grid = torch.stack(torch.meshgrid(pos, pos, indexing = 'ij'))\n        grid = rearrange(grid, 'c i j -> (i j) c')\n        rel_pos = rearrange(grid, 'i ... -> i 1 ...') - rearrange(grid, 'j ... -> 1 j ...')\n        rel_pos += window_size - 1\n        rel_pos_indices = (rel_pos * torch.tensor([2 * window_size - 1, 1])).sum(dim = -1)\n\n        self.register_buffer('rel_pos_indices', rel_pos_indices, persistent = False)\n"}