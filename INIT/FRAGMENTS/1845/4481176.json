{"BEFORE":"        mid_features = head_features * num_heads\n        context_features = default(context_features, features)\n\n        self.norm_in = LayerNorm(features=features, bias=False)\n        self.norm_context = LayerNorm(features=context_features, bias=False)\n\n        self.to_q = nn.Linear(\n            in_features=features, out_features=mid_features, bias=False\n        )\n        self.to_kv = nn.Linear(\n            in_features=context_features, out_features=mid_features * 2, bias=False\n        )\n        self.attention = AttentionBase(\n            features, num_heads=num_heads, head_features=head_features\n        )\n","AFTER":"        self,\n        num_layers: int,\n        channels: int,\n        num_heads: int,\n        head_features: int,\n        multiplier: int,\n        context_features: Optional[int] = None,\n    ):\n        super().__init__()\n\n        self.to_in = nn.Sequential(\n            nn.GroupNorm(num_groups=32, num_channels=channels, eps=1e-6, affine=True),\n            Conv1d(\n                in_channels=channels,\n                out_channels=channels,\n                kernel_size=1,\n            ),\n            Rearrange(\"b c t -> b t c\"),\n        )\n\n        self.blocks = nn.ModuleList(\n            [\n                TransformerBlock(\n                    features=channels,\n                    head_features=head_features,\n                    num_heads=num_heads,\n                    multiplier=multiplier,\n                    context_features=context_features,\n                )\n                for i in range(num_layers)\n            ]\n        )\n\n        self.to_out = nn.Sequential(\n            Rearrange(\"b t c -> b c t\"),\n            Conv1d(\n                in_channels=channels,\n                out_channels=channels,\n                kernel_size=1,\n            ),\n"}