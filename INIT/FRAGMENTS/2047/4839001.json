{"BEFORE":"        cSplitted = cin \/\/ len(k)\n        for i, numCodewords in enumerate(k):\n            setattr(self, f\"codebook{i}\", nn.Parameter(torch.nn.init.kaiming_uniform_(torch.empty(numCodewords, cSplitted))))\n            # setattr(self, f\"tCodebook{i}\", nn.Parameter(torch.nn.init.kaiming_normal_(torch.empty(numCodewords, cin))))\n            # setattr(self, f\"palette{i}\", nn.Parameter(torch.nn.init.kaiming_uniform_(torch.empty(numCodewords, cin))))\n        self._codebookAsKey = nn.ModuleList([nn.Linear(cSplitted, cSplitted) for numCodewords in k])\n        # self._tCodebookAsKey = nn.ModuleList([nn.Linear(cin, cin) for numCodewords in k])\n        self._codebookAsValue = nn.ModuleList([nn.Linear(cSplitted, cSplitted) for numCodewords in k])\n        # self._tCodebookAsValue = nn.ModuleList([nn.Linear(cin, cin) for numCodewords in k])\n        self._xAsQuery = nn.ModuleList([nn.Linear(cSplitted, cSplitted) for numCodewords in k])\n        # self._tXAsQuery = nn.ModuleList([nn.Linear(cin, cin) for numCodewords in k])\n        self._k = k\n        self._scaling = [sqrt(kk) for kk in k]\n        self._d = float(cin) ** 0.5\n        self._c = cin\n        self._position = PositionalEncoding2D(cin, 120, 120)\n","AFTER":"    def __init__(self, layers: int, k: List[int], cin: int, rate: float = 0.1):\n        super().__init__()\n        k = k[0]\n        self._position = PositionalEncoding2D(cin, 120, 120)\n        self._encoder = nn.Transformer(cin, 8, layers, layers, dropout=rate, activation=\"gelu\")\n        setattr(self, \"codebook\", nn.Parameter(torch.nn.init.kaiming_uniform_(torch.empty(k, cin))))\n        self._codebookEncoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(cin, 8, dropout=rate, activation=\"gelu\"), layers)\n        self._select = nn.Linear(cin, k)\n        self._gumbel = GumbelSoftmax()\n        self._decoder = nn.Transformer(cin, 8, layers, layers, dropout=rate, activation=\"gelu\")\n        self._codebookDecoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(cin, 8, dropout=rate, activation=\"gelu\"), layers)\n"}