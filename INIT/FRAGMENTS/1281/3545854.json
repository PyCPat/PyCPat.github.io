{"BEFORE":"        kernel_size = kernel_size or [3, 5]\n        _kernel_valid(kernel_size)\n        if not isinstance(kernel_size, list):\n            kernel_size = [kernel_size] * 2\n        if keep_3x3:\n            dilation = [dilation * (k - 1) \/\/ 2 for k in kernel_size]\n            kernel_size = [3] * len(kernel_size)\n        else:\n            dilation = [dilation] * len(kernel_size)\n        num_paths = len(kernel_size)\n        self.num_paths = num_paths\n        self.split_input = split_input\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        if split_input:\n            assert in_channels % num_paths == 0 and out_channels % num_paths == 0\n            in_channels = in_channels \/\/ num_paths\n            out_channels = out_channels \/\/ num_paths\n        groups = min(out_channels, groups)\n\n        self.paths = nn.ModuleList()\n        for k, d in zip(kernel_size, dilation):\n            p = _get_padding(k, stride, d)\n            self.paths.append(nn.Sequential(OrderedDict([\n                ('conv', nn.Conv2d(\n                    in_channels, out_channels, kernel_size=k, stride=stride, padding=p,\n                    dilation=d, groups=groups, bias=False)),\n                ('bn', norm_layer(out_channels)),\n                ('act', act_layer(inplace=True))\n            ])))\n\n        attn_channels = max(int(out_channels \/ attn_reduction), min_attn_channels)\n","AFTER":"                 drop_block=None, act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d):\n        super(SelectiveKernelConv, self).__init__()\n        kernel_size = kernel_size or [3, 5]\n        _kernel_valid(kernel_size)\n        if not isinstance(kernel_size, list):\n            kernel_size = [kernel_size] * 2\n        if keep_3x3:\n            dilation = [dilation * (k - 1) \/\/ 2 for k in kernel_size]\n            kernel_size = [3] * len(kernel_size)\n        else:\n            dilation = [dilation] * len(kernel_size)\n        num_paths = len(kernel_size)\n        self.num_paths = num_paths\n        self.split_input = split_input\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        if split_input:\n            assert in_channels % num_paths == 0 and out_channels % num_paths == 0\n            in_channels = in_channels \/\/ num_paths\n            out_channels = out_channels \/\/ num_paths\n        groups = min(out_channels, groups)\n\n        conv_kwargs = dict(\n            stride=stride, groups=groups, drop_block=drop_block, act_layer=act_layer, norm_layer=norm_layer)\n        self.paths = nn.ModuleList([\n            ConvBnAct(in_channels, out_channels, kernel_size=k, dilation=d, **conv_kwargs)\n            for k, d in zip(kernel_size, dilation)])\n\n        attn_channels = max(int(out_channels \/ attn_reduction), min_attn_channels)\n        self.attn = SelectiveKernelAttn(out_channels, num_paths, attn_channels)\n        self.drop_block = drop_block\n"}