{"BEFORE":"        fs_kwargs = None,\n        max_checkpoints_keep = 20,\n        **kwargs\n    ):\n        super().__init__()\n        assert not ImagenTrainer.locked, 'ImagenTrainer can only be initialized once per process - for the sake of distributed training, you will now have to create a separate script to train each unet (or a script that accepts unet number as an argument)'\n        assert exists(imagen) ^ exists(imagen_checkpoint_path), 'either imagen instance is passed into the trainer, or a checkpoint path that contains the imagen config'\n\n        # determine filesystem, using fsspec, for saving to local filesystem or cloud\n\n        fs = default(checkpoint_fs, lambda: LocalFileSystem(auto_mkdir = True))\n        self.fs = fs\n\n        assert isinstance(imagen, (Imagen, ElucidatedImagen))\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # elucidated or not\n\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': 'fp16' if fp16 else 'no',\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n            optimizer = Adam(\n                unet.parameters(),\n                lr = unet_lr,\n                eps = unet_eps,\n                betas = (beta1, beta2),\n                **kwargs\n            )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            if not fs.exists(checkpoint_path):\n                self.fs.mkdir(checkpoint_path)\n","AFTER":"        fs_kwargs: dict = None,\n        max_checkpoints_keep = 20,\n        **kwargs\n    ):\n        super().__init__()\n        assert not ImagenTrainer.locked, 'ImagenTrainer can only be initialized once per process - for the sake of distributed training, you will now have to create a separate script to train each unet (or a script that accepts unet number as an argument)'\n        assert exists(imagen) ^ exists(imagen_checkpoint_path), 'either imagen instance is passed into the trainer, or a checkpoint path that contains the imagen config'\n\n        # determine filesystem, using fsspec, for saving to local filesystem or cloud\n\n        self.fs = default(checkpoint_fs, lambda: url_to_fs(checkpoint_path, fs_kwargs))\n\n        assert isinstance(imagen, (Imagen, ElucidatedImagen))\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # elucidated or not\n\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': 'fp16' if fp16 else 'no',\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n            optimizer = Adam(\n                unet.parameters(),\n                lr = unet_lr,\n                eps = unet_eps,\n                betas = (beta1, beta2),\n                **kwargs\n            )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n"}