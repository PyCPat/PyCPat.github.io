{"BEFORE":"                 num_heads=(4, 8, 16), depths=(2, 2, 20), num_classes=1000, mlp_ratio=4., qkv_bias=True, pad_type='',\n                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.5, norm_layer=None, act_layer=None, weight_init='',\n                 global_pool='avg'):\n        \"\"\"\n        Args:\n            img_size (int, tuple): input image size\n            in_chans (int): number of input channels\n            patch_size (int): patch size\n            num_levels (int): number of block hierarchies (T_d in the paper)\n            embed_dims (int, tuple): embedding dimensions of each level\n            num_heads (int, tuple): number of attention heads for each level\n            depths (int, tuple): number of transformer layers for each level\n            num_classes (int): number of classes for classification head\n            mlp_ratio (int): ratio of mlp hidden dim to embedding dim for MLP of transformer layers\n            qkv_bias (bool): enable bias for qkv if True\n            drop_rate (float): dropout rate for MLP of transformer layers, MSA final projection layer, and classifier\n            attn_drop_rate (float): attention dropout rate\n            drop_path_rate (float): stochastic depth rate\n            norm_layer: (nn.Module): normalization layer for transformer layers\n            act_layer: (nn.Module): activation layer in MLP of transformer layers\n            weight_init: (str): weight init scheme\n            global_pool: (str): type of pooling operation to apply to final feature map\n\n        Notes:\n            - Default values follow NesT-B from the original Jax code.\n            - `embed_dims`, `num_heads`, `depths` should be ints or tuples with length `num_levels`.\n            - For those following the paper, Table A1 may have errors!\n                - https:\/\/github.com\/google-research\/nested-transformer\/issues\/2\n        \"\"\"\n        super().__init__()\n\n        for param_name in ['embed_dims', 'num_heads', 'depths']:\n            param_value = locals()[param_name]\n            if isinstance(param_value, collections.abc.Sequence):\n                assert len(param_value) == num_levels, f'Require `len({param_name}) == num_levels`'\n\n        embed_dims = to_ntuple(num_levels)(embed_dims)\n        num_heads = to_ntuple(num_levels)(num_heads)\n        depths = to_ntuple(num_levels)(depths)\n        self.num_classes = num_classes\n        self.num_features = embed_dims[-1]\n        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n        act_layer = act_layer or nn.GELU\n        self.drop_rate = drop_rate\n        self.num_levels = num_levels\n        if isinstance(img_size, collections.abc.Sequence):\n            assert img_size[0] == img_size[1], 'Model only handles square inputs'\n            img_size = img_size[0]\n        assert img_size % patch_size == 0, '`patch_size` must divide `img_size` evenly'\n        self.patch_size = patch_size\n\n        # Number of blocks at each level\n        self.num_blocks = 4**(np.arange(num_levels)[::-1])\n        assert (img_size \/\/ patch_size) % np.sqrt(self.num_blocks[0]) == 0, \\\n                    'First level blocks don\\'t fit evenly. Check `img_size`, `patch_size`, and `num_levels`'\n\n        # Block edge size in units of patches\n        # Hint: (img_size \/\/ patch_size) gives number of patches along edge of image. sqrt(self.num_blocks[0]) is the\n        #  number of blocks along edge of image\n        self.block_size = int((img_size \/\/ patch_size) \/\/ np.sqrt(self.num_blocks[0]))\n        \n        # Patch embedding\n        self.patch_embed = PatchEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dims[0])\n        self.num_patches = self.patch_embed.num_patches\n        self.seq_length = self.num_patches \/\/ self.num_blocks[0]\n\n        # Build up each hierarchical level\n        self.levels = nn.ModuleList([])\n        self.block_aggs = nn.ModuleList([])\n        drop_path_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n        for lix in range(self.num_levels):\n            dpr = drop_path_rates[sum(depths[:lix]):sum(depths[:lix+1])]\n            self.levels.append(NestLevel(\n                self.num_blocks[lix], self.block_size, self.seq_length, num_heads[lix], depths[lix],\n                embed_dims[lix], mlp_ratio, qkv_bias, drop_rate, attn_drop_rate, dpr, norm_layer,\n                act_layer))            \n            if lix < self.num_levels - 1:\n                self.block_aggs.append(BlockAggregation(\n                    embed_dims[lix], embed_dims[lix+1], norm_layer, pad_type=pad_type))\n            else:\n                # Required for zipped iteration over levels and ls_block_agg together\n                self.block_aggs.append(nn.Identity())\n\n        # Final normalization layer\n        self.norm = norm_layer(embed_dims[-1])\n","AFTER":"                 pad_type='', weight_init='', global_pool='avg'):\n        \"\"\"\n        Args:\n            img_size (int, tuple): input image size\n            in_chans (int): number of input channels\n            patch_size (int): patch size\n            num_levels (int): number of block hierarchies (T_d in the paper)\n            embed_dims (int, tuple): embedding dimensions of each level\n            num_heads (int, tuple): number of attention heads for each level\n            depths (int, tuple): number of transformer layers for each level\n            num_classes (int): number of classes for classification head\n            mlp_ratio (int): ratio of mlp hidden dim to embedding dim for MLP of transformer layers\n            qkv_bias (bool): enable bias for qkv if True\n            drop_rate (float): dropout rate for MLP of transformer layers, MSA final projection layer, and classifier\n            attn_drop_rate (float): attention dropout rate\n            drop_path_rate (float): stochastic depth rate\n            norm_layer: (nn.Module): normalization layer for transformer layers\n            act_layer: (nn.Module): activation layer in MLP of transformer layers\n            pad_type: str: Type of padding to use '' for PyTorch symmetric, 'same' for TF SAME\n            weight_init: (str): weight init scheme\n            global_pool: (str): type of pooling operation to apply to final feature map\n\n        Notes:\n            - Default values follow NesT-B from the original Jax code.\n            - `embed_dims`, `num_heads`, `depths` should be ints or tuples with length `num_levels`.\n            - For those following the paper, Table A1 may have errors!\n                - https:\/\/github.com\/google-research\/nested-transformer\/issues\/2\n        \"\"\"\n        super().__init__()\n\n        for param_name in ['embed_dims', 'num_heads', 'depths']:\n            param_value = locals()[param_name]\n            if isinstance(param_value, collections.abc.Sequence):\n                assert len(param_value) == num_levels, f'Require `len({param_name}) == num_levels`'\n\n        embed_dims = to_ntuple(num_levels)(embed_dims)\n        num_heads = to_ntuple(num_levels)(num_heads)\n        depths = to_ntuple(num_levels)(depths)\n        self.num_classes = num_classes\n        self.num_features = embed_dims[-1]\n        self.feature_info = []\n        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n        act_layer = act_layer or nn.GELU\n        self.drop_rate = drop_rate\n        self.num_levels = num_levels\n        if isinstance(img_size, collections.abc.Sequence):\n            assert img_size[0] == img_size[1], 'Model only handles square inputs'\n            img_size = img_size[0]\n        assert img_size % patch_size == 0, '`patch_size` must divide `img_size` evenly'\n        self.patch_size = patch_size\n\n        # Number of blocks at each level\n        self.num_blocks = (4 ** torch.arange(num_levels)).flip(0).tolist()\n        assert (img_size \/\/ patch_size) % math.sqrt(self.num_blocks[0]) == 0, \\\n            'First level blocks don\\'t fit evenly. Check `img_size`, `patch_size`, and `num_levels`'\n\n        # Block edge size in units of patches\n        # Hint: (img_size \/\/ patch_size) gives number of patches along edge of image. sqrt(self.num_blocks[0]) is the\n        #  number of blocks along edge of image\n        self.block_size = int((img_size \/\/ patch_size) \/\/ math.sqrt(self.num_blocks[0]))\n        \n        # Patch embedding\n        self.patch_embed = PatchEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dims[0], flatten=False)\n        self.num_patches = self.patch_embed.num_patches\n        self.seq_length = self.num_patches \/\/ self.num_blocks[0]\n\n        # Build up each hierarchical level\n        levels = []\n        dp_rates = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(depths)).split(depths)]\n        prev_dim = None\n        curr_stride = 4\n        for i in range(len(self.num_blocks)):\n            dim = embed_dims[i]\n            levels.append(NestLevel(\n                self.num_blocks[i], self.block_size, self.seq_length, num_heads[i], depths[i], dim, prev_dim,\n                mlp_ratio, qkv_bias, drop_rate, attn_drop_rate, dp_rates[i], norm_layer, act_layer, pad_type=pad_type))\n            self.feature_info += [dict(num_chs=dim, reduction=curr_stride, module=f'levels.{i}')]\n            prev_dim = dim\n            curr_stride *= 2\n        self.levels = nn.Sequential(*levels)\n"}