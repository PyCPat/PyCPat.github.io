{"BEFORE":"        get_attn = lambda: LSHSelfAttention(dim, heads, bucket_size, n_hashes, causal = causal, dropout = lsh_dropout, post_attn_dropout = post_attn_dropout, attn_chunks = attn_chunks, allow_duplicate_attention = lsh_allow_duplicate_attention, attend_across_buckets = lsh_attend_across_buckets, random_rotations_per_head = random_rotations_per_head, num_mem_kv = num_mem_kv, use_full_attn = use_full_attn, full_attn_thres = full_attn_thres, one_value_head = one_value_head, n_local_attn_heads = n_local_attn_heads)\n        get_ff = lambda: Chunk(ff_chunks, FeedForward(dim, dropout = ff_dropout, activation = ff_activation, mult = ff_mult, glu = ff_glu), along_dim = -2)\n\n        if weight_tie:\n            get_attn = cache_fn(get_attn)\n            get_ff = cache_fn(get_ff)\n\n        blocks = []\n\n        norm_type = ScaleNorm if use_scale_norm else nn.LayerNorm\n\n        residual_fn_wrapper = ReZero if use_rezero else partial(PreNorm, norm_type, dim)\n\n        for _ in range(depth):\n            attn = get_attn()\n            parallel_net = get_attn() if twin_attention else get_ff()\n","AFTER":"    def __init__(self, dim, depth, max_seq_len, heads = 8, bucket_size = 64, n_hashes = 8, ff_chunks = 100, attn_chunks = None, causal = False, weight_tie = False, lsh_dropout = 0., ff_dropout = 0., ff_activation = None, ff_mult = 4, ff_glu = False, post_attn_dropout = 0., layer_dropout = 0., lsh_attend_across_buckets = True, lsh_allow_duplicate_attention = True, random_rotations_per_head = False, twin_attention = False, use_scale_norm = False, use_rezero = False, use_full_attn = False, full_attn_thres = 0, reverse_thres = 0, num_mem_kv = 0, one_value_head = False, n_local_attn_heads = 0, pkm_layers = tuple(), pkm_num_keys = 128):\n        super().__init__()\n        self.dim = dim\n        self.depth = depth\n\n        self.bucket_size = bucket_size\n        self.num_mem_kv = num_mem_kv\n\n        self.twin_attention = twin_attention\n        self.full_attn_thres = full_attn_thres\n\n        get_attn = lambda: LSHSelfAttention(dim, heads, bucket_size, n_hashes, causal = causal, dropout = lsh_dropout, post_attn_dropout = post_attn_dropout, attn_chunks = attn_chunks, allow_duplicate_attention = lsh_allow_duplicate_attention, attend_across_buckets = lsh_attend_across_buckets, random_rotations_per_head = random_rotations_per_head, num_mem_kv = num_mem_kv, use_full_attn = use_full_attn, full_attn_thres = full_attn_thres, one_value_head = one_value_head, n_local_attn_heads = n_local_attn_heads)\n        get_ff = lambda: Chunk(ff_chunks, FeedForward(dim, dropout = ff_dropout, activation = ff_activation, mult = ff_mult, glu = ff_glu), along_dim = -2)\n        get_pkm = lambda: PKM(dim, num_keys = pkm_num_keys)\n\n        if weight_tie:\n            get_attn, get_ff, get_pkm = map(cache_fn, (get_attn, get_ff, get_pkm))\n\n        blocks = []\n\n        norm_type = ScaleNorm if use_scale_norm else nn.LayerNorm\n\n        residual_fn_wrapper = ReZero if use_rezero else partial(PreNorm, norm_type, dim)\n\n        for ind in range(depth):\n            layer_num = ind + 1\n            use_pkm = layer_num in cast_tuple(pkm_layers)\n            parallel_net = None\n\n            attn = get_attn()\n\n            if use_pkm:\n                parallel_net = get_pkm()\n            elif twin_attention:\n                parallel_net = get_attn()\n            else:\n                parallel_net = get_ff()\n\n            f = residual_fn_wrapper(attn)\n"}