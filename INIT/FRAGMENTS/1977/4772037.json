{"BEFORE":"        position_embedding: nn.Module,\n        num_classes: int,\n        num_queries: int,\n        criterion: nn.Module,\n        pixel_mean: List[float],\n        pixel_std: List[float],\n        in_channels: int = 2048,\n        embed_dim: int = 256,\n        aux_loss: bool = True,\n        iter_update: bool = True,\n        query_dim: int = 4,\n        random_refpoints_xy: bool = True,\n        device: str = \"cuda\",\n    ):\n        super(DABDETR, self).__init__()\n        self.backbone = backbone\n        self.in_features = in_features\n        self.transformer = transformer\n        self.position_embedding = position_embedding\n        self.class_embed = nn.Linear(embed_dim, num_classes)\n        self.bbox_embed = MLP(embed_dim, embed_dim, 4, 3)\n        self.refpoint_embed = nn.Embedding(num_queries, query_dim)\n        self.num_classes = num_classes\n        self.num_queries = num_queries\n        self.criterion = criterion\n        self.query_dim = query_dim\n        self.aux_loss = aux_loss\n        self.iter_update = iter_update\n\n        assert self.query_dim in [2, 4]\n\n        self.random_refpoints_xy = random_refpoints_xy\n\n        self.input_proj = nn.Conv2d(in_channels, embed_dim, kernel_size=1)\n\n        if self.iter_update:\n            self.transformer.decoder.bbox_embed = self.bbox_embed\n\n        # normalizer for input raw images\n        self.device = device\n","AFTER":"        in_channels: int,\n        position_embedding: nn.Module,\n        transformer: nn.Module,\n        embed_dim: int,\n        num_classes: int,\n        num_queries: int,\n        criterion: nn.Module,\n        aux_loss: bool = True,\n        pixel_mean: List[float] = [123.675, 116.280, 103.530],\n        pixel_std: List[float] = [58.395, 57.120, 57.375],\n        freeze_anchor_box_centers: bool = True,\n        select_box_nums_for_evaluation: int = 300,\n        device: str = \"cuda\",\n    ):\n        super(DABDETR, self).__init__()\n        # define backbone and position embedding module\n        self.backbone = backbone\n        self.in_features = in_features\n        self.position_embedding = position_embedding\n\n        # project the backbone output feature \n        # into the required dim for transformer block\n        self.input_proj = nn.Conv2d(in_channels, embed_dim, kernel_size=1)\n        \n        # define leanable anchor boxes and transformer module\n        self.transformer = transformer\n        self.anchor_box_embed = nn.Embedding(num_queries, 4)\n\n        # whether to freeze the initilized anchor box centers during training\n        self.freeze_anchor_box_centers = freeze_anchor_box_centers\n\n        # define classification head and box head\n        self.class_embed = nn.Linear(embed_dim, num_classes)\n        self.bbox_embed = MLP(\n            input_dim=embed_dim, \n            hidden_dim=embed_dim, \n            output_dim=4, \n            num_layers=3\n        )\n        self.num_classes = num_classes\n\n        # predict offsets to update anchor boxes after each decoder layer\n        # with shared box embedding head\n        # this is a hack implementation which will be modified in the future\n        self.transformer.decoder.bbox_embed = self.bbox_embed\n\n        # where to calculate auxiliary loss in criterion\n        self.aux_loss = aux_loss\n        self.criterion = criterion\n\n        # normalizer for input raw images\n        self.device = device\n        pixel_mean = torch.Tensor(pixel_mean).to(self.device).view(3, 1, 1)\n        pixel_std = torch.Tensor(pixel_std).to(self.device).view(3, 1, 1)\n        self.normalizer = lambda x: (x - pixel_mean) \/ pixel_std\n\n        # The total nums of selected boxes for evaluation\n        self.select_box_nums_for_evaluation = select_box_nums_for_evaluation\n"}