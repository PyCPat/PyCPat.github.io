{"BEFORE":"            if hasattr(self, f'pos_embed_{i}'):\n                trunc_normal_(getattr(self, f'pos_embed_{i}'), std=.02)\n            trunc_normal_(getattr(self, f'cls_token_{i}'), std=.02)\n","AFTER":"            norm_layer=partial(nn.LayerNorm, eps=1e-6), multi_conv=False, crop_scale=False,\n    ):\n        super().__init__()\n\n        self.num_classes = num_classes\n        self.img_size = to_2tuple(img_size)\n        img_scale = to_2tuple(img_scale)\n        self.img_size_scaled = [tuple([int(sj * si) for sj in self.img_size]) for si in img_scale]\n        self.crop_scale = crop_scale  # crop instead of interpolate for scale\n        num_patches = _compute_num_patches(self.img_size_scaled, patch_size)\n        self.num_branches = len(patch_size)\n        self.embed_dim = embed_dim\n        self.num_features = embed_dim[0]  # to pass the tests\n        self.patch_embed = nn.ModuleList()\n\n        # hard-coded for torch jit script\n        for i in range(self.num_branches):\n            setattr(self, f'pos_embed_{i}', nn.Parameter(torch.zeros(1, 1 + num_patches[i], embed_dim[i])))\n            setattr(self, f'cls_token_{i}', nn.Parameter(torch.zeros(1, 1, embed_dim[i])))\n\n        for im_s, p, d in zip(self.img_size_scaled, patch_size, embed_dim):\n            self.patch_embed.append(\n                PatchEmbed(img_size=im_s, patch_size=p, in_chans=in_chans, embed_dim=d, multi_conv=multi_conv))\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        total_depth = sum([sum(x[-2:]) for x in depth])\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, total_depth)]  # stochastic depth decay rule\n        dpr_ptr = 0\n        self.blocks = nn.ModuleList()\n        for idx, block_cfg in enumerate(depth):\n            curr_depth = max(block_cfg[:-1]) + block_cfg[-1]\n            dpr_ = dpr[dpr_ptr:dpr_ptr + curr_depth]\n            blk = MultiScaleBlock(\n                embed_dim, num_patches, block_cfg, num_heads=num_heads, mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr_, norm_layer=norm_layer)\n            dpr_ptr += curr_depth\n            self.blocks.append(blk)\n\n        self.norm = nn.ModuleList([norm_layer(embed_dim[i]) for i in range(self.num_branches)])\n        self.head = nn.ModuleList([\n            nn.Linear(embed_dim[i], num_classes) if num_classes > 0 else nn.Identity()\n            for i in range(self.num_branches)])\n\n        for i in range(self.num_branches):\n            trunc_normal_(getattr(self, f'pos_embed_{i}'), std=.02)\n"}