{"BEFORE":"        for ind, local_heads in zip(range(depth), n_local_attn_heads):\n            attn = SelfAttention(dim, depth, max_seq_len, heads, local_heads, window_size, causal = causal, local_attn_window_size = local_attn_window_size, attn_dropout = attn_dropout, dropout = attn_layer_dropout, kmeans_ema_decay = kmeans_ema_decay, commitment_factor = commitment_factor)\n            ff = Chunk(ff_chunks, FeedForward(dim, dropout = ff_dropout, glu = ff_glu), along_dim=1)\n\n            attn, ff = map(fn_wrapper, (attn, ff))\n            layers.append(nn.ModuleList([attn, ff]))\n\n            if not receives_context:\n                continue\n\n            context_attn = SelfAttention(dim, depth, max_seq_len, heads, 0, window_size, local_attn_window_size = local_attn_window_size, attn_dropout = attn_dropout, dropout = attn_layer_dropout, kmeans_ema_decay = kmeans_ema_decay, commitment_factor = commitment_factor, receives_context = True, context_window_size = context_window_size)\n            context_ff = Chunk(ff_chunks, FeedForward(dim, dropout = ff_dropout, glu = ff_glu), along_dim=1)\n","AFTER":"    def __init__(self, dim, depth, max_seq_len, heads = 8, window_size = 64, local_attn_window_size = None, causal = False, weight_tie = False, attn_dropout = 0., ff_dropout = 0., attn_layer_dropout = 0., layer_dropout = 0., n_local_attn_heads = 0, ff_glu = False, reversible = False, ff_chunks = 1, kmeans_ema_decay = 0.999, commitment_factor = 1e-4, receives_context = False, context_window_size = None):\n        super().__init__()\n        local_attn_window_size = default(local_attn_window_size, window_size \/\/ 2)\n        if type(n_local_attn_heads) is not tuple:\n            n_local_attn_heads = tuple([n_local_attn_heads] * depth)\n\n        assert len(n_local_attn_heads) == depth, 'local attention heads tuple must have the same length as the depth'\n        assert all([(local_heads <= heads) for local_heads in n_local_attn_heads]), 'number of local attn heads must be less than the maximum number of heads'\n\n        layers = nn.ModuleList([])\n        fn_wrapper = partial(PreNorm, dim)\n\n        get_attn = lambda local_heads: SelfAttention(dim, depth, max_seq_len, heads, local_heads, window_size, causal = causal, local_attn_window_size = local_attn_window_size, attn_dropout = attn_dropout, dropout = attn_layer_dropout, kmeans_ema_decay = kmeans_ema_decay, commitment_factor = commitment_factor)\n        get_ff = lambda: Chunk(ff_chunks, FeedForward(dim, dropout = ff_dropout, glu = ff_glu), along_dim=1)\n        get_context_attn = lambda: SelfAttention(dim, depth, max_seq_len, heads, 0, window_size, local_attn_window_size = local_attn_window_size, attn_dropout = attn_dropout, dropout = attn_layer_dropout, kmeans_ema_decay = kmeans_ema_decay, commitment_factor = commitment_factor, receives_context = True, context_window_size = context_window_size)\n        get_context_ff = lambda: Chunk(ff_chunks, FeedForward(dim, dropout = ff_dropout, glu = ff_glu), along_dim=1)\n\n        if weight_tie:\n            assert len(set(n_local_attn_heads)) == 1, 'you can only weight tie if number of local attention heads for all layers is the same'\n            get_attn, get_ff, get_context_attn, get_context_ff = map(cache_fn, (get_attn, get_ff, get_context_attn, get_context_ff))\n\n        for ind, local_heads in zip(range(depth), n_local_attn_heads):\n            attn = get_attn(local_heads)\n            ff = get_ff()\n\n            attn, ff = map(fn_wrapper, (attn, ff))\n            layers.append(nn.ModuleList([attn, ff]))\n\n            if not receives_context:\n                continue\n\n            context_attn = get_context_attn()\n            context_ff = get_context_ff()\n"}