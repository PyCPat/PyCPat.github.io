{"BEFORE":"        log_moving_average = torch.zeros(1)\n        self.register_buffer(\"log_moving_average\", log_moving_average)\n","AFTER":"    def __init__(self, momentum=0.99, gradient=True):\n        super().__init__()\n        self.register_buffer(\"log_moving_average\", torch.zeros(1))\n        momentum = torch.zeros(1) + momentum\n        self.register_buffer(\"momentum_logit\", momentum.logit())\n\n        class LogMeanExp(torch.autograd.Function):\n            @staticmethod\n            def forward(ctx, input):\n                ctx.save_for_backward(input)\n                self.log_moving_average.data = stop_gradient(\n                    log_interpolate(\n                        self.log_moving_average, logmeanexp(input), self.momentum_logit\n                    )\n                )\n                return self.log_moving_average\n\n            @staticmethod\n            def backward(ctx, grad_output):\n                input = ctx.saved_tensors[0]\n                log_n = np.log(input.numel())\n                moving_average = self.log_moving_average.add(log_n).exp()\n                grad_output = grad_output * input.exp() \/ moving_average\n                return grad_output if gradient else None\n\n        self.logmeanexp = LogMeanExp.apply\n"}