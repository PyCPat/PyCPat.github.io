{"BEFORE":"        self.trunk.reset_classifier(0, global_pool='')\n        feat_size = self.trunk.default_cfg.get('pool_size', None)\n        feature_ndim = 1 if not feat_size else 2\n        prev_chs = self.trunk.num_features\n\n        head_layers = OrderedDict()\n\n        if feature_ndim == 2:\n            assert pool, 'pooling layer needed for 2d feature output'\n            if pool == 'abs_attn':\n                assert feature_ndim == 2\n                head_layers['pool'] = AbsAttentionPool2d(prev_chs, feat_size=feat_size, out_features=embed_dim)\n                prev_chs = embed_dim\n            elif pool == 'rot_attn':\n                assert feature_ndim == 2\n                head_layers['pool'] = RotAttentionPool2d(prev_chs, out_features=embed_dim)\n                prev_chs = embed_dim\n            elif pool == 'avg':\n                assert proj, 'projection layer needed if avg pooling used'\n                head_layers['pool'] = nn.AdaptiveAvgPool2d(1)\n        else:\n            # NOTE timm transformers will be changed in the future to return unpooled\n            # outputs when head is disabled, this is not he case right now and code be needed\n            # here for token extraction or pooling\n            pass\n\n        # NOTE attention pool ends with a projection layer, so proj should usually be set to '' if such pooling is used\n        if proj == 'linear':\n","AFTER":"        self.trunk = timm.create_model(model_name, pretrained=pretrained)\n        feat_size = self.trunk.default_cfg.get('pool_size', None)\n        feature_ndim = 1 if not feat_size else 2\n        if pool in ('abs_attn', 'rot_attn'):\n            assert feature_ndim == 2\n            # if attn pooling used, remove both classifier and default pool\n            self.trunk.reset_classifier(0, global_pool='')\n        else:\n            # reset global pool if pool config set, otherwise leave as network default\n            reset_kwargs = dict(global_pool=pool) if pool else {}\n            self.trunk.reset_classifier(0, **reset_kwargs)\n        prev_chs = self.trunk.num_features\n\n        head_layers = OrderedDict()\n        if pool == 'abs_attn':\n            head_layers['pool'] = AbsAttentionPool2d(prev_chs, feat_size=feat_size, out_features=embed_dim)\n            prev_chs = embed_dim\n        elif pool == 'rot_attn':\n            head_layers['pool'] = RotAttentionPool2d(prev_chs, out_features=embed_dim)\n            prev_chs = embed_dim\n        else:\n            assert proj, 'projection layer needed if other pooling used'\n\n        # NOTE attention pool ends with a projection layer, so proj should usually be set to '' if such pooling is used\n        if proj == 'linear':\n"}