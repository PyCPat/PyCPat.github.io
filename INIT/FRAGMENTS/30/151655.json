{"BEFORE":"            hidden_features = [hidden_features]\n        self.layers = nn.ModuleList()\n        if layer_norm:\n            self.layers.append(nn.LayerNorm(in_features))\n        self.layers.append(nn.Linear(in_features, hidden_features[0]))\n        for i in range(len(hidden_features) - 1):\n            if layer_norm:\n                self.layers.append(nn.LayerNorm(hidden_features[i]))\n            self.layers.append(nn.Linear(hidden_features[i], hidden_features[i+1]))\n        self.layers.append(nn.Linear(hidden_features[-1], out_features))\n","AFTER":"                 n_layers,\n                 layer_norm=False,\n                 activation=F.relu,\n                 edge_drop=0.0,\n                 alpha=0.01,\n                 k=10,\n                 feat_norm=None,\n                 adj_norm_func=GCNAdjNorm,\n                 dropout=0.0):\n        super(APPNP, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.feat_norm = feat_norm\n        self.adj_norm_func = adj_norm_func\n        if type(hidden_features) is int:\n            hidden_features = [hidden_features] * (n_layers - 1)\n        elif type(hidden_features) is list or type(hidden_features) is tuple:\n            assert len(hidden_features) == (n_layers - 1), \"Incompatible sizes between hidden_features and n_layers.\"\n        n_features = [in_features] + hidden_features + [out_features]\n\n        self.layers = nn.ModuleList()\n        for i in range(n_layers):\n            if layer_norm:\n                self.layers.append(nn.LayerNorm(n_features[i]))\n            self.layers.append(nn.Linear(n_features[i], n_features[i + 1]))\n"}