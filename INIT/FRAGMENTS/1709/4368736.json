{"BEFORE":"        downsample_klass = DownsampleWithBlur if antialias_downsample else Downsample\n\n        if cross_embed_downsample:\n            downsample_klass = partial(CrossEmbedLayer, kernel_sizes = cross_embed_downsample_kernel_sizes)\n\n        # scale for resnet skip connections\n\n        self.skip_connect_scale = 1. if not scale_skip_connection else (2 ** -0.5)\n\n        # for debugging purposes\n\n        self.downsample_concat_hiddens_earlier = downsample_concat_hiddens_earlier\n\n        # layers\n\n        self.downs = nn.ModuleList([])\n        self.ups = nn.ModuleList([])\n        num_resolutions = len(in_out)\n\n        layer_params = [num_resnet_blocks, resnet_groups, layer_attns, layer_cross_attns]\n        reversed_layer_params = list(map(reversed, layer_params))\n\n        # downsampling layers\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_cross_attn) in enumerate(zip(in_out, *layer_params)):\n            is_last = ind >= (num_resolutions - 1)\n\n            layer_use_linear_cross_attn = not layer_cross_attn and use_linear_cross_attn\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            transformer_block_klass = TransformerBlock if layer_attn else (LinearAttentionTransformerBlock if use_linear_attn else nn.Identity)\n\n            self.downs.append(nn.ModuleList([\n                downsample_klass(dim_in) if memory_efficient else None,\n                ResnetBlock(dim_in, dim_out, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(dim_out, dim_out, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = dim_out, heads = attn_heads, dim_head = attn_dim_head, ff_mult = ff_mult),\n                downsample_klass(dim_out) if not memory_efficient and not is_last else None,\n            ]))\n\n        # middle layers\n\n        mid_dim = dims[-1]\n\n        self.mid_block1 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n        self.mid_attn = EinopsToAndFrom('b c h w', 'b (h w) c', Residual(Attention(mid_dim, **attn_kwargs))) if attend_at_middle else None\n        self.mid_block2 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n\n        # upsampling klass\n\n        assert not (nearest_neighbor_upsample and bilinear_upsample)\n\n        if bilinear_upsample:\n            upsample_klass = partial(InterpolateUpsample, mode = 'bilinear')\n        elif nearest_neighbor_upsample:\n            upsample_klass = partial(InterpolateUpsample, mode = 'nearest')\n        else:\n            upsample_klass = Upsample\n\n        # upsampling layers\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_cross_attn) in enumerate(zip(reversed(in_out), *reversed_layer_params)):\n            is_last = ind == (len(in_out) - 1)\n            layer_use_linear_cross_attn = not layer_cross_attn and use_linear_cross_attn\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n            transformer_block_klass = TransformerBlock if layer_attn else (LinearAttentionTransformerBlock if use_linear_attn else nn.Identity)\n\n            self.ups.append(nn.ModuleList([\n                ResnetBlock(dim_out * 2, dim_in, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(dim_in, dim_in, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = dim_in, heads = attn_heads, dim_head = attn_dim_head, ff_mult = ff_mult),\n                upsample_klass(dim_in) if not is_last or memory_efficient else nn.Identity()\n            ]))\n","AFTER":"        downsample_klass = Downsample\n\n        if cross_embed_downsample:\n            downsample_klass = partial(CrossEmbedLayer, kernel_sizes = cross_embed_downsample_kernel_sizes)\n\n        # scale for resnet skip connections\n\n        self.skip_connect_scale = 1. if not scale_skip_connection else (2 ** -0.5)\n\n        # layers\n\n        self.downs = nn.ModuleList([])\n        self.ups = nn.ModuleList([])\n        num_resolutions = len(in_out)\n\n        layer_params = [num_resnet_blocks, resnet_groups, layer_attns, layer_cross_attns]\n        reversed_layer_params = list(map(reversed, layer_params))\n\n        # downsampling layers\n\n        skip_connect_dims = [] # keep track of skip connection dimensions\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_cross_attn) in enumerate(zip(in_out, *layer_params)):\n            is_last = ind >= (num_resolutions - 1)\n\n            layer_use_linear_cross_attn = not layer_cross_attn and use_linear_cross_attn\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            transformer_block_klass = TransformerBlock if layer_attn else (LinearAttentionTransformerBlock if use_linear_attn else nn.Identity)\n\n            current_dim = dim_in\n\n            # whether to pre-downsample, from memory efficient unet\n\n            pre_downsample = None\n\n            if memory_efficient:\n                pre_downsample = downsample_klass(dim_in, dim_out)\n                current_dim = dim_out\n\n            skip_connect_dims.append(current_dim)\n\n            self.downs.append(nn.ModuleList([\n                pre_downsample,\n                ResnetBlock(current_dim, current_dim, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(current_dim, current_dim, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = current_dim, heads = attn_heads, dim_head = attn_dim_head, ff_mult = ff_mult),\n                downsample_klass(current_dim, dim_out) if not memory_efficient and not is_last else None,\n            ]))\n\n        # middle layers\n\n        mid_dim = dims[-1]\n\n        self.mid_block1 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n        self.mid_attn = EinopsToAndFrom('b c h w', 'b (h w) c', Residual(Attention(mid_dim, **attn_kwargs))) if attend_at_middle else None\n        self.mid_block2 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n\n        # upsampling layers\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_cross_attn) in enumerate(zip(reversed(in_out), *reversed_layer_params)):\n            is_last = ind == (len(in_out) - 1)\n            layer_use_linear_cross_attn = not layer_cross_attn and use_linear_cross_attn\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n            transformer_block_klass = TransformerBlock if layer_attn else (LinearAttentionTransformerBlock if use_linear_attn else nn.Identity)\n\n            skip_connect_dim = skip_connect_dims.pop()\n\n            self.ups.append(nn.ModuleList([\n                ResnetBlock(dim_out + skip_connect_dim, dim_out, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(dim_out + skip_connect_dim, dim_out, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = dim_out, heads = attn_heads, dim_head = attn_dim_head, ff_mult = ff_mult),\n                Upsample(dim_out, dim_in) if not is_last or memory_efficient else nn.Identity()\n            ]))\n"}