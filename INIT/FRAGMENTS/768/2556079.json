{"BEFORE":"    def __init__(self,\n                 dim,\n                 ffn_type=\"standard\",\n                 ff_mult=4,\n                 dropout=0.):\n        \"\"\"\n        This is a feed-forward network (FFN). Each input (hidden state or embedding) will be ran through the exact same\n        network (weight sharing), in transformers this means each token (word, or sub-word) will be processed through\n        the same FFN in a batch process. Unlike the attention or convolution mechanisms, the input cannot \"see\" the rest\n        of the sequence in this operation.\n        :param dim: The dimension of the FFN (aka the number of features); for transformers they are almost always equal\n        to the dimension of the model (d_model).\n        :param ffn_type: GEGLU and bilinear techniques have 1\/3 more parameters than the standard option.\n        :param ff_mult: Transformers typically use an FFN inner size that is 4 times larger than the input dimension.\n        Apparently, a value of 4 optimizes run-time on current hardware (GPUs).\n        :param dropout: When training a model for 1 epoch only, dropout likely isn't needed, but it's an option.\n        \"\"\"\n        # The GELU non-linearity (aka activation function) is standard for an FFN. The choice of non-linearities don't\n        # have much of an impact on final performance anyway. See page 8 (https:\/\/arxiv.org\/pdf\/2102.11972.pdf)\n        super().__init__()\n\n        inner_dim = int(dim * ff_mult)\n        if ffn_type == \"GEGLU\":\n            self.net = nn.Sequential(\n                GEGLU(dim, inner_dim),\n                nn.Dropout(dropout),\n                nn.Linear(inner_dim, dim)\n            )\n        elif ffn_type == \"standard\":\n            self.net = nn.Sequential(\n                nn.Linear(dim, inner_dim),\n                nn.GELU(),\n                nn.Dropout(dropout),\n                nn.Linear(inner_dim, dim)\n            )\n        elif ffn_type == \"bilinear\":\n            self.net = nn.Sequential(\n                Bilinear(dim, inner_dim),\n                nn.Dropout(dropout),\n                nn.Linear(inner_dim, dim)\n            )\n        else:\n            print(f'ffn_type does not match any available options')\n\n        # despite many varieties of applying layer norms in transformers, pre-norm still seems like the best option\n        self.input_norm = nn.LayerNorm(dim)\n","AFTER":"    def __init__(self, heads, max_length):\n        super().__init__()\n        \"\"\"\n        This builds the matrix seen on the right side of Figure 3 - https:\/\/arxiv.org\/pdf\/2108.12409.pdf . It is a \n        relative position bias (since it biases the attention pattern based on the relative positions of the inputs). \n        Right now, I only have an encoder implementation of this matrix (which isn't mentioned in the paper). The causal\n        version will be added shortly; it's actually just the lower triangle portion.\n        \n        From the paper, Alibi seems to perform on par with RoPE, but it can generalize to longer sequences not seen\n        during training. RoPE cannot do that. EleutherAI discord experiments\/rumors claim that using Alibi and RoPE\n        slightly improves performance; my experiments would agree. \n        \"\"\"\n\n        self.heads = heads\n\n        rows = []\n        for i in range(max_length):  # build the full Alibi relative position bias matrix\n            rows.append(torch.LongTensor([x for x in range(0 - i, max_length - i)]).view(1, -1).abs())\n\n        # This implementation alternates between upper triangular and lower triangular biases. Using the full matrix\n        # doesn't seem to work as well - likely since the forward and backward biases would be identical (i.e. attention\n        # wouldn't be able to tell the difference between a token X spots after or X spots before). However, the lead\n        # author of Alibi claims that using different biases on forward vs backward positions may work.\n\n        lower_tri_rows = -torch.cat(rows, 0).tril()\n        upper_tri_rows = -torch.cat(rows, 0).triu()\n\n        lower_tri_rows = rearrange(lower_tri_rows, 'i j -> () i j')\n        upper_tri_rows = rearrange(upper_tri_rows, 'i j -> () i j')\n        slopes = self._get_slopes(heads=int(heads \/ 2))\n\n        all_rows = []\n        for h_ in range(int(heads \/ 2)):\n            all_rows.append(lower_tri_rows * slopes[h_])\n            all_rows.append(upper_tri_rows * slopes[h_])\n\n        # The resultant bias applies the Alibi position bias looking forward to half of the heads, and backwards to the\n        # other half. Since for each head, only 1 direction contains positional information, you should probably use\n        # RoPE along with Alibi, to give the opposite direction some positional information.\n\n        self.bias = torch.cat(all_rows, dim=0).cuda()  # shape (heads, max_length, max_length)\n"}