<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 During pretraining dropout might be set to 0. However, we might want
        &#47&#47 to apply dropout when fine-tuning on a downstream task. Hence we need
        &#47&#47 to modify the fairseq cfg to activate dropout (if requested).
        <a id="change">if </a>not freeze and dropout is not None<a id="change">:
            </a>overrides<a id="change"> = </a><a id="change">{
                </a>"model": <a id="change">{
                    </a>"dropout": dropout,
                    "encoder_layerdrop": dropout,
                    "dropout_input": dropout,
                    "attention_dropout": dropout<a id="change">,
                }</a><a id="change">
            }</a>
        else:
            overrides = {}

        (</code></pre><h3>After Change</h3><pre><code class='java'>
                logger.warning(
                    "speechbrain.lobes.models.fairseq_wav2vec - wav2vec 2.0 feature extractor is frozen."
                )
                <a id="change">for param</a> in <a id="change">self.model.feature_extractor.parameters():
                    </a>param.requires_grad<a id="change"> = </a>False

        &#47&#47 Randomly initialized layers if pretrain is False
        if not (pretrain):</code></pre>