<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        self.use_res_connect = self.stride == 1 and in_channels == out_channels

        self.inv_block = <a id="change">nn.Sequential(
            nn.Conv2d(</a>in_channels, in_channels * expand_ratio, <a id="change">1</a>, <a id="change">1</a>, <a id="change">0</a><a id="change">, bias=False)</a>,
            nn.BatchNorm2d(in_channels * expand_ratio),
            nn.PReLU(),

            <a id="change">nn.Conv2d(</a>in_channels * expand_ratio, in_channels * expand_ratio, <a id="change">3</a>, stride, <a id="change">1</a><a id="change">,
                      groups=in_channels * expand_ratio, bias=False)</a>,
            nn.BatchNorm2d(in_channels * expand_ratio),
            nn.PReLU(),

            <a id="change">nn.Conv2d(</a>in_channels * expand_ratio, out_channels, <a id="change">1</a>, <a id="change">1</a>, <a id="change">0</a><a id="change">, bias=False)</a>,
            nn.BatchNorm2d(out_channels),
            SELayer(out_channels, 8, nn.PReLU, outp_size)<a id="change">
        )</a>

    def forward(self, x):
        if self.use_res_connect:
            return x + self.inv_block(x)</code></pre><h3>After Change</h3><pre><code class='java'>
            &#47&#47 dw
            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim),
            &#47&#47 pw-linear
            <a id="change">nn.Conv2d(</a>hidden_dim, oup, <a id="change">1</a>, <a id="change">1</a>, <a id="change">0</a><a id="change">, bias=False)</a>,
            nn.BatchNorm2d(oup),
        ])
        self.conv = nn.Sequential(*layers)</code></pre>