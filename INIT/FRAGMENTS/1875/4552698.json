{"BEFORE":"        self.hidden_size = args.hidden_size\n        self.bias = args.bias\n        self.depth = args.depth\n        self.use_layer_norm = args.layer_norm\n        self.dropout = args.dropout\n        self.attention = args.attention\n        self.message_attention = args.message_attention\n        self.global_attention = args.global_attention\n        self.message_attention_heads = args.message_attention_heads\n        self.master_node = args.master_node\n        self.master_dim = args.master_dim\n        self.use_master_as_output = args.use_master_as_output\n        self.deepset = args.deepset\n        self.set2set = args.set2set\n        self.set2set_iters = args.set2set_iters\n        self.args = args\n\n        # Input\n        self.W_i = nn.Linear(self.bond_fdim, self.hidden_size, bias=self.bias)\n\n        # Message passing\n        if self.message_attention:\n            self.num_heads = self.message_attention_heads\n            self.W_h = nn.Linear(self.num_heads * self.hidden_size, self.hidden_size, bias=self.bias)\n            self.W_ma = nn.ModuleList([nn.Linear(self.hidden_size, self.hidden_size, bias=self.bias)\n                                       for _ in range(self.num_heads)])\n            # uncomment this later if you want attention over binput + nei_message? or on atom incoming at end\n            # self.W_ma2 = nn.Linear(hidden_size, 1, bias=self.bias)\n        else:\n            self.W_h = nn.Linear(self.hidden_size, self.hidden_size, bias=self.bias)\n\n        if self.global_attention:\n            self.W_ga1 = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n            self.W_ga2 = nn.Linear(self.hidden_size, self.hidden_size)\n\n        if self.master_node:\n            # self.GRU_master = nn.GRU(self.hidden_size, self.master_dim)\n            self.W_master_in = nn.Linear(self.hidden_size, self.master_dim)\n            self.W_master_out = nn.Linear(self.master_dim, self.hidden_size)\n            # self.layer_norm = nn.LayerNorm(self.hidden_size)\n\n        # Readout\n        if not (self.master_node and self.use_master_as_output):\n            self.W_o = nn.Linear(self.atom_fdim + self.hidden_size, self.hidden_size)\n\n        if self.deepset:\n            self.W_s2s_a = nn.Linear(self.hidden_size, self.hidden_size, bias=self.bias)\n            self.W_s2s_b = nn.Linear(self.hidden_size, self.hidden_size, bias=self.bias)\n\n        if self.set2set:\n            self.set2set_rnn = nn.LSTM(\n                input_size=self.hidden_size,\n                hidden_size=self.hidden_size,\n                dropout=self.dropout,\n                bias=False  # no bias so that an input of all zeros stays all zero\n            )\n\n        if self.attention:\n            self.W_a = nn.Linear(self.hidden_size, self.hidden_size, bias=self.bias)\n            self.W_b = nn.Linear(self.hidden_size, self.hidden_size)\n\n        # Layer norm\n        if self.use_layer_norm:\n            self.layer_norm = nn.LayerNorm(self.hidden_size)\n\n        # Dropout\n        self.dropout_layer = nn.Dropout(p=self.dropout)\n\n        # Activation\n        if args.activation == 'ReLU':\n            self.act_func = nn.ReLU()\n        elif args.activation == 'LeakyReLU':\n            self.act_func = nn.LeakyReLU(0.1)\n        elif args.activation == 'PReLU':\n            self.act_func = nn.PReLU()\n        elif args.activation == 'tanh':\n            self.act_func = nn.Tanh()\n        else:\n            raise ValueError('Activation \"{}\" not supported.'.format(args.activation))\n\n        self.cached_zero_vector = torch.zeros(self.hidden_size).cuda()\n","AFTER":"        self.hidden_size = args.hidden_size\n        self.bias = args.bias\n        self.depth = args.depth\n        self.use_layer_norm = args.layer_norm\n        self.dropout = args.dropout\n        self.attention = args.attention\n        self.message_attention = args.message_attention\n        self.global_attention = args.global_attention\n        self.message_attention_heads = args.message_attention_heads\n        self.master_node = args.master_node\n        self.master_dim = args.master_dim\n        self.use_master_as_output = args.use_master_as_output\n        self.deepset = args.deepset\n        self.set2set = args.set2set\n        self.set2set_iters = args.set2set_iters\n        self.args = args\n\n        # Input\n        self.W_i = nn.Linear(self.bond_fdim, self.hidden_size, bias=self.bias)\n\n        # Message passing\n        if self.message_attention:\n            self.num_heads = self.message_attention_heads\n            self.W_h = nn.Linear(self.num_heads * self.hidden_size, self.hidden_size, bias=self.bias)\n            self.W_ma = nn.ModuleList([nn.Linear(self.hidden_size, self.hidden_size, bias=self.bias)\n                                       for _ in range(self.num_heads)])\n            # uncomment this later if you want attention over binput + nei_message? or on atom incoming at end\n            # self.W_ma2 = nn.Linear(hidden_size, 1, bias=self.bias)\n        else:\n            self.W_h = nn.Linear(self.hidden_size, self.hidden_size, bias=self.bias)\n\n        if self.global_attention:\n            self.W_ga1 = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n            self.W_ga2 = nn.Linear(self.hidden_size, self.hidden_size)\n\n        if self.master_node:\n            # self.GRU_master = nn.GRU(self.hidden_size, self.master_dim)\n            self.W_master_in = nn.Linear(self.hidden_size, self.master_dim)\n            self.W_master_out = nn.Linear(self.master_dim, self.hidden_size)\n            # self.layer_norm = nn.LayerNorm(self.hidden_size)\n\n        # Readout\n        if not (self.master_node and self.use_master_as_output):\n            self.W_o = nn.Linear(self.atom_fdim + self.hidden_size, self.hidden_size)\n\n        if self.deepset:\n            self.W_s2s_a = nn.Linear(self.hidden_size, self.hidden_size, bias=self.bias)\n            self.W_s2s_b = nn.Linear(self.hidden_size, self.hidden_size, bias=self.bias)\n\n        if self.set2set:\n            self.set2set_rnn = nn.LSTM(\n                input_size=self.hidden_size,\n                hidden_size=self.hidden_size,\n                dropout=self.dropout,\n                bias=False  # no bias so that an input of all zeros stays all zero\n            )\n\n        if self.attention:\n            self.W_a = nn.Linear(self.hidden_size, self.hidden_size, bias=self.bias)\n            self.W_b = nn.Linear(self.hidden_size, self.hidden_size)\n\n        # Layer norm\n        if self.use_layer_norm:\n            self.layer_norm = nn.LayerNorm(self.hidden_size)\n\n        # Dropout\n        self.dropout_layer = nn.Dropout(p=self.dropout)\n\n        # Activation\n        if args.activation == 'ReLU':\n            self.act_func = nn.ReLU()\n        elif args.activation == 'LeakyReLU':\n            self.act_func = nn.LeakyReLU(0.1)\n        elif args.activation == 'PReLU':\n            self.act_func = nn.PReLU()\n        elif args.activation == 'tanh':\n            self.act_func = nn.Tanh()\n        else:\n            raise ValueError('Activation \"{}\" not supported.'.format(args.activation))\n\n        self.cached_zero_vector = torch.zeros(self.hidden_size)\n        if args.cuda:\n            self.cached_zero_vector = self.cached_zero_vector.cuda()\n\n    def forward(self, mol_graph: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, List[Tuple[int, int]], List[Tuple[int, int]]]) -> torch.Tensor:\n"}