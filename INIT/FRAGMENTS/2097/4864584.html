<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        assert norm_cfg is None or isinstance(norm_cfg, dict)
        self.conv_cfg = conv_cfg
        self.norm_cfg = norm_cfg
        <a id="change">self.activation</a> = activation
        self.inplace = inplace
        self.order = order
        assert isinstance(self.order, tuple) and len(self.order) == 3
        assert set(order) == set([&quotconv&quot, &quotnorm&quot, &quotact&quot])

        self.with_norm = norm_cfg is not None
        self.with_activation = activation is not None
        &#47&#47 if the conv layer is before a norm layer, bias is unnecessary.
        if bias == &quotauto&quot:
            bias = False if self.with_norm else True
        self.with_bias = bias

        if self.with_norm and self.with_bias:
            warnings.warn(&quotConvModule has norm and bias at the same time&quot)

        &#47&#47 build convolution layer
        self.conv = build_conv_layer(
            conv_cfg,
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
            bias=bias)
        &#47&#47 export the attributes of self.conv to a higher level for convenience
        self.in_channels = self.conv.in_channels
        self.out_channels = self.conv.out_channels
        self.kernel_size = self.conv.kernel_size
        self.stride = self.conv.stride
        self.padding = self.conv.padding
        self.dilation = self.conv.dilation
        self.transposed = self.conv.transposed
        self.output_padding = self.conv.output_padding
        self.groups = self.conv.groups

        &#47&#47 build normalization layers
        if self.with_norm:
            &#47&#47 norm layer is after conv layer
            if order.index(&quotnorm&quot) &gt; order.index(&quotconv&quot):
                norm_channels = out_channels
            else:
                norm_channels = in_channels
            self.norm_name, norm = build_norm_layer(norm_cfg, norm_channels)
            self.add_module(self.norm_name, norm)

        &#47&#47 build activation layer
        if self.with_activation:
            &#47&#47 TODO: introduce `act_cfg` and supports more activation layers
            <a id="change">if </a><a id="change">self.activation not in [&quotrelu&quot]:
                </a><a id="change">raise ValueError(</a>&quot{} is currently not supported.&quot.format(
                    self.activation)<a id="change">)</a>
            <a id="change">if self.activation == &quotrelu&quot</a><a id="change">:
                </a>self.activate<a id="change"> = </a>nn.ReLU(inplace=inplace)

        &#47&#47 Use msra init by default
        self.init_weights()</code></pre><h3>After Change</h3><pre><code class='java'>
        assert act_cfg is None or isinstance(act_cfg, dict)
        self.conv_cfg = conv_cfg
        self.norm_cfg = norm_cfg
        self.act_cfg<a id="change"> = </a>act_cfg
        self.inplace = inplace
        self.order = order
        assert isinstance(self.order, tuple) and len(self.order) == 3
        assert set(order) == set([&quotconv&quot, &quotnorm&quot, &quotact&quot])

        self.with_norm = norm_cfg is not None
        self.with_activation = act_cfg is not None
        &#47&#47 if the conv layer is before a norm layer, bias is unnecessary.
        if bias == &quotauto&quot:
            bias = False if self.with_norm else True
        self.with_bias = bias

        if self.with_norm and self.with_bias:
            warnings.warn(&quotConvModule has norm and bias at the same time&quot)

        &#47&#47 build convolution layer
        self.conv = build_conv_layer(
            conv_cfg,
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
            bias=bias)
        &#47&#47 export the attributes of self.conv to a higher level for convenience
        self.in_channels = self.conv.in_channels
        self.out_channels = self.conv.out_channels
        self.kernel_size = self.conv.kernel_size
        self.stride = self.conv.stride
        self.padding = self.conv.padding
        self.dilation = self.conv.dilation
        self.transposed = self.conv.transposed
        self.output_padding = self.conv.output_padding
        self.groups = self.conv.groups

        &#47&#47 build normalization layers
        if self.with_norm:
            &#47&#47 norm layer is after conv layer
            if order.index(&quotnorm&quot) &gt; order.index(&quotconv&quot):
                norm_channels = out_channels
            else:
                norm_channels = in_channels
            self.norm_name, norm = build_norm_layer(norm_cfg, norm_channels)
            self.add_module(self.norm_name, norm)

        &#47&#47 build activation layer
        if self.with_activation:
            act_cfg_<a id="change"> = </a><a id="change">act_cfg.copy()</a>
            <a id="change">act_cfg_.setdefault(&quotinplace&quot</a>, inplace<a id="change">)</a>
            self.activate<a id="change"> = </a>build_activation_layer(act_cfg_)

        &#47&#47 Use msra init by default
        self.init_weights()</code></pre>