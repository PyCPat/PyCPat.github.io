{"BEFORE":"        print('---------------AST Model Summary---------------')\n        assert timm.__version__ == '0.4.5', 'Please use timm == 0.4.5, the code might not be compatible with newer versions.'\n        print('ImageNet pretraining: {:s}, AudioSet pretraining: {:s}'.format(str(imagenet_pretrain),str(audioset_pretrain)))\n        # override timm input shape restriction\n        timm.models.vision_transformer.PatchEmbed = PatchEmbed\n\n        # if AudioSet pretraining is not used (but ImageNet pretraining may still apply)\n        if audioset_pretrain == False:\n            if model_size == 'tiny224':\n                self.v = timm.create_model('vit_deit_tiny_distilled_patch16_224', pretrained=imagenet_pretrain)\n            elif model_size == 'small224':\n                self.v = timm.create_model('vit_deit_small_distilled_patch16_224', pretrained=imagenet_pretrain)\n            elif model_size == 'base224':\n                self.v = timm.create_model('vit_deit_base_distilled_patch16_224', pretrained=imagenet_pretrain)\n            elif model_size == 'base384':\n                self.v = timm.create_model('vit_deit_base_distilled_patch16_384', pretrained=imagenet_pretrain)\n            else:\n                raise Exception('Model size must be one of tiny224, small224, base224, base384.')\n            self.original_num_patches = self.v.patch_embed.num_patches\n            self.oringal_hw = int(self.original_num_patches ** 0.5)\n            self.original_embedding_dim = self.v.pos_embed.shape[2]\n            self.mlp_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim), nn.Linear(self.original_embedding_dim, label_dim))\n\n            # automatcially get the intermediate shape\n            f_dim, t_dim = self.get_shape(fstride, tstride, input_fdim, input_tdim)\n            num_patches = f_dim * t_dim\n            self.v.patch_embed.num_patches = num_patches\n            print('frequncey stride={:d}, time stride={:d}'.format(fstride, tstride))\n            print('number of patches={:d}'.format(num_patches))\n\n            # the linear projection layer\n            new_proj = torch.nn.Conv2d(1, self.original_embedding_dim, kernel_size=(16, 16), stride=(fstride, tstride))\n            if imagenet_pretrain == True:\n                new_proj.weight = torch.nn.Parameter(torch.sum(self.v.patch_embed.proj.weight, dim=1).unsqueeze(1))\n                new_proj.bias = self.v.patch_embed.proj.bias\n            self.v.patch_embed.proj = new_proj\n\n            # the positional embedding\n            if imagenet_pretrain == True:\n                # get the positional embedding from deit model, skip the first two tokens (cls token and distillation token), reshape it to original 2D shape (24*24).\n                new_pos_embed = self.v.pos_embed[:, 2:, :].detach().reshape(1, self.original_num_patches, self.original_embedding_dim).transpose(1, 2).reshape(1, self.original_embedding_dim, self.oringal_hw, self.oringal_hw)\n                # cut (from middle) or interpolate the second dimension of the positional embedding\n                if t_dim <= self.oringal_hw:\n                    new_pos_embed = new_pos_embed[:, :, :, int(self.oringal_hw \/ 2) - int(t_dim \/ 2): int(self.oringal_hw \/ 2) - int(t_dim \/ 2) + t_dim]\n                else:\n                    new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(self.oringal_hw, t_dim), mode='bilinear')\n                # cut (from middle) or interpolate the first dimension of the positional embedding\n                if f_dim <= self.oringal_hw:\n                    new_pos_embed = new_pos_embed[:, :, int(self.oringal_hw \/ 2) - int(f_dim \/ 2): int(self.oringal_hw \/ 2) - int(f_dim \/ 2) + f_dim, :]\n                else:\n                    new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(f_dim, t_dim), mode='bilinear')\n                # flatten the positional embedding\n                new_pos_embed = new_pos_embed.reshape(1, self.original_embedding_dim, num_patches).transpose(1,2)\n                # concatenate the above positional embedding with the cls token and distillation token of the deit model.\n                self.v.pos_embed = nn.Parameter(torch.cat([self.v.pos_embed[:, :2, :].detach(), new_pos_embed], dim=1))\n            else:\n                # if not use imagenet pretrained model, just randomly initialize a learnable positional embedding\n                # TODO can use sinusoidal positional embedding instead\n                new_pos_embed = nn.Parameter(torch.zeros(1, self.v.patch_embed.num_patches + 2, self.original_embedding_dim))\n                self.v.pos_embed = new_pos_embed\n                trunc_normal_(self.v.pos_embed, std=.02)\n\n        # now load a model that is pretrained on both ImageNet and AudioSet\n        elif audioset_pretrain == True:\n            if audioset_pretrain == True and imagenet_pretrain == False:\n                raise ValueError('currently model pretrained on only audioset is not supported, please set imagenet_pretrain = True to use audioset pretrained model.')\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n            if os.path.exists('..\/..\/pretrained_models\/ast_audioset.pth') == False:\n                # this model performs 0.4593 mAP on the audioset eval set\n                audioset_mdl_url = 'https:\/\/www.dropbox.com\/s\/cv4knew8mvbrnvq\/audioset_0.4593.pth?dl=1'\n                wget.download(audioset_mdl_url, out='..\/..\/pretrained_models\/ast_audioset.pth')\n            sd = torch.load('..\/..\/pretrained_models\/ast_audioset.pth', map_location=device)\n            audio_model = ASTModel(label_dim=527, fstride=10, tstride=10, input_fdim=128, input_tdim=1024, imagenet_pretrain=False, audioset_pretrain=False, model_size='base384')\n            audio_model = torch.nn.DataParallel(audio_model)\n            audio_model.load_state_dict(sd, strict=False)\n            self.v = audio_model.module.v\n            self.original_embedding_dim = self.v.pos_embed.shape[2]\n            self.mlp_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim), nn.Linear(self.original_embedding_dim, label_dim))\n\n            f_dim, t_dim = self.get_shape(fstride, tstride, input_fdim, input_tdim)\n            num_patches = f_dim * t_dim\n            self.v.patch_embed.num_patches = num_patches\n            print('frequncey stride={:d}, time stride={:d}'.format(fstride, tstride))\n            print('number of patches={:d}'.format(num_patches))\n","AFTER":"    def __init__(self, label_dim=527, fstride=10, tstride=10, input_fdim=128, input_tdim=1024, imagenet_pretrain=True, audioset_pretrain=False, model_size='base384', verbose=True):\n\n        super(ASTModel, self).__init__()\n        assert timm.__version__ == '0.4.5', 'Please use timm == 0.4.5, the code might not be compatible with newer versions.'\n\n        if verbose == True:\n            print('---------------AST Model Summary---------------')\n            print('ImageNet pretraining: {:s}, AudioSet pretraining: {:s}'.format(str(imagenet_pretrain),str(audioset_pretrain)))\n        # override timm input shape restriction\n        timm.models.vision_transformer.PatchEmbed = PatchEmbed\n\n        # if AudioSet pretraining is not used (but ImageNet pretraining may still apply)\n        if audioset_pretrain == False:\n            if model_size == 'tiny224':\n                self.v = timm.create_model('vit_deit_tiny_distilled_patch16_224', pretrained=imagenet_pretrain)\n            elif model_size == 'small224':\n                self.v = timm.create_model('vit_deit_small_distilled_patch16_224', pretrained=imagenet_pretrain)\n            elif model_size == 'base224':\n                self.v = timm.create_model('vit_deit_base_distilled_patch16_224', pretrained=imagenet_pretrain)\n            elif model_size == 'base384':\n                self.v = timm.create_model('vit_deit_base_distilled_patch16_384', pretrained=imagenet_pretrain)\n            else:\n                raise Exception('Model size must be one of tiny224, small224, base224, base384.')\n            self.original_num_patches = self.v.patch_embed.num_patches\n            self.oringal_hw = int(self.original_num_patches ** 0.5)\n            self.original_embedding_dim = self.v.pos_embed.shape[2]\n            self.mlp_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim), nn.Linear(self.original_embedding_dim, label_dim))\n\n            # automatcially get the intermediate shape\n            f_dim, t_dim = self.get_shape(fstride, tstride, input_fdim, input_tdim)\n            num_patches = f_dim * t_dim\n            self.v.patch_embed.num_patches = num_patches\n            if verbose == True:\n                print('frequncey stride={:d}, time stride={:d}'.format(fstride, tstride))\n                print('number of patches={:d}'.format(num_patches))\n\n            # the linear projection layer\n            new_proj = torch.nn.Conv2d(1, self.original_embedding_dim, kernel_size=(16, 16), stride=(fstride, tstride))\n            if imagenet_pretrain == True:\n                new_proj.weight = torch.nn.Parameter(torch.sum(self.v.patch_embed.proj.weight, dim=1).unsqueeze(1))\n                new_proj.bias = self.v.patch_embed.proj.bias\n            self.v.patch_embed.proj = new_proj\n\n            # the positional embedding\n            if imagenet_pretrain == True:\n                # get the positional embedding from deit model, skip the first two tokens (cls token and distillation token), reshape it to original 2D shape (24*24).\n                new_pos_embed = self.v.pos_embed[:, 2:, :].detach().reshape(1, self.original_num_patches, self.original_embedding_dim).transpose(1, 2).reshape(1, self.original_embedding_dim, self.oringal_hw, self.oringal_hw)\n                # cut (from middle) or interpolate the second dimension of the positional embedding\n                if t_dim <= self.oringal_hw:\n                    new_pos_embed = new_pos_embed[:, :, :, int(self.oringal_hw \/ 2) - int(t_dim \/ 2): int(self.oringal_hw \/ 2) - int(t_dim \/ 2) + t_dim]\n                else:\n                    new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(self.oringal_hw, t_dim), mode='bilinear')\n                # cut (from middle) or interpolate the first dimension of the positional embedding\n                if f_dim <= self.oringal_hw:\n                    new_pos_embed = new_pos_embed[:, :, int(self.oringal_hw \/ 2) - int(f_dim \/ 2): int(self.oringal_hw \/ 2) - int(f_dim \/ 2) + f_dim, :]\n                else:\n                    new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(f_dim, t_dim), mode='bilinear')\n                # flatten the positional embedding\n                new_pos_embed = new_pos_embed.reshape(1, self.original_embedding_dim, num_patches).transpose(1,2)\n                # concatenate the above positional embedding with the cls token and distillation token of the deit model.\n                self.v.pos_embed = nn.Parameter(torch.cat([self.v.pos_embed[:, :2, :].detach(), new_pos_embed], dim=1))\n            else:\n                # if not use imagenet pretrained model, just randomly initialize a learnable positional embedding\n                # TODO can use sinusoidal positional embedding instead\n                new_pos_embed = nn.Parameter(torch.zeros(1, self.v.patch_embed.num_patches + 2, self.original_embedding_dim))\n                self.v.pos_embed = new_pos_embed\n                trunc_normal_(self.v.pos_embed, std=.02)\n\n        # now load a model that is pretrained on both ImageNet and AudioSet\n        elif audioset_pretrain == True:\n            if audioset_pretrain == True and imagenet_pretrain == False:\n                raise ValueError('currently model pretrained on only audioset is not supported, please set imagenet_pretrain = True to use audioset pretrained model.')\n            if model_size != 'base384':\n                raise ValueError('currently only has base384 AudioSet pretrained model.')\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n            if os.path.exists('..\/..\/pretrained_models\/ast_audioset.pth') == False:\n                # this model performs 0.4593 mAP on the audioset eval set\n                audioset_mdl_url = 'https:\/\/www.dropbox.com\/s\/cv4knew8mvbrnvq\/audioset_0.4593.pth?dl=1'\n                wget.download(audioset_mdl_url, out='..\/..\/pretrained_models\/ast_audioset.pth')\n            sd = torch.load('..\/..\/pretrained_models\/ast_audioset.pth', map_location=device)\n            audio_model = ASTModel(label_dim=527, fstride=10, tstride=10, input_fdim=128, input_tdim=1024, imagenet_pretrain=False, audioset_pretrain=False, model_size='base384', verbose=False)\n            audio_model = torch.nn.DataParallel(audio_model)\n            audio_model.load_state_dict(sd, strict=False)\n            self.v = audio_model.module.v\n            self.original_embedding_dim = self.v.pos_embed.shape[2]\n            self.mlp_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim), nn.Linear(self.original_embedding_dim, label_dim))\n\n            f_dim, t_dim = self.get_shape(fstride, tstride, input_fdim, input_tdim)\n            num_patches = f_dim * t_dim\n            self.v.patch_embed.num_patches = num_patches\n            if verbose == True:\n                print('frequncey stride={:d}, time stride={:d}'.format(fstride, tstride))\n                print('number of patches={:d}'.format(num_patches))\n\n            new_pos_embed = self.v.pos_embed[:, 2:, :].detach().reshape(1, 1212, 768).transpose(1, 2).reshape(1, 768, 12, 101)\n"}