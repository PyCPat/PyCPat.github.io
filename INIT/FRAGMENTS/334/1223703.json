{"BEFORE":"    def __init__(self,n_states:int,n_actions:int,hidden_layers:int):\n        super().__init__()\n        self.input_size=n_states\n        self.output_size=n_actions\n        self.hidden_layers=hidden_layers\n        \n        self.feature_layer=nn.Linear(self.input_size,self.hidden_layers)\n        self.value_layer=nn.Linear(self.hidden_layers,1)#输出value\n        self.advantage_layer=nn.Linear(self.hidden_layers,self.output_size)#输出每个动作的优先值\n","AFTER":"    def __init__(self,\n                 n_states: int,\n                 n_actions: int,\n                 hidden_layers: int,\n                 lr=0.001,\n                 memory_size=100000,\n                 target_replace_iter=100,\n                 batch_size=32,\n                 reward_decay=0.9,\n                 e_greedy=0.9) -> None:\n        super().__init__()\n        self.n_actions = n_actions\n        self.n_states = n_states\n        self.train_net = _DQN(n_states, n_actions, hidden_layers)\n        self.target_net = _DQN(n_states, n_actions, hidden_layers)\n        # 复制参数\n        self.target_net.load_state_dict(self.train_net.state_dict())\n        self.optimizer = optim.RMSprop(\n            self.train_net.parameters(), lr=lr, eps=0.001, alpha=0.95)\n        self.memory = MEMORY_BUFFER_PER(memory_size)\n        self.learn_step_counter = 0\n        self.epsilon = e_greedy\n        self.target_replace_iter = target_replace_iter\n        self.batch_size = batch_size\n        self.gamma = reward_decay\n"}