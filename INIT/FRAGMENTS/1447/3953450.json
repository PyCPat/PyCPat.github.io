{"BEFORE":"    def __init__(self, embed_dim, layer_scale_init=1e-6, drop_path=0.3):\n        super().__init__()\n        self.layers = nn.Sequential(\n            # LayerNorm2d(embed_dim),\n            nn.BatchNorm2d(embed_dim),\n            nn.Conv2d(embed_dim, embed_dim, 1),\n            nn.GELU(),\n            nn.Conv2d(embed_dim, embed_dim, 3, padding=1, groups=embed_dim),\n            nn.GELU(),\n            SqueezeExcitation(embed_dim, embed_dim \/\/ 4),\n            nn.Conv2d(embed_dim, embed_dim, 1)\n        )\n        self.layer_scale = nn.Parameter(torch.ones((embed_dim,1,1)) * layer_scale_init)\n","AFTER":"    def __init__(self, embed_dim, drop_path=0.3, layer_scale_init=1e-6, norm_type='bn'):\n        assert norm_type in ('bn', 'ln')\n        super().__init__()\n        if norm_type == 'ln':\n            # LayerNorm version. Primary format is (N, H, W, C)\n            # follow this approach https:\/\/github.com\/pytorch\/vision\/blob\/main\/torchvision\/models\/convnext.py\n            self.layers = nn.Sequential(\n                nn.LayerNorm(embed_dim),\n                nn.Linear(embed_dim, embed_dim),\n                nn.GELU(),\n                Permute(0, 3, 1, 2),        # (N, H, W, C) -> (N, C, H, W)\n                nn.Conv2d(embed_dim, embed_dim, 3, padding=1, groups=embed_dim),    # dw-conv\n                nn.GELU(),\n                SqueezeExcitation(embed_dim, embed_dim \/\/ 4),\n                Permute(0, 2, 3, 1),        # (N, C, H, W) -> (N, H, W, C)\n                nn.Linear(embed_dim, embed_dim)\n            )\n            self.layer_scale = nn.Parameter(torch.ones(embed_dim) * layer_scale_init)\n\n        else:\n            # BatchNorm version. Primary format is (N, C, H, W)\n            self.layers = nn.Sequential(\n                nn.BatchNorm2d(embed_dim),\n                nn.Conv2d(embed_dim, embed_dim, 1),\n                nn.GELU(),\n                nn.Conv2d(embed_dim, embed_dim, 3, padding=1, groups=embed_dim),\n                nn.GELU(),\n                SqueezeExcitation(embed_dim, embed_dim \/\/ 4),\n                nn.Conv2d(embed_dim, embed_dim, 1)\n            )\n            self.layer_scale = nn.Parameter(torch.ones(embed_dim, 1, 1) * layer_scale_init)\n        \n        self.drop_path = StochasticDepth(drop_path, 'row') if drop_path > 0 else nn.Identity()\n"}