{"BEFORE":"    def __init__(self, num_layers, num_features, k):\n        super(TAGCN, self).__init__()\n        self.num_layers = num_layers\n        self.num_features = num_features\n        self.layers = nn.ModuleList()\n\n        for i in range(num_layers):\n            if i != num_layers - 1:\n                self.layers.append(\n                    TAGraphConvolution(num_features[i], num_features[i + 1], k, activation=F.leaky_relu, dropout=True))\n            else:\n                self.layers.append(TAGraphConvolution(num_features[i], num_features[i + 1], k))\n        self.reset_parameters()\n","AFTER":"    def __init__(self, in_features, out_features, hidden_features, k, activation=F.leaky_relu, dropout=True):\n        super(TAGCN, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        if type(hidden_features) is int:\n            hidden_features = [hidden_features]\n\n        self.layers = nn.ModuleList()\n        self.layers.append(TAGConv(in_features, hidden_features[0], k, activation=activation, dropout=dropout))\n        for i in range(len(hidden_features) - 1):\n            self.layers.append(\n                TAGConv(hidden_features[i], hidden_features[i + 1], k, activation=activation, dropout=dropout))\n        self.layers.append(TAGConv(hidden_features[-1], out_features, k))\n"}