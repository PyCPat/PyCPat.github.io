{"BEFORE":"        blocks = []\n        norm_type = ScaleNorm if use_scale_norm else nn.LayerNorm\n\n        for _ in range(depth):\n            attn = get_attn()\n            parallel_net = get_attn() if twin_attention else get_ff()\n\n            f = WithNorm(norm_type, dim, attn)\n            g = WithNorm(norm_type, dim, parallel_net)\n\n            if not twin_attention and ff_chunks > 1:\n                g = Chunk(ff_chunks, g, along_dim = -2)\n\n            blocks.append(nn.ModuleList([f, g]))\n\n        self.layers = ReversibleSequence(nn.ModuleList(blocks), layer_dropout = layer_dropout)\n        self.layer_modules = list(chain(*[m for m in blocks]))\n","AFTER":"        attns = []\n        norm_type = ScaleNorm if use_scale_norm else nn.LayerNorm\n\n        for _ in range(depth):\n            attn = get_attn()\n            parallel_net = get_attn() if twin_attention else get_ff()\n\n            f = WithNorm(norm_type, dim, attn)\n            g = WithNorm(norm_type, dim, parallel_net)\n\n            if not twin_attention and ff_chunks > 1:\n                g = Chunk(ff_chunks, g, along_dim = -2)\n\n            blocks.append(nn.ModuleList([f, g]))\n            attns.append(attn)\n\n        self.layers = ReversibleSequence(nn.ModuleList(blocks), layer_dropout = layer_dropout)\n        self.layer_modules = attns\n"}