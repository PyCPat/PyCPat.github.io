{"BEFORE":"        attn_channels = max(int(out_channels \/ attn_reduction), min_attn_channels)\n","AFTER":"                 rd_ratio=1.\/16, rd_channels=None, min_rd_channels=16, rd_divisor=8, keep_3x3=True, split_input=True,\n                 drop_block=None, act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d, aa_layer=None):\n        \"\"\" Selective Kernel Convolution Module\n\n        As described in Selective Kernel Networks (https:\/\/arxiv.org\/abs\/1903.06586) with some modifications.\n\n        Largest change is the input split, which divides the input channels across each convolution path, this can\n        be viewed as a grouping of sorts, but the output channel counts expand to the module level value. This keeps\n        the parameter count from ballooning when the convolutions themselves don't have groups, but still provides\n        a noteworthy increase in performance over similar param count models without this attention layer. -Ross W\n\n        Args:\n            in_channels (int):  module input (feature) channel count\n            out_channels (int):  module output (feature) channel count\n            kernel_size (int, list): kernel size for each convolution branch\n            stride (int): stride for convolutions\n            dilation (int): dilation for module as a whole, impacts dilation of each branch\n            groups (int): number of groups for each branch\n            rd_ratio (int, float): reduction factor for attention features\n            min_rd_channels (int): minimum attention feature channels\n            keep_3x3 (bool): keep all branch convolution kernels as 3x3, changing larger kernels for dilations\n            split_input (bool): split input channels evenly across each convolution branch, keeps param count lower,\n                can be viewed as grouping by path, output expands to module out_channels count\n            drop_block (nn.Module): drop block module\n            act_layer (nn.Module): activation layer to use\n            norm_layer (nn.Module): batchnorm\/norm layer to use\n        \"\"\"\n        super(SelectiveKernel, self).__init__()\n        out_channels = out_channels or in_channels\n        kernel_size = kernel_size or [3, 5]  # default to one 3x3 and one 5x5 branch. 5x5 -> 3x3 + dilation\n        _kernel_valid(kernel_size)\n        if not isinstance(kernel_size, list):\n            kernel_size = [kernel_size] * 2\n        if keep_3x3:\n            dilation = [dilation * (k - 1) \/\/ 2 for k in kernel_size]\n            kernel_size = [3] * len(kernel_size)\n        else:\n            dilation = [dilation] * len(kernel_size)\n        self.num_paths = len(kernel_size)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.split_input = split_input\n        if self.split_input:\n            assert in_channels % self.num_paths == 0\n            in_channels = in_channels \/\/ self.num_paths\n        groups = min(out_channels, groups)\n\n        conv_kwargs = dict(\n            stride=stride, groups=groups, drop_block=drop_block, act_layer=act_layer, norm_layer=norm_layer,\n            aa_layer=aa_layer)\n        self.paths = nn.ModuleList([\n            ConvBnAct(in_channels, out_channels, kernel_size=k, dilation=d, **conv_kwargs)\n            for k, d in zip(kernel_size, dilation)])\n\n        attn_channels = rd_channels or make_divisible(\n            out_channels * rd_ratio, min_value=min_rd_channels, divisor=rd_divisor)\n"}