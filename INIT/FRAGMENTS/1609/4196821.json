{"BEFORE":"                 number_of_heads: int,\n                 window_size: int = 7,\n                 shift_size: int = 0,\n                 ff_feature_ratio: int = 4,\n                 dropout: float = 0.0,\n                 dropout_attention: float = 0.0,\n                 dropout_path: float = 0.0,\n                 sequential_self_attention: bool = False,\n                 norm_layer: Type[nn.Module] = nn.LayerNorm) -> None:\n        # Call super constructor\n        super(SwinTransformerBlock, self).__init__()\n        # Save parameters\n        self.in_channels: int = in_channels\n        self.input_resolution: Tuple[int, int] = input_resolution\n        # Catch case if resolution is smaller than the window size\n        if min(self.input_resolution) <= window_size:\n            self.window_size: int = min(self.input_resolution)\n            self.shift_size: int = 0\n            self.make_windows: bool = False\n        else:\n            self.window_size: int = window_size\n            self.shift_size: int = shift_size\n            self.make_windows: bool = True\n        # Init normalization layers\n        self.normalization_1: nn.Module = norm_layer(normalized_shape=in_channels)\n        self.normalization_2: nn.Module = norm_layer(normalized_shape=in_channels)\n        # Init window attention module\n        self.window_attention: WindowMultiHeadAttention = WindowMultiHeadAttention(\n            in_features=in_channels,\n            window_size=self.window_size,\n            number_of_heads=number_of_heads,\n            dropout_attention=dropout_attention,\n            dropout_projection=dropout,\n            sequential_self_attention=sequential_self_attention)\n        # Init dropout layer\n        self.dropout: nn.Module = DropPath(drop_prob=dropout_path) if dropout_path > 0. else nn.Identity()\n        # Init feed-forward network\n        self.feed_forward_network: nn.Module = Mlp(in_features=in_channels,\n                                                   hidden_features=int(in_channels * ff_feature_ratio),\n                                                   drop=dropout,\n                                                   out_features=in_channels)\n        # Make attention mask\n        self.__make_attention_mask()\n","AFTER":"        num_heads: int,\n        feat_size: Tuple[int, int],\n        window_size: Tuple[int, int],\n        shift_size: Tuple[int, int] = (0, 0),\n        mlp_ratio: float = 4.0,\n        drop: float = 0.0,\n        drop_attn: float = 0.0,\n        drop_path: float = 0.0,\n        extra_norm: bool = False,\n        sequential_attn: bool = False,\n        norm_layer: Type[nn.Module] = nn.LayerNorm,\n    ) -> None:\n        super(SwinTransformerBlock, self).__init__()\n        self.dim: int = dim\n        self.feat_size: Tuple[int, int] = feat_size\n        self.target_shift_size: Tuple[int, int] = to_2tuple(shift_size)\n        self.window_size, self.shift_size = self._calc_window_shift(to_2tuple(window_size))\n        self.window_area = self.window_size[0] * self.window_size[1]\n\n        # attn branch\n        self.attn = WindowMultiHeadAttention(\n            dim=dim,\n            num_heads=num_heads,\n            window_size=self.window_size,\n            drop_attn=drop_attn,\n            drop_proj=drop,\n            sequential_attn=sequential_attn,\n        )\n        self.norm1 = norm_layer(dim)\n        self.drop_path1 = DropPath(drop_prob=drop_path) if drop_path > 0.0 else nn.Identity()\n\n        # mlp branch\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=int(dim * mlp_ratio),\n            drop=drop,\n            out_features=dim,\n        )\n        self.norm2 = norm_layer(dim)\n        self.drop_path2 = DropPath(drop_prob=drop_path) if drop_path > 0.0 else nn.Identity()\n\n        # extra norm layer mentioned for Huge\/Giant models in V2 paper (FIXME may be in wrong spot?)\n        self.norm3 = norm_layer(dim) if extra_norm else nn.Identity()\n\n        self._make_attention_mask()\n"}