{"BEFORE":"        input_dim: int = 80,\n        num_heads: int = 4,\n        encoder_dim: int = 144,\n        num_layers: int = 16,\n        conv_kernel_size: int = 31,\n        dropout: float = 0.1,\n        feed_forward_expansion_factor: int = 4,\n        conv_expansion_factor: int = 2,\n        subsampling_factor: int = 4,\n        half_step_residual: bool = True,\n        freq_masks: int = 2,\n        time_masks: int = 10,\n        freq_width: int = 27,\n        time_width: float = 0.05,\n        rnn_type: str = \"lstm\",\n        sos_id: int = 1,\n        eos_id: int = 2,\n        grad_ckpt_batchsize: int = 4,\n    ):\n        super().__init__()\n        self.grad_ckpt_batchsize = grad_ckpt_batchsize\n        self.encoder = ConformerEncoder(\n            input_dim=input_dim,\n            num_heads=num_heads,\n            encoder_dim=encoder_dim,\n            num_layers=num_layers,\n            conv_kernel_size=conv_kernel_size,\n            dropout=dropout,\n            subsampling_factor=subsampling_factor,\n            feed_forward_expansion_factor=feed_forward_expansion_factor,\n            conv_expansion_factor=conv_expansion_factor,\n            freq_masks=freq_masks,\n            time_masks=time_masks,\n            freq_width=freq_width,\n            time_width=time_width,\n            grad_ckpt_batchsize=grad_ckpt_batchsize,\n        )\n        self.decoder = DecoderRNNT(\n            num_classes=num_classes,\n            hidden_state_dim=hidden_state_dim,\n            output_dim=decoder_output_dim,\n            num_layers=decoder_num_layers,\n            rnn_type=rnn_type,\n            sos_id=sos_id,\n            eos_id=eos_id,\n            dropout_p=dropout,\n        )\n        self.fc = nn.Sequential(\n            nn.Linear(encoder_dim << 1, encoder_dim),\n            nn.Tanh(),\n            nn.Linear(encoder_dim, num_classes, bias=False),\n        )\n","AFTER":"        input_dim: int = 80,\n        encoder_dim: int = 512,\n        num_encoder_layers: int = 17,\n        num_attention_heads: int = 8,\n        feed_forward_expansion_factor: int = 4,\n        conv_expansion_factor: int = 2,\n        input_dropout_p: float = 0.1,\n        feed_forward_dropout_p: float = 0.1,\n        attention_dropout_p: float = 0.1,\n        conv_dropout_p: float = 0.1,\n        conv_kernel_size: int = 31,\n        half_step_residual: bool = True,\n        freq_masks: int = 2,\n        time_masks: int = 10,\n        freq_width: int = 27,\n        time_width: int = 0.05,\n    ) -> None:\n        super(Conformer, self).__init__()\n        self.spec_augment = SpecAugment(freq_masks, time_masks, freq_width, time_width)\n        self.encoder = ConformerEncoder(\n            input_dim=input_dim,\n            encoder_dim=encoder_dim,\n            num_layers=num_encoder_layers,\n            num_attention_heads=num_attention_heads,\n            feed_forward_expansion_factor=feed_forward_expansion_factor,\n            conv_expansion_factor=conv_expansion_factor,\n            input_dropout_p=input_dropout_p,\n            feed_forward_dropout_p=feed_forward_dropout_p,\n            attention_dropout_p=attention_dropout_p,\n            conv_dropout_p=conv_dropout_p,\n            conv_kernel_size=conv_kernel_size,\n            half_step_residual=half_step_residual,\n        )\n        self.output_dim = encoder_dim\n"}