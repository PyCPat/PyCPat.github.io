{"BEFORE":"        self,\n        *,\n        dim_text = 512,\n        dim_image = 512,\n        dim_latent = 512,\n        num_text_tokens = 10000,\n        text_enc_depth = 6,\n        text_seq_len = 256,\n        text_heads = 8,\n        num_visual_tokens = 512,\n        visual_enc_depth = 6,\n        visual_heads = 8,\n        visual_image_size = 256,\n        visual_patch_size = 32,\n        channels = 3,\n        use_all_token_embeds = False,\n        decoupled_contrastive_learning = False,\n        extra_latent_projection = False\n    ):\n        super().__init__()\n        self.text_transformer = TextTransformer(\n            dim = dim_text,\n            num_tokens = num_text_tokens,\n            max_seq_len = text_seq_len,\n            depth = text_enc_depth,\n            heads = text_heads\n        )\n\n        self.visual_transformer = VisionTransformer(\n            dim = dim_image,\n            image_size = visual_image_size,\n            patch_size = visual_patch_size,\n            channels = channels,\n            depth = visual_enc_depth,\n            heads = visual_heads\n        )\n\n        self.to_text_latent = nn.Linear(dim_text, dim_latent, bias = False)\n        self.to_visual_latent = nn.Linear(dim_image, dim_latent, bias = False)\n\n        self.temperature = nn.Parameter(torch.tensor(1.))\n\n        # from https:\/\/arxiv.org\/abs\/2111.07783 (FILIP paper)\n        self.use_all_token_embeds = use_all_token_embeds\n\n        # proposed in https:\/\/arxiv.org\/abs\/2110.06848 (DCL) and https:\/\/arxiv.org\/abs\/2110.11316 (CLOOB)\n        self.decoupled_contrastive_learning = decoupled_contrastive_learning\n\n        # proposed in https:\/\/arxiv.org\/abs\/2110.11316 (CLOOB)\n        self.extra_latent_projection = extra_latent_projection\n\n        self.to_text_latent_extra = nn.Linear(dim_text, dim_latent, bias = False)\n        self.to_visual_latent_extra = nn.Linear(dim_image, dim_latent, bias = False)\n","AFTER":"        self.to_text_latent = nn.Linear(dim_text, dim_latent, bias = False)\n\n        # image latent projection\n\n        if downsample_image_embeds:\n            assert use_all_token_embeds, 'must be using all token embeds for contrastive learning in order to downsampling'\n\n            self.to_visual_latent = nn.Sequential(\n                RearrangeImage(),\n                nn.Conv2d(dim_image, dim_image, 4, stride = 2, padding = 1, bias = False, groups = dim_image),\n                nn.Conv2d(dim_image, dim_latent, 1),\n                Rearrange('b c h w -> b (h w) c')\n            )\n        else:\n            self.to_visual_latent = nn.Linear(dim_image, dim_latent, bias = False)\n\n        # temperature\n\n        self.temperature = nn.Parameter(torch.tensor(1.))\n\n        # from https:\/\/arxiv.org\/abs\/2111.07783 (FILIP paper)\n        self.use_all_token_embeds = use_all_token_embeds\n\n        # proposed in https:\/\/arxiv.org\/abs\/2110.06848 (DCL) and https:\/\/arxiv.org\/abs\/2110.11316 (CLOOB)\n        self.decoupled_contrastive_learning = decoupled_contrastive_learning\n\n        # proposed in https:\/\/arxiv.org\/abs\/2110.11316 (CLOOB)\n        self.extra_latent_projection = extra_latent_projection\n\n        self.to_text_latent_extra = copy.deepcopy(self.to_text_latent)\n        self.to_visual_latent_extra = copy.deepcopy(self.to_visual_latent)\n"}