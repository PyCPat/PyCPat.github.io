{"BEFORE":"        kernel_size_factor: int = 1,\n        stride: int = 1,\n        dilation: int = 1,\n        padding: str = \"same\",\n        dropout: float = 0.2,\n        activation=None,\n        residual: bool = True,\n        groups: int = 1,\n        separable: bool = False,\n        heads: int = -1,\n        normalization: str = \"batch\",\n        norm_groups: int = 1,\n        residual_mode: str = \"add\",\n        residual_panes=[],\n        conv_mask: bool = False,\n        se: bool = False,\n        se_reduction_ratio: int = 16,\n        se_context_window=None,\n        se_interpolation_mode: str = \"nearest\",\n        stride_last: bool = False,\n    ):\n        super(JasperBlock, self).__init__()\n\n        if padding != \"same\":\n            raise ValueError(\"currently only 'same' padding is supported\")\n\n        kernel_size_factor = float(kernel_size_factor)\n        if type(kernel_size) in (list, tuple):\n            kernel_size = [\n                compute_new_kernel_size(k, kernel_size_factor) for k in kernel_size\n            ]\n        else:\n            kernel_size = compute_new_kernel_size(kernel_size, kernel_size_factor)\n\n        padding_val = get_same_padding(kernel_size[0], stride[0], dilation[0])\n        self.conv_mask = conv_mask\n        self.separable = separable\n        self.residual_mode = residual_mode\n        self.se = se\n\n        inplanes_loop = inplanes\n        conv = nn.ModuleList()\n\n        for _ in range(repeat - 1):\n            # Stride last means only the last convolution in block will have stride\n            if stride_last:\n                stride_val = [1]\n            else:\n                stride_val = stride\n\n            conv.extend(\n                self._get_conv_bn_layer(\n                    inplanes_loop,\n                    planes,\n                    kernel_size=kernel_size,\n                    stride=stride_val,\n                    dilation=dilation,\n                    padding=padding_val,\n                    groups=groups,\n                    heads=heads,\n                    separable=separable,\n                    normalization=normalization,\n                    norm_groups=norm_groups,\n                )\n            )\n\n            conv.extend(\n                self._get_act_dropout_layer(drop_prob=dropout, activation=activation)\n            )\n\n            inplanes_loop = planes\n\n        conv.extend(\n            self._get_conv_bn_layer(\n                inplanes_loop,\n                planes,\n                kernel_size=kernel_size,\n                stride=stride,\n                dilation=dilation,\n                padding=padding_val,\n                groups=groups,\n                heads=heads,\n                separable=separable,\n                normalization=normalization,\n                norm_groups=norm_groups,\n            )\n        )\n\n        if se:\n            conv.append(\n                SqueezeExcite(\n                    planes,\n                    reduction_ratio=se_reduction_ratio,\n                    context_window=se_context_window,\n                    interpolation_mode=se_interpolation_mode,\n                    activation=activation,\n                )\n            )\n\n        self.mconv = conv\n\n        res_panes = residual_panes.copy()\n        self.dense_residual = residual\n\n        if residual:\n            res_list = nn.ModuleList()\n\n            if residual_mode == \"stride_add\":\n                stride_val = stride\n            else:\n                stride_val = [1]\n\n            if len(residual_panes) == 0:\n                res_panes = [inplanes]\n                self.dense_residual = False\n            for ip in res_panes:\n                res = nn.ModuleList(\n                    self._get_conv_bn_layer(\n                        ip,\n                        planes,\n                        kernel_size=1,\n                        normalization=normalization,\n                        norm_groups=norm_groups,\n                        stride=stride_val,\n                    )\n","AFTER":"        kernel_size: List[int] = [11],\n        kernel_size_factor: float = 1.0,\n        stride: List[int] = [1],\n        dilation: List[int] = [1],\n        dropout: float = 0.2,\n        activation=nn.Hardtanh(min_val=0.0, max_val=20.0),\n        residual: bool = True,\n        groups: int = 1,\n        separable: bool = False,\n        heads: int = -1,\n        normalization: str = \"batch\",\n        norm_groups: int = 1,\n        residual_mode: str = \"add\",\n        residual_panes=[],\n        se: bool = False,\n        se_reduction_ratio: int = 16,\n        se_context_window=None,\n        se_interpolation_mode: str = \"nearest\",\n        stride_last: bool = False,\n    ):\n        super().__init__()\n        if separable and heads != -1:\n            raise ValueError(\n                \"Separable convolutions are not compatible with multiple heads\"\n            )\n\n        kernel_size_factor = float(kernel_size_factor)\n        kernel_size = [\n            compute_new_kernel_size(k, kernel_size_factor) for k in kernel_size\n        ]\n        padding_val = get_same_padding(kernel_size[0], stride[0], dilation[0])\n\n        self.residual_mode = residual_mode\n\n        inplanes_loop = inplanes\n        conv = []\n\n        for _ in range(repeat - 1):\n            # Stride last means only the last convolution in block will have stride\n            if stride_last:\n                stride_val = [1]\n            else:\n                stride_val = stride\n\n            conv.extend(\n                self._get_conv_bn_layer(\n                    inplanes_loop,\n                    planes,\n                    kernel_size=kernel_size,\n                    stride=stride_val,\n                    dilation=dilation,\n                    padding=padding_val,\n                    groups=groups,\n                    heads=heads,\n                    separable=separable,\n                    normalization=normalization,\n                    norm_groups=norm_groups,\n                    bias=False,\n                )\n            )\n\n            conv.extend(\n                self._get_act_dropout_layer(drop_prob=dropout, activation=activation)\n            )\n\n            inplanes_loop = planes\n\n        conv.extend(\n            self._get_conv_bn_layer(\n                inplanes_loop,\n                planes,\n                kernel_size=kernel_size,\n                stride=stride,\n                dilation=dilation,\n                padding=padding_val,\n                groups=groups,\n                heads=heads,\n                separable=separable,\n                normalization=normalization,\n                norm_groups=norm_groups,\n                bias=False,\n            )\n        )\n\n        if se:\n            conv.append(\n                SqueezeExcite(\n                    planes,\n                    reduction_ratio=se_reduction_ratio,\n                    context_window=se_context_window,\n                    interpolation_mode=se_interpolation_mode,\n                    activation=activation,\n                )\n            )\n\n        self.mconv = nn.Sequential(*conv)\n\n        res_panes = residual_panes.copy()\n        self.dense_residual = residual\n\n        if residual:\n            res_list = nn.ModuleList()\n\n            stride_residual = (\n                stride if stride[0] == 1 or stride_last else stride[0] ** repeat\n            )\n            if len(residual_panes) == 0:\n                res_panes = [inplanes]\n                self.dense_residual = False\n            for ip in res_panes:\n                res = nn.Sequential(\n                    *self._get_conv_bn_layer(\n                        ip,\n                        planes,\n                        kernel_size=1,\n                        normalization=normalization,\n                        norm_groups=norm_groups,\n                        stride=stride_residual,\n                        bias=False,\n                    )\n"}