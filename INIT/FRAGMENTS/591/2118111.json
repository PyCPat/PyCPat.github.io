{"BEFORE":"        self.activation = nn.Sigmoid()\n","AFTER":"        self.norm = select_norm(norm, in_channels, 3)\n        self.conv1d = nn.Conv1d(in_channels, out_channels, 1, bias=False)\n\n        self.dual_rnn = nn.ModuleList([])\n        for i in range(num_layers):\n            self.dual_rnn.append(Dual_RNN_Block(out_channels, hidden_channels,\n                                     rnn_type=rnn_type, norm=norm, dropout=dropout,\n                                     bidirectional=bidirectional))\n\n        self.conv2d = nn.Conv2d(\n            out_channels, out_channels*num_spks, kernel_size=1)\n        self.end_conv1x1 = nn.Conv1d(out_channels, in_channels, 1, bias=False)\n        self.prelu = nn.PReLU()\n        self.activation = nn.ReLU()\n         # gated output layer\n        self.output = nn.Sequential(nn.Conv1d(out_channels, out_channels, 1),\n                                    nn.Tanh()\n                                    )\n        self.output_gate = nn.Sequential(nn.Conv1d(out_channels, out_channels, 1),\n                                         nn.Sigmoid()\n                                         )\n"}