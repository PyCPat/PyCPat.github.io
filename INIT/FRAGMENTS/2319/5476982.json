{"BEFORE":"    def __init__(self, hparams):\n        super(Encoder, self).__init__()\n\n        convolutions = []\n        for _ in range(hparams.encoder_n_convolutions):\n            conv_layer = nn.Sequential(\n                ConvNorm(hparams.encoder_embedding_dim,\n                         hparams.encoder_embedding_dim,\n                         kernel_size=hparams.encoder_kernel_size, stride=1,\n                         padding=int((hparams.encoder_kernel_size - 1) \/ 2),\n                         dilation=1, w_init_gain='relu'),\n                nn.BatchNorm1d(hparams.encoder_embedding_dim))\n            convolutions.append(conv_layer)\n        self.convolutions = nn.ModuleList(convolutions)\n\n        self.lstm = nn.LSTM(hparams.encoder_embedding_dim,\n                            int(hparams.encoder_embedding_dim \/ 2), 1,\n","AFTER":"    def __init__(self, embed_dim,\n                 num_convs=3, conv_channels=512, conv_kernel_size=5,\n                 conv_dropout=0.5, blstm_units=512):\n        super(Encoder, self).__init__()\n\n        # convolution layers followed by batch normalization and ReLU activation\n        activations = [nn.ReLU()] * num_convs\n        conv_out_channels = [conv_channels] * num_convs\n        self.conv1ds = BatchNormConv1dStack(embed_dim, conv_out_channels, kernel_size=conv_kernel_size,\n                                            stride=1, padding=(conv_kernel_size -1) \/\/ 2,\n                                            activations=activations, dropout=conv_dropout)\n\n        # 1 layer Bi-directional LSTM\n        self.lstm = nn.LSTM(conv_channels, blstm_units \/\/ 2, 1, batch_first=True, bidirectional=True)\n"}