{"BEFORE":"            hidden_features = [hidden_features]\n\n        self.layers = nn.ModuleList()\n        if layer_norm:\n            self.layers.append(nn.LayerNorm(in_features))\n\n        self.layers.append(TAGConv(in_features=in_features,\n                                   out_features=hidden_features[0],\n                                   k=k,\n                                   batch_norm=batch_norm,\n                                   activation=activation,\n                                   dropout=dropout))\n        for i in range(len(hidden_features) - 1):\n            if layer_norm:\n                self.layers.append(nn.LayerNorm(hidden_features[i]))\n            self.layers.append(TAGConv(in_features=hidden_features[i],\n                                       out_features=hidden_features[i + 1],\n                                       k=k,\n                                       batch_norm=batch_norm,\n                                       activation=activation,\n                                       dropout=dropout))\n        self.layers.append(TAGConv(in_features=hidden_features[-1],\n                                   out_features=out_features,\n                                   k=k,\n                                   batch_norm=None,\n                                   activation=None,\n                                   dropout=0.0))\n","AFTER":"                 n_layers,\n                 k,\n                 activation=F.leaky_relu,\n                 feat_norm=None,\n                 adj_norm_func=GCNAdjNorm,\n                 layer_norm=False,\n                 batch_norm=False,\n                 dropout=0.0):\n        super(TAGCN, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.feat_norm = feat_norm\n        self.adj_norm_func = adj_norm_func\n        if type(hidden_features) is int:\n            hidden_features = [hidden_features] * (n_layers - 1)\n        elif type(hidden_features) is list or type(hidden_features) is tuple:\n            assert len(hidden_features) == (n_layers - 1), \"Incompatible sizes between hidden_features and n_layers.\"\n        n_features = [in_features] + hidden_features + [out_features]\n\n        self.layers = nn.ModuleList()\n        for i in range(n_layers):\n            if layer_norm:\n                self.layers.append(nn.LayerNorm(n_features[i]))\n"}