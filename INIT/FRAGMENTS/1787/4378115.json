{"BEFORE":"        im_channels = self.image_dims[0]\n        kernel_dim = 3\n        filters = (60, 120, 240, 480, 960)\n\n        # Images downscaled to 500 x 1000 + randomly cropped to 256 x 256\n        assert image_dims == (im_channels, 256, 256), 'Crop image to 256 x 256!'\n\n        # Layer \/ normalization options\n        cnn_kwargs = dict(stride=2, padding=0, padding_mode='reflect')\n        norm_kwargs = dict(momentum=0.1, affine=True, track_running_stats=False)\n        self.activation = getattr(F, activation)  # (leaky_relu, relu, elu)\n        \n        if channel_norm is True:\n            interlayer_norm = normalization.ChannelNorm2D_wrap\n        else:\n            interlayer_norm = normalization.InstanceNorm2D_wrap\n\n        self.pre_pad = nn.ReflectionPad2d(3)\n        self.asymmetric_pad = nn.ReflectionPad2d((0,1,1,0))  # Slower than tensorflow?\n        self.post_pad = nn.ReflectionPad2d(1)\n\n        heights = (2**i for i in range(4,9))[::-1]\n        widths = heights\n        H1, H2, H3, H4, H5 = heights\n        W1, W2, W3, W4, W5 = widths \n\n        # (256,256) -> (256,256), with implicit padding\n        self.conv_block1 = nn.Sequential(\n            self.pre_pad,\n            nn.Conv2d(im_channels, filters[0], kernel_size=(7,7), stride=1),\n            interlayer_norm((batch_size, filters[0], H1, W1), **norm_kwargs),\n            self.activation,\n        )\n\n        # (256,256) -> (128,128)\n        self.conv_block2 = nn.Sequential(\n            self.asymmetric_pad,\n            nn.Conv2d(filters[0], filters[1], kernel_dim, **cnn_kwargs),\n            interlayer_norm((batch_size, filters[1], H2, W2), **norm_kwargs),\n            self.activation,\n        )\n\n        # (128,128) -> (64,64)\n        self.conv_block3 = nn.Sequential(\n            self.asymmetric_pad,\n            nn.Conv2d(filters[1], filters[2], kernel_dim, **cnn_kwargs),\n            interlayer_norm((batch_size, filters[2], H3, W3), **norm_kwargs),\n            self.activation,\n        )\n\n        # (64,64) -> (32,32)\n        self.conv_block4 = nn.Sequential(\n            self.asymmetric_pad,\n            nn.Conv2d(filters[2], filters[3], kernel_dim, **cnn_kwargs),\n            interlayer_norm((batch_size, filters[3], H4, W4), **norm_kwargs),\n            self.activation,\n        )\n\n        # (32,32) -> (16,16)\n        self.conv_block5 = nn.Sequential(\n            self.asymmetric_pad,\n            nn.Conv2d(filters[3], filters[4], kernel_dim, **cnn_kwargs),\n            interlayer_norm((batch_size, filters[4], H5, W5), **norm_kwargs),\n            self.activation,\n","AFTER":"        im_channels = image_dims[0]\n        assert image_dims == (im_channels, 256, 256), 'Crop image to 256 x 256!'\n\n        # Layer \/ normalization options\n        cnn_kwargs = dict(stride=2, padding=0, padding_mode='reflect')\n        norm_kwargs = dict(momentum=0.1, affine=True, track_running_stats=True)\n        activation_d = dict(relu='ReLU', elu='ELU', leaky_relu='LeakyReLU')\n        self.activation = getattr(nn, activation_d[activation])  # (leaky_relu, relu, elu)\n        \n        if channel_norm is True:\n            self.interlayer_norm = normalization.ChannelNorm2D_wrap\n        else:\n            self.interlayer_norm = normalization.InstanceNorm2D_wrap\n\n        self.pre_pad = nn.ReflectionPad2d(3)\n        self.asymmetric_pad = nn.ReflectionPad2d((0,1,1,0))  # Slower than tensorflow?\n        self.post_pad = nn.ReflectionPad2d(1)\n\n        heights = [2**i for i in range(4,9)][::-1]\n        widths = heights\n        H1, H2, H3, H4, H5 = heights\n        W1, W2, W3, W4, W5 = widths \n\n        # (256,256) -> (256,256), with implicit padding\n        self.conv_block1 = nn.Sequential(\n            self.pre_pad,\n            nn.Conv2d(im_channels, filters[0], kernel_size=(7,7), stride=1),\n            self.interlayer_norm((batch_size, filters[0], H1, W1), **norm_kwargs),\n            self.activation(),\n        )\n\n        # (256,256) -> (128,128)\n        self.conv_block2 = nn.Sequential(\n            self.asymmetric_pad,\n            nn.Conv2d(filters[0], filters[1], kernel_dim, **cnn_kwargs),\n            self.interlayer_norm((batch_size, filters[1], H2, W2), **norm_kwargs),\n            self.activation(),\n        )\n\n        # (128,128) -> (64,64)\n        self.conv_block3 = nn.Sequential(\n            self.asymmetric_pad,\n            nn.Conv2d(filters[1], filters[2], kernel_dim, **cnn_kwargs),\n            self.interlayer_norm((batch_size, filters[2], H3, W3), **norm_kwargs),\n            self.activation(),\n        )\n\n        # (64,64) -> (32,32)\n        self.conv_block4 = nn.Sequential(\n            self.asymmetric_pad,\n            nn.Conv2d(filters[2], filters[3], kernel_dim, **cnn_kwargs),\n            self.interlayer_norm((batch_size, filters[3], H4, W4), **norm_kwargs),\n            self.activation(),\n        )\n\n        # (32,32) -> (16,16)\n        self.conv_block5 = nn.Sequential(\n            self.asymmetric_pad,\n            nn.Conv2d(filters[3], filters[4], kernel_dim, **cnn_kwargs),\n            self.interlayer_norm((batch_size, filters[4], H5, W5), **norm_kwargs),\n            self.activation(),\n"}