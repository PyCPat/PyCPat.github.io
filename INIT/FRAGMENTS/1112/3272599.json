{"BEFORE":"        random_fourier_features = False,\n        learned_sinusoidal_dim = 16,\n        init_img_transform: callable = None,\n        final_img_itransform: callable = None,\n        patch_size = 1,\n    ):\n        super().__init__()\n\n        # for initial dwt transform (or whatever transform researcher wants to try here)\n\n        if exists(init_img_transform) and exists(final_img_itransform):\n            init_shape = torch.Size(1, 1, 32, 32)\n            mock_tensor = torch.randn(init_shape)\n            assert final_img_itransform(init_img_transform(mock_tensor)).shape == init_shape\n\n        self.init_img_transform = default(init_img_transform, identity)\n        self.final_img_itransform = default(final_img_itransform, identity)\n\n        input_channels = channels\n\n        # whether to do initial patching, as alternative to dwt\n\n        self.patchify = self.unpatchify = identity\n\n        if patch_size > 1:\n            input_channels = channels * (patch_size ** 2)\n            self.patchify = nn.Conv2d(channels, input_channels, patch_size, stride = patch_size)\n            self.unpatchify = nn.ConvTranspose2d(input_channels, channels, patch_size, stride = patch_size)\n\n        # determine dimensions\n\n        init_dim = default(init_dim, dim)\n        self.init_conv = nn.Conv2d(input_channels, init_dim, 7, padding = 3)\n\n        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n        in_out = list(zip(dims[:-1], dims[1:]))\n\n        resnet_block = partial(ResnetBlock, groups = resnet_block_groups)\n\n        # time embeddings\n\n        time_dim = dim * 4\n\n        sinu_pos_emb = RandomOrLearnedSinusoidalPosEmb(learned_sinusoidal_dim, random_fourier_features)\n        fourier_dim = learned_sinusoidal_dim + 1\n\n        self.time_mlp = nn.Sequential(\n            sinu_pos_emb,\n            nn.Linear(fourier_dim, time_dim),\n            nn.GELU(),\n            nn.Linear(time_dim, time_dim)\n        )\n\n        # layers\n\n        self.downs = nn.ModuleList([])\n        self.ups = nn.ModuleList([])\n        num_resolutions = len(in_out)\n\n        for ind, (dim_in, dim_out) in enumerate(in_out):\n            is_last = ind >= (num_resolutions - 1)\n\n            self.downs.append(nn.ModuleList([\n                resnet_block(dim_in, dim_in, time_emb_dim = time_dim),\n                resnet_block(dim_in, dim_in, time_emb_dim = time_dim),\n                LinearAttention(dim_in),\n                Downsample(dim_in, dim_out) if not is_last else nn.Conv2d(dim_in, dim_out, 3, padding = 1)\n            ]))\n\n        mid_dim = dims[-1]\n\n        self.vit = Transformer(\n            dim = mid_dim,\n            time_cond_dim = time_dim,\n            depth = vit_depth,\n            dim_head = attn_dim_head,\n            heads = attn_heads,\n            ff_mult = ff_mult,\n            dropout = vit_dropout\n        )\n\n        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n            is_last = ind == (len(in_out) - 1)\n\n            self.ups.append(nn.ModuleList([\n                resnet_block(dim_out + dim_in, dim_out, time_emb_dim = time_dim),\n                resnet_block(dim_out + dim_in, dim_out, time_emb_dim = time_dim),\n                LinearAttention(dim_out),\n                Upsample(dim_out, dim_in) if not is_last else  nn.Conv2d(dim_out, dim_in, 3, padding = 1)\n","AFTER":"        downsample_factor = 2,\n        channels = 3,\n        vit_depth = 6,\n        vit_dropout = 0.2,\n        attn_dim_head = 32,\n        attn_heads = 4,\n        ff_mult = 4,\n        resnet_block_groups = 8,\n        learned_sinusoidal_dim = 16,\n        init_img_transform: callable = None,\n        final_img_itransform: callable = None,\n        patch_size = 1,\n    ):\n        super().__init__()\n\n        # for initial dwt transform (or whatever transform researcher wants to try here)\n\n        if exists(init_img_transform) and exists(final_img_itransform):\n            init_shape = torch.Size(1, 1, 32, 32)\n            mock_tensor = torch.randn(init_shape)\n            assert final_img_itransform(init_img_transform(mock_tensor)).shape == init_shape\n\n        self.init_img_transform = default(init_img_transform, identity)\n        self.final_img_itransform = default(final_img_itransform, identity)\n\n        input_channels = channels\n\n        # whether to do initial patching, as alternative to dwt\n\n        self.patchify = self.unpatchify = identity\n\n        if patch_size > 1:\n            input_channels = channels * (patch_size ** 2)\n            self.patchify = nn.Conv2d(channels, input_channels, patch_size, stride = patch_size)\n            self.unpatchify = nn.ConvTranspose2d(input_channels, channels, patch_size, stride = patch_size)\n\n        # determine dimensions\n\n        init_dim = default(init_dim, dim)\n        self.init_conv = nn.Conv2d(input_channels, init_dim, 7, padding = 3)\n\n        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n        in_out = list(zip(dims[:-1], dims[1:]))\n\n        resnet_block = partial(ResnetBlock, groups = resnet_block_groups)\n\n        # time embeddings\n\n        time_dim = dim * 4\n\n        sinu_pos_emb = LearnedSinusoidalPosEmb(learned_sinusoidal_dim)\n        fourier_dim = learned_sinusoidal_dim + 1\n\n        self.time_mlp = nn.Sequential(\n            sinu_pos_emb,\n            nn.Linear(fourier_dim, time_dim),\n            nn.GELU(),\n            nn.Linear(time_dim, time_dim)\n        )\n\n        # downsample factors\n\n        downsample_factor = cast_tuple(downsample_factor, len(dim_mults))\n        assert len(downsample_factor) == len(dim_mults)\n\n        # layers\n\n        self.downs = nn.ModuleList([])\n        self.ups = nn.ModuleList([])\n        num_resolutions = len(in_out)\n\n        for ind, ((dim_in, dim_out), factor) in enumerate(zip(in_out, downsample_factor)):\n            is_last = ind >= (num_resolutions - 1)\n\n            self.downs.append(nn.ModuleList([\n                resnet_block(dim_in, dim_in, time_emb_dim = time_dim),\n                resnet_block(dim_in, dim_in, time_emb_dim = time_dim),\n                LinearAttention(dim_in),\n                Downsample(dim_in, dim_out, factor = factor)\n            ]))\n\n        mid_dim = dims[-1]\n\n        self.vit = Transformer(\n            dim = mid_dim,\n            time_cond_dim = time_dim,\n            depth = vit_depth,\n            dim_head = attn_dim_head,\n            heads = attn_heads,\n            ff_mult = ff_mult,\n            dropout = vit_dropout\n        )\n\n        for ind, ((dim_in, dim_out), factor) in enumerate(zip(reversed(in_out), reversed(downsample_factor))):\n            is_last = ind == (len(in_out) - 1)\n\n            self.ups.append(nn.ModuleList([\n                Upsample(dim_out, dim_in, factor = factor),\n                resnet_block(dim_in * 2, dim_in, time_emb_dim = time_dim),\n                resnet_block(dim_in * 2, dim_in, time_emb_dim = time_dim),\n                LinearAttention(dim_in),\n"}