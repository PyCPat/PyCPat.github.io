{"BEFORE":"                 dropout=0.5):\n        super().__init__()\n\n        self.signal_encoder = SignalEncoderLinear(hidden_dim, projection_dim)\n\n        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nheads)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_encoder_layers)\n\n        self.mlp_head = nn.Sequential(\n            nn.Linear(hidden_dim * projection_dim, mlp_head_dims[0]),\n            nn.BatchNorm1d(mlp_head_dims[0]),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(mlp_head_dims[0], mlp_head_dims[1]),\n            nn.BatchNorm1d(mlp_head_dims[1]),\n            nn.GELU(),\n            nn.Dropout(dropout)\n        )\n\n        self.dropout = nn.Dropout(dropout)\n        self.classifier = nn.Linear(mlp_head_dims[1], num_classes)\n","AFTER":"                 dropout=0.5, prediction=False):\n        super().__init__()\n        self.signal_encoder = SignalEncoderLinear(hidden_dim, projection_dim, learnable=True)\n        # self.signal_encoder = SignalEncoderConv(hidden_dim, projection_dim, learnable=False)\n\n        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nheads, dropout=dropout, dim_feedforward=1024, activation='gelu')\n        encoder_norm = nn.LayerNorm(hidden_dim)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n\n        self.mlp_head = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, num_classes)\n        )\n\n        if prediction:\n            self.mlp_head = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, 6),\n            )\n\n    def forward(self, inputs):\n"}