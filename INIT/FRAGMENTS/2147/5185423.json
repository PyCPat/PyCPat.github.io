{"BEFORE":"        inner_dim = dim_head *  heads\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n\n        self.attend = nn.Softmax(dim = -1)\n        self.dropout = nn.Dropout(dropout)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim),\n            nn.Dropout(dropout)\n        )\n","AFTER":"        image_height, image_width = pair(image_size)\n        patch_height, patch_width = pair(patch_size)\n\n        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n        \n        num_patches = (image_height \/\/ patch_height) * (image_width \/\/ patch_width)\n        patch_dim = channels * patch_height * patch_width\n\n        self.dim = dim\n        self.num_patches = num_patches\n\n        self.to_patch_embedding = nn.Sequential(\n            Rearrange('b c (h p1) (w p2) -> b h w (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n            nn.LayerNorm(patch_dim),\n            nn.Linear(patch_dim, dim),\n            nn.LayerNorm(dim),\n        )\n\n        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n\n        self.to_latent = nn.Identity()\n        self.linear_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_classes)\n        )\n"}