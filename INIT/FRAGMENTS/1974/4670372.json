{"BEFORE":"        use_bert_layer_norm: bool = False,\n        use_gelu: bool = True,\n        add_bias_kv: bool = False,\n        add_zero_attn: bool = False,\n    ) -> None:\n\n        super().__init__()\n        # Initialize parameters\n        self.embedding_dim = embedding_dim\n        self.dropout = dropout\n        self.activation_dropout = activation_dropout\n        self.normalize_before = encoder_normalize_before\n\n        # Initialize blocks\n        self.activation_fn = gelu if use_gelu else F.relu\n        self.self_attn = MultiheadAttention(\n            self.embedding_dim,\n            num_attention_heads,\n            dropout=attention_dropout,\n            add_bias_kv=add_bias_kv,\n            add_zero_attn=add_zero_attn,\n        )\n\n        # layer norm associated with the self attention layer\n        self.self_attn_layer_norm = (\n            BertLayerNorm(self.embedding_dim)\n            if use_bert_layer_norm\n            else LayerNorm(self.embedding_dim, eps=1e-12)\n        )\n        self.fc1 = nn.Linear(self.embedding_dim, ffn_embedding_dim)\n        self.fc2 = nn.Linear(ffn_embedding_dim, self.embedding_dim)\n\n        # layer norm associated with the position wise feed-forward NN\n        self.final_layer_norm = (\n            BertLayerNorm(self.embedding_dim)\n            if use_bert_layer_norm\n            else LayerNorm(self.embedding_dim, eps=1e-12)\n        )\n","AFTER":"        self.embedding_dim = embedding_dim\n        self.dropout = dropout\n        self.activation_dropout = activation_dropout\n\n        # Initialize blocks\n        self.activation_fn = utils.get_activation_fn(activation_fn)\n        self.self_attn = MultiheadAttention(\n            self.embedding_dim,\n            num_attention_heads,\n            dropout=attention_dropout,\n            add_bias_kv=add_bias_kv,\n            add_zero_attn=add_zero_attn,\n        )\n\n        # layer norm associated with the self attention layer\n        self.self_attn_layer_norm = LayerNorm(self.embedding_dim)\n        self.fc1 = nn.Linear(self.embedding_dim, ffn_embedding_dim)\n        self.fc2 = nn.Linear(ffn_embedding_dim, self.embedding_dim)\n\n        # layer norm associated with the position wise feed-forward NN\n        self.final_layer_norm = LayerNorm(self.embedding_dim)\n"}