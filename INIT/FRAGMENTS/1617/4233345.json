{"BEFORE":"        self.layer_size = config.layer_size  # self.hidden_dim, 之前这里没有改\n        self.num_token_type = config.num_token_type  # 实体类型的综述\n        self.config = config\n        if embedding_pre is not None:\n            print(\"use pretrained embeddings\")\n            self.word_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_pre), freeze=False)\n        else:\n            self.word_embedding = nn.Embedding(config.vocab_size, config.embedding_dim, padding_idx=config.pad_token_id)\n        # self.word_embedding = nn.Embedding(config.vocab_size, config.embedding_dim)\n        self.token_type_embedding = nn.Embedding(config.num_token_type, config.token_type_dim)\n        self.gru = nn.GRU(config.embedding_dim, config.hidden_dim_lstm, num_layers=config.num_layers, batch_first=True,\n                          bidirectional=True)\n        self.is_train = True\n        if USE_CUDA:\n            self.weights_rel = (torch.ones(self.config.num_relations) * 100).cuda()\n        else:\n            self.weights_rel = torch.ones(self.config.num_relations) * 100\n        self.weights_rel[0] = 1\n\n        self.V_ner = nn.Parameter(torch.rand((config.num_token_type, self.layer_size)))\n        self.U_ner = nn.Parameter(torch.rand((self.layer_size, 2 * self.hidden_dim)))\n        self.b_s_ner = nn.Parameter(torch.rand(self.layer_size))\n        self.b_c_ner = nn.Parameter(torch.rand(config.num_token_type))\n\n        self.U_head = nn.Parameter(torch.rand((self.layer_size, self.hidden_dim * 2 + self.config.token_type_dim)))\n        self.W_head = nn.Parameter(torch.rand((self.layer_size, self.hidden_dim * 2 + self.config.token_type_dim)))\n        self.V_head = nn.Parameter(torch.rand(self.layer_size, len(self.config.relations)))\n        self.b_s_head = nn.Parameter(torch.rand(self.layer_size))\n","AFTER":"        self.rel_embedding = nn.Embedding(config.num_relations, config.rel_emb_size)\n        self.gru = nn.GRU(config.embedding_dim, config.hidden_dim_lstm, num_layers=config.num_layers, batch_first=True,\n                          bidirectional=True)\n        self.is_train = True\n        if USE_CUDA:\n            self.weights_rel = (torch.ones(self.config.num_relations) * 100).cuda()\n        else:\n            self.weights_rel = torch.ones(self.config.num_relations) * 100\n        self.weights_rel[0] = 1\n\n        self.V_ner = nn.Parameter(torch.rand((config.num_token_type, self.layer_size)))\n        self.U_ner = nn.Parameter(torch.rand((self.layer_size, 2 * self.hidden_dim)))\n        self.b_s_ner = nn.Parameter(torch.rand(self.layer_size))\n        self.b_c_ner = nn.Parameter(torch.rand(config.num_token_type))\n\n        # self.U_head = nn.Parameter(torch.rand((self.layer_size, self.hidden_dim * 2 + self.config.token_type_dim)))\n        # self.W_head = nn.Parameter(torch.rand((self.layer_size, self.hidden_dim * 2 + self.config.token_type_dim)))\n        # self.V_head = nn.Parameter(torch.rand(self.layer_size, len(self.config.relations)))\n        # self.b_s_head = nn.Parameter(torch.rand(self.layer_size))\n        #self.b_c_head = nn.Parameter(torch.rand(config.num_relations))\n        \n        self.dropout_embedding_layer = torch.nn.Dropout(config.dropout_embedding)\n        self.dropout_head_layer = torch.nn.Dropout(config.dropout_head)\n        self.dropout_ner_layer = torch.nn.Dropout(config.dropout_ner)\n        self.dropout_lstm_layer = torch.nn.Dropout(config.dropout_lstm)\n        self.crf_model = CRF(self.num_token_type, batch_first=True)\n        \n        self.selection_u = nn.Linear(self.hidden_dim * 2 + self.config.token_type_dim, config.rel_emb_size)\n        self.selection_v = nn.Linear(self.hidden_dim * 2 + self.config.token_type_dim, config.rel_emb_size)\n        self.selection_uv = nn.Linear(2*config.rel_emb_size, config.rel_emb_size)\n"}