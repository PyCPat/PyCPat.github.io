<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    expansion = 4

    def __init__(self, in_planes, planes, stride=1):
        <a id="change">super(</a>Bottleneck, self<a id="change">)</a>.__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)</code></pre><h3>After Change</h3><pre><code class='java'>
        super().__init__()
        assert image_size % patch_size == 0, &quotimage dimensions must be divisible by the patch size&quot
        num_patches = (image_size // patch_size) ** 2
        patch_dim = channels * patch_size<a id="change"> ** 2</a>
        assert num_patches &gt; MIN_NUM_PATCHES, f&quotyour number of patches ({num_patches}) is way too small for attention to be effective. try decreasing your patch size&quot

        self.patch_size = patch_size

        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))
        self.patch_to_embedding = <a id="change">nn.Linear(</a>patch_dim, dim<a id="change">)</a>
        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))
        self.dropout = nn.Dropout(emb_dropout)

        self.transformer = Transformer(dim, depth, heads, mlp_dim, dropout)</code></pre>