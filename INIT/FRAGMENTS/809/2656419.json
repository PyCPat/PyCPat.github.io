{"BEFORE":"    def __init__(self,F_g,F_l,F_int):\n        super(Attention_block,self).__init__()\n        self.W_g = nn.Sequential(\n            nn.Conv2d(F_g, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n            nn.BatchNorm2d(F_int)\n            )\n        \n        self.W_x = nn.Sequential(\n            nn.Conv2d(F_l, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n\n        self.psi = nn.Sequential(\n            nn.Conv2d(F_int, 1, kernel_size=1,stride=1,padding=0,bias=True),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n        \n        self.relu = nn.ReLU(inplace=True)\n","AFTER":"    def __init__(self, channels, num_groups = 32, skip_connection_scale=1, swish=1.0, skip_path=False):\n        super(ResNetBlock, self).__init__()\n        \n        self.main_path = nn.Sequential(\n            nn.GroupNorm(num_groups, channels),\n            Swish(swish),\n            nn.Conv2d(channels, channels, 3, 1, 1),\n            nn.GroupNorm(num_groups, channels),\n            Swish(swish),\n            nn.Conv2d(channels, channels, 3, 1, 1)\n        )\n        self.skip_path = skip_path\n        if skip_path:\n            self.skip = nn.Conv2d(channels, channels, 1, 1, 0)\n        self.skip_conn_scale = skip_connection_scale\n"}