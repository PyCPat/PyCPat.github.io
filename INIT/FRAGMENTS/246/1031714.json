{"BEFORE":"        range_vec = torch.arange(max_position)\n        distance_mat = range_vec[None, :] - range_vec[:, None]  # 列数-行数\n        distance_mat_clipped = torch.clamp(distance_mat, -max_relative_position, max_relative_position)\n        final_mat = distance_mat_clipped + max_relative_position\n\n        # sinusoid_encoding编码的位置矩阵\n        embeddings_table = get_sinusoid_encoding_table(vocab_size, embedding_size)\n\n        # 老的实现方式\n        # flat_relative_positions_matrix = final_mat.view(-1)\n        # one_hot_relative_positions_matrix = torch.nn.functional.one_hot(flat_relative_positions_matrix, num_classes=vocab_size).float()\n        # position_embeddings = torch.matmul(one_hot_relative_positions_matrix, embeddings_table)\n        # my_shape = list(final_mat.size())\n        # my_shape.append(embedding_size)\n        # position_embeddings = position_embeddings.view(my_shape)\n\n        position_embeddings = torch.take_along_dim(embeddings_table, final_mat.flatten().unsqueeze(1), dim=0)\n        position_embeddings = position_embeddings.reshape(*final_mat.shape, embeddings_table.shape[-1])  # [seq_len, seq_len, hdsz]\n","AFTER":"    def __init__(self, qlen, klen, embedding_size, max_relative_position=127):\n        super(RelativePositionsEncoding, self).__init__()\n        # 生成相对位置矩阵\n        vocab_size = max_relative_position * 2 + 1\n        distance_mat = torch.arange(klen)[None, :] - torch.arange(qlen)[:, None]  # 列数-行数, [query_len, key_len]\n        distance_mat_clipped = torch.clamp(distance_mat, -max_relative_position, max_relative_position)\n        final_mat = distance_mat_clipped + max_relative_position\n\n        # sinusoid_encoding编码的位置矩阵\n        embeddings_table = get_sinusoid_encoding_table(vocab_size, embedding_size)\n\n        # 实现方式1\n        # flat_relative_positions_matrix = final_mat.view(-1)\n        # one_hot_relative_positions_matrix = torch.nn.functional.one_hot(flat_relative_positions_matrix, num_classes=vocab_size).float()\n        # position_embeddings = torch.matmul(one_hot_relative_positions_matrix, embeddings_table)\n        # my_shape = list(final_mat.size())\n        # my_shape.append(embedding_size)\n        # position_embeddings = position_embeddings.view(my_shape)\n\n        # 实现方式2\n        # position_embeddings = torch.take_along_dim(embeddings_table, final_mat.flatten().unsqueeze(1), dim=0)\n        # position_embeddings = position_embeddings.reshape(*final_mat.shape, embeddings_table.shape[-1])  # [seq_len, seq_len, hdsz]\n        # self.register_buffer('position_embeddings', position_embeddings)\n        \n        # 实现方式3\n        position_embeddings = nn.Embedding.from_pretrained(embeddings_table, freeze=True)(final_mat)\n"}