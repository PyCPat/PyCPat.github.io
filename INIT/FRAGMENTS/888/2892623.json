{"BEFORE":"            attn = Attention(dim = dim, heads = heads, dim_head = dim_head, dropout = attn_dropout)\n            ff = FeedForward(dim = dim, dropout = ff_dropout)\n\n            shared_kv_proj = default(shared_kv_proj, attn.to_kv)\n            attn.to_kv = shared_kv_proj\n\n            self.layers.append(nn.ModuleList([\n                Residual(PreNorm(dim, attn)),\n                Residual(PreNorm(dim, ff))\n            ]))\n","AFTER":"            attn = Attention(dim = dim, heads = heads, dim_head = dim_head, dropout = attn_dropout)\n            ff = FeedForward(dim = dim, dropout = ff_dropout)\n\n            shared_kv_proj = default(shared_kv_proj, attn.to_kv)\n            attn.to_kv = shared_kv_proj\n\n            attn, ff = map(lambda fn: Residual(PreNorm(dim, fn)), (attn, ff))\n\n            if seq_len == 1:\n                memory_is_empty = lambda *args, **kwargs: not exists(kwargs['memory'])\n                attn = SkipIf(memory_is_empty, attn)\n\n            self.layers.append(nn.ModuleList([\n                attn,\n                ff\n            ]))\n"}