{"BEFORE":"            self,\n            inplanes: int,\n            planes: int,\n            stride: int = 1,\n            downsample: Optional[nn.Module] = None,\n            groups: int = 1,\n            base_width: int = 64,\n            dilation: int = 1,\n            norm_layer: Optional[Callable[..., nn.Module]] = None\n    ) -> None:\n        super(Bottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width \/ 64.)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n","AFTER":"    def __init__(self, in_channels, out_channels, stride,\n                 cardinality, base_width, widen_factor):\n        \"\"\" Constructor\n        Args:\n            in_channels: input channel dimensionality\n            out_channels: output channel dimensionality\n            stride: conv stride. Replaces pooling layer.\n            cardinality: num of convolution groups.\n            base_width: base number of channels in each group.\n            widen_factor: factor to reduce the input dimensionality before convolution.\n        \"\"\"\n        super().__init__()\n        width_ratio = out_channels \/ (widen_factor * 64.)\n        D = cardinality * int(base_width * width_ratio)\n        self.conv_reduce = nn.Conv2d(\n            in_channels, D, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn_reduce = nn.BatchNorm2d(D, momentum=0.001)\n        self.conv_conv = nn.Conv2d(D, D,\n                                   kernel_size=3, stride=stride, padding=1,\n                                   groups=cardinality, bias=False)\n        self.bn = nn.BatchNorm2d(D, momentum=0.001)\n        self.act = mish\n        self.conv_expand = nn.Conv2d(\n            D, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn_expand = nn.BatchNorm2d(out_channels, momentum=0.001)\n\n        self.shortcut = nn.Sequential()\n        if in_channels != out_channels:\n            self.shortcut.add_module('shortcut_conv',\n                                     nn.Conv2d(in_channels, out_channels,\n                                               kernel_size=1,\n                                               stride=stride,\n                                               padding=0,\n                                               bias=False))\n            self.shortcut.add_module(\n                'shortcut_bn', nn.BatchNorm2d(out_channels, momentum=0.001))\n\n    def forward(self, x):\n"}