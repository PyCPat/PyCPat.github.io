{"BEFORE":"                self.head = nn.Linear(embed_dims[3], num_classes)\n            else:\n                # CoaT-Lite series: Use feature of last scale for classification.\n                self.head = nn.Linear(embed_dims[3], num_classes)\n","AFTER":"        self.embed_dims = embed_dims\n        self.num_features = embed_dims[-1]\n        self.num_classes = num_classes\n\n        # Patch embeddings.\n        img_size = to_2tuple(img_size)\n        self.patch_embed1 = PatchEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=in_chans,\n            embed_dim=embed_dims[0], norm_layer=nn.LayerNorm)\n        self.patch_embed2 = PatchEmbed(\n            img_size=[x \/\/ 4 for x in img_size], patch_size=2, in_chans=embed_dims[0],\n            embed_dim=embed_dims[1], norm_layer=nn.LayerNorm)\n        self.patch_embed3 = PatchEmbed(\n            img_size=[x \/\/ 8 for x in img_size], patch_size=2, in_chans=embed_dims[1],\n            embed_dim=embed_dims[2], norm_layer=nn.LayerNorm)\n        self.patch_embed4 = PatchEmbed(\n            img_size=[x \/\/ 16 for x in img_size], patch_size=2, in_chans=embed_dims[2],\n            embed_dim=embed_dims[3], norm_layer=nn.LayerNorm)\n\n        # Class tokens.\n        self.cls_token1 = nn.Parameter(torch.zeros(1, 1, embed_dims[0]))\n        self.cls_token2 = nn.Parameter(torch.zeros(1, 1, embed_dims[1]))\n        self.cls_token3 = nn.Parameter(torch.zeros(1, 1, embed_dims[2]))\n        self.cls_token4 = nn.Parameter(torch.zeros(1, 1, embed_dims[3]))\n\n        # Convolutional position encodings.\n        self.cpe1 = ConvPosEnc(dim=embed_dims[0], k=3)\n        self.cpe2 = ConvPosEnc(dim=embed_dims[1], k=3)\n        self.cpe3 = ConvPosEnc(dim=embed_dims[2], k=3)\n        self.cpe4 = ConvPosEnc(dim=embed_dims[3], k=3)\n\n        # Convolutional relative position encodings.\n        self.crpe1 = ConvRelPosEnc(Ch=embed_dims[0] \/\/ num_heads, h=num_heads, window=crpe_window)\n        self.crpe2 = ConvRelPosEnc(Ch=embed_dims[1] \/\/ num_heads, h=num_heads, window=crpe_window)\n        self.crpe3 = ConvRelPosEnc(Ch=embed_dims[2] \/\/ num_heads, h=num_heads, window=crpe_window)\n        self.crpe4 = ConvRelPosEnc(Ch=embed_dims[3] \/\/ num_heads, h=num_heads, window=crpe_window)\n\n        # Disable stochastic depth.\n        dpr = drop_path_rate\n        assert dpr == 0.0\n        \n        # Serial blocks 1.\n        self.serial_blocks1 = nn.ModuleList([\n            SerialBlock(\n                dim=embed_dims[0], num_heads=num_heads, mlp_ratio=mlp_ratios[0], qkv_bias=qkv_bias,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr, norm_layer=norm_layer, \n                shared_cpe=self.cpe1, shared_crpe=self.crpe1\n            )\n            for _ in range(serial_depths[0])]\n        )\n\n        # Serial blocks 2.\n        self.serial_blocks2 = nn.ModuleList([\n            SerialBlock(\n                dim=embed_dims[1], num_heads=num_heads, mlp_ratio=mlp_ratios[1], qkv_bias=qkv_bias,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr, norm_layer=norm_layer, \n                shared_cpe=self.cpe2, shared_crpe=self.crpe2\n            )\n            for _ in range(serial_depths[1])]\n        )\n\n        # Serial blocks 3.\n        self.serial_blocks3 = nn.ModuleList([\n            SerialBlock(\n                dim=embed_dims[2], num_heads=num_heads, mlp_ratio=mlp_ratios[2], qkv_bias=qkv_bias,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr, norm_layer=norm_layer, \n                shared_cpe=self.cpe3, shared_crpe=self.crpe3\n            )\n            for _ in range(serial_depths[2])]\n        )\n\n        # Serial blocks 4.\n        self.serial_blocks4 = nn.ModuleList([\n            SerialBlock(\n                dim=embed_dims[3], num_heads=num_heads, mlp_ratio=mlp_ratios[3], qkv_bias=qkv_bias,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr, norm_layer=norm_layer, \n                shared_cpe=self.cpe4, shared_crpe=self.crpe4\n            )\n            for _ in range(serial_depths[3])]\n        )\n\n        # Parallel blocks.\n        self.parallel_depth = parallel_depth\n        if self.parallel_depth > 0:\n            self.parallel_blocks = nn.ModuleList([\n                ParallelBlock(\n                    dims=embed_dims, num_heads=num_heads, mlp_ratios=mlp_ratios, qkv_bias=qkv_bias,\n                    drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr, norm_layer=norm_layer,\n                    shared_crpes=(self.crpe1, self.crpe2, self.crpe3, self.crpe4)\n                )\n                for _ in range(parallel_depth)]\n            )\n        else:\n            self.parallel_blocks = None\n\n        # Classification head(s).\n        if not self.return_interm_layers:\n            if self.parallel_blocks is not None:\n                self.norm2 = norm_layer(embed_dims[1])\n                self.norm3 = norm_layer(embed_dims[2])\n            else:\n                self.norm2 = self.norm3 = None\n            self.norm4 = norm_layer(embed_dims[3])\n\n            if self.parallel_depth > 0:\n                # CoaT series: Aggregate features of last three scales for classification.\n                assert embed_dims[1] == embed_dims[2] == embed_dims[3]\n                self.aggregate = torch.nn.Conv1d(in_channels=3, out_channels=1, kernel_size=1)\n                self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n            else:\n                # CoaT-Lite series: Use feature of last scale for classification.\n                self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n"}