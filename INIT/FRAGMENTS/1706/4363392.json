{"BEFORE":"            self.BatchNorm = nn.BatchNorm2d\n            self.AvgPool = nn.AvgPool2d\n            self.adaptive_avg_pool = F.adaptive_avg_pool2d\n            self.output_size = (1, 1)\n            self.conv_stride = (conv1_t_stride, 2)\n        elif num_dimensions == 3:\n            self.Conv = nn.Conv3d\n            self.MaxPool = nn.MaxPool3d\n            self.BatchNorm = nn.BatchNorm3d\n            self.AvgPool = nn.AvgPool3d\n            self.adaptive_avg_pool = F.adaptive_avg_pool3d\n            self.output_size = (1, 1, 1)\n            self.conv_stride = (conv1_t_stride, 2, 2)\n        else:\n            sys.exit(\"Only 2D or 3D convolutions are supported.\")\n\n        # First convolution\n        self.features = [\n            (\n                \"conv1\",\n                self.Conv(\n                    num_channels,\n                    num_init_features,\n                    kernel_size=conv1_t_size,\n                    stride=self.conv_stride,\n                    padding=conv1_t_size \/\/ 2,\n                    bias=False,\n                ),\n            ),\n            (\"norm1\", self.BatchNorm(num_init_features)),\n            (\"relu1\", nn.ReLU(inplace=True)),\n        ]\n        if not no_max_pool:\n            self.features.append(\n                (\"pool1\", self.MaxPool(kernel_size=3, stride=2, padding=1))\n            )\n        self.features = nn.Sequential(OrderedDict(self.features))\n\n        self.final_convolution_layer = get_final_layer(final_convolution_layer)\n\n        # Each denseblock\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(\n                num_layers=num_layers,\n                num_input_features=num_features,\n                bn_size=bn_size,\n                growth_rate=growth_rate,\n                drop_rate=drop_rate,\n                batch_norm=self.BatchNorm,\n                conv=self.Conv,\n            )\n            self.features.add_module(\"denseblock{}\".format(i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n            if i != len(block_config) - 1:\n                trans = _Transition(\n                    num_input_features=num_features,\n                    num_output_features=num_features \/\/ 2,\n                    BatchNorm=self.BatchNorm,\n                    Conv=self.Conv,\n                    AvgPool=self.AvgPool,\n                )\n                self.features.add_module(\"transition{}\".format(i + 1), trans)\n                num_features = num_features \/\/ 2\n\n        # Final batch norm\n        self.features.add_module(\"norm5\", self.BatchNorm(num_features))\n\n        for m in self.modules():\n            if isinstance(m, self.Conv):\n                m.weight = nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n            elif isinstance(m, self.BatchNorm):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n        # Linear layer\n        self.classifier = nn.Linear(num_features, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, self.Conv):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n            elif isinstance(m, self.BatchNorm):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n","AFTER":"        norm_type=\"batch\"\n    ):\n\n        super().__init__()\n\n        if num_dimensions == 2:\n            self.Conv = nn.Conv2d\n            self.MaxPool = nn.MaxPool2d\n            if norm_type == \"instance\":\n                self.Norm = nn.InstanceNorm2d\n            else:\n                self.Norm = nn.BatchNorm2d\n            self.AvgPool = nn.AvgPool2d\n            self.adaptive_avg_pool = F.adaptive_avg_pool2d\n            self.output_size = (1, 1)\n            self.conv_stride = (conv1_t_stride, 2)\n        elif num_dimensions == 3:\n            self.Conv = nn.Conv3d\n            self.MaxPool = nn.MaxPool3d\n            if norm_type == \"instance\":\n                self.Norm = nn.InstanceNorm3d\n            else:\n                self.Norm = nn.BatchNorm3d\n            self.AvgPool = nn.AvgPool3d\n            self.adaptive_avg_pool = F.adaptive_avg_pool3d\n            self.output_size = (1, 1, 1)\n            self.conv_stride = (conv1_t_stride, 2, 2)\n        else:\n            sys.exit(\"Only 2D or 3D convolutions are supported.\")\n\n        # First convolution\n        self.features = [\n            (\n                \"conv1\",\n                self.Conv(\n                    num_channels,\n                    num_init_features,\n                    kernel_size=conv1_t_size,\n                    stride=self.conv_stride,\n                    padding=conv1_t_size \/\/ 2,\n                    bias=False,\n                ),\n            ),\n            (\"norm1\", self.Norm(num_init_features)),\n            (\"relu1\", nn.ReLU(inplace=True)),\n        ]\n        if not no_max_pool:\n            self.features.append(\n                (\"pool1\", self.MaxPool(kernel_size=3, stride=2, padding=1))\n            )\n        self.features = nn.Sequential(OrderedDict(self.features))\n\n        self.final_convolution_layer = get_final_layer(final_convolution_layer)\n\n        # Each denseblock\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(\n                num_layers=num_layers,\n                num_input_features=num_features,\n                bn_size=bn_size,\n                growth_rate=growth_rate,\n                drop_rate=drop_rate,\n                norm=self.Norm,\n                conv=self.Conv,\n            )\n            self.features.add_module(\"denseblock{}\".format(i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n            if i != len(block_config) - 1:\n                trans = _Transition(\n                    num_input_features=num_features,\n                    num_output_features=num_features \/\/ 2,\n                    Norm=self.Norm,\n                    Conv=self.Conv,\n                    AvgPool=self.AvgPool,\n                )\n                self.features.add_module(\"transition{}\".format(i + 1), trans)\n                num_features = num_features \/\/ 2\n\n        # Final batch norm\n        self.features.add_module(\"norm5\", self.Norm(num_features))\n\n        for m in self.modules():\n            if isinstance(m, self.Conv):\n                m.weight = nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n            elif isinstance(m, self.Norm):\n                if isinstance(self.Norm, nn.BatchNorm2d) or isinstance(self.Norm, nn.BatchNorm3d):\n                    m.weight.data.fill_(1)\n                    m.bias.data.zero_()\n\n        # Linear layer\n        self.classifier = nn.Linear(num_features, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, self.Conv):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n            elif isinstance(m, self.Norm):\n                if isinstance(self.Norm, nn.BatchNorm2d) or isinstance(self.Norm, nn.BatchNorm3d):\n                    nn.init.constant_(m.weight, 1)\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n"}