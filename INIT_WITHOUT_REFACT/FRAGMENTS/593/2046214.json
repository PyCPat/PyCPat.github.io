{"BEFORE":"          self.encoderModel = encoder_model\n          ### New layers:\n\n          #self.lstm = nn.LSTM(embedding_size, 256, batch_first=True,bidirectional=True, num_layers=2)\n          #self.linear = nn.Linear(256*2, number_of_labels)\n          self.lstm = nn.LSTM(input_size=embedding_size, hidden_size=200, batch_first=True, bidirectional=True, num_layers=2)\n\n          self.perceptron = nn.Sequential(\n                          nn.Linear(200*2, 200),\n                          nn.ReLU(),\n                          #nn.Linear(400, 200),\n                          #nn.ReLU(),\n                          nn.Linear(200, 200),\n                          #nn.Dropout(p=0.5),\n                          #nn.Linear(200, 100),\n                          nn.ReLU(),\n","AFTER":"    def __init__(self, number_of_labels, model_choice, embedding_size, dropout_layer, frozen):\n          super(CustomBERTModel, self).__init__()\n\n          if model_choice == \"t5-3b\":\n\n            tokenizer = T5Tokenizer.from_pretrained(model_choice, model_max_length=512)\n            model_encoding = T5EncoderModel.from_pretrained(model_choice)\n            embedding_size = 1024\n            self.encoderModel = model_encoding\n\n          else:\n\n            tokenizer = AutoTokenizer.from_pretrained(model_choice, model_max_length=512)\n                                                      #attention_probs_dropout_prob=0.5)\n                                                      #hidden_dropout_prob=0.5)\n            model_encoding = AutoModel.from_pretrained(model_choice)\n            embedding_size = 768\n            self.encoderModel = model_encoding\n\n\n\n          if frozen == True:\n            for param in self.encoderModel.parameters():\n                param.requires_grad = False\n\n\n          ### New layers:\n\n          #self.lstm = nn.LSTM(embedding_size, 256, batch_first=True,bidirectional=True, num_layers=2)\n          #self.linear = nn.Linear(256*2, number_of_labels)\n\n\n          self.lstm = nn.LSTM(input_size=embedding_size, hidden_size=200, batch_first=True, \n"}