{"BEFORE":"        is_cross_attention = encoder_hidden_states is not None\r\n        if is_cross_attention and past_key_value is not None:\r\n            key_layer = past_key_value[0]\r\n            value_layer = past_key_value[1]\r\n            attention_mask = encoder_attention_mask\r\n        elif is_cross_attention:\r\n            key_layer = self.transpose_for_qk_scores(self.k(encoder_hidden_states))\r\n            value_layer = self.transpose_for_v_scores(self.v(encoder_hidden_states))\r\n            attention_mask = encoder_attention_mask\r\n        elif past_key_value is not None:\r\n            key_layer = self.transpose_for_qk_scores(self.k(hidden_states))\r\n            value_layer = self.transpose_for_v_scores(self.v(hidden_states))\r\n            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\r\n            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\r\n        else:\r\n            key_layer = self.transpose_for_qk_scores(self.k(hidden_states))\r\n            value_layer = self.transpose_for_v_scores(self.v(hidden_states))\r\n        # query_layer shape: [batch_size, num_attention_heads, query_len, attention_head_size]\r\n        # key_layer shape: [batch_size, num_attention_heads, key_len, attention_head_size]\r\n        # value_layer shape: [batch_size, num_attention_heads, value_len, attention_head_size]\r\n\r\n        if self.p_bias == 'rotary' and self.position_encoding_2d:  # chatglm独有逻辑\r\n            q1, q2 = query_layer.chunk(2, dim=(query_layer.ndim - 1))\r\n            k1, k2 = key_layer.chunk(2, dim=(key_layer.ndim - 1))\r\n            q1 = self.relative_positions_encoding(q1, model_kwargs['position_ids'][:, 0, :])\r\n            k1 = self.relative_positions_encoding(k1, model_kwargs['position_ids'][:, 0, :])\r\n            q2 = self.relative_positions_encoding(q2, model_kwargs['position_ids'][:, 1, :])\r\n            k2 = self.relative_positions_encoding(k2, model_kwargs['position_ids'][:, 1, :])\r\n            query_layer = torch.concat([q1, q2], dim=(q1.ndim - 1))\r\n            key_layer = torch.concat([k1, k2], dim=(k1.ndim - 1))\r\n        elif self.p_bias == 'rotary' and not self.position_encoding_2d:  # 原rotary逻辑\r\n            query_layer = self.relative_positions_encoding(query_layer, model_kwargs['position_ids'])\r\n            key_layer = self.relative_positions_encoding(key_layer, model_kwargs['position_ids'])\r\n\r\n        if self.is_decoder:\r\n","AFTER":"        if self.p_bias == 'rotary':\r\n            # rotary有cache情况下，需要先rope后再和past_key_value concat\r\n            key_layer = self.transpose_for_qk_scores(self.k(hidden_states))\r\n            value_layer = self.transpose_for_v_scores(self.v(hidden_states))\r\n        elif (encoder_hidden_states is not None) and past_key_value is not None:\r\n            key_layer = past_key_value[0]\r\n            value_layer = past_key_value[1]\r\n            attention_mask = encoder_attention_mask\r\n        elif encoder_hidden_states is not None:\r\n            key_layer = self.transpose_for_qk_scores(self.k(encoder_hidden_states))\r\n            value_layer = self.transpose_for_v_scores(self.v(encoder_hidden_states))\r\n            attention_mask = encoder_attention_mask\r\n        elif past_key_value is not None:\r\n            key_layer = self.transpose_for_qk_scores(self.k(hidden_states))\r\n            value_layer = self.transpose_for_v_scores(self.v(hidden_states))\r\n            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\r\n            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\r\n        else:\r\n            key_layer = self.transpose_for_qk_scores(self.k(hidden_states))\r\n            value_layer = self.transpose_for_v_scores(self.v(hidden_states))\r\n        # query_layer shape: [batch_size, num_attention_heads, query_len, attention_head_size]\r\n        # key_layer shape: [batch_size, num_attention_heads, key_len, attention_head_size]\r\n        # value_layer shape: [batch_size, num_attention_heads, value_len, attention_head_size]\r\n\r\n        if self.p_bias == 'rotary':\r\n            if self.position_encoding_2d:  # chatglm独有逻辑\r\n                q1, q2 = query_layer.chunk(2, dim=(query_layer.ndim - 1))\r\n                k1, k2 = key_layer.chunk(2, dim=(key_layer.ndim - 1))\r\n                q1 = self.relative_positions_encoding(q1, model_kwargs['position_ids'][:, 0, :])\r\n                k1 = self.relative_positions_encoding(k1, model_kwargs['position_ids'][:, 0, :])\r\n                q2 = self.relative_positions_encoding(q2, model_kwargs['position_ids'][:, 1, :])\r\n                k2 = self.relative_positions_encoding(k2, model_kwargs['position_ids'][:, 1, :])\r\n                query_layer = torch.concat([q1, q2], dim=(q1.ndim - 1))\r\n                key_layer = torch.concat([k1, k2], dim=(k1.ndim - 1))\r\n            else:  # 原rotary逻辑\r\n                query_layer = self.relative_positions_encoding(query_layer, model_kwargs['position_ids'])\r\n                key_layer = self.relative_positions_encoding(key_layer, model_kwargs['position_ids'])\r\n            if past_key_value is not None:  # 过了rope再concat\r\n                key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\r\n                value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\r\n\r\n        if self.is_decoder:\r\n"}