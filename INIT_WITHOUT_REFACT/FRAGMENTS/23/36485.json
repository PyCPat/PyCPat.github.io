{"BEFORE":"        if(use_lma and (q_chunk_size is None or kv_chunk_size is None)):\n            raise ValueError(\n                \"If use_lma is specified, q_chunk_size and kv_chunk_size must \"\n                \"be provided\"\n            )\n        if(use_memory_efficient_kernel and use_lma):\n            raise ValueError(\n                \"Choose one of use_memory_efficient_kernel and use_lma\"\n            )\n\n        # [*, H, Q\/K, C_hidden]\n        q, k, v = self._prep_qkv(q_x, kv_x)\n\n        # [*, Q, H, C_hidden]\n        if(use_memory_efficient_kernel):\n            if(len(biases) > 2):\n                raise ValueError(\n                    \"If use_memory_efficient_kernel is True, you may only \"\n                    \"provide up to two bias terms\"\n                )\n            o = attention_core(q, k, v, *((biases + [None] * 2)[:2]))\n            o = o.transpose(-2, -3)\n        elif(use_lma):\n            biases = [\n                b.expand(b.shape[:-2] + (q_x.shape[-2],) + (kv_x.shape[-2],)) \n                for b in biases\n            ]\n            o = _lma(q, k, v, biases, q_chunk_size, kv_chunk_size)\n            o = o.transpose(-2, -3)\n        else:\n            o = _attention(q, k, v, biases)\n","AFTER":"        use_flash: bool = False,\n        flash_mask: Optional[torch.Tensor] = None,\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            q_x:\n                [*, Q, C_q] query data\n            kv_x:\n                [*, K, C_k] key data\n            biases:\n                List of biases that broadcast to [*, H, Q, K]\n            use_memory_efficient_kernel:\n                Whether to use a custom memory-efficient attention kernel.\n                This should be the default choice for most. If none of the\n                \"use_<...>\" flags are True, a stock PyTorch implementation\n                is used instead\n            use_lma:\n                Whether to use low-memory attention (Staats & Rabe 2021). If\n                none of the \"use_<...>\" flags are True, a stock PyTorch \n                implementation is used instead\n            lma_q_chunk_size:\n                Query chunk size (for LMA)\n            lma_kv_chunk_size:\n                Key\/Value chunk size (for LMA)\n        Returns\n            [*, Q, C_q] attention update\n        \"\"\"\n        if(use_lma and (q_chunk_size is None or kv_chunk_size is None)):\n            raise ValueError(\n                \"If use_lma is specified, q_chunk_size and kv_chunk_size must \"\n                \"be provided\"\n            )\n\n        if(use_flash and biases is not None):\n            raise ValueError(\n                \"use_flash is incompatible with the bias option. For masking, \"\n                \"use flash_mask instead\"\n            )\n\n        attn_options = [use_memory_efficient_kernel, use_lma, use_flash]\n        if(sum(attn_options) > 1):\n            raise ValueError(\n                \"Choose at most one alternative attention algorithm\"\n            )\n\n        if(biases is None):\n            biases = []\n        \n        # [*, H, Q\/K, C_hidden]\n        q, k, v = self._prep_qkv(q_x, kv_x)\n\n        # [*, Q, H, C_hidden]\n        if(use_memory_efficient_kernel):\n            if(len(biases) > 2):\n                raise ValueError(\n                    \"If use_memory_efficient_kernel is True, you may only \"\n                    \"provide up to two bias terms\"\n                )\n            o = attention_core(q, k, v, *((biases + [None] * 2)[:2]))\n            o = o.transpose(-2, -3)\n        elif(use_lma):\n            biases = [\n                b.expand(b.shape[:-2] + (q_x.shape[-2],) + (kv_x.shape[-2],)) \n                for b in biases\n            ]\n            o = _lma(q, k, v, biases, lma_q_chunk_size, lma_kv_chunk_size)\n            o = o.transpose(-2, -3)\n        elif(use_flash):\n            o = _flash_attn(q, k, v, flash_mask)\n        else:\n            o = _attention(q, k, v, biases)\n            o = o.transpose(-2, -3)\n"}