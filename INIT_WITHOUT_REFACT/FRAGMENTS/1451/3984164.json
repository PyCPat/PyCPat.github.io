{"BEFORE":"        if sum(emb.dim for emb in encoder_embeddings) != args.dimension:\n            raise ValueError('Hidden dimension must be equal to the sum of the embedding sizes to use IdentityEncoder')\n\n        self.encoder_embeddings = CombinedEmbedding(numericalizer, encoder_embeddings, args.dimension,\n","AFTER":"        self.args = args\n        self.pad_idx = numericalizer.pad_id\n\n        self.encoder_embeddings = CombinedEmbedding(numericalizer, encoder_embeddings, args.dimension,\n                                                    trained_dimension=0,\n                                                    project=False,\n                                                    finetune_pretrained=args.train_encoder_embeddings)\n\n        if self.args.rnn_layers > 0 and self.args.rnn_dimension != self.args.dimension:\n            self.dropout = nn.Dropout(args.dropout_ratio)\n            self.projection = nn.Linear(self.encoder_embeddings.dimension, self.args.rnn_dimension, bias=False)\n        else:\n            self.dropout = None\n            self.projection = None\n\n    def forward(self, batch):\n"}