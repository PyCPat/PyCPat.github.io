{"BEFORE":"        dim_batch = past.size()[0]\n        weight_read = torch.FloatTensor(dim_batch, len(self.memory_past)).cuda()\n        zero_padding = torch.zeros(1, dim_batch, self.dim_embedding_key * 2).cuda()\n        prediction = torch.Tensor().cuda()\n        present_temp = past[:, -1].unsqueeze(1)\n\n        # temporal encoding for past\n        past = torch.transpose(past, 1, 2)\n        story_embed = self.relu(self.conv_past(past))\n        story_embed = torch.transpose(story_embed, 1, 2)\n        output_past, state_past = self.encoder_past(story_embed)\n\n        # scene encoding\n        scene = scene.permute(0, 3, 1, 2)\n        scene_1 = self.convScene_1(scene)\n        scene_2 = self.convScene_2(scene_1)\n\n        for i in range(dim_batch):\n            weight_read[i] = self.similarity(self.memory_past, state_past[:, i]).unsqueeze(0)\n\n        # weight_read[torch.arange(dim_batch)] = self.similarity(self.memory_past, state_past[:,torch.arange(dim_batch)]).unsqueeze(0)\n        index_max = torch.sort(weight_read, descending=True)[1].cpu()\n","AFTER":"        past_normalized = F.normalize(self.memory_past, p=2, dim=1)\n        state_normalized = F.normalize(state_past.squeeze(), p=2, dim=1)\n        weight_read = torch.matmul(past_normalized, state_normalized.transpose(0,1)).transpose(0,1)\n"}