{"BEFORE":"            hidden_features = [hidden_features]\n        self.batchnorm = nn.BatchNorm1d(in_features)\n        self.in_conv = nn.Linear(in_features, hidden_features[0])\n        self.out_conv = nn.Linear(hidden_features[-1], out_features)\n        self.activation = activation\n        self.layers = nn.ModuleList()\n\n        for i in range(len(hidden_features) - 1):\n            if layer_norm:\n                self.layers.append(nn.LayerNorm(hidden_features[i]))\n            self.layers.append(SGConv(hidden_features[i], hidden_features[i + 1]))\n","AFTER":"                 feat_norm=None,\n                 adj_norm_func=GCNAdjNorm,\n                 layer_norm=False,\n                 k=4,\n                 dropout=0.0):\n        super(SGCN, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.feat_norm = feat_norm\n        self.adj_norm_func = adj_norm_func\n        if type(hidden_features) is int:\n            hidden_features = [hidden_features]\n        self.batch_norm = nn.BatchNorm1d(in_features)\n        self.in_conv = nn.Linear(in_features, hidden_features[0])\n        self.out_conv = nn.Linear(hidden_features[-1], out_features)\n        self.activation = activation\n        self.layers = nn.ModuleList()\n\n        for i in range(len(hidden_features) - 1):\n            if layer_norm:\n                self.layers.append(nn.LayerNorm(hidden_features[i]))\n            self.layers.append(SGConv(in_features=hidden_features[i],\n                                      out_features=hidden_features[i + 1],\n                                      k=k))\n\n        if dropout > 0.0:\n            self.dropout = nn.Dropout(dropout)\n        else:\n            self.dropout = None\n\n    @property\n"}