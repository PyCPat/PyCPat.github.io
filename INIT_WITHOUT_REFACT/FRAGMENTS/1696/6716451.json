{"BEFORE":"        dot[:, i, i] = TOKEN_SELF_ATTN_VALUE\n\n        if input_mask is not None:\n            mq = input_mask[:, :, None]\n            mk = F.pad(input_mask, (0, seq_len - mq.shape[1]), 'constant', True)[:, None, :]\n            mask = mq * mk\n            masked_value = -torch.finfo(dot.dtype).max\n            dot.masked_fill_(~mask, masked_value)\n\n        if self.causal:\n            i, j = torch.triu_indices(t, t, 1)\n            dot[:, i, j] = float('-inf')\n","AFTER":"        dot[:, i, i] = TOKEN_SELF_ATTN_VALUE\n        masked_value = max_neg_value(dot)\n\n        if input_mask is not None:\n            mask = input_mask[:, :, None] * input_mask[:, None, :]\n            mask = F.pad(mask, (0, seq_len - mask.shape[-1]), 'constant', True)\n            dot.masked_fill_(~mask, masked_value)\n\n        if self.causal:\n            i, j = torch.triu_indices(t, t, 1)\n            dot[:, i, j] = masked_value\n"}