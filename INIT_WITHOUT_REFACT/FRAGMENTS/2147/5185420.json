{"BEFORE":"        inner_dim = dim_head * heads\n        project_out = not (heads == 1 and dim_head == dim)\n\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n\n        self.attend = nn.Softmax(dim=-1)\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim),\n            nn.Dropout(dropout)\n        ) if project_out else nn.Identity()\n","AFTER":"    def __init__(self, token_channels, heads=8, dim_head=64, channel_mlp=None, dropout=0., AddPosEmb2Value=True):\n        \"\"\"\n        :param dim: input tokens channel, input token size = [bz, token_num, channel]\n        :param heads:\n        :param dim_head: projection dim of qkv\n        :param dropout:\n\n        like what Vit encoder does, AddPosEmb2Value = True\n        Vit,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(https:\/\/arxiv.org\/abs\/2010.11929)\n\n        \"\"\"\n        super().__init__()\n        if channel_mlp is None:\n            channel_mlp = token_channels\n        self.AddPosEmb2Value = AddPosEmb2Value\n\n        self.norm1 = nn.LayerNorm(token_channels)\n        self.Attention = MultiHeadAttention(token_channels, heads, dim_head, dropout)\n        self.norm2 = nn.LayerNorm(token_channels)\n        self.FFN = FeedForward(token_channels, channel_mlp, dropout=dropout)\n"}