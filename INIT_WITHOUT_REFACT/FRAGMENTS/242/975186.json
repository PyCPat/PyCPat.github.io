{"BEFORE":"        for num_neurons in head_layers[:-1]:\r\n            sequential_layers.append(nn.Linear(last_layer, num_neurons))\r\n            #TODO: use Batchnormalization?\r\n            sequential_layers.append(nn.ReLU(inplace=True))\r\n            last_layer = num_neurons\r\n        \r\n        #the last layer without activation\r\n        sequential_layers.append(nn.Linear(last_layer, head_layers[-1]))\r\n        last_layer = head_layers[-1]\r\n\r\n        head = nn.Sequential(\r\n            *sequential_layers\r\n          )\r\n        self.resnet18.fc = head\r\n","AFTER":"        for num_neurons in head_layers:\r\n            sequential_layers.append(nn.Linear(last_layer, num_neurons))\r\n            sequential_layers.append(nn.BatchNorm1d(num_neurons))\r\n            sequential_layers.append(nn.ReLU(inplace=True))\r\n            last_layer = num_neurons\r\n        \r\n        #the last layer without activation\r\n        #TODO: is this correct? check one classe representation framework paper\/code\r\n        # sequential_layers.append(nn.Linear(last_layer, head_layers[-1]))\r\n        # last_layer = head_layers[-1]\r\n\r\n        head = nn.Sequential(\r\n            *sequential_layers\r\n          )\r\n        self.resnet18.fc = nn.Identity()\r\n        self.head = head\r\n"}