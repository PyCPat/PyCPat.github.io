{"BEFORE":"        mem = cmem = None\n        if memories is not None:\n            mem, cmem = memories\n\n        num_memory_layers = len(self.memory_layers)\n        mem = default(mem, lambda: torch.empty(num_memory_layers, b, 0, d, **to(x)))\n        cmem = default(cmem, lambda: torch.empty(num_memory_layers, b, 0, d, **to(x)))\n\n        total_len = mem.shape[2] + cmem.shape[2] + self.seq_len\n        pos_emb = self.pos_emb[:, (self.seq_len - t):total_len]\n\n        next_mem = []\n        next_cmem = []\n        aux_loss = torch.tensor(0., requires_grad = True, **to(x))\n\n        mem_iter = iterate_tensor(mem)\n        cmem_iter = iterate_tensor(cmem)\n","AFTER":"        memories = default(memories, (None, None))\n        mem, cmem = memories\n\n        num_memory_layers = len(self.memory_layers)\n        init_empty_mem = lambda: torch.empty(num_memory_layers, b, 0, d, **to(x))\n        mem = default(mem, init_empty_mem)\n        cmem = default(cmem, init_empty_mem)\n\n        total_len = mem.shape[2] + cmem.shape[2] + self.seq_len\n        pos_emb = self.pos_emb[:, (self.seq_len - t):total_len]\n\n        next_mem = []\n        next_cmem = []\n        aux_loss = torch.tensor(0., requires_grad = True, **to(x))\n\n        mem_iter, cmem_iter = map(iterate_tensor, (mem, cmem))\n"}