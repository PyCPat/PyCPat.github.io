{"BEFORE":"        return cls_heads, reg_heads, center_heads\n","AFTER":"        self.batch_size, _, _, _ = inputs.shape\n        device = inputs.device\n        [C3, C4, C5] = self.backbone(inputs)\n\n        del inputs\n\n        features = self.fpn([C3, C4, C5])\n\n        del C3, C4, C5\n\n        self.fpn_feature_sizes = []\n        cls_heads, reg_heads, center_heads = [], [], []\n        for feature, scale in zip(features, self.scales):\n            self.fpn_feature_sizes.append([feature.shape[3], feature.shape[2]])\n            cls_outs = self.cls_head(feature)\n            # [N,num_classes,H,W] -> [N,H,W,num_classes]\n            cls_outs = cls_outs.permute(0, 2, 3, 1).contiguous()\n            cls_heads.append(cls_outs)\n\n            reg_outs, center_outs = self.regcenter_head(feature)\n            # [N,4,H,W] -> [N,H,W,4]\n            reg_outs = reg_outs.permute(0, 2, 3, 1).contiguous()\n            reg_outs = reg_outs * scale\n            reg_heads.append(reg_outs)\n            # [N,1,H,W] -> [N,H,W,1]\n            center_outs = center_outs.permute(0, 2, 3, 1).contiguous()\n            center_heads.append(center_outs)\n\n        del features\n\n        self.fpn_feature_sizes = torch.tensor(\n            self.fpn_feature_sizes).to(device)\n\n        batch_positions = self.positions(self.batch_size,\n                                         self.fpn_feature_sizes)\n\n        # if input size:[B,3,640,640]\n        # features shape:[[B, 256, 80, 80],[B, 256, 40, 40],[B, 256, 20, 20],[B, 256, 10, 10],[B, 256, 5, 5]]\n        # cls_heads shape:[[B, 80, 80, 80],[B, 40, 40, 80],[B, 20, 20, 80],[B, 10, 10, 80],[B, 5, 5, 80]]\n        # reg_heads shape:[[B, 80, 80, 4],[B, 40, 40, 4],[B, 20, 20, 4],[B, 10, 10, 4],[B, 5, 5, 4]]\n        # center_heads shape:[[B, 80, 80, 1],[B, 40, 40, 1],[B, 20, 20, 1],[B, 10, 10, 1],[B, 5, 5, 1]]\n        # batch_positions shape:[[B, 80, 80, 2],[B, 40, 40, 2],[B, 20, 20, 2],[B, 10, 10, 2],[B, 5, 5, 2]]\n\n        return cls_heads, reg_heads, center_heads, batch_positions\n"}