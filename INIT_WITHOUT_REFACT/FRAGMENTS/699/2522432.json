{"BEFORE":"                nn.LayerNorm(model_dim),\n","AFTER":"        assert (model_dim % heads) == 0, 'model dimension must be divisible by number of heads'\n        self.embed = InputEmbedding(time_features, model_dim, kernel_size = embed_kernel_size, dropout = dropout)\n\n        self.encoder_layers = nn.ModuleList([])\n\n        for ind in range(layers):\n            is_last_layer = (ind - 1) == layers\n\n            self.encoder_layers.append(nn.ModuleList([\n                FrequencyAttention(K = K, dropout = dropout),\n                MHESA(dim = model_dim, heads = heads, dropout = dropout),\n                FeedForwardBlock(dim = model_dim) if not is_last_layer else None,\n                Level(time_features = time_features, model_dim = model_dim)\n            ]))\n\n        self.growth_dampening_module = GrowthDampening(dim = model_dim, heads = heads)\n\n        self.latents_to_time_features = nn.Linear(model_dim, time_features)\n"}