{"BEFORE":"        x = self.project_in(x)\n\n        quantize, embed_ind = self._codebook(x)\n\n        if self.training:\n            commit_loss = F.mse_loss(quantize.detach(), x) * self.commitment\n            quantize = x + (quantize - x).detach()\n        else:\n            commit_loss = torch.tensor([0.], device = x.device)\n\n        quantize = self.project_out(quantize)\n\n        if need_transpose:\n            quantize = rearrange(quantize, 'b d n -> b n d')\n\n        return quantize, embed_ind, commit_loss\n","AFTER":"        device, codebook_size = x.device, self.codebook_size\n\n        need_transpose = not self.channel_last\n\n        if need_transpose:\n            x = rearrange(x, 'b n d -> b d n')\n\n        x = self.project_in(x)\n\n        quantize, embed_ind = self._codebook(x)\n\n        if self.training:\n            quantize = x + (quantize - x).detach()\n\n        loss = torch.tensor([0.], device = device)\n\n        if self.training:\n            if self.commitment > 0:\n                commit_loss = F.mse_loss(quantize.detach(), x)\n                loss = loss + commit_loss * self.commitment\n\n            if self.orthogonal_reg_weight > 0:\n                orthogonal_reg_loss = orthgonal_loss_fn(self.codebook)\n                loss = loss + orthogonal_reg_loss * self.orthogonal_reg_weight\n\n        quantize = self.project_out(quantize)\n\n        if need_transpose:\n            quantize = rearrange(quantize, 'b d n -> b n d')\n\n        return quantize, embed_ind, loss\n"}