<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                batchWiseLogit = logit.reshape(len(logit), -1, logit.shape[-1])

                &#47&#47 [n, k]
                summedProb<a id="change"> = </a><a id="change">batchWiseLogit.mean(1</a><a id="change">)</a>.sigmoid()

                target = torch.ones_like(summedProb)<a id="change"> / </a>2.0
                &#47&#47 [n, ]
                reg = F.binary_cross_entropy(summedProb, target, reduction=&quotnone&quot).sum(-1)

                &#47&#47 [n, k] -&gt; [n, ]
                diversity = batchWiseLogit.var(1).sum(-1)
                reg<a id="change"> -= </a>diversity

                &#47&#47 posterior = OneHotCategorical(logits=summedLogit, validate_args=False)
                &#47&#47 prior = OneHotCategorical(probs=torch.ones_like(summedLogit) / summedLogit.shape[-1], validate_args=False)</code></pre><h3>After Change</h3><pre><code class='java'>
            for latent, q in zip(latents, quantizeds):
                l2QLoss.append(F.mse_loss(latent.detach(), q, reduction=&quotnone&quot).mean(axis=(1, 2, 3)))
                l1QLoss.append(F.l1_loss(latent.detach(), q, reduction=&quotnone&quot).mean(axis=(1, 2, 3)))
                <a id="change">l2QLoss.append(</a>0.1 * F.mse_loss(latent, q.detach(), reduction=&quotnone&quot).mean(axis=(1, 2, 3))<a id="change">)</a>
                l1QLoss.append(0.1 * F.l1_loss(latent, q.detach(), reduction=&quotnone&quot).mean(axis=(1, 2, 3)))

        l1QLoss = sum(l1QLoss)
        l2QLoss = sum(l2QLoss)</code></pre>