{"BEFORE":"        x = self.norm(x)\n\n        # axial mask\n\n        w_mask = h_mask = None\n\n        if exists(mask):\n            w_mask = rearrange(mask, 'b h w -> (b w) h')\n            h_mask = rearrange(mask, 'b h w -> (b h) w')\n\n        # calculate attention bias\n\n        attn_bias = None\n        if exists(self.edges_to_attn_bias) and exists(edges):\n            attn_bias = self.edges_to_attn_bias(edges)\n\n        # axial attention\n\n        out = 0\n\n        if self.col_attn:\n            w_x = rearrange(x, 'b h w d -> (b w) h d')\n            if exists(attn_bias):\n                attn_bias = repeat(attn_bias, 'b h i j -> (b x) h i j', x = w)\n\n            tie_dim = w if self.global_query_attn else None\n            w_out = self.attn(w_x, mask = w_mask, attn_bias = attn_bias, tie_dim = tie_dim)\n            w_out = rearrange(w_out, '(b w) h d -> b h w d', h = h, w = w)\n\n            out += w_out\n\n        if self.row_attn:\n            h_x = rearrange(x, 'b h w d -> (b h) w d')\n            if exists(attn_bias):\n                attn_bias = repeat(attn_bias, 'b h i j -> (b x) h i j', x = h)\n\n            tie_dim = h if self.global_query_attn else None\n            h_out = self.attn(h_x, mask = h_mask, attn_bias = attn_bias, tie_dim = tie_dim)\n            h_out = rearrange(h_out, '(b h) w d -> b h w d', h = h, w = w)\n\n            out += h_out\n\n        return out\n","AFTER":"        x = self.norm(x)\n\n        # axial attention\n\n        if self.col_attn:\n            axial_dim = w\n            mask_fold_axial_eq = 'b h w -> (b w) h'\n            input_fold_eq = 'b h w d -> (b w) h d'\n            output_fold_eq = '(b w) h d -> b h w d'\n\n        elif self.row_attn:\n            axial_dim = h\n            mask_fold_axial_eq = 'b h w -> (b h) w'\n            input_fold_eq = 'b h w d -> (b h) w d'\n            output_fold_eq = '(b h) w d -> b h w d'\n\n        x = rearrange(x, input_fold_eq)\n\n        if exists(mask):\n            mask = rearrange(mask, mask_fold_axial_eq)\n\n        attn_bias = None\n        if exists(self.edges_to_attn_bias) and exists(edges):\n            attn_bias = self.edges_to_attn_bias(edges)\n            attn_bias = repeat(attn_bias, 'b h i j -> (b x) h i j', x = axial_dim)\n\n        tie_dim = axial_dim if self.global_query_attn else None\n\n        out = self.attn(x, mask = mask, attn_bias = attn_bias, tie_dim = tie_dim)\n        out = rearrange(out, output_fold_eq, h = h, w = w)\n"}