{"BEFORE":"        for logit in logits:\n            # [N, H, W, K] -> [N, K]\n            # logit = logit.mean(dim=(1, 2))\n            posterior = Categorical(logits=logit)\n            prior = Categorical(logits=torch.zeros_like(logit))\n            reg = torch.distributions.kl_divergence(posterior, prior).mean()\n            regs.append(reg)\n        regs = sum(regs) \/ len(logits)\n","AFTER":"        regs = list()\n\n        n, h, w, k = logits[0].shape\n\n        # codes: [m, n, h, w]; logits: m * list(n, h, w, k); codeFreqMap: m * list([n, h, w])\n        for code, logit, freqMap in zip(codes.permute(1, 0, 2, 3), logits, codeFreqMap):\n            needRegMask = (freqMap > (float(h * w) \/ k)).float()\n            sample = torch.distributions.Categorical(logits=torch.zeros_like(logit)).sample()\n            logit = logit.permute(0, 3, 1, 2)\n            ceReg = F.cross_entropy(logit, sample, reduction=\"none\") * needRegMask\n            cePush = F.cross_entropy(logit, code, reduction=\"none\") * (1 - needRegMask)\n            regs.append(ceReg.mean() + cePush.mean())\n        # # [m, n, h, w] and m * list(n, h, w, k) logits and [n, k] frequencies\n        # for code, logit, freq in zip(codes.permute(1, 0, 2, 3), logits, codeFreq):\n        #     # perturb code by the most rare codes with 0.1 probability\n        #     p = 1. \/ freq.shape[-1]\n        #     freq[freq < 1.] = 1.\n        #     weight = p \/ (freq + 1e-6)\n        #     dropoutMask = torch.distributions.Bernoulli(probs=torch.tensor(0.1, device=logit.device)).sample((n, h, w))\n        #     # [n, k] probs sample (h, w) -> [n, h, w]\n        #     sample = torch.distributions.Categorical(probs=weight).sample(((dropoutMask > 0.5).int().sum(), ))\n        #     code[dropoutMask > 0.5] = sample\n        #     # input: [n, k, h, w], target: [n, h, w] -> [n, h, w]\n        #     ce = F.cross_entropy(logit.permute(0, 3, 1, 2), code, reduction=\"none\")\n        #     # [n, k]\n        #     weight = torch.zeros((n, k), device=ce.device)\n        #     for i, c in enumerate(code):\n        #         # frequency for all k entries --- sum up to h * w\n        #         frequency = torch.bincount(c.flatten(), minlength=k)\n        #         p = 1. \/ len(torch.unique(c.flatten()))\n        #         # weights, higher frequency, lower weights\n        #         weight[i] = p \/ (frequency + 1e-6)\n        #     # indexing frequency by code to get per code weight\n        #     ix = torch.arange(n, device=logit.device)[:, None, None].expand_as(code)\n        #     # [n, h, w] weight sum to 1 every row, peer-to-peer weighting `ce` to balance the loss\n        #     weight = weight[[ix, code]]\n        #     ce = (ce * weight).mean()\n        #     regs.append(ce)\n\n        regs = sum(regs) \/ len(regs)\n"}