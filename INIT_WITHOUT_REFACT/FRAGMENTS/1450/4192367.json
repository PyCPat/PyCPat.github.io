{"BEFORE":"        quantizeds = list()\n        codes = list()\n        logits = list()\n        for xRaw in latents:\n            n, c, h, w = xRaw.shape\n            # [1, k, c]\n            codebook = getattr(self, \"codebook\")[None, ...]\n            # [n, c, h, w] -> [n, h, w, c]\n            encoderIn = xRaw.permute(0, 2, 3, 1)\n            # [n, h, w, c] -> [n, h*w, c]\n            encoderIn = self._position(encoderIn).reshape(n, -1, c)\n            # [1, k, c]\n            codebookQ = self._codebookQuery(codebook)\n            # [n, h*w, c]\n            x = self._encoder(encoderIn, codebookQ)\n            # [n, h*w, k]\n            logit = self._select(x)\n\n            logit = logit - logit.mean(1, keepdim=True)\n            logit = logit \/ logit.std(1, keepdim=True)\n\n            meanLogit = logit.detach().mean(1)\n\n            # [n, k], 3Ïƒ rule\n            bernoulli = Bernoulli(logits=meanLogit - 3.0)\n            # [h*w, n, k] -> [n, h*w, k] (0 or 1 -> choose or not choose)\n            randomFalseMask = bernoulli.sample((logit.shape[1], )).permute(1, 0, 2)\n            randomFalseMask *= -1e9\n            bernoulli = Bernoulli(logits=-meanLogit - 3.0)\n            randomTrueMask = bernoulli.sample((logit.shape[1], )).permute(1, 0, 2)\n            randomTrueMask *= 1e9\n            maskedLogit = logit + randomFalseMask + randomTrueMask\n\n            sample = F.gumbel_softmax(maskedLogit, temp, True)\n            # [1, k, c]\n            codewords = self._codebookEncoder(codebook)\n            # [n, h*w, c]\n            quantized = sample @ codewords[0, ...]\n            # [n, h*w, c]\n            posistedQuantized = self._position(quantized.reshape(n, h, w, c)).reshape(n, -1, c)\n            # [1, k, c]\n            decodedCodes = self._codebookDecoder(codebook)\n            # [n, c, h, w]\n            deTransformed = self._decoder(posistedQuantized, decodedCodes).reshape(n, h, w, c).permute(0, 3, 1, 2)\n\n            # [n, c, h, w]\n            quantizeds.append(deTransformed)\n            codes.append(sample.argmax(-1).reshape(n, h, w))\n            logits.append(logit.reshape(n, h, w, -1))\n        return quantizeds, codes, logits\n","AFTER":"        quantizeds = list()\n        codes = list()\n        logits = list()\n        xs = list()\n        for xRaw in latents:\n            n, c, h, w = xRaw.shape\n            # [1, k, c]\n            codebook = getattr(self, \"codebook\")[None, ...]\n            # [n, c, h, w] -> [n, h, w, c]\n            encoderIn = xRaw.permute(0, 2, 3, 1)\n            # [n, h, w, c] -> [n, h*w, c]\n            encoderIn = self._position(encoderIn).reshape(n, -1, c)\n            # [1, k, c]\n            codebookQ = self._codebookQuery(codebook)\n            # [n, h*w, c]\n            x = self._encoder(encoderIn, codebookQ)\n            xs.append(x)\n            # [n, h*w, k]\n            logit = self._select(x)\n\n            # [k]\n            bernoulli = Bernoulli(probs=maskProb)\n            # [n, h*w, k] (0 or 1 -> choose or not choose)\n            randomFalseMask = bernoulli.sample((n, h*w, )).bool()\n\n            maskedLogit = logit.masked_fill(randomFalseMask, -1e9)\n\n            # randomFalseMask *= -1e9\n            # maskedLogit = logit + randomFalseMask # + randomTrueMask\n\n            sample = F.gumbel_softmax(maskedLogit, 1.0, True)\n            # [1, k, c]\n            codewords = self._codebookEncoder(codebook)\n            # [n, h*w, c]\n            quantized = sample @ codewords[0, ...]\n            # [n, h*w, c]\n            posistedQuantized = self._position(quantized.reshape(n, h, w, c)).reshape(n, -1, c)\n            # [1, k, c]\n            decodedCodes = self._codebookDecoder(codebook)\n            # [n, c, h, w]\n            deTransformed = self._decoder(posistedQuantized, decodedCodes).reshape(n, h, w, c).permute(0, 3, 1, 2)\n\n            # [n, c, h, w]\n            quantizeds.append(deTransformed)\n            codes.append(sample.argmax(-1).reshape(n, h, w))\n            logits.append(logit.reshape(n, h, w, -1))\n        return quantizeds, codes, logits, xs\n"}