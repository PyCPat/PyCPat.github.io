{"BEFORE":"        p = ModelParams()\n        assert p.nb_erb % 8 == 0, \"erb_bins should be divisible by 8\"\n        self.stages = p.stages\n        self.freq_bins = p.fft_size \/\/ 2 + 1\n        self.erb_bins = p.nb_erb\n        self.df_bins = p.nb_df\n        self.erb_fb: Tensor\n        self.erb_comp = MagCompression(self.erb_bins)\n        self.cplx_comp = ComplexCompression(self.df_bins)\n        self.register_buffer(\"erb_fb\", erb_fb, persistent=False)\n        self.erb_stage = FreqStage(\n            1,\n            1,\n            nn.Sigmoid,\n            p.conv_ch,\n            p.nb_erb,\n            p.erb_hidden_dim,\n            depth=3,\n            global_skip=p.global_skip,\n        )\n        self.mask = Mask(erb_inv_fb, post_filter=p.mask_pf)\n        refinement_act = {\"tanh\": nn.Tanh, \"identity\": nn.Identity}[p.refinement_act.lower()]\n        self.refinement_stages = nn.ModuleList(\n            [\n                FreqStage(\n                    in_ch=2,\n                    out_ch=2,\n                    out_act=refinement_act,\n                    width=p.conv_ch,\n                    num_freqs=p.nb_df,\n                    hidden_dim=p.refinement_hidden_dim,\n                    depth=depth,\n                    patch_size=2 ** (i + 1),\n                    downsample_hprev=i >= 1,\n                    out_init_scale=2**-i,\n                    global_skip=p.global_skip,\n                )\n                for i, depth in enumerate(self.stages)\n            ]\n        )\n        self.lsnr_net = LSNRNet(p.conv_ch * 2, lsnr_min=p.lsnr_min, lsnr_max=p.lsnr_max)\n        # SNR offsets on which each refinement layer is activated\n        self.refinement_snr_min = -10\n        self.refinement_snr_max = (100, 10, 5, 0, -5, -5, -5, -5)\n        # Add a bunch of '-5' SNRs to support currently a maximum of 8 refinement layers.\n        assert len(self.stages) <= 8\n","AFTER":"        p = ModelParams()\n        assert p.nb_erb % 8 == 0, \"erb_bins should be divisible by 8\"\n        self.stages = p.stages\n        self.freq_bins = p.fft_size \/\/ 2 + 1\n        self.erb_bins = p.nb_erb\n        self.df_bins = p.nb_df\n        self.erb_fb: Tensor\n        self.erb_comp = MagCompression(self.erb_bins)\n        self.cplx_comp = ComplexCompression(self.df_bins)\n        self.register_buffer(\"erb_fb\", erb_fb, persistent=False)\n        assert p.erb_depth <= 6\n        self.erb_stage = FreqStage(\n            in_ch=1,\n            out_ch=1,\n            out_act=nn.Sigmoid,\n            num_freqs=p.nb_erb,\n            hidden_dim=p.erb_hidden_dim,\n            width_mult=p.width_mult,\n            depth_mult=p.depth_mult,\n            depth=p.erb_depth,\n        )\n        self.mask = Mask(erb_inv_fb, post_filter=p.mask_pf)\n        refinement_act = {\"tanh\": nn.Tanh, \"identity\": nn.Identity}[p.refinement_act.lower()]\n        assert p.refinement_depth <= 6\n        strides = [2, 2, 2, 2, 1, 2] if p.refinement_depth == 6 else None\n        self.refinement_stage = FreqStage(\n            in_ch=2,\n            out_ch=2,\n            out_act=refinement_act,\n            num_freqs=p.nb_df,\n            hidden_dim=p.refinement_hidden_dim,\n            width_mult=p.width_mult,\n            depth_mult=p.depth_mult,\n            depth=p.refinement_depth,\n            fstrides=strides,\n        )\n        self.refinement_op = ComplexMul() if p.refinement_op == \"mul\" else ComplexAdd()\n        self.lsnr_net = LSNRNet(self.erb_stage.max_width, lsnr_min=p.lsnr_min, lsnr_max=p.lsnr_max)\n"}