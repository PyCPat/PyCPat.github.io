{"BEFORE":"        exp_input = torch.arange(0, d_model, 2) * (-math.log(10000.0) \/ d_model)\r\n        \r\n        div_term = torch.exp(exp_input) # Returns a new tensor with the exponential of the elements of exp_input\r\n        \r\n        pe = torch.zeros(max_seq_len, d_model)\r\n\r\n        pe[:, 0::2] = torch.sin(position * div_term)\r\n        \r\n        pe[:, 1::2] = torch.cos(position * div_term) # torch.Size([58, 512])\r\n\r\n        pe = pe.unsqueeze(0).transpose(0, 1) # torch.Size([58, 1, 512])\r\n","AFTER":"        batch_first: bool=False\r\n        ):\r\n\r\n        \"\"\"\r\n        Parameters:\r\n            dropout: the dropout rate\r\n            max_seq_len: the maximum length of the input sequences\r\n            d_model: The dimension of the output of sub-layers in the model \r\n                     (Vaswani et al, 2017)\r\n        \"\"\"\r\n\r\n        super().__init__()\r\n\r\n        self.d_model = d_model\r\n        \r\n        self.dropout = nn.Dropout(p=dropout)\r\n\r\n        self.batch_first = batch_first\r\n\r\n        self.x_dim = 1 if batch_first else 0\r\n\r\n        # copy pasted from PyTorch tutorial\r\n        position = torch.arange(max_seq_len).unsqueeze(1)\r\n        \r\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) \/ d_model))\r\n        \r\n        pe = torch.zeros(max_seq_len, 1, d_model)\r\n        \r\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\r\n        \r\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\r\n"}