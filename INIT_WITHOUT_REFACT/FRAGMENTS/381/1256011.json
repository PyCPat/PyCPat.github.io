{"BEFORE":"    def __init__(self,nhiddens=3,ndim=64,dp=0.1):\n        \n        # lossnet is pair of [batch,L] -> dist [batch]\n        # classifnet goes dist [batch] -> pred [batch,2] == evaluate BCE with low-capacity\n        \n        super(classifnet, self).__init__()\n        nhiddens += 1\n        self.nhiddens = nhiddens\n        MLP = []\n        for ihiddens in range(self.nhiddens):\n            if ihiddens==0:\n                fin = 1\n            else:\n                fin = ndim\n            if ihiddens<nhiddens-1:\n                MLP.append(nn.Linear(fin,ndim))\n                MLP.append(nn.BatchNorm1d(ndim))\n                MLP.append(nn.LeakyReLU())\n                if dp!=0:\n                    MLP.append(nn.Dropout(p=dp))\n            else:\n                # last linear maps to binary class probabilities ; loss includes LogSoftmax\n                MLP.append(nn.Linear(fin,2))\n        self.MLP = nn.Sequential(*MLP)\n","AFTER":"    def __init__(self,ndim=[16,6],dp=0.1,BN=1):\n        \n        # lossnet is pair of [batch,L] -> dist [batch]\n        # classifnet goes dist [batch] -> pred [batch,2] == evaluate BCE with low-capacity\n        \n        super(classifnet, self).__init__()\n        n_layers = 2\n        MLP = []\n        for ilayer in range(n_layers):\n            if ilayer==0:\n                fin = 1\n            else:\n                fin = ndim[ilayer-1]\n            MLP.append(nn.Linear(fin,ndim[ilayer]))\n            if BN==1:\n                MLP.append(nn.BatchNorm1d(ndim[ilayer]))\n            MLP.append(nn.LeakyReLU())\n            if dp!=0:\n                MLP.append(nn.Dropout(p=dp))\n        # last linear maps to binary class probabilities ; loss includes LogSoftmax\n        MLP.append(nn.Linear(ndim[ilayer],2))\n"}