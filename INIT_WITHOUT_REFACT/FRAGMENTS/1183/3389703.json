{"BEFORE":"        assert len(mlp_hidden_mults) == 2, 'final mlp fixed at 2 layers for now'\n\n        # categories related calculations\n\n        self.num_categories = len(categories)\n        self.num_unique_categories = sum(categories)\n\n        # for automatically offsetting unique category ids to the correct position in the categories embedding table\n\n        categories_offset = F.pad(torch.tensor(list(categories)), (1, 0), value = 0).cumsum(dim = -1)[:-1]\n        self.register_buffer('categories_offset', categories_offset)\n\n        self.categorical_embeds = nn.Embedding(self.num_unique_categories, dim)\n\n        # continuous\n\n        assert continuous_mean_std.shape == (num_continuous, 2), f'continuous_mean_std must have a shape of ({num_continuous}, 2) where the last dimension contains the mean and variance respectively'\n        self.register_buffer('continuous_mean_std', continuous_mean_std)\n\n        self.norm = nn.LayerNorm(num_continuous)\n        self.num_continuous = num_continuous\n\n        # attention layers\n\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Residual(PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head))),\n                Residual(PreNorm(dim, FeedForward(dim))),\n            ]))\n\n        # mlp to logits\n\n        input_size = (dim * self.num_categories) + num_continuous\n        l = input_size \/\/ 8\n        mult1, mult2 = mlp_hidden_mults\n\n        self.mlp = nn.Sequential(\n            nn.Linear(input_size, l * mult1),\n            nn.ReLU(),\n            nn.Linear(l * mult1, l * mult2),\n            nn.ReLU(),\n            nn.Linear(l * mult2, dim_out)\n        )\n","AFTER":"        input_size = (dim * self.num_categories) + num_continuous\n        l = input_size \/\/ 8\n\n        hidden_dimensions = list(map(lambda t: l * t, mlp_hidden_mults))\n        all_dimensions = [input_size, *hidden_dimensions, dim_out]\n        self.mlp = MLP(all_dimensions)\n"}