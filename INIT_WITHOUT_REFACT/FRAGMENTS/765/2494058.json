{"BEFORE":"        with torch.cuda.amp.autocast() if self.mixed_precision is True and evaluation is False else dummy_context_mgr() as mp:\n            h = x\n            for index, blocklist in enumerate(self.blocks):\n                for block in blocklist:\n                    h = block(h)\n            h = self.conv(h)\n            if self.d_spectral_norm is False:\n                h = self.bn(h)\n            h = self.activation(h)\n            h = torch.sum(h, dim=[2, 3])\n\n            if self.conditional_strategy == 'no':\n                authen_output = torch.squeeze(self.linear1(h))\n                return authen_output\n\n            elif self.conditional_strategy in ['ContraGAN', 'Proxy_NCA_GAN', 'NT_Xent_GAN']:\n                authen_output = torch.squeeze(self.linear1(h))\n                cls_proxy = self.embedding(label)\n                cls_embed = self.linear2(h)\n                if self.nonlinear_embed:\n                    cls_embed = self.linear3(self.activation(cls_embed))\n                if self.normalize_embed:\n                    cls_proxy = F.normalize(cls_proxy, dim=1)\n                    cls_embed = F.normalize(cls_embed, dim=1)\n                return cls_proxy, cls_embed, authen_output\n\n            elif self.conditional_strategy == 'ProjGAN':\n                authen_output = torch.squeeze(self.linear1(h))\n                proj = torch.sum(torch.mul(self.embedding(label), h), 1)\n                return authen_output + proj\n\n            elif self.conditional_strategy == 'ACGAN':\n                authen_output = torch.squeeze(self.linear1(h))\n                cls_output = self.linear4(h)\n                return cls_output, authen_output\n\n            else:\n                raise NotImplementedError\n","AFTER":"        with torch.cuda.amp.autocast() if self.mixed_precision and not eval else misc.dummy_context_mgr() as mp:\n            embed, proxy, cls_output = None, None, None\n            h = x\n            for index, blocklist in enumerate(self.blocks):\n                for block in blocklist:\n                    h = block(h)\n            h = self.conv(h)\n            if not self.apply_d_sn:\n                h = self.bn(h)\n            h = self.activation(h)\n            h = torch.sum(h, dim=[2, 3])\n\n            adv_output = torch.squeeze(self.linear1(h))\n            if self.d_cond_mtd == \"AC\":\n                if self.normalize_d_embed:\n                    for W in self.linear2.parameters():\n                        W = F.normalize(W, dim=1)\n                    h = F.normalize(h, dim=1)\n                cls_output = self.linear2(h)\n            elif self.d_cond_mtd == \"PD\":\n                adv_output = adv_output + torch.sum(torch.mul(self.embedding(label), h), 1)\n            elif self.d_cond_mtd == \"2C\":\n                embed = self.linear2(h)\n                proxy = self.embedding(label)\n                if self.normalize_d_embed:\n                    embed = F.normalize(embed, dim=1)\n                    proxy = F.normalize(proxy, dim=1)\n            else:\n                raise NotImplementedError\n            return {\"adv_output\": adv_output, \"embed\": embed, \"proxy\": proxy, \"cls_output\": cls_output, \"label\": label}\n"}