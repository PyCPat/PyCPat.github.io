{"BEFORE":"                 locality_strength=1., use_local_init=True):\n        super().__init__()\n        self.num_heads = num_heads\n        self.dim = dim\n        head_dim = dim \/\/ num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qk = nn.Linear(dim, dim * 2, bias=qkv_bias)       \n        self.v = nn.Linear(dim, dim, bias=qkv_bias)       \n        \n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.pos_proj = nn.Linear(3, num_heads)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.locality_strength = locality_strength\n        self.gating_param = nn.Parameter(torch.ones(self.num_heads))\n        self.apply(self._init_weights)\n        if use_local_init:\n            self.local_init(locality_strength=locality_strength)\n\n    def _init_weights(self, m):\n","AFTER":"        self.locality_strength = locality_strength\n\n        self.qk = nn.Linear(dim, dim * 2, bias=qkv_bias)\n        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.pos_proj = nn.Linear(3, num_heads)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.locality_strength = locality_strength\n        self.gating_param = nn.Parameter(torch.ones(self.num_heads))\n        self.rel_indices: torch.Tensor = torch.zeros(1, 1, 1, 3)  # silly torchscript hack, won't work with None\n"}