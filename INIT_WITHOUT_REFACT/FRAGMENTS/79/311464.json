{"BEFORE":"        attention_bias = (1 - attention_mask.to(torch.float32)) * -1e12\n        attention_weights: FloatTensor = torch.einsum(\n            'bqhc,bkhc->bhqk',\n            queries, \n            keys\n        )\n        if attention_bias.ndim == 3:\n            attention_bias = attention_bias[:, None, :attention_weights.shape[-2], :attention_weights.shape[-1]]\n        elif attention_bias.ndim == 2:\n            attention_bias = attention_bias[:, None, None, :attention_weights.shape[-1]]\n        attention_weights += attention_bias\n","AFTER":"        attention_bias = (1 - attention_mask.to(torch.float32)) * -1e12\n        attention_weights: FloatTensor = torch.einsum(\n            'bqhc,bkhc->bhqk',\n            queries, \n            keys\n        )\n        while attention_bias.ndim < 4: \n            attention_bias = attention_bias.unsqueeze(1)\n        attention_weights += attention_bias\n"}