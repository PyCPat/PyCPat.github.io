{"BEFORE":"        conv = []\r\n        head = 1\r\n\r\n        for hid, num_head, act in zip(hids, num_heads, acts):\r\n            conv.append(GATConv(in_features * head,\r\n                                hid,\r\n                                num_heads=num_head,\r\n                                bias=bias,\r\n                                feat_drop=dropout,\r\n                                attn_drop=dropout,\r\n                                activation=activations.get(act)))\r\n            conv.append(nn.Flatten(1))\r\n","AFTER":"                 bn: bool = False,\r\n                 includes=['num_heads']):\r\n        r\"\"\"\r\n        Parameters\r\n        ----------\r\n        in_features : int, \r\n            the input dimmensions of model\r\n        out_features : int, \r\n            the output dimensions of model\r\n        hids : list, optional\r\n            the number of hidden units of each hidden layer, by default [8]\r\n        num_heads : list, optional\r\n            the number of attention heads of each hidden layer, by default [8]\r\n        acts : list, optional\r\n            the activaction function of each hidden layer, by default ['elu']\r\n        dropout : float, optional\r\n            the dropout ratio of model, by default 0.5\r\n        bias : bool, optional\r\n            whether to use bias in the layers, by default True      \r\n        bn: bool, optional\r\n            whether to use `BatchNorm1d` after the convolution layer, by default False            \r\n        \"\"\"\r\n        super().__init__()\r\n\r\n        conv = []\r\n        head = 1\r\n\r\n        for hid, num_head, act in zip(hids, num_heads, acts):\r\n            conv.append(GATConv(in_features * head,\r\n                                hid,\r\n                                num_heads=num_head,\r\n                                bias=bias,\r\n                                feat_drop=dropout,\r\n                                attn_drop=dropout,\r\n                                activation=None))\r\n            conv.append(nn.Flatten(1))\r\n            if bn:\r\n                conv.append(nn.BatchNorm1d(hid * num_head))\r\n            conv.append(activations.get(act))            \r\n"}