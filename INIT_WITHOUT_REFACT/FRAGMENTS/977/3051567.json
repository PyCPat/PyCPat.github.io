{"BEFORE":"        no_batch = (x.ndim == 1)\n        if no_batch:\n            x = rearrange(x, 'n -> () n')\n\n        if x.dtype == torch.long:\n            x = F.one_hot(x, num_classes = self.num_alphabet)\n\n        x = self._trunk(x.float())\n","AFTER":"        return_embeddings = False\n    ):\n        dtype = x.dtype\n\n        if x.dtype == torch.long:\n            x = F.one_hot(x, num_classes = self.num_alphabet).float()\n\n        no_batch = x.ndim == 2\n\n        if no_batch:\n            x = rearrange(x, '... -> () ...')\n\n        x = self._trunk(x)\n        out = map_values(lambda fn: fn(x), self._heads)\n\n        if no_batch:\n            out = map_values(lambda t: rearrange(t, '() ... -> ...'), out)\n            x = rearrange(x, '() ... -> ...')\n\n        if return_embeddings:\n            return out, x\n\n        return out\n"}