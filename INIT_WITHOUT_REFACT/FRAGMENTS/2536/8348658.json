{"BEFORE":"        qk = F.normalize(qk, 2, dim=-1)\n\n        dot = torch.einsum('bie,bje->bij', q, qk)\n\n        # qk attention requires tokens not attend to self\n        i = torch.arange(t)\n        dot[:, i, i] = -1e-5 \n","AFTER":"        q = qk[:, 0:query_len]\n        qk = F.normalize(qk, 2, dim=-1).type(q.type())\n\n        dot = torch.einsum('bie,bje->bij', q, qk)\n\n        # qk attention requires tokens not attend to self\n        i = torch.arange(t)\n        dot[:, i, i] = TOKEN_SELF_MASK_VALUE\n"}