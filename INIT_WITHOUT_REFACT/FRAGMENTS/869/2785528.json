{"BEFORE":"        shared_network_layers = []\n\n        # first apply convolution layers + flattening\n\n        for layer_spec in BASIC_CNN_ARCH:\n            shared_network_layers.append(nn.Conv2d(self.input_channel, layer_spec['out_dim'],\n                                                   kernel_size=layer_spec['kernel_size'], stride=layer_spec['stride']))\n            shared_network_layers.append(nn.ReLU())\n            self.input_channel = layer_spec['out_dim']\n        shared_network_layers.append(nn.Flatten())\n\n        # now customise the dense layers to handle an appropriate-sized conv output\n        dense_in_dim, = compute_output_shape(observation_space, shared_network_layers)\n        dense_arch = [\n            # this input size is accurate for Atari, but will be ovewritten for other envs\n            {'in_dim': 64*7*7},\n        ]\n        dense_arch[0]['in_dim'] = dense_in_dim\n        dense_arch[-1]['out_dim'] = representation_dim\n","AFTER":"        deconv_layers = []\n        self.architecture_definition = NETWORK_ARCHITECTURE_DEFINITIONS['BasicCNN']\n\n        # first apply convolution layers + flattening\n\n        for layer_spec in self.architecture_definition:\n            deconv_layers.append(nn.Conv2d(self.input_channel, layer_spec['out_dim'],\n                                                   kernel_size=layer_spec['kernel_size'], stride=layer_spec['stride']))\n            deconv_layers.append(nn.ReLU())\n            self.input_channel = layer_spec['out_dim']\n        self.deconvolution = nn.Sequential(*deconv_layers)\n        dense_layers = []\n        dense_layers.append(nn.Flatten())\n\n        # now customise the dense layers to handle an appropriate-sized conv output\n        dense_in_dim, = compute_output_shape(observation_space, deconv_layers + [nn.Flatten()])\n        dense_arch = [{'in_dim': dense_in_dim, 'out_dim': representation_dim}]\n        print(dense_arch)\n"}