<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        If ``cache`` is set to True, ``feat`` and ``graph`` should not change during
        training, or you will get wrong results.
        
        <a id="change">with </a><a id="change">graph.local_scope():

            </a>if self._cached and self._cached_h is not None:
                feat = self._cached_h
            else:
                &#47&#47 compute normalization
                degs = graph.in_degrees().float().clamp(min=1)
                norm = th.pow(degs, -0.5).to(feat.device).unsqueeze(1)

                &#47&#47 compute (D^-0.5 * A * D^-0.5)^k X
                for _ in range(self._k):
                    feat = feat * norm
                    graph.ndata[&quoth&quot] = feat
                    graph.update_all(fn.copy_src(&quoth&quot, &quotm&quot),
                                     fn.sum(&quotm&quot, &quoth&quot))
                    feat = graph.ndata.pop(&quoth&quot)
                    feat = feat * norm

                &#47&#47 cache feature
                if self._cached:
                    self._cached_h = feat

            if weight is not None:
                if self.weight is not None:
                    raise DGLError(&quotExternal weight is provided while at the same time the&quot
                                   &quot module has defined its own weight parameter. Please&quot
                                   &quot create the module with flag weight=False.&quot)
            else:
                weight = self.weight

            if weight is not None:
                feat<a id="change"> = </a>th.matmul(feat, weight)

            if self.bias is not None:
                feat = feat + self.bias</code></pre><h3>After Change</h3><pre><code class='java'>
            if self._cached:
                self._cached_h = feat

        <a id="change">return </a>self.linear(feat)
    
    def extra_repr(self):
        Set the extra representation of the module,</code></pre>