{"BEFORE":"        slf_attn_mask = get_attn_key_pad_mask(seq_k=src_seq, seq_q=src_seq)\n        non_pad_mask = get_non_pad_mask(src_seq)\n\n        # -- Forward\n        enc_output = self.src_word_emb(src_seq) + self.position_enc(src_pos)\n\n        for enc_layer in self.layer_stack:\n            enc_output, enc_slf_attn = enc_layer(\n                enc_output,\n                non_pad_mask=non_pad_mask,\n                slf_attn_mask=slf_attn_mask)\n            if return_attns:\n                enc_slf_attn_list += [enc_slf_attn]\n\n        return enc_output, non_pad_mask\n","AFTER":"        batch_size, max_len = src_seq.shape[0], src_seq.shape[1]\n        \n        # -- Prepare masks\n        slf_attn_mask = mask.unsqueeze(1).expand(-1, max_len, -1)\n\n        # -- Forward\n        enc_output = self.src_word_emb(src_seq) + self.position_enc[:, :max_len, :].expand(batch_size, -1, -1)\n\n        for enc_layer in self.layer_stack:\n            enc_output, enc_slf_attn = enc_layer(\n                enc_output,\n                mask=mask,\n                slf_attn_mask=slf_attn_mask)\n            if return_attns:\n                enc_slf_attn_list += [enc_slf_attn]\n\n        return enc_output\n"}