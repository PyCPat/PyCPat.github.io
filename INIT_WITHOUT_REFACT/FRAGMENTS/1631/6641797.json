{"BEFORE":"        z_q = torch.einsum('b n h w, n d -> b d h w', soft_one_hot, self.embedding.weight)\n\n        # + kl divergence to the prior loss\n        qy = F.softmax(z, dim=1)\n        loss = self.kl_weight * torch.sum(qy * torch.log(qy * self.num_tokens + 1e-10), dim=1).mean()\n\n        encoding_indices = soft_one_hot.argmax(dim=1)\n\n        return z_q, loss, (None, None, encoding_indices)\n","AFTER":"        z_q = torch.einsum('b n h w, n d -> b d h w', soft_one_hot, self.embedding.weight)\n\n        # + kl divergence to the prior loss\n        qy = F.softmax(z, dim=1)\n        loss = self.kl_weight * torch.sum(qy * torch.log(qy * self.num_tokens + 1e-10), dim=1).mean()\n\n        encoding_indices = soft_one_hot.argmax(dim=1)\n        encodings = F.one_hot(encoding_indices, self.num_tokens).type(z.dtype)\n        avg_probs = torch.mean(encodings, dim=0)\n        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n\n        return z_q, loss, (perplexity, encodings, encoding_indices)\n"}