{"BEFORE":"        batch_size, seg_len, embed_dim = x.shape\n        mem_len = h.shape[1] - seg_len\n        total_len = h.shape[1]\n        \n        # compute projections of input and memory embeddings\n        q = self.w_q(x).view(batch_size, -1, seg_len, embed_dim)\n        k = self.w_k(h).view(batch_size, -1, total_len, embed_dim)\n        v = self.w_v(h).view(batch_size, -1, total_len, embed_dim)\n        r_emb = self.w_r(self.R[-total_len:]).view(1, -1, total_len, embed_dim)\n        \n        # the \"XL specific\" way of computing the pre-softmax attention score\n        ac = torch.einsum(\"bhid,bhjd->bhij\", q + self.u1, k)\n        bd = torch.einsum(\"bhid,bhjd->bhij\", q + self.u2, r_emb)\n        bd = self.circulant_shift(bd, -seg_len+1)\n        \n        # computing the attention scores\n        att_score = ac + bd\n        att_score = att_score.tril(mem_len) \/ embed_dim**0.5\n        att_score[att_score == 0] = float(\"-inf\")\n        att_score = torch.softmax(att_score, dim=-1)\n        \n        # compute output\n        att = (att_score @ v).view(batch_size, seg_len, -1)\n        return self.layer_norm(self.mlp(att) + x)\n","AFTER":"        batch_size, seg_len, _ = x.shape\n        mem_len = h.shape[1] - seg_len\n        total_len = h.shape[1]\n        \n        # compute projections of input and memory embeddings\n        q = self.w_q(x).view(batch_size, -1, seg_len, self.embed_dim)\n        k = self.w_k(h).view(batch_size, -1, total_len, self.embed_dim)\n        v = self.w_v(h).view(batch_size, -1, total_len, self.embed_dim)\n        r_emb = self.w_r(self.R[-total_len:]).view(1, -1, total_len, self.embed_dim)\n        \n        # the \"XL specific\" way of computing the pre-softmax attention score\n        ac = torch.einsum(\"bhid,bhjd->bhij\", q + self.u1, k)\n        bd = torch.einsum(\"bhid,bhjd->bhij\", q + self.u2, r_emb)\n        bd = self.circulant_shift(bd, -seg_len+1)\n        \n        # computing the attention scores\n        att_score = ac + bd\n        att_score = att_score.tril(mem_len) \/ self.embed_dim**0.5\n        att_score[att_score == 0] = float(\"-inf\")\n        att_score = torch.softmax(att_score, dim=-1)\n        #att_score = self.dropout(att_score)\n        \n        # compute output\n        att = (att_score @ v).view(batch_size, seg_len, -1)\n        out = self.dropout(self.mlp(att))\n        return self.layer_norm(out + x)\n"}