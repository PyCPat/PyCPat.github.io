{"BEFORE":"            if shared_label is None:\n                shared_label = self.shared(label)\n            else:\n                pass\n            z = torch.cat([shared_label, z], 1)\n\n            act = self.linear0(z)\n            act = act.view(-1, self.in_dims[0], self.bottom, self.bottom)\n            for index, blocklist in enumerate(self.blocks):\n                for block in blocklist:\n                    if isinstance(block, ops.SelfAttention):\n                        act = block(act)\n                    else:\n                        act = block(act, z)\n","AFTER":"        affine_list = []\n        with torch.cuda.amp.autocast() if self.mixed_precision and not eval else misc.dummy_context_mgr() as mp:\n            if self.MODEL.info_type != \"N\/A\":\n                if self.MODEL.g_info_injection == \"concat\":\n                    z = self.info_mix_linear(z)\n                elif self.MODEL.g_info_injection == \"cBN\":\n                    z, z_info = z[:, :self.z_dim], z[:, self.z_dim:]\n                    affine_list.append(self.info_proj_linear(z_info))\n\n            if self.g_cond_mtd != \"W\/O\":\n                if shared_label is None:\n                    shared_label = self.shared(label)\n                affine_list.append(shared_label)\n            if len(affine_list) == 0:\n                affine = z\n            else:\n                affine = torch.cat(affine_list + [z], 1)\n\n            act = self.linear0(z)\n            act = act.view(-1, self.in_dims[0], self.bottom, self.bottom)\n            for index, blocklist in enumerate(self.blocks):\n                for block in blocklist:\n                    if isinstance(block, ops.SelfAttention):\n                        act = block(act)\n                    else:\n                        act = block(act, affine)\n"}