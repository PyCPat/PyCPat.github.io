{"BEFORE":"        use_lsh = not self.use_full_attn or kv_len <= self.full_attn_thres\n        assert not use_lsh or (kv_len % self.bucket_size == 0), f'Sequence length needs to be divisible by target bucket size - {self.bucket_size}'\n\n        x = torch.cat((x, mem, keys), dim=1)\n        qk = self.toqk(x)\n        v = self.tov(x)\n\n        def merge_heads(v):\n            return v.view(b, kv_len, h, -1).transpose(1, 2).reshape(b * h, kv_len, -1)\n\n        def split_heads(v):\n            return v.view(b, h, t, -1).transpose(1, 2).contiguous()\n\n        qk = merge_heads(qk)\n        v = merge_heads(v)\n\n        attn_fn = self.lsh_attn if use_lsh else self.full_attn\n        partial_attn_fn = partial(attn_fn, query_len = t, input_mask = input_mask)\n        attn_out, buckets = process_inputs_chunk(partial_attn_fn, qk, v, chunks=self.attn_chunks)\n\n        out = split_heads(attn_out).view(b, t, e)\n","AFTER":"        use_full_attn = self.use_full_attn or kv_len <= self.full_attn_thres\n\n        if not use_full_attn:\n            assert not use_full_attn and (kv_len % self.bucket_size == 0), f'Sequence length needs to be divisible by target bucket size - {self.bucket_size}'\n\n        x = torch.cat((x, mem, keys), dim=1)\n        qk = self.toqk(x)\n        v = self.tov(x)\n\n        def merge_heads(v):\n            return v.view(b, kv_len, h, -1).transpose(1, 2).reshape(b * h, kv_len, -1)\n\n        def split_heads(v):\n            return v.view(b, h, t, -1).transpose(1, 2).contiguous()\n\n        qk = merge_heads(qk)\n        v = merge_heads(v)\n\n        attn_fn = self.lsh_attn if not use_full_attn else self.full_attn\n        partial_attn_fn = partial(attn_fn, query_len = t, input_mask = input_mask)\n        out, attn, buckets = process_inputs_chunk(partial_attn_fn, qk, v, chunks=self.attn_chunks)\n        out = split_heads(out).view(b, t, e)\n\n        if self.callback is not None:\n            self.callback(attn.reshape(b, h, t, -1), buckets.reshape(b, h, -1))\n\n        return self.to_out(out)\n"}