{"BEFORE":"            loss1 = F.binary_cross_entropy(torch.sigmoid(logits), labels.float())\n","AFTER":"            log_logits = F.log_softmax(logits, dim=1)\n            labels_scaled = labels \/ labels.sum(dim=1, keepdim=True)\n            loss1 = - (labels_scaled * log_logits).sum(dim=1)\n            loss1 = loss1.mean()\n"}