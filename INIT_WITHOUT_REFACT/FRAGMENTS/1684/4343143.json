{"BEFORE":"            raise NotImplementedError\n","AFTER":"        self.out_dims = d_out_dims_collection[str(img_size)]\n        down = d_down[str(img_size)]\n\n        self.blocks = []\n        for index in range(len(self.in_dims)):\n            if index == 0:\n                self.blocks += [[DiscOptBlock(in_channels=self.in_dims[index],\n                                              out_channels=self.out_dims[index],\n                                              d_spectral_norm=d_spectral_norm,\n                                              activation_fn=activation_fn,\n                                              synchronized_bn=synchronized_bn)]]\n            else:\n                self.blocks += [[DiscBlock(in_channels=self.in_dims[index],\n                                           out_channels=self.out_dims[index],\n                                           d_spectral_norm=d_spectral_norm,\n                                           activation_fn=activation_fn,\n                                           synchronized_bn=synchronized_bn,\n                                           downsample=down[index])]]\n\n            if index+1 == attention_after_nth_dis_block and attention is True:\n                self.blocks += [[Self_Attn(self.out_dims[index], d_spectral_norm)]]\n        \n        self.blocks = nn.ModuleList([nn.ModuleList(block) for block in self.blocks])\n        \n        self.LN = nn.LayerNorm([self.out_dims[-1], img_size\/\/(2**n_down), img_size\/\/(2**n_down)])\n        \n        if activation_fn == \"ReLU\":\n            self.activation = nn.ReLU(inplace=True)\n        elif activation_fn == \"Leaky_ReLU\":\n            self.activation = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n        elif activation_fn == \"ELU\":\n            self.activation = nn.ELU(alpha=1.0, inplace=True)\n        elif activation_fn == \"GELU\":\n            self.activation = nn.GELU()\n        else:\n            raise NotImplementedError\n"}