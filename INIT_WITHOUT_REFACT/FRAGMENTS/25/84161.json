{"BEFORE":"        mask=None,\n        dropout=0.,\n    ):\n        \"\"\"\n        TBD\n        :param dim:\n        :param heads:\n        :param causal:\n        :param mask:\n        :param dropout:\n        \"\"\"\n        super().__init__()\n        assert dim % heads == 0\n        dim_head = int(dim \/ heads)\n        self.scale = dim_head ** -0.5\n        self.num_heads = heads\n        self.causal = causal\n        self.mask = mask\n\n        qk_dim = v_dim = dim_head * heads\n\n        self.to_q = nn.Linear(dim, qk_dim, bias=False)\n        self.to_k = nn.Linear(dim, qk_dim, bias=False)\n        self.to_v = nn.Linear(dim, v_dim, bias=False)\n\n        self.dropout = nn.Dropout(dropout)\n","AFTER":"        qk_dim = v_dim = dim_head * heads\n\n        if use_previous_attention:\n            # If we use the attention pattern from the last attention layer, we don't need queries and keys\n            self.to_v = nn.Linear(dim, v_dim, bias=False)\n\n        else:\n            # Standard attention layer that will calculate the attention pattern from queries and keys\n            self.to_q = nn.Linear(dim, qk_dim, bias=False)\n            self.to_k = nn.Linear(dim, qk_dim, bias=False)\n            self.to_v = nn.Linear(dim, v_dim, bias=False)\n\n        self.attn_fn = F.softmax\n"}