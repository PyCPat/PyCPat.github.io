<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            batch, permutation_dim, -1
        )

        <a id="change">if self.combining_operation == "mean"</a><a id="change">:
            </a>e<a id="change"> = </a>iid_embeddings.mean(dim=1)
        elif self.combining_operation == "sum":
            e = iid_embeddings.sum(dim=1)
        else:
            <a id="change">raise </a><a id="change">ValueError("combining_operation must be in [&quotsum&quot, &quotmean&quot]."</a><a id="change">)</a>

        embedding = self.fc_subnet(e)

        return embedding</code></pre><h3>After Change</h3><pre><code class='java'>
        batch, permutation_dim, _ = x.shape

        &#47&#47 if no NaNs for padding varying trial lengths we can batch the computation
        <a id="change">if </a>not torch.isnan(x).any()<a id="change">:
            </a>trial_embeddings = self.trial_net(x.view(batch * permutation_dim, -1)).view(
                batch, permutation_dim, -1
            )
            combined_embedding = self.combining_function(trial_embeddings, dim=1)
            trial_counts = torch.ones(batch, 1, dtype=torch.float32) * permutation_dim

        &#47&#47 otherwise we need to loop over the batch to account for varying trial lengths
        else:
            combined_embedding = []
            trial_counts = torch.zeros(batch, 1)
            for i in range(batch):
                &#47&#47 remove NaNs
                valid_x<a id="change"> = x[i, ~torch.isnan(x[i, :, 0]), :]</a>
                trial_counts[i] = valid_x.shape[0]
                trial_embeddings = self.trial_net(valid_x)
                &#47&#47 apply combining operation over permutation dimension
                combined_embedding.append(</code></pre>