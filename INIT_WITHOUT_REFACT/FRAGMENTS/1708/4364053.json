{"BEFORE":"                 num_classes=80,\n                 embed_dim=768,\n                 depth=12,\n                 num_heads=12,\n                 mlp_ratio=4.,\n                 qkv_bias=False,\n                 qk_scale=None,\n                 drop_rate=0.,\n                 attn_drop_rate=0.,\n                 drop_path_rate=0.,\n                 hybrid_backbone=None,\n                 norm_layer=None,\n                 init_values=None,\n                 use_checkpoint=False,\n                 use_abs_pos_emb=False,\n                 use_rel_pos_bias=False,\n                 use_shared_rel_pos_bias=False,\n                 out_indices=[11],\n                 interval=3,\n                 pretrained=None,\n                 aggregation='attn'):\n        super().__init__()\n        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n\n        if hybrid_backbone is not None:\n            self.patch_embed = HybridEmbed(\n                hybrid_backbone,\n                img_size=img_size,\n                in_chans=in_chans,\n                embed_dim=embed_dim)\n        else:\n            self.patch_embed = PatchEmbed(\n                img_size=img_size,\n                patch_size=patch_size,\n                in_chans=in_chans,\n                embed_dim=embed_dim)\n\n        num_patches = self.patch_embed.num_patches\n\n        self.out_indices = out_indices\n\n        if use_abs_pos_emb:\n            self.pos_embed = nn.Parameter(\n                torch.zeros(1, num_patches, embed_dim))\n        else:\n            self.pos_embed = None\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)\n               ]  # stochastic depth decay rule\n        self.use_rel_pos_bias = use_rel_pos_bias\n        self.use_checkpoint = use_checkpoint\n        self.blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                drop=drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[i],\n                norm_layer=norm_layer,\n                init_values=init_values,\n                window_size=(14, 14) if\n                ((i + 1) % interval != 0\n                 or aggregation != 'attn') else self.patch_embed.patch_shape,\n                window=((i + 1) % interval != 0),\n                aggregation=aggregation) for i in range(depth)\n        ])\n\n        if self.pos_embed is not None:\n            trunc_normal_(self.pos_embed, std=.02)\n\n        self.norm = norm_layer(embed_dim)\n\n        self.pretrained = pretrained\n        self._register_load_state_dict_pre_hook(self._prepare_checkpoint_hook)\n","AFTER":"        drop_path_rate=0.0,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        act_layer=nn.GELU,\n        use_abs_pos=True,\n        use_rel_pos=False,\n        rel_pos_zero_init=True,\n        window_size=0,\n        window_block_indexes=(),\n        residual_block_indexes=(),\n        use_act_checkpoint=False,\n        pretrain_img_size=224,\n        pretrain_use_cls_token=True,\n        pretrained=None,\n    ):\n        \"\"\"\n        Args:\n            img_size (int): Input image size.\n            patch_size (int): Patch size.\n            in_chans (int): Number of input image channels.\n            embed_dim (int): Patch embedding dimension.\n            depth (int): Depth of ViT.\n            num_heads (int): Number of attention heads in each ViT block.\n            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n            drop_path_rate (float): Stochastic depth rate.\n            norm_layer (nn.Module): Normalization layer.\n            act_layer (nn.Module): Activation layer.\n            use_abs_pos (bool): If True, use absolute positional embeddings.\n            use_rel_pos (bool): If True, add relative positional embeddings to the attention map.\n            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n            window_size (int): Window size for window attention blocks.\n            window_block_indexes (list): Indexes for blocks using window attention.\n            residual_block_indexes (list): Indexes for blocks using conv propagation.\n            use_act_checkpoint (bool): If True, use activation checkpointing.\n            pretrain_img_size (int): input image size for pretraining models.\n            pretrain_use_cls_token (bool): If True, pretrainig models use class token.\n        \"\"\"\n        super().__init__()\n        self.pretrain_use_cls_token = pretrain_use_cls_token\n        self.use_act_checkpoint = use_act_checkpoint\n\n        self.patch_embed = PatchEmbed(\n            kernel_size=(patch_size, patch_size),\n            stride=(patch_size, patch_size),\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n        )\n\n        if use_abs_pos:\n            # Initialize absolute positional embedding with pretrain image size.\n            num_patches = (pretrain_img_size \/\/ patch_size) * (\n                pretrain_img_size \/\/ patch_size)\n            num_positions = (num_patches +\n                             1) if pretrain_use_cls_token else num_patches\n            self.pos_embed = nn.Parameter(\n                torch.zeros(1, num_positions, embed_dim))\n        else:\n            self.pos_embed = None\n\n        # stochastic depth decay rule\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n\n        self.blocks = nn.ModuleList()\n        for i in range(depth):\n            block = Block(\n                dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                drop_path=dpr[i],\n                norm_layer=norm_layer,\n                act_layer=act_layer,\n                use_rel_pos=use_rel_pos,\n                rel_pos_zero_init=rel_pos_zero_init,\n                window_size=window_size if i in window_block_indexes else 0,\n                use_residual_block=i in residual_block_indexes,\n                input_size=(img_size \/\/ patch_size, img_size \/\/ patch_size),\n            )\n            self.blocks.append(block)\n\n        if self.pos_embed is not None:\n            trunc_normal_(self.pos_embed, std=0.02)\n\n        self.apply(self._init_weights)\n"}