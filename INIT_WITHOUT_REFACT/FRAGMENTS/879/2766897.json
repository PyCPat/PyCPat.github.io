{"BEFORE":"        logits = self.linear(z)\n        logits = logits + logits.transpose(-2, -3)\n        return logits\n","AFTER":"        float16_enabled = (torch.get_autocast_gpu_dtype() == torch.float16)\n        if float16_enabled and torch.is_autocast_enabled():\n            with torch.cuda.amp.autocast(enabled=False):\n                return self._forward(z.float())\n        else:\n            return self._forward(z)\n        \nclass TMScoreHead(nn.Module):\n"}