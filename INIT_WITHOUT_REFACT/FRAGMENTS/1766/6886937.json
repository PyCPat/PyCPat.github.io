{"BEFORE":"    def forward(self, input, device):\n        # input shape [batch_size, timestep, feature_dim]\n        batch_size = input.size(0)\n        time_step = input.size(1)\n        feature_dim = input.size(2)\n\n        cur_h = Variable(torch.zeros(batch_size, self.hidden_dim)).to(device)\n        inputse_att = []\n        convse_att = []\n        h = []\n\n        conv_input = input.permute(0, 2, 1)\n        conv_res1 = self.nn_conv1(conv_input)\n        conv_res3 = self.nn_conv3(conv_input)\n        conv_res5 = self.nn_conv5(conv_input)\n\n        conv_res = torch.cat((conv_res1, conv_res3, conv_res5), dim=1)\n        conv_res = self.relu(conv_res)\n\n        for cur_time in range(time_step):\n            convse_res, cur_convatt = self.nn_convse(\n                conv_res[:, :, : cur_time + 1], device=device\n            )\n            inputse_res, cur_inputatt = self.nn_inputse(\n                input[:, : cur_time + 1, :].permute(0, 2, 1), device=device\n            )\n            cur_input = torch.cat((convse_res[:, :, -1], inputse_res[:, :, -1]), dim=-1)\n\n            cur_h = self.rnn(cur_input, cur_h)\n            h.append(cur_h)\n            convse_att.append(cur_convatt)\n            inputse_att.append(cur_inputatt)\n\n        h = torch.stack(h).permute(1, 0, 2)\n        h_reshape = h.contiguous().view(batch_size * time_step, self.hidden_dim)\n        if self.dropout > 0.0:\n            h_reshape = self.nn_dropout(h_reshape)\n        output = self.nn_output(h_reshape)\n        output = self.sigmoid(output)\n        output = output.contiguous().view(batch_size, time_step, self.output_dim)\n        return output, inputse_att\n","AFTER":"        batch_size = input.size(0)\n        time_step = input.size(1)\n        feature_dim = input.size(2)\n\n        cur_h = Variable(torch.zeros(batch_size, self.hidden_dim))\n        inputse_att = []\n        convse_att = []\n        h = []\n\n        conv_input = input.permute(0, 2, 1)\n        conv_res1 = self.nn_conv1(conv_input)\n        conv_res3 = self.nn_conv3(conv_input)\n        conv_res5 = self.nn_conv5(conv_input)\n\n        conv_res = torch.cat((conv_res1, conv_res3, conv_res5), dim=1)\n        conv_res = self.relu(conv_res)\n\n        for cur_time in range(time_step):\n            convse_res, cur_convatt = self.nn_convse(conv_res[:, :, : cur_time + 1])\n            inputse_res, cur_inputatt = self.nn_inputse(\n                input[:, : cur_time + 1, :].permute(0, 2, 1)\n            )\n            cur_input = torch.cat((convse_res[:, :, -1], inputse_res[:, :, -1]), dim=-1)\n\n            cur_h = self.rnn(cur_input, cur_h)\n            h.append(cur_h)\n            convse_att.append(cur_convatt)\n            inputse_att.append(cur_inputatt)\n\n        h = torch.stack(h).permute(1, 0, 2)\n        output = h.contiguous().view(batch_size, time_step, self.hidden_dim)\n        if self.dropout > 0.0:\n            output = self.nn_dropout(output)\n        # output = self.nn_output(h_reshape)\n        # output = self.sigmoid(output)\n        # output = output.contiguous().view(batch_size, time_step, self.output_dim)\n        # output = h_reshape.contiguous().view(batch_size, time_step, self.hidden_dim)\n        # return output, inputse_att\n        return output\n"}