{"BEFORE":"        elif self.p_bias == 'deberta_v2':  # deberta_v2\r\n            self.pos_att_type = kwargs.get('pos_att_type')\r\n            self.relative_positions = RelativePositionsEncodingDebertaV2(qlen=kwargs.get('max_position'), \r\n                                                                         klen=kwargs.get('max_position'), \r\n                                                                         position_buckets=kwargs.get('position_buckets'),\r\n                                                                         max_position=kwargs.get('max_position'))\r\n            self.relative_positions_encoding = nn.Embedding(kwargs.get('max_position'), self.hidden_size)\r\n            self.norm_rel_ebd = [x.strip() for x in kwargs.get(\"norm_rel_ebd\", \"none\").lower().split(\"|\")]\r\n            if \"layer_norm\" in self.norm_rel_ebd:\r\n                self.LayerNorm = LayerNorm(self.hidden_size, kwargs.get('layer_norm_eps', 1e-12))\r\n\r\n    def transpose_for_scores(self, x):\r\n","AFTER":"    def __init__(self, hidden_size, num_attention_heads, attention_probs_dropout_prob, dropout_rate=0.1, attention_scale=True,\r\n                 return_attention_scores=False, bias=True, **kwargs):\r\n        super(MultiHeadAttentionLayer, self).__init__()\r\n        self.hidden_size = hidden_size\r\n        self.num_attention_heads = num_attention_heads\r\n        # assert hidden_size % num_attention_heads == 0  # 旧逻辑，t5_pegasus_small中不可以整除\r\n        # 兼容t5_pegasus_small\r\n        if kwargs.get('attention_head_size'):\r\n            self.attention_head_size = kwargs.get('attention_head_size')\r\n        else:\r\n            self.attention_head_size = int(hidden_size \/ num_attention_heads)\r\n        self.inner_dim = self.num_attention_heads * self.attention_head_size  # 新逻辑\r\n        self.attention_scale = attention_scale\r\n        self.return_attention_scores = return_attention_scores\r\n\r\n        self.bias = bias\r\n        self.q = nn.Linear(hidden_size, self.inner_dim, bias=bias)\r\n        self.k = nn.Linear(hidden_size, self.inner_dim, bias=bias)\r\n        self.v = nn.Linear(hidden_size, self.inner_dim, bias=bias)\r\n        self.o = nn.Linear(self.inner_dim, hidden_size, bias=bias)\r\n        self.dropout = nn.Dropout(attention_probs_dropout_prob)\r\n\r\n        self.a_bias, self.p_bias = kwargs.get('a_bias'), kwargs.get('p_bias')\r\n\r\n        if self.p_bias == 'typical_relative':  # nezha\r\n            self.relative_positions_encoding = RelativePositionsEncoding(qlen=kwargs.get('max_position'),\r\n                                                                         klen=kwargs.get('max_position'),\r\n                                                                         embedding_size=self.attention_head_size,\r\n                                                                         max_relative_position=kwargs.get('max_relative_position'))\r\n        elif self.p_bias == 'rotary':  # roformer\r\n            self.relative_positions_encoding = RoPEPositionEncoding(max_position=kwargs.get('max_position'), embedding_size=self.attention_head_size)\r\n        elif self.p_bias == 't5_relative':  # t5\r\n            self.relative_positions = RelativePositionsEncodingT5(qlen=kwargs.get('max_position'), \r\n                                                                  klen=kwargs.get('max_position'), \r\n                                                                  relative_attention_num_buckets=kwargs.get('relative_attention_num_buckets'), \r\n                                                                  is_decoder=kwargs.get('is_decoder'))\r\n            self.relative_positions_encoding = nn.Embedding(kwargs.get('relative_attention_num_buckets'), self.num_attention_heads)\r\n\r\n        elif self.p_bias == 'deberta_v2':  # deberta_v2\r\n            # 配置文件\r\n            self.share_att_key = kwargs.get(\"share_att_key\", False)\r\n            self.position_buckets = kwargs.get(\"position_buckets\", -1)\r\n            self.max_relative_positions = kwargs.get(\"max_relative_positions\", -1)\r\n            if self.max_relative_positions < 1:\r\n                self.max_relative_positions = kwargs.get('max_position_embeddings')\r\n            self.pos_ebd_size = self.max_relative_positions\r\n            if self.position_buckets > 0:\r\n                self.pos_ebd_size = self.position_buckets\r\n\r\n            # position_embedding\r\n            self.pos_att_type = kwargs.get('pos_att_type', [])\r\n            self.relative_positions = RelativePositionsEncodingDebertaV2(qlen=kwargs.get('max_position'), \r\n                                                                         klen=kwargs.get('max_position'), \r\n                                                                         position_buckets=kwargs.get('position_buckets'),\r\n                                                                         max_position=kwargs.get('max_position'))\r\n            self.relative_positions_encoding = nn.Embedding(kwargs.get('max_position'), self.hidden_size)\r\n            self.norm_rel_ebd = [x.strip() for x in kwargs.get(\"norm_rel_ebd\", \"none\").lower().split(\"|\")]\r\n            if \"layer_norm\" in self.norm_rel_ebd:\r\n                self.layernorm = nn.LayerNorm(self.hidden_size, kwargs.get('layer_norm_eps', 1e-12), elementwise_affine=True)\r\n\r\n            self.pos_dropout = StableDropout(dropout_rate)\r\n"}