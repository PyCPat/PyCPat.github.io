{"BEFORE":"        mask = get_mask_subset_with_prob(~no_mask, self.mask_prob)\n\n        # get mask indices\n        mask_indices = torch.nonzero(mask, as_tuple=True)\n\n        # mask input with mask tokens with probability of `replace_prob` (keep tokens the same with probability 1 - replace_prob)\n        masked_input = input.clone().detach()\n\n        # if random token probability > 0 for mlm\n        if self.random_token_prob > 0:\n            assert self.num_tokens is not None, 'Number of tokens (num_tokens) must be passed to Electra for randomizing tokens during masked language modeling'\n\n            random_token_prob = prob_mask_like(input, self.random_token_prob)\n            random_tokens = torch.randint(0, self.num_tokens, input.shape, device=input.device)\n            random_no_mask = mask_with_tokens(random_tokens, self.mask_ignore_token_ids)\n            random_token_prob &= ~random_no_mask\n            random_indices = torch.nonzero(random_token_prob, as_tuple=True)\n            masked_input[random_indices] = random_tokens[random_indices]\n\n        # [mask] input\n        masked_input = masked_input.masked_fill(mask * replace_prob, self.mask_token_id)\n\n        # set inverse of mask to padding tokens for labels\n        gen_labels = input.masked_fill(~mask, self.pad_token_id)\n","AFTER":"        mask = get_mask_subset_with_prob(~no_mask, self.mask_prob)\n\n        # get mask indices\n        mask_indices = torch.nonzero(mask, as_tuple=True)\n\n        # mask input with mask tokens with probability of `replace_prob` (keep tokens the same with probability 1 - replace_prob)\n        masked_input = input.clone().detach()\n\n        # set inverse of mask to padding tokens for labels\n        gen_labels = input.masked_fill(~mask, self.pad_token_id)\n\n        # clone the mask, for potential modification if random tokens are involved\n        # not to be mistakened for the mask above, which is for all tokens, whether not replaced nor replaced with random tokens\n        masking_mask = mask.clone()\n\n        # if random token probability > 0 for mlm\n        if self.random_token_prob > 0:\n            assert self.num_tokens is not None, 'Number of tokens (num_tokens) must be passed to Electra for randomizing tokens during masked language modeling'\n\n            random_token_prob = prob_mask_like(input, self.random_token_prob)\n            random_tokens = torch.randint(0, self.num_tokens, input.shape, device=input.device)\n            random_no_mask = mask_with_tokens(random_tokens, self.mask_ignore_token_ids)\n            random_token_prob &= ~random_no_mask\n            masked_input = torch.where(random_token_prob, random_tokens, masked_input)\n\n            # remove random token prob mask from masking mask\n            masking_mask = masking_mask & ~random_token_prob\n\n        # [mask] input\n        masked_input = masked_input.masked_fill(masking_mask * replace_prob, self.mask_token_id)\n"}