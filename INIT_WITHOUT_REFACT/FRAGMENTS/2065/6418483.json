{"BEFORE":"            attn = SelfAttention(dim, depth, max_seq_len, heads, local_heads, window_size, causal = causal, attn_dropout = attn_dropout, dropout = attn_layer_dropout)\n            ff = FeedForward(dim, dropout = ff_dropout, glu = ff_glu)\n\n            attn = PreNorm(dim, attn)\n            ff = PreNorm(dim, ff)\n\n            layers.append(nn.ModuleList([attn, ff]))\n\n        execute_type = ReversibleSequence if reversible else SequentialSequence\n        self.layers = execute_type(layers)\n        self.pad_to_window_size = window_size\n\n        register_kmeans_update_on_backwards(self)        \n","AFTER":"    def __init__(self, dim, depth, max_seq_len, heads = 8, window_size = 64, causal = False, attn_dropout = 0., ff_dropout = 0., attn_layer_dropout = 0., layer_dropout = 0., n_local_attn_heads = 0, ff_glu = False, reversible = False, ff_chunks = 1):\n        super().__init__()\n        if type(n_local_attn_heads) is not tuple:\n            n_local_attn_heads = tuple([n_local_attn_heads] * depth)\n\n        assert len(n_local_attn_heads) == depth, 'local attention heads tuple must have the same length as the depth'\n        assert all([local_heads <= heads for local_heads in n_local_attn_heads]), 'number of local attn heads must be less than the maximum number of heads'\n\n        layers = nn.ModuleList([])\n        fn_wrapper = partial(PreNorm, dim)\n\n        for ind, local_heads in zip(range(depth), n_local_attn_heads):\n            attn = SelfAttention(dim, depth, max_seq_len, heads, local_heads, window_size, causal = causal, attn_dropout = attn_dropout, dropout = attn_layer_dropout)\n            ff = Chunk(ff_chunks, FeedForward(dim, dropout = ff_dropout, glu = ff_glu))\n\n            attn, ff = map(fn_wrapper, (attn, ff))\n            layers.append(nn.ModuleList([attn, ff]))\n\n        execute_type = ReversibleSequence if reversible else SequentialSequence\n        self.layers = execute_type(layers, layer_dropout = layer_dropout)\n        self.pad_to_window_size = window_size\n\n        self.remove_handle_fn = register_kmeans_update_on_backwards(self)        \n"}