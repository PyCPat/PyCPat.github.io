<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        tar_candit_pro = self.prob_mlp(feat_in_repeat).squeeze(-1)          &#47&#47 [batch_size, self.N_tar, 1]
        tar_offset_mean = self.mean_mlp(feat_in_repeat)                     &#47&#47 [batch_size, self.N_tar, 2]
        print("tar_candit_pro size: ", tar_candit_pro.size())
        print("tar_offset_mean size: ", <a id="change">tar_offset_mean.size()</a>)

        &#47&#47 compute the prob. of normal distribution
        d_x_dist = Normal(tar_offset_mean[:, :, 0], torch.tensor([1.0]))    &#47&#47 [batch_size, self.N_tar]</code></pre><h3>After Change</h3><pre><code class='java'>
        _, N, _ = tar_candidate.size()

        &#47&#47 stack the target candidates to the end of input feature
        feat_in_repeat = torch.cat([feat_in.repeat(1, N, 1), <a id="change">tar_candidate.float()</a>], dim=2)
        &#47&#47 feat_in_repeat = torch.cat([feat_in.repeat(1, N, 1), tar_candidate.float()], dim=2)
        &#47&#47 print("feat_in_repeat size: ", feat_in_repeat.size())

        &#47&#47 compute probability for each candidate
        tar_candit_prob = self.prob_mlp(feat_in_repeat).squeeze(-1)          &#47&#47 [batch_size, self.N_tar, 1]
        tar_offset_mean = self.mean_mlp(feat_in_repeat)                     &#47&#47 [batch_size, self.N_tar, 2]
        &#47&#47 print("tar_candit_pro size: ", tar_candit_prob.size())
        &#47&#47 print("tar_offset_mean size: ", tar_offset_mean.size())

        &#47&#47 compute the prob. of normal distribution
        d_x_dist = Normal(tar_offset_mean[:, :, 0], torch.tensor([1.0], device=self.device))    &#47&#47 [batch_size, self.N_tar]
        d_y_dist = Normal(tar_offset_mean[:, :, 1], torch.tensor([1.0], device=self.device))    &#47&#47 [batch_size, self.N_tar]

        d_x = d_x_dist.sample()
        d_y = d_y_dist.sample()

        &#47&#47 p = tar_candit_pro * d_x_dist.log_prob(d_x) * d_y_dist.log_prob(d_y)
        _, indices = tar_candit_prob.topk(self.M, dim=1)

        <a id="change">return </a>tar_candit_prob<a id="change">, d_x, d_y, indices</a>

    &#47&#47 todo: offset_gt for every tar_candidate
    def loss(self,
             feat_in: torch.Tensor,</code></pre>