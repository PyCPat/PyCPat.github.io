{"BEFORE":"        x += self.drop_out_1(self.masked_multihead_attention(after_norm_1, after_norm_1, after_norm_1, mask=decoder_mask)) # (B, L, d_model)\r\n        after_norm_2 = self.layer_norm_2(x) # (B, L, d_model)\r\n        x += self.drop_out_2(self.multihead_attention(after_norm_2, encoder_output, encoder_output, mask=encoder_mask)) # (B, L, d_model)\r\n        after_norm_3 = self.layer_norm_3(x) # (B, L, d_model)\r\n        x += self.drop_out_3(self.feed_forward(after_norm_3)) # (B, L, d_model)\r\n","AFTER":"        x = x + self.drop_out_1(self.masked_multihead_attention(after_norm_1, after_norm_1, after_norm_1, mask=decoder_mask)) # (B, L, d_model)\r\n        after_norm_2 = self.layer_norm_2(x) # (B, L, d_model)\r\n        x = x + self.drop_out_2(self.multihead_attention(after_norm_2, encoder_output, encoder_output, mask=encoder_mask)) # (B, L, d_model)\r\n        after_norm_3 = self.layer_norm_3(x) # (B, L, d_model)\r\n        x = x + self.drop_out_3(self.feed_forward(after_norm_3)) # (B, L, d_model)\r\n"}