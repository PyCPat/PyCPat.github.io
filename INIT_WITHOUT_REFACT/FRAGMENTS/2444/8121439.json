{"BEFORE":"        x += self.drop_out_1(self.multihead_attention(after_norm_1, after_norm_1, after_norm_1, mask=encoder_mask)) # (B, L, d_model)\r\n        after_norm_2 = self.layer_norm_2(x) # (B, L, d_model)\r\n        x += self.drop_out_2(self.feed_forward(after_norm_2)) # (B, L, d_model)\r\n","AFTER":"        x = x + self.drop_out_1(self.multihead_attention(after_norm_1, after_norm_1, after_norm_1, mask=encoder_mask)) # (B, L, d_model)\r\n        after_norm_2 = self.layer_norm_2(x) # (B, L, d_model)\r\n        x = x + self.drop_out_2(self.feed_forward(after_norm_2)) # (B, L, d_model)\r\n"}