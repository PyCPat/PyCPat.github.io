{"BEFORE":"            audio = torch.cat([self.input_cache.to(audio.device), audio], dim=-1)\n            windows = util.slice_windows(audio, self.window_size, self.hop_size)\n            assert windows.shape[1] == (audio.shape[-1] - self.offset) \/\/ self.hop_size + 1\n\n            self.offset = self.hop_size - (audio.shape[-1] - self.offset) % self.hop_size\n            self.input_cache = audio[-(self.hop_size - self.offset):]\n\n            # f0\n            f0 = yin_frame(windows, self.sample_rate, self.pitch_min, self.pitch_max)\n            # loudness\n            comp_spec = torch.fft.rfft(windows, dim=-1)\n            loudness = spec_loudness(comp_spec, self.a_weighting)\n            print(f0.shape, loudness.shape)\n            # estimator\n            x = {'f0': f0[:,:,None], 'loud': loudness[:,:,None]} # batch=1, n_frames=windows.shape[1], 2\n            est_param = self.estimator(x)\n            params_dict = self.synth.fill_params(est_param, x)\n            render_length = (windows.shape[1]+1)*self.hop_size # accounting for previous frame\n            resyn_audio, outputs = self.synth(params_dict, render_length)\n            # output cache (delay)\n            resyn_audio = torch.cat([self.output_cache.to(audio.device), resyn_audio], dim=-1)\n            if resyn_audio.shape[-1] > orig_len:\n                resyn_audio = resyn_audio[:, :orig_len]\n                self.output_cache = resyn_audio[orig_len:]\n","AFTER":"            orig_len = audio.shape[-1]\n            # input cache\n            audio = torch.cat([self.input_cache.to(audio.device), audio], dim=-1)\n            windows = util.slice_windows(audio, self.window_size, self.hop_size, pad=False)\n\n            self.offset = self.hop_size - ((orig_len - self.offset) % self.hop_size)\n            self.input_cache = audio[:, -(self.window_size - self.offset):]\n\n            # f0\n            f0 = yin_frame(windows, self.sample_rate, self.pitch_min, self.pitch_max)\n            # loudness\n            comp_spec = torch.fft.rfft(windows, dim=-1)\n            loudness = spec_loudness(comp_spec, self.a_weighting)\n            # estimator\n            x = {'f0': f0[:,:,None], 'loud': loudness[:,:,None]} # batch=1, n_frames=windows.shape[1], 2\n            est_param = self.estimator(x)\n            params_dict = self.synth.fill_params(est_param, x)\n            render_length = windows.shape[1]*self.hop_size # last_of_prev_frame<->0th window<-...->last window\n            resyn_audio, outputs = self.synth(params_dict, render_length)\n            # output cache (delay)\n            resyn_audio = torch.cat([self.output_cache.to(audio.device), resyn_audio], dim=-1)\n            if resyn_audio.shape[-1] > orig_len:\n                self.output_cache = resyn_audio[:, orig_len:]\n"}