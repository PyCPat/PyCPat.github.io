{"BEFORE":"    def __init__(self, input_size, channels, dim_k, dim_ff=256, dim_d=None, dropout_ff=0.15, nhead=4, depth=1, dropout=0.1, activation=\"gelu\", use_pos_emb=True, checkpoint_level=\"C0\", parameter_sharing=\"layerwise\", k_reduce_by_layer=0, full_attention=False, include_ff=True, w_o_intermediate_dim=None):\n        super(Linformer, self).__init__()\n        assert activation == \"gelu\" or activation == \"relu\", \"Only gelu and relu activations supported for now\"\n        assert checkpoint_level == \"C0\" or checkpoint_level == \"C1\" or checkpoint_level == \"C2\", \"Checkpoint level has to be either C0, C1, or C2.\"\n        assert parameter_sharing == \"none\" or parameter_sharing == \"headwise\" or parameter_sharing == \"kv\" or parameter_sharing == \"layerwise\", \"The `parameter_sharing` flag has to be either 'none', 'headwise', 'kv', or 'layerwise'.\"\n        assert channels % nhead == 0 if dim_d is None else True, \"If `dim_d` is not set to a custom value, `channels` must be divisible by `nhead`!\"\n\n        self.layers = nn.ModuleList()\n        self.input_size = input_size\n        self.channels = channels\n        self.checkpoint_level = checkpoint_level\n        self.pos_emb = PositionalEmbedding(channels) if use_pos_emb else None\n        self.depth = depth\n        self.nhead = nhead\n\n        head_dim = channels \/\/ nhead if dim_d is None else dim_d\n\n        E_proj = get_EF(input_size, dim_k)\n\n        get_attn = lambda curr_dim_k: MHAttention(input_size, head_dim, channels, curr_dim_k, nhead, dropout, activation, checkpoint_level, parameter_sharing, E_proj, E_proj, full_attention, w_o_intermediate_dim)\n        get_ff = lambda: FeedForward(channels, dim_ff, dropout_ff)\n        get_norm = lambda: nn.LayerNorm(channels)\n\n        for index in range(depth):\n            self.layers.append(nn.ModuleList([get_attn(max(1, dim_k - index*k_reduce_by_layer)), get_norm()]))\n            if include_ff:\n                self.layers.append(nn.ModuleList([get_ff(), get_norm()]))\n\n    def forward(self, tensor, **kwargs):\n","AFTER":"        layers = nn.ModuleList()\n        self.input_size = input_size\n        self.channels = channels\n        self.checkpoint_level = checkpoint_level\n        self.depth = depth\n        self.nhead = nhead\n\n        head_dim = channels \/\/ nhead if dim_d is None else dim_d\n\n        E_proj = get_EF(input_size, dim_k)\n\n        get_attn = lambda curr_dim_k: MHAttention(input_size, head_dim, channels, curr_dim_k, nhead, dropout, activation, checkpoint_level, parameter_sharing, E_proj, E_proj, full_attention, w_o_intermediate_dim)\n        get_ff = lambda: FeedForward(channels, dim_ff, dropout_ff)\n        get_norm = lambda: nn.LayerNorm(channels)\n\n        for index in range(depth):\n            attn_layer = get_attn(max(1, dim_k - index*k_reduce_by_layer))\n            ff_layer = get_ff()\n\n            attn_layer, ff_layer = map(lambda fn: Residual(PreNorm(channels, fn)), (attn_layer, ff_layer))\n\n            if include_ff:\n                layers.extend([attn_layer, ff_layer])\n            else:\n                layers.extend([attn_layer])\n\n        self.seq = layers\n"}