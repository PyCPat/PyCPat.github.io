{"BEFORE":"        dim_batch = past.size()[0]\n        zero_padding = torch.zeros(1, dim_batch, self.dim_embedding_key * 2).cuda()\n        prediction = torch.Tensor().cuda()\n        present_temp = past[:, -1].unsqueeze(1)\n\n        # past temporal encoding\n        past = torch.transpose(past, 1, 2)\n        story_embed = self.relu(self.conv_past(past))\n        story_embed = torch.transpose(story_embed, 1, 2)\n        output_past, state_past = self.encoder_past(story_embed)\n\n        # scene encoding\n        scene = scene.permute(0, 3, 1, 2)\n        scene_1 = self.convScene_1(scene)\n        scene_2 = self.convScene_2(scene_1)\n\n        # Cosine similarity and memory read\n        past_normalized = F.normalize(self.memory_past, p=2, dim=1)\n        state_normalized = F.normalize(state_past.squeeze(), p=2, dim=1)\n        self.weight_read = torch.matmul(past_normalized, state_normalized.transpose(0,1)).transpose(0,1)\n        self.index_max = torch.sort(self.weight_read, descending=True)[1].cpu()[:,:self.num_prediction]\n\n        for i_track in range(self.num_prediction):\n            present = present_temp\n            prediction_single = torch.Tensor().cuda()\n            ind = self.index_max[:, i_track]\n            info_future = self.memory_fut[ind]\n            info_total = torch.cat((state_past, info_future.unsqueeze(0)), 2)\n            input_dec = info_total\n            state_dec = zero_padding\n            for i in range(self.future_len):\n                output_decoder, state_dec = self.decoder(input_dec, state_dec)\n                displacement_next = self.FC_output(output_decoder)\n                coords_next = present + displacement_next.squeeze(0).unsqueeze(1)\n                prediction_single = torch.cat((prediction_single, coords_next), 1)\n                present = coords_next\n                input_dec = zero_padding\n\n            # Iteratively refine predictions using context\n            for i_refine in range(1):\n                pred_map = prediction_single + 90\n                pred_map = pred_map.unsqueeze(2)\n                indices = pred_map.permute(0, 2, 1, 3)\n\n                # rescale between -1 and 1\n                indices = 2 * (indices \/ 180) - 1\n                output = F.grid_sample(scene_2, indices, mode='nearest')\n                output = output.squeeze(2).permute(0, 2, 1)\n\n                state_rnn = state_past\n                output_rnn, state_rnn = self.RNN_scene(output, state_rnn)\n                prediction_refine = self.fc_refine(state_rnn).view(dim_batch, 40, 2)\n                prediction_single = prediction_single + prediction_refine\n\n            prediction = torch.cat((prediction, prediction_single.unsqueeze(1)), 1)\n\n        return prediction\n","AFTER":"        dim_batch = past.size()[0]\n        zero_padding = torch.zeros(1, dim_batch*self.num_prediction, self.dim_embedding_key * 2).cuda()\n        prediction = torch.Tensor().cuda()\n        present_temp = past[:, -1].unsqueeze(1)\n\n        # past temporal encoding\n        past = torch.transpose(past, 1, 2)\n        story_embed = self.relu(self.conv_past(past))\n        story_embed = torch.transpose(story_embed, 1, 2)\n        output_past, state_past = self.encoder_past(story_embed)\n\n        # scene encoding\n        scene = scene.permute(0, 3, 1, 2)\n        scene_1 = self.convScene_1(scene)\n        scene_2 = self.convScene_2(scene_1)\n\n        # Cosine similarity and memory read\n        past_normalized = F.normalize(self.memory_past, p=2, dim=1)\n        state_normalized = F.normalize(state_past.squeeze(), p=2, dim=1)\n        self.weight_read = torch.matmul(past_normalized, state_normalized.transpose(0,1)).transpose(0,1)\n        self.index_max = torch.sort(self.weight_read, descending=True)[1].cpu()[:,:self.num_prediction]\n\n        present = present_temp.repeat_interleave(self.num_prediction, dim=0)\n        state_past = state_past.repeat_interleave(self.num_prediction, dim=1)\n        scene_2 = scene_2.repeat_interleave(self.num_prediction, dim=0)\n        ind = self.index_max.flatten()\n        info_future = self.memory_fut[ind]\n        info_total = torch.cat((state_past, info_future.unsqueeze(0)), 2)\n        input_dec = info_total\n        state_dec = zero_padding\n        for i in range(self.future_len):\n            output_decoder, state_dec = self.decoder(input_dec, state_dec)\n            displacement_next = self.FC_output(output_decoder)\n            coords_next = present + displacement_next.squeeze(0).unsqueeze(1)\n            prediction = torch.cat((prediction, coords_next), 1)\n            present = coords_next\n            input_dec = zero_padding\n\n        # Iteratively refine predictions using context\n        for i_refine in range(1):\n            pred_map = prediction + 90\n            pred_map = pred_map.unsqueeze(2)\n            indices = pred_map.permute(0, 2, 1, 3)\n            # rescale between -1 and 1\n            indices = 2 * (indices \/ 180) - 1\n            output = F.grid_sample(scene_2, indices, mode='nearest')\n            output = output.squeeze(2).permute(0, 2, 1)\n\n            state_rnn = state_past\n            output_rnn, state_rnn = self.RNN_scene(output, state_rnn)\n            prediction_refine = self.fc_refine(state_rnn).view(-1, 40, 2)\n            prediction = prediction + prediction_refine\n\n        return prediction\n"}