<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        if key_padding_mask is not None:
            &#47&#47 don&quott attend to padding symbols
            <a id="change">attn_weights</a> = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)
            if self.onnx_trace:
                attn_weights = torch.where(
                    key_padding_mask.unsqueeze(1).unsqueeze(2),
                    torch.Tensor([float("-Inf")]),
                    attn_weights.float()
                ).type_as(attn_weights)
            else:
                attn_weights = <a id="change">attn_weights.float().masked_fill(
                    key_padding_mask.unsqueeze(1).unsqueeze(2),
                    float(&quot-inf&quot),
                ).type_as(</a>attn_weights<a id="change">)</a>  &#47&#47 FP16 support: cast to float and back
            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)

        attn_weights = utils.softmax(</code></pre><h3>After Change</h3><pre><code class='java'>

        if key_padding_mask is not None:
            &#47&#47 don&quott attend to padding symbols
            <a id="change">attn_weights</a> = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)
            if self.onnx_trace:
                attn_weights = torch.where(
                    key_padding_mask.unsqueeze(1).unsqueeze(2),
                    torch.Tensor([float("-Inf")]),
                    attn_weights.float()
                ).type_as(attn_weights)
            else:
                attn_weights = <a id="change">attn_weights.masked_fill(
                    key_padding_mask.unsqueeze(1).unsqueeze(</a>2<a id="change">)</a>,
                    <a id="change">float(</a>&quot-inf&quot<a id="change">),
                )</a>
            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)

        attn_weights = utils.softmax(
            attn_weights, dim=-1, onnx_trace=self.onnx_trace,</code></pre>