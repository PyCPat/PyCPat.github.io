{"BEFORE":"        self.convolution_module = nn.Sequential(\n            # pointwise\n            nn.Conv1d(\n                input_size, 2 * input_size, kernel_size=1, stride=1, bias=bias\n            ),\n            nn.GLU(dim=1),\n            # depthwise\n            nn.Conv1d(\n                input_size,\n                input_size,\n                kernel_size=kernel_size,\n                stride=1,\n                padding=(kernel_size - 1) \/\/ 2,\n                groups=input_size,\n                bias=bias,\n            ),\n            nn.BatchNorm1d(input_size),\n            activation(),\n            # pointwise\n            nn.Conv1d(\n                input_size, input_size, kernel_size=1, stride=1, bias=bias\n            ),\n            nn.Dropout(dropout),\n        )\n","AFTER":"        self, input_size, kernel_size, bias=True, activation=Swish, dropout=0.1, causal=False, dilation=1\n    ):\n        super().__init__()\n        \n        self.causal = causal \n        \n        if self.causal:\n            self.padding = (kernel_size - 1) * 2 ** (dilation -1)\n        else:\n            self.padding = (kernel_size - 1) * 2 ** (dilation-1) \/\/ 2\n\n        \n        \n        self.layer_norm = nn.LayerNorm(input_size)\n        self.bottleneck = nn.Sequential(\n            # pointwise\n            nn.Conv1d(\n                input_size, 2 * input_size, kernel_size=1, stride=1, bias=bias\n            ),\n            nn.GLU(dim=1))\n            # depthwise\n        self.conv = nn.Conv1d(\n                input_size,\n                input_size,\n                kernel_size=kernel_size,\n                stride=1,\n                padding=self.padding,\n                dilation=dilation,\n                groups=input_size,\n                bias=bias,\n            )\n        \n        self.after_conv = nn.Sequential(\n            nn.BatchNorm1d(input_size),\n            activation(),\n            # pointwise\n            nn.Conv1d(\n                input_size, input_size, kernel_size=1, stride=1, bias=bias\n            ),\n            nn.Dropout(dropout)\n        )\n"}