{"BEFORE":"        normed_embeds = []\n        normed_ngram_embeds = []\n\n        embeds = rearrange(embeds, 'b n (h d) -> b n h d', h = num_heads)\n\n        for head_num, prime, input_ids_per_head, embed, ngram_emb, embed_norm, ngram_norm in zip(range(num_heads), self.primes, cluster_ids.unbind(dim = -1), embeds.unbind(dim = -2), self.embeddings, self.input_norm, self.ngram_norm):\n            ngram_ids = get_bigram_ids(input_ids_per_head, unigram_vocab_size, segment_pos)\n            ngram_ids_for_head = multi_way_hash_ids(ngram_ids, head_num + 1, head_num + 1, prime, vocab_size)\n\n            ngram_embed = ngram_emb(ngram_ids_for_head)\n            normed_ngram_embed = ngram_norm(ngram_embed)\n            normed_ngram_embeds.append(normed_ngram_embed)\n\n            normed_embed = embed_norm(embed)\n            normed_embeds.append(normed_embed)\n\n        normed_embeds = torch.stack(normed_embeds, dim = -2)\n        normed_ngram_embeds = torch.stack(normed_ngram_embeds, dim = -2)\n","AFTER":"        ngram_embeds = []\n\n        for head_num, prime, input_ids_per_head, embed, ngram_emb in zip(range(num_heads), self.primes, cluster_ids.unbind(dim = -1), embeds.unbind(dim = -2), self.embeddings):\n            ngram_ids = get_bigram_ids(input_ids_per_head, unigram_vocab_size, segment_pos)\n            ngram_ids_for_head = multi_way_hash_ids(ngram_ids, head_num + 1, head_num + 1, prime, vocab_size)\n\n            ngram_embed = ngram_emb(ngram_ids_for_head)\n            ngram_embeds.append(ngram_embed)\n\n        embeds = rearrange(embeds, 'b n (h d) -> b n h d', h = num_heads)\n        normed_embeds = self.embeds_layernorm(embeds)\n\n        ngram_embeds = torch.stack(ngram_embeds, dim = -2)\n        normed_ngram_embeds = self.ngram_layernorm(ngram_embeds)\n"}