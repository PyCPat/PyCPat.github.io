{"BEFORE":"        output_text, _ = self.RNNs[0](X_text)\n        output_audio, _ = self.RNNs[1](X_audio)\n        output_visual, _ = self.RNNs[2](X_visual)\n\n        batch_size = output_text.size(0)\n\n        # (batch, num_directions * hidden_size)\n        output_text = output_text[:, -1, :]\n        output_audio = output_audio[:, -1, :]\n        output_visual = output_visual[:, -1, :]\n\n        # (num_classes, 300)\n        text_emo_vecs = self.textEmoEmbs(torch.LongTensor(list(range(self.num_classes))))\n        visual_emo_vecs = self.affineVisual(text_emo_vecs)\n        audio_emo_vecs = self.affineAudio(text_emo_vecs)\n\n        text_emo_vecs = text_emo_vecs.unsqueeze(0).repeat(batch_size, 1, 1)\n        visual_emo_vecs = visual_emo_vecs.unsqueeze(0).repeat(batch_size, 1, 1)\n        audio_emo_vecs = audio_emo_vecs.unsqueeze(0).repeat(batch_size, 1, 1)\n\n        text_attn_feature = self.attention(output_text, text_emo_vecs)\n        visual_attn_feature = self.attention(output_visual, visual_emo_vecs)\n        audio_attn_feature = self.attention(output_audio, audio_emo_vecs)\n\n        # TODO: try residual connection\n\n        logits = self.out(torch.cat((text_attn_feature, visual_attn_feature, audio_attn_feature), dim=1))\n","AFTER":"        batch_size = X_text.size(0)\n        logits = None\n        if 't' in self.modalities:\n            output_text, _ = self.RNNs[0](X_text)\n            output_text = output_text[:, -1, :]\n            text_emo_vecs_origin = self.textEmoEmbs(torch.LongTensor(list(range(self.num_classes))).to(self.device))\n            text_emo_vecs = text_emo_vecs_origin.unsqueeze(0).repeat(batch_size, 1, 1)\n            text_attn_weights = self.attention(output_text, text_emo_vecs)\n            logits = text_attn_weights if logits is None else logits + text_attn_weights\n\n        if 'a' in self.modalities:\n            output_audio, _ = self.RNNs[1](X_audio)\n            output_audio = output_audio[:, -1, :]\n            audio_emo_vecs = self.affineAudio(text_emo_vecs_origin)\n            audio_emo_vecs = audio_emo_vecs.unsqueeze(0).repeat(batch_size, 1, 1)\n            audio_attn_weights = self.attention(output_audio, audio_emo_vecs)\n            logits = audio_attn_weights if logits is None else logits + audio_attn_weights\n\n        if 'v' in self.modalities:\n            output_visual, _ = self.RNNs[2](X_visual)\n            output_visual = output_visual[:, -1, :]\n            visual_emo_vecs = self.affineVisual(text_emo_vecs_origin)\n            visual_emo_vecs = visual_emo_vecs.unsqueeze(0).repeat(batch_size, 1, 1)\n            visual_attn_weights = self.attention(output_visual, visual_emo_vecs)\n            logits = visual_attn_weights if logits is None else logits + visual_attn_weights\n\n        return logits\n"}