{"BEFORE":"        total_len = h.shape[1]\n        \n        # compute projections of input and memory embeddings\n        q = self.w_q(x).view(batch_size, -1, seg_len, embed_dim)\n        kv = self.w_kv(h).view(2*batch_size, -1, total_len, embed_dim)\n        k, v = kv.chunk(2, dim=0)\n        k = k.transpose(2, 3) # only using transposed k below\n        \n        # relative distance between two tokens is max total_len\n        pos = self.pos[-total_len:]\n        r = self.w_r(pos).view(-1, total_len, embed_dim)\n        \n        # compute relative positional encodings\n        b = q @ r.transpose(1, 2)\n        b = self.circulant_shift(b, -seg_len+1)\n        \n        # u1 and u2 should be identical for all query vectors\n        u1 = self.u1.repeat(1, seg_len, 1)\n        u2 = self.u2.repeat(1, seg_len, 1)\n        \n        # this is the XL specific way of computing the attention score\n        #att_mask = att_mask.unsqueeze(1).unsqueeze(-1).repeat(1,10,1,total_len)\n        att_score = q @ k + b + u1 @ k + u2 @ r.transpose(1, 2)\n","AFTER":"        total_len = h.shape[1]\n        \n        # compute projections of input and memory embeddings\n        q = self.w_q(x).view(batch_size, -1, seg_len, embed_dim)\n        kv = self.w_kv(h).view(2*batch_size, -1, total_len, embed_dim)\n        r_emb = self.w_r(self.R[-total_len:]).view(1, -1, total_len, embed_dim)\n        k, v = kv.chunk(2, dim=0)\n        \n        # the \"XL specific\" way of computing the pre-softmax attention score\n        AC = torch.einsum(\"bhid,bhjd->bhij\", q + self.u1, k)\n        BD = torch.einsum(\"bhid,bhjd->bhij\", q + self.u2, r_emb)\n        BD = self.circulant_shift(BD, -seg_len+1)\n        \n        # computing the attention scores\n        att_score = AC + BD\n"}