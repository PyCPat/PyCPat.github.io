{"BEFORE":"        assert dim % heads == 0\n        dim_head = int(dim \/ heads)\n        self.scale = dim_head ** -0.5\n        self.num_heads = heads\n\n        qk_dim = v_dim = dim_head * heads\n\n        if use_previous_attention:\n","AFTER":"                 previous_attention_bool=False,\n                 pre_norm_bool=True,\n                 post_norm_bool=False,\n                 ):\n        \"\"\"\n        Standard attention function, with a few features. Lazy attention (set previous_attention_bool=True) allows us\n        to skip calculating a new attention map, and re-use the last attention map: https:\/\/arxiv.org\/abs\/2102.12702 .\n        When not using lazy attention, we can use residual attention (https:\/\/arxiv.org\/abs\/2012.11747) by giving this\n        module previous_attn_dots, which are the dots from the last attention layer.\n\n        :param dim: Input and output dimension size (typically it is d_model)\n        :param attn_dim: Dimension size of attention (typically it is equal to dim)\n        :param num_heads: Number of attention heads\n        :param previous_attention_bool: Whether or not to re-use the last attention map\n        :param pre_norm_bool: Apply layer normalization before attention\n        :param post_norm_bool: Apply layer normalization after attention\n        \"\"\"\n        super().__init__()\n\n        # Config\n        dim_head = int(attn_dim \/ num_heads)\n        assert attn_dim % num_heads == 0, \"The attention dimension size (attn_dim) must divide evenly into num_heads\"\n\n        self.scale = dim_head ** -0.5\n        self.num_heads = num_heads\n        self.previous_attention_bool = previous_attention_bool\n        self.pre_norm_bool = pre_norm_bool\n        self.post_norm_bool = post_norm_bool\n\n        # Functions\n        if self.previous_attention_bool:\n            # If we use the attention pattern from the last attention layer, we don't need queries and keys\n            self.to_v = nn.Linear(dim, attn_dim, bias=False)\n\n        else:\n            # Standard attention layer that will calculate the attention pattern from queries and keys\n            self.to_q = nn.Linear(dim, attn_dim, bias=False)\n            self.to_k = nn.Linear(dim, attn_dim, bias=False)\n            self.to_v = nn.Linear(dim, attn_dim, bias=False)\n\n        if self.pre_norm_bool:\n            self.pre_norm = nn.LayerNorm(dim)\n\n        if self.post_norm_bool:\n            self.post_norm = nn.LayerNorm(dim)\n\n        self.attn_fn = F.softmax\n"}