{"BEFORE":"        output = None\n\n        inputs = self.embedding(inputs)\n        inputs = self.positional_encoding(inputs)\n\n        for layer in self.layers:\n            output, attn = layer(inputs, inputs_mask)\n            self_attns.append(attn)\n            inputs = output\n","AFTER":"        output = self.input_dropout(self.embedding(inputs) * self.logit_scale + self.pos_encoding(inputs))\n\n        non_pad_mask = get_pad_mask(inputs, input_lengths=input_lengths).eq(False)\n        length = inputs.size(1)\n        self_attn_mask = get_pad_mask(inputs, input_lengths).squeeze(-1).unsqueeze(1).expand(-1, length, -1)\n\n        for layer in self.layers:\n            output, attn = layer(output, non_pad_mask, self_attn_mask)\n"}