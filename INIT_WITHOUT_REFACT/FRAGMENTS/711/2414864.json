{"BEFORE":"        self.shapes = {\n            'P0': [160, 160, 256],\n            'P1': [80, 80, 128],\n            'P2': [40, 40, 64],\n            'P3': [20, 20, 32],\n            'P4': [10, 10, 16],\n            'P5': [5, 5, 8]\n        }\n\n        self.d_model = d_model\n        self.nhead = nhead\n\n        attn_masks = self.generate_attn_masks()\n\n        decoder_layer = FocusedDecoderLayer(\n            d_model, dim_feedforward, dropout, activation, nhead, attn_masks, config['obj_self_attn']\n","AFTER":"        anchors=None\n    ):\n        super().__init__()\n        self.bbox_props = bbox_props\n        self.config = config\n\n        self.d_model = d_model\n        self.nhead = nhead\n        reg_head = MLP(d_model, d_model, 6, 3)\n\n        decoder_layer = FocusedDecoderLayer(\n            d_model, dim_feedforward, dropout, activation, nhead, config['obj_self_attn'], \n            config, bbox_props, anchors\n        )\n        self.decoder = FocusedDecoderModel(decoder_layer, reg_head, num_decoder_layers, return_intermediate_dec)\n"}