{"BEFORE":"    def __init__(self, n_in, n_out=1, layers=(), dropout_rate=0.0, context=1, n_in_base=4, mix_base=0, join='cat'):\n        super(CNNPairedLayer, self).__init__()\n        self.context = context\n        self.mix_base = mix_base\n        self.join = join\n        if len(layers)>0 and layers[0]==0:\n            layers = ()\n\n        if join=='cat':\n            n = n_in*2 # concat\n        else:\n            n = n_in # add or mul\n        n += n_in_base*mix_base*2\n        \n        l = []\n        for m in layers:\n            l += [ \n                nn.Conv2d(n, m, context, padding=context\/\/2), \n                nn.GroupNorm(1, m),\n                nn.CELU(), \n                nn.Dropout(p=dropout_rate) ]\n            n = m\n        l += [ nn.Conv2d(n, n_out, context, padding=context\/\/2), nn.GroupNorm(1, n_out) ]\n","AFTER":"    def __init__(self, n_in, n_out=1, layers=(), ksize=(), dropout_rate=0.0):\n        super(CNNPairedLayer, self).__init__()\n        if len(layers)>0 and layers[0]==0:\n            layers = ()\n\n        l = []\n        for m, k in zip(layers, ksize):\n            l += [ \n                nn.Conv2d(n_in, m, k, padding=k\/\/2), \n                nn.GroupNorm(1, m),\n                nn.CELU(), \n                nn.Dropout(p=dropout_rate) ]\n            n_in = m\n        self.net = nn.Sequential(*l)\n        self.linear = nn.Linear(n_in, n_out)\n"}