{"BEFORE":"        image_embed = perceptor.encode_image(into)\n\n        latents, soft_one_hot_classes = self.model.latents()\n        num_latents = latents.shape[0]\n        latent_thres = self.model.latents.model.thresh_lat\n\n        lat_loss =  torch.abs(1 - torch.std(latents, dim=1)).mean() + \\\n                    torch.abs(torch.mean(latents, dim = 1)).mean() + \\\n                    4 * torch.max(torch.square(latents).mean(), latent_thres)\n\n\n        for array in latents:\n            mean = torch.mean(array)\n            diffs = array - mean\n            var = torch.mean(torch.pow(diffs, 2.0))\n            std = torch.pow(var, 0.5)\n            zscores = diffs \/ std\n            skews = torch.mean(torch.pow(zscores, 3.0))\n            kurtoses = torch.mean(torch.pow(zscores, 4.0)) - 3.0\n\n            lat_loss = lat_loss + torch.abs(kurtoses) \/ num_latents + torch.abs(skews) \/ num_latents\n\n        cls_loss = ((50 * torch.topk(soft_one_hot_classes, largest = False, dim = 1, k = 999)[0]) ** 2).mean()\n\n        sim_loss = -self.loss_coef * torch.cosine_similarity(text_embed, image_embed, dim = -1).mean()\n","AFTER":"    def forward(self, text_embeds, text_min_embeds=[], return_loss = True):\n        width, num_cutouts = self.image_size, self.num_cutouts\n\n        out = self.model()\n\n        if not return_loss:\n            return out\n\n        pieces = []\n        for ch in range(num_cutouts):\n            size = int(width * torch.zeros(1,).normal_(mean=.8, std=.3).clip(.5, .95))\n            offsetx = torch.randint(0, width - size, ())\n            offsety = torch.randint(0, width - size, ())\n            apper = out[:, :, offsetx:offsetx + size, offsety:offsety + size]\n            if (self.experimental_resample):\n                apper = resample(apper, (224, 224))\n            else:\n                apper = F.interpolate(apper, (224, 224), **self.interpolation_settings)\n            pieces.append(apper)\n\n        into = torch.cat(pieces)\n        into = normalize_image(into)\n\n        image_embed = perceptor.encode_image(into)\n\n        latents, soft_one_hot_classes = self.model.latents()\n        num_latents = latents.shape[0]\n        latent_thres = self.model.latents.model.thresh_lat\n\n        lat_loss =  torch.abs(1 - torch.std(latents, dim=1)).mean() + \\\n                    torch.abs(torch.mean(latents, dim = 1)).mean() + \\\n                    4 * torch.max(torch.square(latents).mean(), latent_thres)\n\n\n        for array in latents:\n            mean = torch.mean(array)\n            diffs = array - mean\n            var = torch.mean(torch.pow(diffs, 2.0))\n            std = torch.pow(var, 0.5)\n            zscores = diffs \/ std\n            skews = torch.mean(torch.pow(zscores, 3.0))\n            kurtoses = torch.mean(torch.pow(zscores, 4.0)) - 3.0\n\n            lat_loss = lat_loss + torch.abs(kurtoses) \/ num_latents + torch.abs(skews) \/ num_latents\n\n        cls_loss = ((50 * torch.topk(soft_one_hot_classes, largest = False, dim = 1, k = 999)[0]) ** 2).mean()\n\n        results = []\n        for txt_embed in text_embeds:\n            results.append(self.sim_txt_to_img(txt_embed, image_embed))\n        for txt_min_embed in text_min_embeds:\n            results.append(self.sim_txt_to_img(txt_min_embed, image_embed, \"min\"))\n        sim_loss = sum(results).mean()\n"}