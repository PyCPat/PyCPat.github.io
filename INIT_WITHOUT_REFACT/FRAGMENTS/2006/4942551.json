{"BEFORE":"        layers = []\n        if \"fc_layer\" in self.custom_config[\"model_arch_args\"]:\n            self.obs_size = self.full_obs_space['obs'].shape[0]\n            input_dim = self.obs_size\n            for out_dim in self.encoder_layer_dim:\n                layers.append(\n                    SlimFC(in_size=input_dim,\n                           out_size=out_dim,\n                           initializer=normc_initializer(1.0),\n                           activation_fn=self.activation))\n                input_dim = out_dim\n        elif \"conv_layer\" in self.custom_config[\"model_arch_args\"]:\n            self.obs_size = self.full_obs_space['obs'].shape\n            input_dim = self.obs_size[2]\n            for i in range(self.custom_config[\"model_arch_args\"][\"conv_layer\"]):\n                conv_f = nn.Conv2d(\n                    in_channels=input_dim,\n                    out_channels=self.custom_config[\"model_arch_args\"][\"out_channel_layer_{}\".format(i)],\n                    kernel_size=self.custom_config[\"model_arch_args\"][\"kernel_size_layer_{}\".format(i)],\n                    stride=self.custom_config[\"model_arch_args\"][\"stride_layer_{}\".format(i)],\n                    padding=self.custom_config[\"model_arch_args\"][\"padding_layer_{}\".format(i)],\n                )\n                relu_f = nn.ReLU()\n                pool_f = nn.MaxPool2d(kernel_size=self.custom_config[\"model_arch_args\"][\"pool_size_layer_{}\".format(i)])\n\n                layers.append(conv_f)\n                layers.append(relu_f)\n","AFTER":"        layers = []\n        if \"fc_layer\" in self.custom_config[\"model_arch_args\"]:\n            self.obs_size = self.full_obs_space['obs'].shape[0]\n            input_dim = self.obs_size\n            for out_dim in self.encoder_layer_dim:\n                layers.append(\n                    SlimFC(in_size=input_dim,\n                           out_size=out_dim,\n                           initializer=normc_initializer(1.0),\n                           activation_fn=self.activation))\n                input_dim = out_dim\n        elif \"conv_layer\" in self.custom_config[\"model_arch_args\"]:\n            self.obs_size = self.full_obs_space['obs'].shape\n            input_dim = self.obs_size[2]\n            for i in range(self.custom_config[\"model_arch_args\"][\"conv_layer\"]):\n                layers.append(\n                    SlimConv2d(\n                        in_channels=input_dim,\n                        out_channels=self.custom_config[\"model_arch_args\"][\"out_channel_layer_{}\".format(i)],\n                        kernel=self.custom_config[\"model_arch_args\"][\"kernel_size_layer_{}\".format(i)],\n                        stride=self.custom_config[\"model_arch_args\"][\"stride_layer_{}\".format(i)],\n                        padding=self.custom_config[\"model_arch_args\"][\"padding_layer_{}\".format(i)],\n                        activation_fn=self.activation\n                    )\n                )\n                pool_f = nn.MaxPool2d(kernel_size=self.custom_config[\"model_arch_args\"][\"pool_size_layer_{}\".format(i)])\n                layers.append(pool_f)\n\n                input_dim = self.custom_config[\"model_arch_args\"][\"out_channel_layer_{}\".format(i)]\n\n        else:\n            raise ValueError(\"fc_layer\/conv layer not in model arch args\")\n"}