{"BEFORE":"        context = einsum('bhdn,bhen->bhde', k, v)\n\n        content_out = einsum('bhde,bhdn->bhen', context, q)\n","AFTER":"        context = einsum('bhdn,bhen->bhde', k, v)\n\n        content_q = q if not self.norm_queries else q.softmax(dim=-2)\n\n        content_out = einsum('bhde,bhdn->bhen', context, content_q)\n        content_out = content_out.reshape(b, -1, x, y)\n        content_out = self.to_out(content_out)\n\n        # todo: compute relative position attentions and sum to content_out\n\n        if exists(self.rel_pos_length):\n            row_attn_map = einsum('bhdn,ld->bhnl', q, self.rel_rows)\n            column_attn_map = einsum('bhdn,ld->bhnl', q, self.rel_columns)\n\n        return content_out\n"}