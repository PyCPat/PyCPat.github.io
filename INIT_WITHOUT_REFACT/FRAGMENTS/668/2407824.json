{"BEFORE":"    def __init__(self,Nl,Nc,Nh,M, H,Cov,Mean,device, RC=2,Stat_comp=False,alpha = 1e-1):\n        super(compNet_1D_size_stat, self).__init__()\n        \n        self.Nl = Nl;\n        self.Nc = Nc;\n        self.Nh = Nh;\n        self.M = M;\n        self.device = device\n        self.RC = RC\n        self.Cov = Cov\n        self.Mean = Mean\n        self.Stat_comp = Stat_comp\n        #H =  H[0,0,:,:M]\n \n        self.H = H[:,:M]\n        self.H2 = H\n\n        #print(self.H.device)\n        #self.even_index = range(0,2*M*n,2);\n        #self.uneven_index = range(1,2*M*n,2);\n        \n#        #-- Hadamard patterns (full basis)\n#        if type(H)==type(None):\n#            H = Hadamard_Transform_Matrix(self.n)\n#        H = n*H; #fht hadamard transform needs to be normalized\n#        Pmat = np.zeros((M*n,n*n))\n#        P_ind = []\n#        for i in range(n):\n#            for j in range(M):\n#                P_ind.append(i*n+j)\n#        for i in range(M*n):\n#            Pmat[i] = H[P_ind[i]]\n#            \n#        Pinv2 = np.zeros((n*n,M*n))\n#        P_ind = []\n#        for i in range(n):\n#            for j in range(M):\n#                P_ind.append(i*n+j)\n#        for i in range(M*n):\n#            Pinv2[:,i] = Pinv[:,P_ind[i]]\n#        Pinv = Pinv2\n#        #-- Hadamard patterns (undersampled basis)\n#        #Var = Cov2Var(Cov)\n#        #Perm = Permutation_Matrix(Var)\n#        #Pmat = np.dot(Perm,H);\n#        #Pmat = H[:M,:];#Pmat[:M,:];\n#        Pconv = matrix2conv(Pmat);\n        \n\n        #-- Denoising parameters \n        #Sigma = np.dot(Perm,np.dot(Cov,np.transpose(Perm)));\n        #diag_index = np.diag_indices(n**2);\n        #Sigma = Sigma[diag_index];\n        #Sigma = n**2\/4*Sigma[:M]; #(H = nH donc Cov = n**2 Cov)!\n        ##Sigma = Sigma[:M];\n        #Sigma = torch.Tensor(Sigma);\n        #self.sigma = Sigma.view(1,1,M);\n        \n\n        #P1 = np.zeros((n**2,1));\n        #P1[0] = n**2;\n        #mean = n*np.reshape(Mean,(self.n**2,1))+P1;\n        #mu = (1\/2)*np.dot(Perm, mean);\n        ##mu = np.dot(Perm, np.reshape(Mean, (n**2,1)))\n        #mu1 = torch.Tensor(mu[:M]);\n        #self.mu_1 = mu1.view(1,1,M);\n\n        #-- Measurement preprocessing\n#        self.Patt = Pconv;\n#        P, T = split(Pconv, 1);\n#        self.P = P;\n#        self.T = T;\n#        self.P.bias.requires_grad = False;\n#        self.P.weight.requires_grad = False;\n#        self.Patt.bias.requires_grad = False;\n#        self.Patt.weight.requires_grad = False;\n#        self.T.weight.requires_grad=False;\n#        self.T.weight.requires_grad=False;\n\n        #-- Pseudo-inverse to determine levels of noise.\n        #if np.shape(Pinv)[0]==0:\n        #    Pinv = torch.from_numpy(Pinv)\n        #else:\n        if Stat_comp:\n            Pinv = torch.pinverse(self.H2, rcond=alpha)\n        else:\n            Pinv = torch.pinverse(self.H, rcond=alpha)\n        Pinv = Pinv.float()\n        self.Pinv = Pinv#(1\/n**2)*np.transpose(Pmat);\n        \n        if Stat_comp :\n            Pt = torch.transpose(self.H2,0,1)\n        else :\n            Pt = torch.transpose(self.H,0,1)\n        Pt = Pt.float()\n        self.Pt = Pt\/self.Nh\n        self.fc1 = Pt\/self.Nh\n                \n        x_flat = np.ones((1,1,Nl,Nc))\n        x_flat = torch.Tensor(x_flat)\n        x_flat = x_flat.float()\n        x_flat = x_flat.to(self.H.device)\n        (b,c,h,w) = x_flat.size()\n        if Stat_comp:\n            m_flat = torch.matmul(x_flat,self.H2)\n        else :\n            m_flat = torch.matmul(x_flat,self.H)\n        x_flat = torch.matmul(m_flat,self.Pt)\n        x_flat = x_flat.view(b*c,1,h,w)\n        self.flat = x_flat\n","AFTER":"            print(\"Statistic completion\")\n"}