{"BEFORE":"      x = encoder(x, input_mask)\n\n    encoder_logit = x.clone()\n\n    for decoder in self.decoders:\n      # target, encoder_output, encoder_mask)\n      target = decoder(target, x, input_mask)\n\n    lm_logits = self.lm_head(x)\n\n    loss = None\n    if labels is not None:\n      # Shift so that tokens < n predict n\n      shift_logits = lm_logits[..., :-1, :].contiguous()\n      shift_labels = labels[..., 1:].contiguous()\n\n      # Flatten the tokens\n      loss_fct = CrossEntropyLoss()\n      loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n\n    return lm_logits, loss, encoder_logit, x\n","AFTER":"      target = decoder(target, x, input_mask)\n\n    lm_logits = self.lm_head(target)\n\n    loss = None\n    if labels is not None:\n      # Shift so that tokens < n predict n\n      shift_logits = lm_logits[..., :-1, :].contiguous()\n      shift_labels = labels[..., 1:].contiguous()\n\n      # Flatten the tokens\n      loss_fct = CrossEntropyLoss()\n      loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n\n    return lm_logits, loss\n"}