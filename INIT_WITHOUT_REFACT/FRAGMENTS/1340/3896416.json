{"BEFORE":"        sim = einsum('b i d, b j d -> b i j', q, k)\n\n        if self.causal:\n            sim = sim \/ rearrange(torch.arange(seq_len, device = device) + 1, '... -> ... 1')\n        else:\n            sim = sim \/ seq_len\n\n        if exists(rel_pos_bias):\n","AFTER":"        mask = None\n    ):\n        seq_len, device = x.shape[-2], x.device\n\n        normed_x = self.norm(x)\n        v, gate = self.to_hidden(normed_x).chunk(2, dim = -1)\n\n        qk = self.to_qk(normed_x)\n        q, k = self.offsetscale(qk)\n\n        sim = einsum('b i d, b j d -> b i j', q, k) \/ seq_len\n\n        if exists(rel_pos_bias):\n            sim = sim + rel_pos_bias\n\n        attn = F.relu(sim) ** 2\n        attn = self.dropout(attn)\n\n        if exists(mask):\n            mask = rearrange(mask, 'b j -> b 1 j')\n            attn = attn.masked_fill(~mask, 0.)\n\n        if self.causal:\n"}