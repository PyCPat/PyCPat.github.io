{"BEFORE":"        text_embeddings = self.text_embeddings(text_tokens) + text_pos\r\n\r\n        if img_tokens is not None:\r\n            img_pos = self.get_image_pos_embeddings(img_tokens, past_length=0)\r\n            image_embeddings = self.image_embeddings(img_tokens) + img_pos\r\n            embeddings = torch.cat((text_embeddings, image_embeddings), dim=1)\r\n        else:\r\n            embeddings = text_embeddings\r\n            \r\n        attention_mask = torch.tril(\r\n            torch.ones((embeddings.shape[0], 1, self.total_seq_length, self.total_seq_length), device=self.device)\r\n        )\r\n        attention_mask = attention_mask[:, :, :embeddings.shape[1], :embeddings.shape[1]]\r\n\r\n        transformer_output, present_cache = self.transformer(\r\n            embeddings, attention_mask,\r\n            cache=None, use_cache=False\r\n        )\r\n\r\n        logits = self.to_logits(transformer_output)\r\n        return logits[:, -self.image_seq_length:, :]\r\n","AFTER":"        text_pos = self.text_pos_embeddings(torch.arange(text_tokens.shape[1], device=self.device))\r\n        text_embeddings = self.text_token_embedding(text_tokens) + text_pos\r\n\r\n        seq_pos = self.seq_pos_embeddings(torch.arange(seg_tokens.shape[1], device=self.device))\r\n        seq_embeddings = self.seg_token_embedding(seg_tokens) + text_pos\r\n\r\n        embeddings = torch.cat((text_embeddings, seq_embeddings), dim=1)\r\n        if img_tokens is not None:\r\n            img_pos = self.get_image_pos_embeddings(img_tokens)\r\n            image_embeddings = self.image_embeddings(img_tokens) + img_pos\r\n            embeddings = torch.cat((embeddings, image_embeddings), dim=1)\r\n            \r\n        attention_mask = torch.tril(\r\n            torch.ones((embeddings.shape[0], 1, self.total_length, self.total_length), device=self.device)\r\n        )\r\n        attention_mask = attention_mask[:, :, :embeddings.shape[1], :embeddings.shape[1]]\r\n\r\n        transformer_output, present_cache = self.transformer(\r\n            embeddings, attention_mask,\r\n            cache=None, use_cache=False\r\n        )\r\n\r\n        logits = self.to_logits(transformer_output)\r\n        return logits[:, -self.image_length:, :]\r\n"}