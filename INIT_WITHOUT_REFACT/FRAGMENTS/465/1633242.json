{"BEFORE":"        out = self.non_lin_3(out)\n        # out: (BB, embedding_dim , NN)\n\n        # Pooling\n        # out: (BB, embedding_dim, NN)\n        if self._pooling_func_name == 'max':\n            out = torch.max(out, -1, keepdim=True)[0]\n        elif self._pooling_func_name == 'mean':\n            out = torch.mean(out, keepdim=True, dim=-1)\n        elif self._pooling_func_name == 'sum':\n            out = torch.sum(out, keepdim=True, dim=-1)\n        else:\n            raise ValueError(f\"Pooling function {self._pooling_func_name} is not yet supported!\")\n\n        # out: (BB, embedding_dim, 1)\n        out = torch.flatten(out, start_dim=-2)\n\n        # out: (BB, embedding_dim)\n        out = self.fc1(out)\n        if self._use_batch_norm and batch_size > 1:\n            out = self.bn4(out)\n        out = self.non_lin_4(out)\n\n        # out: (BB, embedding_dim\/\/2)\n        out = self.fc2(out)\n        if self._use_batch_norm and batch_size > 1:\n            out = self.bn5(out)\n        out = self.non_lin_5(out)\n\n        # out: (BB, embedding_dim\/\/4)\n        out = self.fc3(out)\n        # out: (BB, num_features ** 2)\n\n        identity = torch.Tensor(torch.flatten(torch.eye(self._num_features),\n                                              start_dim=-2)).to(torch.float32).to(out.device)\n","AFTER":"    def forward(self, input_tensor, masking_tensor: Optional[torch.Tensor]):\n        \"\"\"Forward pass through the transformer module\n\n        :param input_tensor: Input to the network (BB, KK, NN)\n        :param masking_tensor: Optional masking tensor for the pooling operation (BB, NN).\n        :return: A transformation matrix of the form (BB, KK, KK)\n        \"\"\"\n\n        batch_size = input_tensor.shape[0]\n\n        # input_tensor: (BB, KK, NN)\n        out = self.conv1(input_tensor)\n        if self._use_batch_norm and batch_size > 1:\n            out = self.bn1(out)\n        out = self.non_lin_1(out)\n\n        # out: (BB, embedding_dim \/\/ 16, NN)\n        out = self.conv2(out)\n        if self._use_batch_norm and batch_size > 1:\n            out = self.bn2(out)\n        out = self.non_lin_2(out)\n\n        # out: (BB, embedding_dim \/\/ 8, NN)\n        out = self.conv3(out)\n        if self._use_batch_norm and batch_size > 1:\n            out = self.bn3(out)\n        out = self.non_lin_3(out)\n        # out: (BB, embedding_dim , NN)\n\n        # Pooling\n        # out: (BB, embedding_dim, NN)\n        masking_input = {'in_tensor': out.transpose(2, 1)}\n        if self.use_masking:\n            masking_input['mask_tensor'] = masking_tensor\n        out = self.pooling_block(masking_input)['masking_out']\n        # output_tensor: (BB, embedding_dim)\n\n        # out: (BB, embedding_dim)\n        out = self.fc1(out)\n        if self._use_batch_norm and batch_size > 1:\n            out = self.bn4(out)\n        out = self.non_lin_4(out)\n\n        # out: (BB, embedding_dim\/\/2)\n        out = self.fc2(out)\n        if self._use_batch_norm and batch_size > 1:\n            out = self.bn5(out)\n        out = self.non_lin_5(out)\n\n        # out: (BB, embedding_dim\/\/4)\n        out = self.fc3(out)\n        # out: (BB, num_features ** 2)\n\n        identity = torch.flatten(torch.eye(self._num_features),\n                                 start_dim=-2).to(torch.float32).to(out.device)\n"}