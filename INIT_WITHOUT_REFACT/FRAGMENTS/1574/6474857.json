{"BEFORE":"    def forward(self, g, h, e, EigVecs, EigVals):\n\n        # input embedding\n        h = self.embedding_h(h)\n        e = self.embedding_e_real(e)\n\n        PosEnc = torch.cat((EigVecs.unsqueeze(2), EigVals), dim=2).float()  # (Num nodes) x (Num Eigenvectors) x 2\n        empty_mask = torch.isnan(PosEnc)  # (Num nodes) x (Num Eigenvectors) x 2\n\n        PosEnc[empty_mask] = 0  # (Num nodes) x (Num Eigenvectors) x 2\n        PosEnc = torch.transpose(PosEnc, 0, 1).float()  # (Num Eigenvectors) x (Num nodes) x 2\n        PosEnc = self.linear_A(PosEnc)  # (Num Eigenvectors) x (Num nodes) x PE_dim\n\n        # 1st Transformer: Learned PE\n        PosEnc = self.PE_Transformer(src=PosEnc, src_key_padding_mask=empty_mask[:, :, 0])\n\n        # remove masked sequences\n        PosEnc[torch.transpose(empty_mask, 0, 1)[:, :, 0]] = float('nan')\n\n        # Sum pooling\n        PosEnc = torch.nansum(PosEnc, 0, keepdim=False)\n\n        # Concatenate learned PE to input embedding\n        h = torch.cat((h, PosEnc), 1)\n\n        h = self.in_feat_dropout(h)\n\n        # Second Transformer\n        for conv in self.layers:\n            h, e = conv(g, h, e)\n        g.ndata['h'] = h\n\n        if self.readout == \"sum\":\n            hg = dgl.sum_nodes(g, 'h')\n        elif self.readout == \"max\":\n            hg = dgl.max_nodes(g, 'h')\n        elif self.readout == \"mean\":\n            hg = dgl.mean_nodes(g, 'h')\n        else:\n            hg = dgl.mean_nodes(g, 'h')  # default readout is mean nodes\n\n        sig = nn.Sigmoid()\n\n        return sig(self.MLP_layer(hg))\n","AFTER":"    def forward(self, g):\n\n        # input embedding\n        h = self.embedding_h(g.ndata['f'])\n        e = self.embedding_e_real(g.edata['w'])\n\n        PosEnc = g.ndata['pos_enc']  # (Num nodes) x (Num Eigenvectors) x 2\n        empty_mask = torch.isnan(PosEnc)  # (Num nodes) x (Num Eigenvectors) x 2\n\n        PosEnc[empty_mask] = 0  # (Num nodes) x (Num Eigenvectors) x 2\n        PosEnc = torch.transpose(PosEnc, 0, 1).float()  # (Num Eigenvectors) x (Num nodes) x 2\n        PosEnc = self.linear_A(PosEnc)  # (Num Eigenvectors) x (Num nodes) x PE_dim\n\n        # 1st Transformer: Learned PE\n        PosEnc = self.PE_Transformer(src=PosEnc, src_key_padding_mask=empty_mask[:, :, 0])\n\n        # remove masked sequences\n        PosEnc[torch.transpose(empty_mask, 0, 1)[:, :, 0]] = float('nan')\n\n        # Sum pooling\n        PosEnc = torch.nansum(PosEnc, 0, keepdim=False)\n\n        # Concatenate learned PE to input embedding\n        h = torch.cat((h, PosEnc), 1)\n\n        h = self.in_feat_dropout(h)\n\n        # Second Transformer\n        for conv in self.layers:\n            h, e = conv(g, h, e)\n        g.ndata['h'] = h\n\n        readouts_to_cat = [dgl.readout_nodes(g, 'f', op=aggr) for aggr in self.readout_aggregators]\n        readout = torch.cat(readouts_to_cat, dim=-1)\n        return self.output(readout)\n"}