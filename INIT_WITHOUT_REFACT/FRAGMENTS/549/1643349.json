{"BEFORE":"    def __init__(self, model:nn.Module, feat_size:int, num_classes:int=10)->None:\n        super(AddonNN, self).__init__()\n\n        # keep everything but the last layer \n        self.featurizer = nn.Sequential(*list(model.classifier.children())[:-1])\n        \n        # freeze the featurizer\n        for param in self.featurizer.parameters():\n            param.requires_grad_ = False\n\n        # create small network that will take features as input\n        # TODO: got to figure out what is input size\n        self.addon_nn = nn.Sequential(\n            nn.Linear(feat_size, 4096), \n            nn.ReLU(), \n            nn.Linear(4096, 4096), \n            nn.ReLU(), \n            nn.Linear(4096, num_classes)) \n","AFTER":"    def __init__(self, model:Network, num_classes:int=10, stack_num=1)->None:\n        super(AddonNN, self).__init__()\n\n        assert stack_num in set((1,2,3))\n\n        self.FEAT_SIZE = {1: 32768, 2: 16384, 3: 32768}\n        \n        self.stack_num = stack_num\n        self.featurizer = model\n\n        # freeze the featurizer\n        for param in self.featurizer.parameters():\n            param.requires_grad = False\n\n        # create small network that will take features as input\n        self.addon_nn = nn.Linear(self.FEAT_SIZE[stack_num], num_classes)\n"}