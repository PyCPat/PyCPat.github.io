{"BEFORE":"        self,\n        *,\n        dim,\n        depth,\n        seq_len,\n        heads,\n        dim_head,\n        attn_dropout,\n        ff_dropout,\n        global_column_attn = False\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PairwiseAttentionBlock(dim = dim, seq_len = seq_len, heads = heads, dim_head = dim_head, dropout = attn_dropout, global_column_attn = global_column_attn),\n                FeedForward(dim = dim, dropout = ff_dropout),\n                MsaAttentionBlock(dim = dim, seq_len = seq_len, heads = heads, dim_head = dim_head, dropout = attn_dropout),\n                FeedForward(dim = dim, dropout = ff_dropout),\n            ]))\n\n    def forward(\n","AFTER":"        self,\n        *,\n        depth,\n        **kwargs\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([EvoformerBlock(**kwargs) for _ in range(depth)])\n"}