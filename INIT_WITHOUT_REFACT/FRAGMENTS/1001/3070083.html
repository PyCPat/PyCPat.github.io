<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                   for i, t in enumerate([query, key, value]))

        &#47&#47 scale dot product attention
        product<a id="change"> = </a>paddle.matmul(x=q, y=k, transpose_y=True)
        scaling = float(self.head_dim)**-0.5
        product = product * scaling

        if attn_mask is not None:
            &#47&#47 Support bool or int mask
            attn_mask = _convert_attention_mask(attn_mask, product.dtype)
            product = product + attn_mask
        weights = F.softmax(product)
        if self.dropout:
            weights = F.dropout(
                weights,
                self.dropout,
                training=self.training,
                mode="upscale_in_train")
        out<a id="change"> = </a><a id="change">paddle.matmul(</a>weights, v<a id="change">)</a>

        &#47&#47 combine heads
        out = paddle.transpose(out, perm=[0, 2, 1, 3])
        out = paddle.reshape(x=out, shape=[0, 0, out.shape[2] * out.shape[3]])</code></pre><h3>After Change</h3><pre><code class='java'>
                self.dropout,
                training=self.training,
                mode="upscale_in_train")
        out<a id="change"> = </a>torch.matmul(weights, v)    &#47&#47 [N, num_heads, HW, head_dim]

        &#47&#47 combine heads
        out = out.permute([0, 2, 1, 3])    &#47&#47 [N, HW, num_heads, head_dim]
        N, HW, _, _ = out.shape
        out = torch.reshape(out, <a id="change">[</a>N, HW, out.shape[2]<a id="change"> * </a>out.shape[3]<a id="change"></a>])    &#47&#47 [N, HW, embed_dim]

        &#47&#47 project to output
        out = self.out_proj(out)    &#47&#47 [N, HW, embed_dim]</code></pre>