{"BEFORE":"        product = paddle.matmul(x=q, y=k, transpose_y=True)\n        scaling = float(self.head_dim)**-0.5\n        product = product * scaling\n\n        if attn_mask is not None:\n            # Support bool or int mask\n            attn_mask = _convert_attention_mask(attn_mask, product.dtype)\n            product = product + attn_mask\n        weights = F.softmax(product)\n        if self.dropout:\n            weights = F.dropout(\n                weights,\n                self.dropout,\n                training=self.training,\n                mode=\"upscale_in_train\")\n        out = paddle.matmul(weights, v)\n\n        # combine heads\n        out = paddle.transpose(out, perm=[0, 2, 1, 3])\n        out = paddle.reshape(x=out, shape=[0, 0, out.shape[2] * out.shape[3]])\n","AFTER":"        product = q.matmul(k.permute([0, 1, 3, 2]))    # [N, num_heads, HW, HW]\n        scaling = float(self.head_dim)**-0.5\n        product = product * scaling\n\n        if attn_mask is not None:\n            # Support bool or int mask\n            attn_mask = _convert_attention_mask(attn_mask, product.dtype)\n            product = product + attn_mask\n        # paddle的softmax dim默认是-1，所以这里显式写上-1\n        weights = F.softmax(product, dim=-1)\n        if self.dropout:\n            weights = F.dropout(\n                weights,\n                self.dropout,\n                training=self.training,\n                mode=\"upscale_in_train\")\n        out = torch.matmul(weights, v)    # [N, num_heads, HW, head_dim]\n\n        # combine heads\n        out = out.permute([0, 2, 1, 3])    # [N, HW, num_heads, head_dim]\n        N, HW, _, _ = out.shape\n        out = torch.reshape(out, [N, HW, out.shape[2] * out.shape[3]])    # [N, HW, embed_dim]\n"}