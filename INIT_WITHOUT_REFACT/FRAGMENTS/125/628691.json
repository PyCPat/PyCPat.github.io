{"BEFORE":"        self.num_features = in_chs\n        self.features = nn.Sequential(blocks)\n\n        # Using 1x1 conv for the FC layer to allow the extra pooling scheme\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        num_features = self.num_features * self.global_pool.feat_mult()\n        self.classifier = nn.Conv2d(num_features, num_classes, kernel_size=1, bias=True)\n","AFTER":"        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        self.b = b\n        assert output_stride == 32  # FIXME look into dilation support\n        bw_factor = 1 if small else 4\n        blocks = OrderedDict()\n\n        # conv1\n        blocks['conv1_1'] = ConvBnAct(\n            in_chans, num_init_features, kernel_size=3 if small else 7, stride=2, norm_kwargs=dict(eps=.001))\n        blocks['conv1_pool'] = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.feature_info = [dict(num_chs=num_init_features, reduction=2, module='features.conv1_1')]\n\n        # conv2\n        bw = 64 * bw_factor\n        inc = inc_sec[0]\n        r = (k_r * bw) \/\/ (64 * bw_factor)\n        blocks['conv2_1'] = DualPathBlock(num_init_features, r, r, bw, inc, groups, 'proj', b)\n        in_chs = bw + 3 * inc\n        for i in range(2, k_sec[0] + 1):\n            blocks['conv2_' + str(i)] = DualPathBlock(in_chs, r, r, bw, inc, groups, 'normal', b)\n            in_chs += inc\n        self.feature_info += [dict(num_chs=in_chs, reduction=4, module=f'features.conv2_{k_sec[0]}')]\n\n        # conv3\n        bw = 128 * bw_factor\n        inc = inc_sec[1]\n        r = (k_r * bw) \/\/ (64 * bw_factor)\n        blocks['conv3_1'] = DualPathBlock(in_chs, r, r, bw, inc, groups, 'down', b)\n        in_chs = bw + 3 * inc\n        for i in range(2, k_sec[1] + 1):\n            blocks['conv3_' + str(i)] = DualPathBlock(in_chs, r, r, bw, inc, groups, 'normal', b)\n            in_chs += inc\n        self.feature_info += [dict(num_chs=in_chs, reduction=8, module=f'features.conv3_{k_sec[1]}')]\n\n        # conv4\n        bw = 256 * bw_factor\n        inc = inc_sec[2]\n        r = (k_r * bw) \/\/ (64 * bw_factor)\n        blocks['conv4_1'] = DualPathBlock(in_chs, r, r, bw, inc, groups, 'down', b)\n        in_chs = bw + 3 * inc\n        for i in range(2, k_sec[2] + 1):\n            blocks['conv4_' + str(i)] = DualPathBlock(in_chs, r, r, bw, inc, groups, 'normal', b)\n            in_chs += inc\n        self.feature_info += [dict(num_chs=in_chs, reduction=16, module=f'features.conv4_{k_sec[2]}')]\n\n        # conv5\n        bw = 512 * bw_factor\n        inc = inc_sec[3]\n        r = (k_r * bw) \/\/ (64 * bw_factor)\n        blocks['conv5_1'] = DualPathBlock(in_chs, r, r, bw, inc, groups, 'down', b)\n        in_chs = bw + 3 * inc\n        for i in range(2, k_sec[3] + 1):\n            blocks['conv5_' + str(i)] = DualPathBlock(in_chs, r, r, bw, inc, groups, 'normal', b)\n            in_chs += inc\n        self.feature_info += [dict(num_chs=in_chs, reduction=32, module=f'features.conv5_{k_sec[3]}')]\n\n        def _fc_norm(f, eps): return BatchNormAct2d(f, eps=eps, act_layer=fc_act, inplace=False)\n        blocks['conv5_bn_ac'] = CatBnAct(in_chs, norm_layer=_fc_norm)\n\n        self.num_features = in_chs\n        self.features = nn.Sequential(blocks)\n\n        # Using 1x1 conv for the FC layer to allow the extra pooling scheme\n        self.global_pool, self.classifier = create_classifier(\n            self.num_features, self.num_classes, pool_type=global_pool, use_conv=True)\n"}