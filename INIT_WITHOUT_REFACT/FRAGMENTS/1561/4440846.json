{"BEFORE":"        for enc_layer in self.layers:\n            output, attention = enc_layer(\n                output,\n                src_mask=src_mask,\n                src_key_padding_mask=src_key_padding_mask,\n                pos_embs=pos_embs,\n            )\n            attention_lst.append(attention)\n        output = self.norm(output)\n\n        return output, attention_lst\n","AFTER":"        output_hidden_states: bool = False,\n    ):\n        \"\"\"\n        Arguments\n        ----------\n        src : tensor\n            The sequence to the encoder layer (required).\n        src_mask : tensor\n            The mask for the src sequence (optional).\n        src_key_padding_mask : tensor\n            The mask for the src keys per batch (optional).\n        \"\"\"\n        output = src\n        if self.layerdrop_prob > 0.:\n            keep_probs = self.rng.random(len(self.layers))\n        else:\n            keep_probs = None\n        attention_lst = []\n        if output_hidden_states:\n            hidden_states = {}\n        for i, enc_layer in enumerate(self.layers):\n            if not self.training or self.layerdrop_prob == 0. or keep_probs[i] > self.layerdrop_prob:\n                output, attention = enc_layer(\n                    output,\n                    src_mask=src_mask,\n                    src_key_padding_mask=src_key_padding_mask,\n                    pos_embs=pos_embs,\n                )\n                if output_hidden_states:\n                    hidden_states[i] = enc_layer\n                attention_lst.append(attention)\n        output = self.norm(output)\n\n        if not output_hidden_states:\n            return output, attention_lst\n        else:\n            return output, attention_lst, hidden_states\n\n\nclass TransformerDecoderLayer(nn.Module):\n"}