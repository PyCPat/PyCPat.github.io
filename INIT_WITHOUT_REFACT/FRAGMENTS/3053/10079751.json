{"BEFORE":"    def forward(self, images, restored, latents, logits, quantizeds, cv):\n        l2Loss = F.mse_loss(restored, images, reduction='none').mean(axis=(1, 2, 3))\n        l1Loss = F.l1_loss(restored, images, reduction='none').mean(axis=(1, 2, 3))\n        ssimLoss = 1 - self._msssim((restored + 1), (images + 1))\n\n        regs = list()\n        if logits is not None:\n            for logit in logits:\n                # N, H, W, K -> N, HW, K\n                unNormlogit = logit.reshape(len(logit), -1, logit.shape[-1])\n\n                # [n, k]\n                summedLogit = unNormlogit.mean(1)\n\n                posterior = OneHotCategorical(logits=summedLogit, validate_args=False)\n                prior = OneHotCategorical(probs=torch.ones_like(summedLogit) \/ summedLogit.shape[-1], validate_args=False)\n                reg = cv * torch.distributions.kl_divergence(posterior, prior)\n                # reg = compute_penalties(unNormlogit, allowed_entropy=0.1, individual_entropy_coeff=cv, allowed_js=4.0, js_coeff=cv, cv_coeff=cv, eps=Consts.Eps)\n                regs.append(reg)\n","AFTER":"        regs = list()\n        if logits is not None:\n            for logit in logits:\n                # N, H, W, K -> N, HW, K\n                batchWiseLogit = logit.reshape(len(logit), -1, logit.shape[-1])\n\n                posterior = OneHotCategorical(logits=batchWiseLogit)\n                prior = OneHotCategorical(probs=torch.ones_like(batchWiseLogit) \/ batchWiseLogit.shape[-1])\n                regs.append(torch.distributions.kl_divergence(posterior, prior).sum(-1) + compute_penalties(batchWiseLogit, allowed_entropy=0.1, individual_entropy_coeff=1.0, allowed_js=4.0, js_coeff=1.0, cv_coeff=1.0, eps=Consts.Eps))\n"}