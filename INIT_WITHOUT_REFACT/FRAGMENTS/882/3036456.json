{"BEFORE":"        dropout_dim: int = 1,\n        inplanes: int = 128,\n        downsample_kernel_size: int = 3,\n        input_3x3: bool = True\n    ) -> None:\n\n        super().__init__()\n\n        relu_type: Type[nn.ReLU] = Act[Act.RELU]\n        conv_type: Type[Union[nn.Conv1d, nn.Conv2d, nn.Conv3d]] = Conv[Conv.CONV, spatial_dims]\n        pool_type: Type[Union[nn.MaxPool1d, nn.MaxPool2d, nn.MaxPool3d]] = Pool[Pool.MAX, spatial_dims]\n        norm_type: Type[Union[nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d]] = Norm[Norm.BATCH, spatial_dims]\n        dropout_type: Type[Union[nn.Dropout, nn.Dropout2d, nn.Dropout3d]] = Dropout[Dropout.DROPOUT, dropout_dim]\n\n        self.inplanes = inplanes\n        self.spatial_dims = spatial_dims\n\n        layer0_modules: List[Tuple[str, Any]]\n\n        if input_3x3:\n            layer0_modules = [\n                (\n                    \"conv1\",\n                    conv_type(in_channels=in_channels, out_channels=64, kernel_size=3, stride=2, padding=1, bias=False),\n                ),\n                (\"bn1\", norm_type(num_features=64)),\n                (\"relu1\", relu_type(inplace=True)),\n                (\"conv2\", conv_type(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)),\n                (\"bn2\", norm_type(num_features=64)),\n                (\"relu2\", relu_type(inplace=True)),\n                (\n                    \"conv3\",\n                    conv_type(in_channels=64, out_channels=inplanes, kernel_size=3, stride=1, padding=1, bias=False),\n                ),\n                (\"bn3\", norm_type(num_features=inplanes)),\n                (\"relu3\", relu_type(inplace=True)),\n            ]\n        else:\n            layer0_modules = [\n                (\n                    \"conv1\",\n                    conv_type(\n                        in_channels=in_channels, out_channels=inplanes, kernel_size=7, stride=2, padding=3, bias=False\n                    ),\n                ),\n                (\"bn1\", norm_type(num_features=inplanes)),\n                (\"relu1\", relu_type(inplace=True)),\n            ]\n\n        layer0_modules.append((\"pool\", pool_type(kernel_size=3, stride=2, ceil_mode=True)))\n        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n        self.layer1 = self._make_layer(\n            block, planes=64, blocks=layers[0], groups=groups, reduction=reduction, downsample_kernel_size=1\n        )\n        self.layer2 = self._make_layer(\n            block,\n            planes=128,\n            blocks=layers[1],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n        )\n        self.layer3 = self._make_layer(\n            block,\n            planes=256,\n            blocks=layers[2],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n        )\n        self.layer4 = self._make_layer(\n            block,\n            planes=512,\n            blocks=layers[3],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n        )\n        # self.adaptive_avg_pool = avg_pool_type(1)\n        self.dropout = dropout_type(dropout_prob) if dropout_prob is not None else None\n        # self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n        self.num_features = 512 * block.expansion\n","AFTER":"        num_layers: int = -1\n    ) -> None:\n\n        super().__init__()\n\n        relu_type: Type[nn.ReLU] = Act[Act.RELU]\n        conv_type: Type[Union[nn.Conv1d, nn.Conv2d, nn.Conv3d]] = Conv[Conv.CONV, spatial_dims]\n        pool_type: Type[Union[nn.MaxPool1d, nn.MaxPool2d, nn.MaxPool3d]] = Pool[Pool.MAX, spatial_dims]\n        norm_type: Type[Union[nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d]] = Norm[Norm.BATCH, spatial_dims]\n\n        self.inplanes = inplanes\n        self.spatial_dims = spatial_dims\n\n        layer0_modules: List[Tuple[str, Any]]\n\n        if input_3x3:\n            layer0_modules = [\n                (\n                    \"conv1\",\n                    conv_type(in_channels=in_channels, out_channels=64, kernel_size=3, stride=2, padding=1, bias=False),\n                ),\n                (\"bn1\", norm_type(num_features=64)),\n                (\"relu1\", relu_type(inplace=True)),\n                (\"conv2\", conv_type(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)),\n                (\"bn2\", norm_type(num_features=64)),\n                (\"relu2\", relu_type(inplace=True)),\n                (\n                    \"conv3\",\n                    conv_type(in_channels=64, out_channels=inplanes, kernel_size=3, stride=1, padding=1, bias=False),\n                ),\n                (\"bn3\", norm_type(num_features=inplanes)),\n                (\"relu3\", relu_type(inplace=True)),\n            ]\n        else:\n            layer0_modules = [\n                (\n                    \"conv1\",\n                    conv_type(\n                        in_channels=in_channels, out_channels=inplanes, kernel_size=7, stride=2, padding=3, bias=False\n                    ),\n                ),\n                (\"bn1\", norm_type(num_features=inplanes)),\n                (\"relu1\", relu_type(inplace=True)),\n            ]\n\n        layer0_modules.append((\"pool\", pool_type(kernel_size=3, stride=2, ceil_mode=True)))\n\n        layer0 = nn.Sequential(OrderedDict(layer0_modules))\n        layer1 = self._make_layer(\n            block,\n            planes=64,\n            blocks=layers[0],\n            stride=strides[0],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=1\n        )\n        layer2 = self._make_layer(\n            block,\n            planes=128,\n            blocks=layers[1],\n            stride=strides[1],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n        )\n        layer3 = self._make_layer(\n            block,\n            planes=256,\n            blocks=layers[2],\n            stride=strides[2],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n        )\n        layer4 = self._make_layer(\n            block,\n            planes=512,\n            blocks=layers[3],\n            stride=strides[3],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n        )\n        all_layers = [layer0, layer1, layer2, layer3, layer4]\n        self.layers = nn.ModuleList(all_layers[:num_layers + 1])\n"}