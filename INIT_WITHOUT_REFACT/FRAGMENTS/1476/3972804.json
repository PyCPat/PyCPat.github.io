{"BEFORE":"        self.feature_info = []  # NOTE: there will be no stride == 2 feature if stem_stride == 4\n        dpr = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(cfg.depths)).split(cfg.depths)]\n        prev_chs = stem_chs\n        net_stride = stem_stride\n        dilation = 1\n        expected_var = 1.0\n        stages = []\n        for stage_idx, stage_depth in enumerate(cfg.depths):\n            stride = 1 if stage_idx == 0 and stem_stride > 2 else 2\n            self.feature_info += [dict(\n                num_chs=prev_chs, reduction=net_stride, module=f'stages.{stage_idx}.0.act1' if stride == 2 else '')]\n            if net_stride >= output_stride and stride > 1:\n                dilation *= stride\n                stride = 1\n            net_stride *= stride\n            first_dilation = 1 if dilation in (1, 2) else 2\n\n            blocks = []\n            for block_idx in range(cfg.depths[stage_idx]):\n                first_block = block_idx == 0 and stage_idx == 0\n                out_chs = make_divisible(cfg.channels[stage_idx] * cfg.width_factor, cfg.ch_div)\n                blocks += [NormalizationFreeBlock(\n                    in_chs=prev_chs, out_chs=out_chs,\n                    alpha=cfg.alpha,\n                    beta=1. \/ expected_var ** 0.5,  # NOTE: beta used as multiplier in block\n                    stride=stride if block_idx == 0 else 1,\n                    dilation=dilation,\n                    first_dilation=first_dilation,\n                    group_size=cfg.group_size,\n                    bottle_ratio=1. if cfg.efficient and first_block else cfg.bottle_ratio,\n                    efficient=cfg.efficient,\n                    ch_div=cfg.ch_div,\n                    attn_layer=attn_layer,\n                    attn_gain=cfg.attn_gain,\n                    act_layer=act_layer,\n                    conv_layer=conv_layer,\n                    drop_path_rate=dpr[stage_idx][block_idx],\n                    skipinit=cfg.skipinit,\n                )]\n                if block_idx == 0:\n                    expected_var = 1.  # expected var is reset after first block of each stage\n                expected_var += cfg.alpha ** 2   # Even if reset occurs, increment expected variance\n                first_dilation = dilation\n                prev_chs = out_chs\n            stages += [nn.Sequential(*blocks)]\n        self.stages = nn.Sequential(*stages)\n\n        if cfg.efficient and cfg.num_features:\n            # The paper NFRegNet models have an EfficientNet-like final head convolution.\n            self.num_features = make_divisible(cfg.width_factor * cfg.num_features, cfg.ch_div)\n            self.final_conv = conv_layer(prev_chs, self.num_features, 1)\n        else:\n            self.num_features = prev_chs\n            self.final_conv = nn.Identity()\n        # FIXME not 100% clear on gamma subtleties final conv\/final act in case where it's in stdconv\n        self.final_act = act_layer()\n        self.feature_info += [dict(num_chs=self.num_features, reduction=net_stride, module='final_act')]\n\n        self.head = ClassifierHead(self.num_features, num_classes, pool_type=global_pool, drop_rate=self.drop_rate)\n\n        for n, m in self.named_modules():\n            if 'fc' in n and isinstance(m, nn.Linear):\n                nn.init.zeros_(m.weight)\n","AFTER":"        self.stem, stem_stride, stem_feat = create_stem(\n            in_chans, stem_chs, cfg.stem_type, conv_layer=conv_layer, act_layer=act_layer)\n\n        self.feature_info = [dict(num_chs=stem_chs, reduction=2, module=stem_feat)] if stem_stride == 4 else []\n        drop_path_rates = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(cfg.depths)).split(cfg.depths)]\n        prev_chs = stem_chs\n        net_stride = stem_stride\n        dilation = 1\n        expected_var = 1.0\n        stages = []\n        for stage_idx, stage_depth in enumerate(cfg.depths):\n            stride = 1 if stage_idx == 0 and stem_stride > 2 else 2\n            if stride == 2:\n                self.feature_info += [dict(num_chs=prev_chs, reduction=net_stride, module=f'stages.{stage_idx}.0.act1')]\n            if net_stride >= output_stride and stride > 1:\n                dilation *= stride\n                stride = 1\n            net_stride *= stride\n            first_dilation = 1 if dilation in (1, 2) else 2\n\n            blocks = []\n            for block_idx in range(cfg.depths[stage_idx]):\n                first_block = block_idx == 0 and stage_idx == 0\n                out_chs = make_divisible(cfg.channels[stage_idx] * cfg.width_factor, cfg.ch_div)\n                blocks += [NormFreeBlock(\n                    in_chs=prev_chs, out_chs=out_chs,\n                    alpha=cfg.alpha,\n                    beta=1. \/ expected_var ** 0.5,  # NOTE: beta used as multiplier in block\n                    stride=stride if block_idx == 0 else 1,\n                    dilation=dilation,\n                    first_dilation=first_dilation,\n                    group_size=cfg.group_size,\n                    bottle_ratio=1. if cfg.reg and first_block else cfg.bottle_ratio,\n                    ch_div=cfg.ch_div,\n                    reg=cfg.reg,\n                    extra_conv=cfg.extra_conv,\n                    skipinit=cfg.skipinit,\n                    attn_layer=attn_layer,\n                    attn_gain=cfg.attn_gain,\n                    act_layer=act_layer,\n                    conv_layer=conv_layer,\n                    drop_path_rate=drop_path_rates[stage_idx][block_idx],\n                )]\n                if block_idx == 0:\n                    expected_var = 1.  # expected var is reset after first block of each stage\n                expected_var += cfg.alpha ** 2   # Even if reset occurs, increment expected variance\n                first_dilation = dilation\n                prev_chs = out_chs\n            stages += [nn.Sequential(*blocks)]\n        self.stages = nn.Sequential(*stages)\n\n        if cfg.num_features:\n            # The paper NFRegNet models have an EfficientNet-like final head convolution.\n            self.num_features = make_divisible(cfg.width_factor * cfg.num_features, cfg.ch_div)\n            self.final_conv = conv_layer(prev_chs, self.num_features, 1)\n            # FIXME not 100% clear on gamma subtleties final conv\/final act in case where it's pushed into stdconv\n        else:\n            self.num_features = prev_chs\n            self.final_conv = nn.Identity()\n        self.final_act = act_layer(inplace=cfg.num_features > 0)\n        self.feature_info += [dict(num_chs=self.num_features, reduction=net_stride, module='final_act')]\n\n        self.head = ClassifierHead(self.num_features, num_classes, pool_type=global_pool, drop_rate=self.drop_rate)\n\n        for n, m in self.named_modules():\n            if 'fc' in n and isinstance(m, nn.Linear):\n                if cfg.zero_init_fc:\n                    nn.init.zeros_(m.weight)\n                else:\n                    nn.init.normal_(m.weight, 0., .01)\n                if m.bias is not None:\n"}