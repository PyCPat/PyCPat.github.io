{"BEFORE":"        self.dropoute = kwargs.get(\"dropoute\", .7)\n        self.rnn_size = kwargs.get(\"rnn_size\", 100)\n        self.rnn_layers = kwargs.get(\"rnn_layers\", 2)\n        self.rnn_dropouti = kwargs.get(\"rnn_dropouti\", .5)\n        self.rnn_dropouto = kwargs.get(\"rnn_dropouto\", .5)\n        self.rnn_dropoutw = kwargs.get(\"rnn_dropoutw\", .5)\n        self.pack = kwargs.get(\"pack\", True)\n        self.no_rnn = kwargs.get(\"no_rnn\", False)\n        self.bidir = kwargs.get(\"bidir\", False)\n\n        self.lockdrop = LockedDropout()  # den kserw ti einai auto\n        self.idrop = nn.Dropout(self.rnn_dropouti)\n        self.hdrop = nn.Dropout(self.rnn_dropoutw)\n        dropout=0.5\n        self.drop = nn.Dropout(dropout)\n        ############################################\n        # Layers\n        ############################################\n        self.embedding = EmbeddingDropout(num_embeddings=ntokens,\n                                          embedding_dim=self.emb_size,\n                                          embedding_dropout=self.dropoute)\n\n        self.rnn = RNNModel(input_size=self.emb_size,\n                            rnn_size=self.rnn_size,\n                            rnn_layers=self.rnn_layers,\n                            bidirectional=self.bidir,\n                            pack=self.pack)\n\n        if self.no_rnn == False:\n            self.attention_size = self.rnn_size\n        else:\n            self.attention_size = self.emb_size\n\n        # self.attention = SelfAttention(self.attention_size, baseline=True)\n\n        self.classes = nn.Linear(self.attention_size, nclasses)\n","AFTER":"        self.rnn_size = kwargs.get(\"rnn_size\", 100)\n        self.rnn_layers = kwargs.get(\"rnn_layers\", 1)\n\n        # dropouts\n        self.dropoute = kwargs.get(\"dropoute\", .3) # embedding layer dropout\n        self.rnn_dropouti = kwargs.get(\"rnn_dropouti\", .5) # rnn input dropout\n        self.rnn_wdrop = kwargs.get(\"rnn_wdrop\", .5) # rnn recurrent dropout\n        self.rnn_dropouth = kwargs.get(\"rnn_dropouth\", .5) # rnn output dropout\n        self.rnn_dropout = kwargs.get(\"rnn_dropout\", .5) # not sure yet\n\n        self.pack = kwargs.get(\"pack\", True)\n        # self.no_rnn = kwargs.get(\"no_rnn\", False)\n        self.bidir = kwargs.get(\"bidir\", False)\n        self.attention = kwargs.get(\"attention\", False)\n\n        if not isinstance(self.rnn_size, list):\n            self.rnn_size = [self.rnn_size]\n\n        ############################################\n        # Layers\n        ############################################\n        self.embedding = EmbeddingDropout(num_embeddings=ntokens,\n                                          embedding_dim=self.emb_size,\n                                          embedding_dropout=self.dropoute)\n\n\n        self.rnn = RNNModule(ninput=self.emb_size,\n                             nhidden=self.rnn_size,\n                             nlayers=self.rnn_layers,\n                             dropouti=self.rnn_dropouti,\n                             dropouth=self.rnn_dropouth,\n                             dropout=self.rnn_dropout,\n                             wdrop=self.rnn_wdrop)\n\n        if self.attention:\n            self.attention_size = self.rnn_size[-1]\n\n            self.attention = SelfAttention(self.attention_size, baseline=True)\n\n            self.classes = nn.Linear(self.attention_size, nclasses)\n        else:\n            self.classes = nn.Linear(self.rnn_size[-1], nclasses)\n\n    def initialize_embeddings(self, embs, trainable=False):\n"}