{"BEFORE":"        stacks = list(range(e_layers, 1, -1)) # you can customize here\n        encoders = [\n            Encoder(\n                [\n                    EncoderLayer(\n                        AttentionLayer(Attn(False, factor, attention_dropout=dropout), \n                                    d_model, n_heads),\n                        d_model,\n                        d_ff,\n                        dropout=dropout,\n                        activation=activation\n                    ) for l in range(el)\n                ],\n                [\n                    ConvLayer(\n                        d_model\n                    ) for l in range(el-1)\n                ],\n","AFTER":"                output_attention = False, distil=True,\n                device=torch.device('cuda:0')):\n        super(InformerStack, self).__init__()\n        self.pred_len = out_len\n        self.attn = attn\n        self.output_attention = output_attention\n\n        # Encoding\n        self.enc_embedding = DataEmbedding(enc_in, d_model, embed, data, dropout)\n        self.dec_embedding = DataEmbedding(dec_in, d_model, embed, data, dropout)\n        # Attention\n        Attn = ProbAttention if attn=='prob' else FullAttention\n        # Encoder\n\n        stacks = list(range(e_layers, 2, -1)) # you can customize here\n        encoders = [\n            Encoder(\n                [\n                    EncoderLayer(\n                        AttentionLayer(Attn(False, factor, attention_dropout=dropout, output_attention=output_attention), \n                                    d_model, n_heads),\n                        d_model,\n                        d_ff,\n                        dropout=dropout,\n                        activation=activation\n                    ) for l in range(el)\n                ],\n                [\n                    ConvLayer(\n                        d_model\n                    ) for l in range(el-1)\n                ] if distil else None,\n"}