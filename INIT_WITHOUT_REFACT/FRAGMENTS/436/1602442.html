<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.layers = ReversibleSequence(layers)

    def forward(self, x, **kwargs):
        x<a id="change"> = </a>torch.cat([x, x], dim = -1)
        x<a id="change"> = </a><a id="change">self.layers(</a>x<a id="change">, **kwargs)</a>
        <a id="change">return </a>torch.stack(x.chunk(2, dim=-1)).mean(dim=0)

class LinearAttentionTransformerLM(nn.Module):
    def __init__(self, num_tokens, dim, depth, max_seq_len, heads = 8, causal = False, one_kv_head = False, ff_chunks = 1):</code></pre><h3>After Change</h3><pre><code class='java'>
        self.layers = execute_type(layers)

    def forward(self, x, **kwargs):
        <a id="change">return </a><a id="change">self.layers(</a>x<a id="change">, **kwargs)</a>

class LinearAttentionTransformerLM(nn.Module):
    def __init__(self, num_tokens, dim, depth, max_seq_len, heads = 8, causal = False, one_kv_head = False, reversible = False, ff_chunks = 1):
        super().__init__()</code></pre>