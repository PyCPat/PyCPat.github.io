{"BEFORE":"        self.block_dim = self.lbmgr.get_block_dim(self.rank)\n        self.comm_func = reduce_forward\n\n        if blk_embed is not None:\n            weights = blk_embed.get_weights(detach=True)\n            base_embedding_dim = blk_embed.get_base_embedding_dim()\n            assert weights[0].size() == (sum([self.field_dims[i] for i in self.group]), self.block_dim), \\\n                'passed embedding layer dimensions are wrong: {x1} vs {x2} \\\n                    '.format(x1=weights[0].size(), x2=(sum([self.field_dims[i] for i in self.group]), self.block_dim))\n            if self.block_dim != self.embedding_dim:\n                assert weights[1].size() == (self.embedding_dim, self.block_dim), \\\n                    'passed linear layer dimensions are wrong: {x1} vs {x2} \\\n                    '.format(x1=weights[1].size(), x2=(self.embedding_dim, self.block_dim))\n            if base_embedding_dim != self.embedding_dim:\n                DISTLogger.warning('Base embedding dimension provided by blk_embed is different from \\\n                    default or manually passed. Will overwrite by blk_embed.base_embedding_dim')\n                self.embedding_dim = base_embedding_dim\n            self.embed = BlockEmbeddingBag.from_pretrained(\n                                                weights=weights,\n                                                base_embedding_dim=base_embedding_dim,\n                                                freeze=freeze)\n        else:\n            self.embed = BlockEmbeddingBag(\n                                    sum([self.field_dims[i] for i in self.group]), \n                                    block_embedding_dim=self.block_dim,\n                                    base_embedding_dim=self.embedding_dim,\n                                    mode=mode,\n                                    *args,\n                                    **kwargs)\n\n    def _shard_tensor(self, _input: Tensor) -> Tensor:\n","AFTER":"                enable_qr: bool = False,\n                lbmgr: Optional[LoadBalanceManager] = None,\n                freeze: bool = False,\n                device = None,\n                *args,\n                **kwargs):\n        super().__init__()\n        self.field_dims = field_dims\n        self.embedding_dim = embedding_dim\n        self.mode = mode\n        self.device = device if device is not None else torch.device('cuda', torch.cuda.current_device())\n\n        # Decide number of nodes\n        self.parallel_mode = ParallelMode.DEFAULT if parallel_mode is None else parallel_mode\n        self.world_size = DISTMGR.get_world_size(self.parallel_mode)\n        self.rank = DISTMGR.get_rank(self.parallel_mode)\n        self.num_groups = self.world_size # default setting\n\n        if lbmgr is not None:\n            self.lbmgr = lbmgr\n            self.field_dims = lbmgr.get_field_dims()\n            self.embedding_dim = lbmgr.get_base_dim()\n        else:\n            self.lbmgr = LoadBalanceManager(field_dims, self.num_groups, embedding_dim)\n\n        self.group = self.lbmgr.get_group(self.rank)\n        self.block_dim = self.lbmgr.get_block_dim(self.rank)\n        self.qr_bucket_size = self.lbmgr.get_qr_bucket_size(self.rank)\n        self.offsets = torch.tensor((0,*np.cumsum(np.array( \\\n            self.field_dims, dtype=np.long)[self.group])[:-1]), device=self.device)\n        \n        self.comm_func = reduce_forward\n        cls = BlockEmbeddingBag if not enable_qr else QREmbeddingBag\n\n        if pretrain_embed is not None:\n            if not enable_qr:\n                assert isinstance(pretrain_embed, cls)\n                weights = pretrain_embed.get_weights(detach=True)\n                base_embedding_dim = pretrain_embed.get_base_embedding_dim()\n                assert weights[0].size() == (sum([self.field_dims[i] for i in self.group]), self.block_dim), \\\n                    'passed embedding layer dimensions are wrong: {x1} vs {x2} \\\n                        '.format(x1=weights[0].size(), x2=(sum([self.field_dims[i] for i in self.group]), self.block_dim))\n                if self.block_dim != self.embedding_dim:\n                    assert weights[1].size() == (self.embedding_dim, self.block_dim), \\\n                        'passed linear layer dimensions are wrong: {x1} vs {x2} \\\n                        '.format(x1=weights[1].size(), x2=(self.embedding_dim, self.block_dim))\n                if base_embedding_dim != self.embedding_dim:\n                    DISTLogger.warning('Base embedding dimension provided by blk_embed is different from \\\n                        default or manually passed. Will overwrite by blk_embed.base_embedding_dim')\n                    self.embedding_dim = base_embedding_dim\n                self.embed = cls.from_pretrained(weights=weights,\n                                            base_embedding_dim=base_embedding_dim,\n                                            freeze=freeze)\n            else:\n                assert isinstance(pretrain_embed, cls)\n                weights = pretrain_embed.get_weights(detach=True)\n                num_embeddings = pretrain_embed.get_num_embeddings()\n                assert num_embeddings == sum([self.field_dims[i] for i in self.group]), \\\n                    f'passed embedding layer have wrong number of embeddings: {num_embeddings} vs \\\n                        {sum([self.field_dims[i] for i in self.group])}'\n                assert weights[0].size() == weights[1].size() == (self.qr_bucket_size, self.embedding_dim), \\\n                    'either q- or r-embedding layer has wrong input dimensions: {x1} vs {x2} vs {x3} \\\n                        '.format(x1=weights[0].size(), x2=weights[0].size(), \n                                x3=(self.qr_bucket_size, self.embedding_dim))\n                self.embed = cls.from_pretrained(weights=weights,\n                                            num_embeddings=num_embeddings,\n                                            freeze=freeze)\n        else:\n            if not enable_qr:\n                self.embed = cls(sum([self.field_dims[i] for i in self.group]), \n                                block_embedding_dim=self.block_dim,\n                                embedding_dim=self.embedding_dim,\n                                mode=mode,\n                                *args,\n                                **kwargs)\n            else:\n                self.embed = cls(sum([self.field_dims[i] for i in self.group]), \n                                num_buckets=self.qr_bucket_size,\n                                embedding_dim=self.embedding_dim,\n                                mode=mode,\n                                *args,\n                                **kwargs)\n\n    def _shard_tensor(self, _input: Tensor) -> Tensor:\n"}