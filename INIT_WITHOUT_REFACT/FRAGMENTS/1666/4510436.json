{"BEFORE":"    def __init__(self, N=24, dim=768, codebook_size=1024):\r\n        super(BidirectionalTransformer, self).__init__()\r\n\r\n        self.tok_emb = nn.Embedding(codebook_size, dim)\r\n        self.pos_emb = nn.Parameter(torch.zeros(1, 512, dim))\r\n        self.EncoderLayers = nn.ModuleList([Encoder(dim) for _ in range(N)])\r\n        self.Token_Prediction = nn.Linear(in_features=dim, out_features=codebook_size)\r\n","AFTER":"    def __init__(self, args):\r\n        super(BidirectionalTransformer, self).__init__()\r\n        self.num_image_tokens = args.num_image_tokens\r\n        self.tok_emb = nn.Embedding(args.num_codebook_vectors + 2, args.dim)\r\n        # self.pos_emb = PositionalEmbedding(args.dim, self.num_image_tokens + 1)\r\n        self.pos_emb = nn.init.trunc_normal_(nn.Parameter(torch.zeros(self.num_image_tokens + 1, args.dim)), 0., 0.02)\r\n        # self.register_buffer(\"pos_emb\", nn.init.trunc_normal_(nn.Parameter(torch.zeros(1024, args.dim)), 0., 0.02))\r\n        self.blocks = nn.Sequential(*[Encoder(args.dim, args.hidden_dim) for _ in range(args.n_layers)])\r\n        self.Token_Prediction = nn.Sequential(*[\r\n            nn.Linear(in_features=args.dim, out_features=args.dim),\r\n            nn.GELU(),\r\n            nn.LayerNorm(args.dim, eps=1e-12)\r\n        ])\r\n        self.bias = nn.Parameter(torch.zeros(self.num_image_tokens+1, args.num_codebook_vectors + 2))\r\n        self.ln = nn.LayerNorm(args.dim, eps=1e-12)\r\n        self.drop = nn.Dropout(p=0.1)\r\n"}