{"BEFORE":"        end_net = []\n        \n        end_net.append(nn.ReLU())\n        end_net.append(nn.Conv1d(skip_channels, hidden_channels, kernel_size=1, stride=1, bias=False))\n        end_net.append(nn.ReLU())\n        end_net.append(nn.Conv1d(hidden_channels, out_channels, kernel_size=1, stride=1, bias=False))\n        \n        if output_nonlinear is not None:\n            if output_nonlinear == 'tanh':\n                end_net.append(nn.Tanh())\n            elif output_nonlinear == 'softmax':\n                end_net.append(nn.Softmax(dim=1))\n            else:\n                raise ValueError(\"Not support {}\".format(output_nonlinear))\n        \n        self.end_net = nn.Sequential(*end_net)\n","AFTER":"            if output_nonlinear == 'softmax':\n                kwargs = {\n                    \"dim\": 1\n                }\n            else:\n                kwargs = {}\n            \n            module = choose_nonlinear(output_nonlinear, **kwargs)\n            end_net.append(module)\n"}