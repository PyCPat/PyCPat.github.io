{"BEFORE":"        emb_dim = default(emb_dim, dim)\n        self.token_emb = nn.Embedding(num_tokens, emb_dim)\n        self.pos_emb = nn.Embedding(max_seq_len, emb_dim)\n","AFTER":"    def __init__(self, num_tokens, dim, depth, max_seq_len, heads = 8, bucket_size = 64, n_hashes = 8, ff_chunks = 100, attn_chunks = None, causal = False, weight_tie = False, lsh_dropout = 0., random_rotations_per_head = False, twin_attention = False, use_scale_norm = False, use_full_attn = False, full_attn_thres = 0, num_mem_kv = 0, emb_dim = None, return_embeddings = False, fixed_position_emb = False):\n        super().__init__()\n        emb_dim = default(emb_dim, dim)\n        self.token_emb = nn.Embedding(num_tokens, emb_dim)\n        self.pos_emb = FixedPositionEmbedding(emb_dim) if fixed_position_emb else nn.Embedding(max_seq_len, emb_dim)\n"}