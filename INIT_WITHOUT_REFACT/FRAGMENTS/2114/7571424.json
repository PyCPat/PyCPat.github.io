{"BEFORE":"        if shared_label is None:\n            shared_label = self.shared(label)\n        else:\n            pass\n        z = torch.cat([shared_label, z], 1)\n\n        act = self.linear0(z)\n        act = act.view(-1, self.in_dims[0], self.bottom, self.bottom)\n        for index, blocklist in enumerate(self.blocks):\n            for block in blocklist:\n                if isinstance(block, ops.SelfAttention):\n                    act = block(act)\n                else:\n                    act = block(act, z)\n\n        act = self.bn4(act)\n        act = self.activation(act)\n        act = self.conv2d5(act)\n        out = self.tanh(act)\n","AFTER":"    def forward(self, z, label, shared_label=None, eval=False):\n        with torch.cuda.amp.autocast() if self.mixed_precision and not eval else misc.dummy_context_mgr() as mp:\n            if shared_label is None:\n                shared_label = self.shared(label)\n            else:\n                pass\n            z = torch.cat([shared_label, z], 1)\n\n            act = self.linear0(z)\n            act = act.view(-1, self.in_dims[0], self.bottom, self.bottom)\n            for index, blocklist in enumerate(self.blocks):\n                for block in blocklist:\n                    if isinstance(block, ops.SelfAttention):\n                        act = block(act)\n                    else:\n                        act = block(act, z)\n\n            act = self.bn4(act)\n            act = self.activation(act)\n            act = self.conv2d5(act)\n            out = self.tanh(act)\n        return out\n"}