{"BEFORE":"        emb[torch.isnan(emb)] = 0.0\n\n        e = torch.matmul(self.edge_emb, self.W_e).view(-1,\n                                                       self.num_heads, self.edge_feats)\n\n        row = g.edges()[0]\n        col = g.edges()[1]\n        tp = g.edata['_TYPE']\n        # tp = g.edge_type\n\n        h_l = (self.a_l * emb).sum(dim=-1)[row]\n        h_r = (self.a_r * emb).sum(dim=-1)[col]\n        h_e = (self.a_e * e).sum(dim=-1)[tp]\n\n        edge_attention = self.leakyrelu(h_l + h_r + h_e)\n        edge_attention = edge_softmax(g, edge_attention)\n\n        if 'alpha' in g.edata.keys():\n            res_attn = g.edata['alpha']\n            edge_attention = edge_attention * \\\n                             (1 - self.beta) + res_attn * self.beta\n\n        with g.local_scope():\n            h_prime = []\n            emb = emb.permute(1, 0, 2).contiguous()\n            for i in range(self.num_heads):\n                g.edata['alpha'] = edge_attention[:, i]\n                g.srcdata.update({'emb': emb[i]})\n                g.update_all(Fn.u_mul_e('emb', 'alpha', 'm'),\n                             Fn.sum('m', 'emb'))\n                h_prime.append(g.ndata['emb'])\n            h_output = torch.cat(h_prime, dim=1)\n","AFTER":"        emb[torch.isnan(emb)] = 0.0\n\n        e = torch.matmul(self.edge_emb, self.W_e).view(-1,\n                                                       self.num_heads, self.edge_feats)\n\n        row = g.edges()[0]\n        col = g.edges()[1]\n        tp = g.edata['_TYPE']\n        # tp = g.edge_type\n\n        h_l = (self.a_l * emb).sum(dim=-1)[row]\n        h_r = (self.a_r * emb).sum(dim=-1)[col]\n        h_e = (self.a_e * e).sum(dim=-1)[tp]\n\n        edge_attention = self.leakyrelu(h_l + h_r + h_e)\n        edge_attention = edge_softmax(g, edge_attention)\n\n        if 'alpha' in g.edata.keys():\n            res_attn = g.edata['alpha']\n            edge_attention = edge_attention * \\\n                             (1 - self.beta) + res_attn * self.beta\n        if self.num_heads == 1:\n            edge_attention = edge_attention[:, 0]\n            edge_attention = edge_attention.unsqueeze(1)\n\n        with g.local_scope():\n            emb = emb.permute(0, 2, 1).contiguous()\n            g.edata['alpha'] = edge_attention\n            g.srcdata['emb'] = emb\n            g.update_all(Fn.u_mul_e('emb', 'alpha', 'm'),\n                         Fn.sum('m', 'emb'))\n            # g.apply_edges(Fn.u_mul_e('emb', 'alpha', 'm'))\n            h_output = g.ndata['emb'].view(-1, self.out_features * self.num_heads)\n"}