{"BEFORE":"        alignments = []\n        stop_tokens = []\n        t = 0\n        memory_input = initial_memory\n        while True:\n            if t > 0:\n                if greedy:\n                    memory_input = outputs[-1]\n                else:\n                    memory_input = memory[t-1]\n            # Prenet\n            processed_memory = self.prenet(memory_input)\n            # Attention RNN\n            attention_rnn_hidden, current_context_vec, alignment = self.attention_rnn(\n                processed_memory, current_context_vec, attention_rnn_hidden, inputs)\n            # Concat RNN output and attention context vector\n            decoder_input = self.project_to_decoder_in(\n                torch.cat((attention_rnn_hidden, current_context_vec), -1))\n            # Pass through the decoder RNNs\n            for idx in range(len(self.decoder_rnns)):\n                decoder_rnn_hiddens[idx] = self.decoder_rnns[idx](\n                    decoder_input, decoder_rnn_hiddens[idx])\n                # Residual connectinon\n                decoder_input = decoder_rnn_hiddens[idx] + decoder_input\n            decoder_output = decoder_input\n            # predict mel vectors from decoder vectors\n            output = self.proj_to_mel(decoder_output)\n            stop_input = output\n            # predict stop token\n            stop_token, stopnet_rnn_hidden = self.stopnet(stop_input, stopnet_rnn_hidden)\n            outputs += [output]\n            alignments += [alignment]\n            stop_tokens += [stop_token]\n            t += 1\n            if (not greedy and self.training) or (greedy and memory is not None):\n                if t >= T_decoder:\n                    break\n            else:\n                if t > inputs.shape[1]\/2 and stop_token > 0.8:\n                    break\n                elif t > self.max_decoder_steps:\n                    print(\" !! Decoder stopped with 'max_decoder_steps'. \\\n                          Something is probably wrong.\")\n                    break\n        assert greedy or len(outputs) == T_decoder\n        # Back to batch first\n        alignments = torch.stack(alignments).transpose(0, 1)\n        outputs = torch.stack(outputs).transpose(0, 1).contiguous()\n        stop_tokens = torch.stack(stop_tokens).transpose(0, 1)\n        return outputs, alignments, stop_tokens\n","AFTER":"        B = inputs.size(0)\n        T = inputs.size(1)\n        # Run greedy decoding if memory is None\n        greedy = not self.training\n        if memory is not None:\n            # Grouping multiple frames if necessary\n            if memory.size(-1) == self.memory_dim:\n                memory = memory.view(B, memory.size(1) \/\/ self.r, -1)\n                \" !! Dimension mismatch {} vs {} * {}\".format(memory.size(-1),\n                                                              self.memory_dim, self.r)\n            T_decoder = memory.size(1)\n        # go frame - 0 frames tarting the sequence\n        initial_memory = inputs.data.new(B, self.memory_dim * self.r).zero_()\n        # Init decoder states\n        attention_rnn_hidden = inputs.data.new(B, 256).zero_()\n        decoder_rnn_hiddens = [inputs.data.new(B, 256).zero_()\n            for _ in range(len(self.decoder_rnns))]\n        current_context_vec = inputs.data.new(B, 256).zero_()\n        stopnet_rnn_hidden = inputs.data.new(B, self.r * self.memory_dim).zero_()\n        attention_vec = memory.data.new(B, T).zero_()\n        attention_vec_cum = memory.data.new(B, T).zero_()\n        # Time first (T_decoder, B, memory_dim)\n        if memory is not None:\n            memory = memory.transpose(0, 1)\n        outputs = []\n        attentions = []\n        stop_tokens = []\n        t = 0\n        memory_input = initial_memory\n        while True:\n            if t > 0:\n                if greedy:\n                    memory_input = outputs[-1]\n                else:\n                    memory_input = memory[t-1]\n            # Prenet\n            processed_memory = self.prenet(memory_input)\n            # Attention RNN\n            attention_vec_cat = torch.cat((attention_vec.unsqueeze(1),\n                                               attention_vec_cum.unsqueeze(1)),\n                                           dim=1)\n            attention_rnn_hidden, current_context_vec, attention = self.attention_rnn(\n                processed_memory, current_context_vec, attention_rnn_hidden, inputs, attention_vec_cat)\n            attention_vec_cum += attention_vec\n            attention_vec_cum \/= (t + 1)\n            # Concat RNN output and attention context vector\n            decoder_input = self.project_to_decoder_in(\n                torch.cat((attention_rnn_hidden, current_context_vec), -1))\n            # Pass through the decoder RNNs\n            for idx in range(len(self.decoder_rnns)):\n                decoder_rnn_hiddens[idx] = self.decoder_rnns[idx](\n                    decoder_input, decoder_rnn_hiddens[idx])\n                # Residual connectinon\n                decoder_input = decoder_rnn_hiddens[idx] + decoder_input\n            decoder_output = decoder_input\n            # predict mel vectors from decoder vectors\n            output = self.proj_to_mel(decoder_output)\n            stop_input = output\n            # predict stop token\n            stop_token, stopnet_rnn_hidden = self.stopnet(stop_input, stopnet_rnn_hidden)\n            outputs += [output]\n            attentions += [attention]\n            stop_tokens += [stop_token]\n            t += 1\n            if (not greedy and self.training) or (greedy and memory is not None):\n                if t >= T_decoder:\n                    break\n            else:\n                if t > inputs.shape[1]\/2 and stop_token > 0.6:\n                    break\n                elif t > self.max_decoder_steps:\n                    print(\" !! Decoder stopped with 'max_decoder_steps'. \\\n                          Something is probably wrong.\")\n                    break\n        assert greedy or len(outputs) == T_decoder\n        # Back to batch first\n        attentions = torch.stack(attentions).transpose(0, 1)\n        outputs = torch.stack(outputs).transpose(0, 1).contiguous()\n        stop_tokens = torch.stack(stop_tokens).transpose(0, 1)\n        return outputs, attentions, stop_tokens\n"}