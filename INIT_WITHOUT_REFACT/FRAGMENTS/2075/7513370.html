<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        KW = self.w_k(K)
        KW = torch.einsum("nk,bnc-&gt;bkc", E, KW)
        QW = self.w_q(Q)
        QW = <a id="change">torch.einsum("bnc,bkc-&gt;bnk"</a>, QW, KW<a id="change">)</a>
        P_bar = QW/torch.sqrt(torch.tensor(self.dim, dtype=torch.float32))
        P_bar = P_bar.softmax(dim=-1)
        P_bar = self.dropout(P_bar)
</code></pre><h3>After Change</h3><pre><code class='java'>
    def forward(self, Q, K, V, E, F):
        KW = self.w_k(K)
        KW = torch.matmul(E, KW)
        KW = <a id="change">torch.transpose(</a>KW, <a id="change">1</a>, <a id="change">2</a><a id="change">)</a>
        QW = self.w_q(Q)
        QW = torch.matmul(QW, KW)

        &#47&#47 TODO: Possibly change the dtype?
        P_bar = QW/torch.sqrt(torch.tensor(self.dim, dtype=torch.float32))
        P_bar = P_bar.softmax(dim=-1)
        P_bar = self.dropout(P_bar)

        VW = self.w_v(V)
        VW = torch.matmul(F, VW)
        out_tensor<a id="change"> = </a>torch.matmul(P_bar, VW)

        return out_tensor
</code></pre>