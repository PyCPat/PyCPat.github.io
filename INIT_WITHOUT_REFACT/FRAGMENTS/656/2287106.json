{"BEFORE":"        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n\n        # 我们一旦生成了<end>就已经完成了解码\n        # 因此需要解码的长度实际是 lengths - 1\n        decode_lengths = (caption_lengths - 1).tolist()\n        # 新建两个张量用于存放 word predicion scores and alphas\n        global device\n        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n\n        # 在每一个时间步根据解码器的前一个状态以及经过attention加权后的encoder输出进行解码\n        for t in range(max(decode_lengths)):\n            batch_size_t = sum([l > t for l in decode_lengths])\n            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n                                                                h[:batch_size_t])\n            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n            attention_weighted_encoding = gate * attention_weighted_encoding\n            h, c = self.decode_step(\n                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n","AFTER":"    def forward(self, encoder_out, encoded_captions, caption_lengths,p = 1):\n        \"\"\"\n        Forward propagation.\n        :param encoder_out: encoder的输出 (batch_size, enc_image_size, enc_image_size, encoder_dim)\n        :param encoded_captions: caption的编码张量,不是字符串！ (batch_size, max_caption_length)\n        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n        \"\"\"\n        self.p = p\n        batch_size = encoder_out.size(0)\n        encoder_dim = encoder_out.size(1)#这里和普通的resnet输出的不同，resnet是最后一个维度是C\n        vocab_size = self.vocab_size\n\n        # 把特征图展平作为上下文向量\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n        num_pixels = encoder_out.size(1)\n\n        # Sort input data by decreasing lengths; why? apparent below\n        caption_lengths, sort_ind = caption_lengths.sort(dim=0, descending=True)\n        # print('sort_ind',sort_ind,'encoder_out',encoder_out.shape,'encoder_captions',encoded_captions.shape)\n        encoder_out = encoder_out[sort_ind]\n        encoded_captions = encoded_captions[sort_ind]\n\n        # Embedding\n        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n\n        # 初始化GRU状态\n        # h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n        h = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n\n        # 我们一旦生成了<end>就已经完成了解码\n        # 因此需要解码的长度实际是 lengths - 1\n        decode_lengths = (caption_lengths - 1).tolist()\n        # 新建两个张量用于存放 word predicion scores and alphas\n        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n\n        # 在每一个时间步根据解码器的前一个状态以及经过attention加权后的encoder输出进行解码\n        for t in range(max(decode_lengths)):\n            #decode_lengths是解码长度降序的排列,batch_size_t求出当前时间步中需要进行解码的数量\n            batch_size_t = sum([l > t for l in decode_lengths])\n            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n                                                                h[:batch_size_t])\n            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n            attention_weighted_encoding = gate * attention_weighted_encoding\n            # h, c = self.decode_step(\n            #     torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n            #     (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n            #teahcer forcing\n            if t==1 or (np.random.rand() < self.p) :\n                h = self.decode_step(\n                    torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n                    h[:batch_size_t])  # (batch_size_t, decoder_dim)\n            else:\n                h = self.decode_step(\n                    torch.cat([self.embedding(torch.argmax(predictions[:batch_size_t, t, :],dim = 1)), attention_weighted_encoding], dim=1),\n                    h[:batch_size_t])  # (batch_size_t, decoder_dim)\n            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n"}