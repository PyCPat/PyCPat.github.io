{"BEFORE":"        print(\"EncOutput\", enc_output.shape)\n        hidden_with_time_axis = hidden.permute(1, 0, 2)\n        print(\"HiddenTimeaxis:\", hidden_with_time_axis.shape)\n        # score: (batch_size, max_length, hidden_dim)\n        score = torch.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n\n        # attention_weights shape == (batch_size, max_length, 1)\n        # we get 1 at the last axis because we are applying score to self.V\n        attention_weights = torch.softmax(self.V(score), dim=1)\n\n        # context_vector shape after sum == (batch_size, hidden_dim)\n        context_vector = attention_weights * enc_output\n        context_vector = torch.sum(context_vector, dim=1)\n        # context_vector: batch_size, 1, hidden_dim\n        context_vector = context_vector.unsqueeze(1)\n        ## -------------------------\n\n        # x shape after embedding == (batch_size, 1, dec_embed_dim)\n        x = self.embedding(x)\n\n        # x shape after concatenation == (batch_size, 1, dec_embed_dim + hidden_size)\n        x = torch.cat((context_vector, x), -1)\n\n        # passing the concatenated vector to the GRU\n        # output: (batch_size, 1, hidden_size)\n        # state: 1, batch_size, hidden_size\n        output, state = self.gru(x)\n\n        # output shape == (batch_size * 1, hidden_size)\n        output =  output.view(-1, output.size(2))\n\n        # output shape == (batch_size * 1, output_dim)\n        output = self.fc(output)\n\n        return output, state\n","AFTER":"        x = self.embedding(x)\n\n        # x (batch_size, 1, dec_embed_dim + hidden_size) -> after attention\n        x = self.attention( x, hidden, enc_output)\n\n        # passing the concatenated vector to the GRU\n        # output: (batch_size, 1, hidden_size)\n        # hidden: 1, batch_size, hidden_size\n        output, hidden = self.gru(x)\n\n        # output shape == (batch_size * 1, hidden_size)\n        output =  output.view(-1, output.size(2))\n\n        # output shape == (batch_size * 1, output_dim)\n        output = self.fc(output)\n\n        return output, hidden\n"}