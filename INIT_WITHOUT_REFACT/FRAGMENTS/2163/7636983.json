{"BEFORE":"        buckets = self.hash_vectors(n_buckets, qk)\n        # We use the same vector as both a query and a key.\n        assert int(buckets.shape[1]) == self.n_hashes * seqlen\n\n        ticker = torch.arange(self.n_hashes * seqlen, device=device).unsqueeze(0).expand_as(buckets)\n        buckets_and_t = seqlen * buckets + (ticker % seqlen)\n        buckets_and_t = buckets_and_t.detach()\n\n        # Hash-based sort (\"s\" at the start of variable names means \"sorted\")\n        sbuckets_and_t, sticker = sort_key_val(buckets_and_t, ticker, dim=-1)\n        _, undo_sort = sort_key_val(sticker, ticker, dim=-1)\n        del ticker\n\n        sbuckets_and_t = sbuckets_and_t.detach()\n        sticker = sticker.detach()\n        undo_sort = undo_sort.detach()\n\n        st = (sticker % seqlen)\n        sqk = batched_index_select(qk, st)\n        sv = batched_index_select(v, st)\n\n        # Split off a \"bin\" axis so that attention only occurs within chunks.\n        chunk_size = self.n_hashes * n_buckets\n        bq_t = bkv_t = torch.reshape(st, (batch_size, chunk_size, -1))\n        bqk = torch.reshape(sqk, (batch_size, chunk_size, -1, dim))\n        bv = torch.reshape(sv, (batch_size, chunk_size, -1, dim))\n\n        # Hashing operates on unit-length vectors. Unnormalized query vectors are\n        # fine because they effectively provide a learnable temperature for the\n        # attention softmax, but normalizing keys is needed so that similarity for\n        # the purposes of attention correctly corresponds to hash locality.\n        bq = bqk\n        bk = F.normalize(bqk, p=2, dim=-1).type(bq.type())\n\n        # Allow each chunk to attend within itself, and also one chunk back. Chunk\n        # boundaries might occur in the middle of a sequence of items from the\n        # same bucket, so this increases the chances of attending to relevant items.\n        def look_one_back(x):\n            x_extra = torch.cat([x[:, -1:, ...], x[:, :-1, ...]], dim=1)\n            return torch.cat([x, x_extra], dim=2)\n\n        bk = look_one_back(bk)\n        bv = look_one_back(bv)\n        bkv_t = look_one_back(bkv_t)\n\n        # Dot-product attention.\n        dots = torch.einsum('bhie,bhje->bhij', bq, bk) * (dim ** -0.5)\n        masked_value = max_neg_value(dots)\n\n        # Input mask for padding in variable lengthed sequences\n        if input_mask is not None:\n            input_mask = F.pad(input_mask, (0, seqlen - input_mask.shape[1]), 'constant', True)\n            mq = input_mask.gather(1, st).reshape((batch_size, chunk_size, -1))\n            mkv = look_one_back(mq)\n            mask = mq[:, :, :, None] * mkv[:, :, None, :]\n            dots.masked_fill_(~mask, masked_value)\n            del mask\n\n        # Causal masking\n        if self.causal:\n            mask = bq_t[:, :, :, None] < bkv_t[:, :, None, :].clamp(max=query_len - 1)\n            dots.masked_fill_(mask, masked_value)\n            del mask\n\n        # Mask out attention to self except when no other targets are available.\n        self_mask = bq_t[:, :, :, None] == bkv_t[:, :, None, :]\n        dots.masked_fill_(self_mask, TOKEN_SELF_ATTN_VALUE)\n        del self_mask\n\n        # Mask out attention to other hash buckets.\n        if not self._attend_across_buckets:\n            bq_buckets = bkv_buckets = torch.reshape(sbuckets_and_t \/\/ seqlen, (batch_size, chunk_size, -1))\n            bkv_buckets = look_one_back(bkv_buckets)\n            bucket_mask = bq_buckets[:, :, :, None] != bkv_buckets[:, :, None, :]\n            dots.masked_fill_(bucket_mask, masked_value)\n            del bucket_mask\n\n        # Don't double-count query-key pairs across multiple rounds of hashing.\n        # There are two possible strategies here. (1) The default is to count how\n        # many times a query-key pair is repeated, and to lower its log-prob\n        # correspondingly at each repetition. (2) When hard_k is set, the code\n        # instead masks all but the first occurence of each query-key pair.\n        if not self._allow_duplicate_attention:\n            locs1 = undo_sort \/\/ bq_t.shape[-1]\n            locs2 = (locs1 + 1) % chunk_size\n            if not self._attend_across_buckets:\n                locs1 = buckets * chunk_size + locs1\n                locs2 = buckets * chunk_size + locs2\n            locs = torch.cat([\n                torch.reshape(locs1, (batch_size, self.n_hashes, seqlen)),\n                torch.reshape(locs2, (batch_size, self.n_hashes, seqlen)),\n            ], 1).permute((0, 2, 1))\n\n            slocs = batched_index_select(locs, st)\n            b_locs = torch.reshape(slocs, (batch_size, chunk_size, -1, 2 * self.n_hashes))\n\n            b_locs1 = b_locs[:, :, :, None, :self.n_hashes]\n\n            bq_locs = b_locs1.expand(b_locs.shape[:3] + (2, self.n_hashes))\n            bq_locs = torch.reshape(bq_locs, b_locs.shape)\n            bkv_locs = look_one_back(b_locs)\n\n            dup_counts = (bq_locs[:, :, :, None, :] == bkv_locs[:, :, None, :, :])\n            # for memory considerations, chunk summation of last dimension for counting duplicates\n            dup_counts = chunked_sum(dup_counts, chunks=(self.n_hashes * batch_size))\n            dup_counts = dup_counts.detach()\n            assert dup_counts.shape == dots.shape\n            dots = dots - torch.log(dup_counts + 1e-9)\n            del dup_counts\n\n        # Softmax.\n        dots_logsumexp = torch.logsumexp(dots, dim=-1, keepdim=True)\n        dots = torch.exp(dots - dots_logsumexp).type(dots.type())\n        dots = self.dropout(dots)\n\n        bo = torch.einsum('buij,buje->buie', dots, bv)\n        so = torch.reshape(bo, (batch_size, -1, dim))\n        slogits = torch.reshape(dots_logsumexp, (batch_size, -1,))\n\n        class UnsortLogits(Function):\n            @staticmethod\n            def forward(ctx, so, slogits):\n                so = so.detach()\n                slogits = slogits.detach()\n                o = batched_index_select(so, undo_sort)\n                _, logits = sort_key_val(sticker, slogits, dim=-1)\n                return o, logits\n\n            @staticmethod\n            def backward(ctx, grad_x, grad_y):\n                so_grad = batched_index_select(grad_x, sticker)\n                _, slogits_grad = sort_key_val(buckets_and_t, grad_y, dim=-1)\n                return so_grad, slogits_grad\n\n        o, logits = UnsortLogits.apply(so, slogits)\n        o = torch.reshape(o, (batch_size, self.n_hashes, seqlen, dim))\n        logits = torch.reshape(logits, (batch_size, self.n_hashes, seqlen, 1))\n\n        if query_len != seqlen:\n            query_slice = (slice(None), slice(None), slice(0, query_len))\n            o, logits = o[query_slice], logits[query_slice]\n\n        probs = torch.exp(logits - torch.logsumexp(logits, dim=1, keepdim=True))\n        out = torch.sum(o * probs, dim=1)\n        return out, buckets\n","AFTER":"        query_len = default(query_len, seqlen)\n        device = qk.device\n\n        n_buckets = seqlen \/\/ self.bucket_size\n\n        buckets = self.hash_vectors(n_buckets, qk)\n        # We use the same vector as both a query and a key.\n        assert int(buckets.shape[1]) == self.n_hashes * seqlen\n\n        ticker = torch.arange(self.n_hashes * seqlen, device=device).unsqueeze(0).expand_as(buckets)\n        buckets_and_t = seqlen * buckets + (ticker % seqlen)\n        buckets_and_t = buckets_and_t.detach()\n\n        # Hash-based sort (\"s\" at the start of variable names means \"sorted\")\n        sbuckets_and_t, sticker = sort_key_val(buckets_and_t, ticker, dim=-1)\n        _, undo_sort = sort_key_val(sticker, ticker, dim=-1)\n        del ticker\n\n        sbuckets_and_t = sbuckets_and_t.detach()\n        sticker = sticker.detach()\n        undo_sort = undo_sort.detach()\n\n        st = (sticker % seqlen)\n        sqk = batched_index_select(qk, st)\n        sv = batched_index_select(v, st)\n\n        # Split off a \"bin\" axis so that attention only occurs within chunks.\n        chunk_size = self.n_hashes * n_buckets\n        bq_t = bkv_t = torch.reshape(st, (batch_size, chunk_size, -1))\n        bqk = torch.reshape(sqk, (batch_size, chunk_size, -1, dim))\n        bv = torch.reshape(sv, (batch_size, chunk_size, -1, dim))\n\n        # Hashing operates on unit-length vectors. Unnormalized query vectors are\n        # fine because they effectively provide a learnable temperature for the\n        # attention softmax, but normalizing keys is needed so that similarity for\n        # the purposes of attention correctly corresponds to hash locality.\n        bq = bqk\n        bk = F.normalize(bqk, p=2, dim=-1).type(bq.type())\n\n        # Allow each chunk to attend within itself, and also one chunk back. Chunk\n        # boundaries might occur in the middle of a sequence of items from the\n        # same bucket, so this increases the chances of attending to relevant items.\n        def look_one_back(x):\n            x_extra = torch.cat([x[:, -1:, ...], x[:, :-1, ...]], dim=1)\n            return torch.cat([x, x_extra], dim=2)\n\n        bk = look_one_back(bk)\n        bv = look_one_back(bv)\n        bkv_t = look_one_back(bkv_t)\n\n        # Dot-product attention.\n        dots = torch.einsum('bhie,bhje->bhij', bq, bk) * (dim ** -0.5)\n        masked_value = max_neg_value(dots)\n\n        # Input mask for padding in variable lengthed sequences\n        if input_mask is not None:\n            input_mask = F.pad(input_mask, (0, seqlen - input_mask.shape[1]), 'constant', True)\n            mq = input_mask.gather(1, st).reshape((batch_size, chunk_size, -1))\n            mkv = look_one_back(mq)\n            mask = mq[:, :, :, None] * mkv[:, :, None, :]\n            dots.masked_fill_(~mask, masked_value)\n            del mask\n\n        # Causal masking\n        if self.causal:\n            mask = bq_t[:, :, :, None] < bkv_t[:, :, None, :].clamp(max=query_len - 1)\n            dots.masked_fill_(mask, masked_value)\n            del mask\n\n        # Mask out attention to self except when no other targets are available.\n        self_mask = bq_t[:, :, :, None] == bkv_t[:, :, None, :]\n        dots.masked_fill_(self_mask, TOKEN_SELF_ATTN_VALUE)\n        del self_mask\n\n        # Mask out attention to other hash buckets.\n        if not self._attend_across_buckets:\n            bq_buckets = bkv_buckets = torch.reshape(sbuckets_and_t \/\/ seqlen, (batch_size, chunk_size, -1))\n            bkv_buckets = look_one_back(bkv_buckets)\n            bucket_mask = bq_buckets[:, :, :, None] != bkv_buckets[:, :, None, :]\n            dots.masked_fill_(bucket_mask, masked_value)\n            del bucket_mask\n\n        # Don't double-count query-key pairs across multiple rounds of hashing.\n        # There are two possible strategies here. (1) The default is to count how\n        # many times a query-key pair is repeated, and to lower its log-prob\n        # correspondingly at each repetition. (2) When hard_k is set, the code\n        # instead masks all but the first occurence of each query-key pair.\n        if not self._allow_duplicate_attention:\n            locs1 = undo_sort \/\/ bq_t.shape[-1]\n            locs2 = (locs1 + 1) % chunk_size\n            if not self._attend_across_buckets:\n                locs1 = buckets * chunk_size + locs1\n                locs2 = buckets * chunk_size + locs2\n            locs = torch.cat([\n                torch.reshape(locs1, (batch_size, self.n_hashes, seqlen)),\n                torch.reshape(locs2, (batch_size, self.n_hashes, seqlen)),\n            ], 1).permute((0, 2, 1))\n\n            slocs = batched_index_select(locs, st)\n            b_locs = torch.reshape(slocs, (batch_size, chunk_size, -1, 2 * self.n_hashes))\n\n            b_locs1 = b_locs[:, :, :, None, :self.n_hashes]\n\n            bq_locs = b_locs1.expand(b_locs.shape[:3] + (2, self.n_hashes))\n            bq_locs = torch.reshape(bq_locs, b_locs.shape)\n            bkv_locs = look_one_back(b_locs)\n\n            dup_counts = (bq_locs[:, :, :, None, :] == bkv_locs[:, :, None, :, :])\n            # for memory considerations, chunk summation of last dimension for counting duplicates\n            dup_counts = chunked_sum(dup_counts, chunks=(self.n_hashes * batch_size))\n            dup_counts = dup_counts.detach()\n            assert dup_counts.shape == dots.shape\n            dots = dots - torch.log(dup_counts + 1e-9)\n            del dup_counts\n\n        # Softmax.\n        dots_logsumexp = torch.logsumexp(dots, dim=-1, keepdim=True)\n        dots = torch.exp(dots - dots_logsumexp).type(dots.type())\n        dots = self.dropout(dots)\n\n        bo = torch.einsum('buij,buje->buie', dots, bv)\n        so = torch.reshape(bo, (batch_size, -1, dim))\n        slogits = torch.reshape(dots_logsumexp, (batch_size, -1,))\n\n        class UnsortLogits(Function):\n            @staticmethod\n            def forward(ctx, so, slogits):\n                so = so.detach()\n                slogits = slogits.detach()\n                o = batched_index_select(so, undo_sort)\n                _, logits = sort_key_val(sticker, slogits, dim=-1)\n                return o, logits\n\n            @staticmethod\n            def backward(ctx, grad_x, grad_y):\n                so_grad = batched_index_select(grad_x, sticker)\n                _, slogits_grad = sort_key_val(buckets_and_t, grad_y, dim=-1)\n                return so_grad, slogits_grad\n\n        o, logits = UnsortLogits.apply(so, slogits)\n        o = torch.reshape(o, (batch_size, self.n_hashes, seqlen, dim))\n        logits = torch.reshape(logits, (batch_size, self.n_hashes, seqlen, 1))\n\n        if query_len != seqlen:\n            query_slice = (slice(None), slice(None), slice(0, query_len))\n            o, logits = o[query_slice], logits[query_slice]\n\n        probs = torch.exp(logits - torch.logsumexp(logits, dim=1, keepdim=True))\n        out = torch.sum(o * probs, dim=1)\n\n        # compute unsorted matrix\n        attn = torch.empty(0, device=device)\n\n        if self._return_attn:\n            attn_unsort = ((bq_t * seqlen)[:, :, :, None] + bkv_t[:, :, None, :])\n            attn_unsort = attn_unsort.view(-1, 2 * self.bucket_size * self.bucket_size)\n            unsorted_dots = scatter_sum(dots.view_as(attn_unsort), attn_unsort)\n            unsorted_dots = unsorted_dots.reshape(batch_size, self.n_hashes, n_buckets, seqlen, seqlen).sum(dim=2)\n            attn = torch.sum(unsorted_dots[:, :, 0:query_len, :] * probs, dim=1)\n\n        # return output, attention matrix, and bucket distribution\n        return out, attn, buckets\n"}