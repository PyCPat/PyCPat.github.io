{"BEFORE":"        check_shape(ids, 'b s d') # b - batch, s - spatial dimension, d - depth dimension\n\n        b, space, depth, device = *ids.shape, ids.device\n        assert space <= self.max_spatial_seq_len, 'spatial dimension is greater than the max_spatial_seq_len set'\n        assert depth <= self.max_depth_seq_len, 'depth dimension is greater than the max_depth_seq_len set'\n\n        tokens = self.token_emb(ids)\n\n        spatial_pos = self.spatial_pos_emb(torch.arange(space, device = device))\n        depth_pos = self.depth_pos_emb(torch.arange(depth, device = device))\n\n        tokens_with_depth_pos = tokens + depth_pos\n\n        # spatial tokens is tokens with depth pos reduced along depth dimension + spatial positions\n        spatial_tokens = reduce(tokens_with_depth_pos, 'b s d f -> b s f', 'sum') + spatial_pos\n\n        spatial_tokens = torch.cat((\n            repeat(self.spatial_start_token, 'f -> b 1 f', b = b),\n            spatial_tokens\n        ), dim = -2)\n\n        spatial_tokens = spatial_tokens[:, :-1]\n\n        spatial_tokens = self.spatial_transformer(spatial_tokens)\n\n        spatial_tokens = rearrange(spatial_tokens, 'b s f -> b s 1 f')\n\n        # spatial tokens become the start tokens of the depth dimension\n\n        depth_tokens = torch.cat((spatial_tokens, tokens_with_depth_pos), dim = -2)\n        depth_tokens = depth_tokens[:, :, :-1]\n\n        depth_tokens = rearrange(depth_tokens, '... n d -> (...) n d')\n\n        depth_tokens = self.depth_transformer(depth_tokens)\n\n        depth_tokens = rearrange(depth_tokens, '(b s) d f -> b s d f', b = b)\n        logits = self.to_logits(depth_tokens)\n\n        if not return_loss:\n            return logits\n\n        assert self.training\n\n        preds = rearrange(logits, 'b s d c -> b c (s d)')\n","AFTER":"        assert ids.ndim in {2, 3}\n        ids_orig_ndim = ids.ndim\n\n        if ids.ndim == 2:\n            # allow for ids to be given in the shape of (batch, seq)\n            # in which case it will be auto-padded to the next nearest multiple of depth seq len\n            seq_len = ids.shape[-1]\n            padding = remainder_to_mult(seq_len, self.depth_seq_len)\n            ids = F.pad(ids, (0, padding), value = self.pad_id)\n            ids = rearrange(ids, 'b (s d) -> b s d', d = self.depth_seq_len)\n\n        b, space, depth, device = *ids.shape, ids.device\n        assert space <= self.max_spatial_seq_len, 'spatial dimension is greater than the max_spatial_seq_len set'\n        assert depth == self.depth_seq_len, 'depth dimension must be equal to depth_seq_len'\n\n        tokens = self.token_emb(ids)\n\n        spatial_pos = self.spatial_pos_emb(torch.arange(space, device = device))\n        depth_pos = self.depth_pos_emb(torch.arange(depth, device = device))\n\n        tokens_with_depth_pos = tokens + depth_pos\n\n        # spatial tokens is tokens with depth pos reduced along depth dimension + spatial positions\n\n        spatial_tokens = reduce(tokens_with_depth_pos, 'b s d f -> b s f', 'sum') + spatial_pos\n\n        spatial_tokens = torch.cat((\n            repeat(self.spatial_start_token, 'f -> b 1 f', b = b),\n            spatial_tokens\n        ), dim = -2)\n\n        spatial_tokens = spatial_tokens[:, :-1]\n\n        spatial_tokens = self.spatial_transformer(spatial_tokens)\n\n        spatial_tokens = rearrange(spatial_tokens, 'b s f -> b s 1 f')\n\n        # spatial tokens become the start tokens of the depth dimension\n\n        depth_tokens = torch.cat((spatial_tokens, tokens_with_depth_pos), dim = -2)\n        depth_tokens = depth_tokens[:, :, :-1]\n\n        depth_tokens = rearrange(depth_tokens, '... n d -> (...) n d')\n\n        depth_tokens = self.depth_transformer(depth_tokens)\n\n        depth_tokens = rearrange(depth_tokens, '(b s) d f -> b s d f', b = b)\n        logits = self.to_logits(depth_tokens)\n\n        if ids_orig_ndim == 2:\n            logits = rearrange(logits, 'b ... n -> b (...) n')\n            logits = logits[:, :seq_len]\n\n        if not return_loss:\n            return logits\n\n        assert self.training\n\n        preds = rearrange(logits, 'b ... c -> b c (...)')\n\n        labels = rearrange(ids, 'b s d -> b (s d)')\n        labels = labels[:, :preds.shape[-1]]\n"}