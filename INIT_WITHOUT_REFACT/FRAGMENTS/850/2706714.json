{"BEFORE":"        if self.training or self.normal_instance_normalization:\n            return self.forward_normal(x)\n\n        else:\n            assert y_anchor is not None\n            assert x_anchor is not None\n\n            if self.collection_mode:\n                x_var, x_mean = torch.var_mean(x, dim=(2, 3))  # [B, C]\n                x_std = torch.sqrt(x_var + self.eps)\n                # x_anchor, y_anchor = [B], [B]\n                # table = [H, W, C]\n                # update std and mean to corresponing coordinates\n                self.mean_table[y_anchor, x_anchor] = x_mean\n                self.std_table[y_anchor, x_anchor] = x_std\n                x_mean = x_mean.unsqueeze(-1).unsqueeze(-1)\n                x_std = x_std.unsqueeze(-1).unsqueeze(-1)\n\n            else:\n\n                def multiply_kernel(x):\n                    x = x * self.kernel  # [1, C, H, W] * [H, W] = [1, C, H, W]\n                    x = x.sum(dim=(2, 3), keepdim=True)  # [1, C, 1, 1]\n                    return x\n\n                # currently, could support batch size = 1 for\n                # kernelized instance normalization\n                assert x.shape[0] == 1\n\n                top = y_anchor\n                down = y_anchor + 2 * self.padding + 1\n                left = x_anchor\n                right = x_anchor + 2 * self.padding + 1\n                x_mean = self.padded_mean_table[\n                    :, :, top:down, left:right\n                ]  # 1, C, H, W\n                x_std = self.padded_std_table[\n                    :, :, top:down, left:right\n                ]  # 1, C, H, W\n                x_mean = multiply_kernel(x_mean)\n                x_std = multiply_kernel(x_std)\n\n            x = (x - x_mean) \/ x_std * self.weight + self.bias\n","AFTER":"    def forward(self, x, y_anchor=None, x_anchor=None, mode=1):\n        if self.training or x_anchor is None or y_anchor is None:\n            return self.forward_normal(x)\n\n        else:\n            if mode == self.Mode.PHASE_CACHING:\n                x_var, x_mean = torch.var_mean(x, dim=(2, 3))  # [B, C]\n                x_std = torch.sqrt(x_var + self.eps)\n                # x_anchor, y_anchor = [B], [B]\n                # table = [H, W, C]\n                # update std and mean to corresponing coordinates\n                self.mean_table[y_anchor, x_anchor] = x_mean\n                self.std_table[y_anchor, x_anchor] = x_std\n                x_mean = x_mean.unsqueeze(-1).unsqueeze(-1)\n                x_std = x_std.unsqueeze(-1).unsqueeze(-1)\n\n            elif mode == self.Mode.PHASE_INFERENCE:\n\n                def multiply_kernel(x):\n                    x = x * self.kernel  # [1, C, H, W] * [H, W] = [1, C, H, W]\n                    x = x.sum(dim=(2, 3), keepdim=True)  # [1, C, 1, 1]\n                    return x\n\n                # currently, could support batch size = 1 for\n                # kernelized instance normalization\n                assert x.shape[0] == 1\n\n                top = y_anchor\n                down = y_anchor + 2 * self.padding + 1\n                left = x_anchor\n                right = x_anchor + 2 * self.padding + 1\n                x_mean = self.padded_mean_table[\n                    :, :, top:down, left:right\n                ]  # 1, C, H, W\n                x_std = self.padded_std_table[\n                    :, :, top:down, left:right\n                ]  # 1, C, H, W\n                x_mean = multiply_kernel(x_mean)\n                x_std = multiply_kernel(x_std)\n\n            else:\n                raise ValueError(f'Unknown mode: {mode}.')\n\n            x = (x - x_mean) \/ x_std * self.weight + self.bias\n"}