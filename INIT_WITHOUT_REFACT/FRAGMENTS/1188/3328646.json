{"BEFORE":"            attn = LSHSelfAttention(emb, heads, bucket_size, n_hashes, causal = causal)\n            ff_net = FeedForward(emb)\n","AFTER":"    def __init__(self, emb, depth, max_seq_len, num_tokens = 10000, heads = 8, bucket_size = 64, n_hashes = 8, ff_chunks = 100, causal = False, weight_tie = False):\n        super().__init__()\n        self.emb = emb\n        self.depth = depth\n        self.token_emb = nn.Embedding(num_tokens, emb)\n        self.pos_emb = nn.Embedding(max_seq_len, emb)\n\n        get_attn = lambda: LSHSelfAttention(emb, heads, bucket_size, n_hashes, causal = causal)\n        get_ff = lambda: FeedForward(emb)\n\n        if weight_tie:\n            get_attn = cache_fn(get_attn)\n            get_ff = cache_fn(get_ff)\n\n        blocks = []\n\n        for _ in range(depth):\n            attn = get_attn()\n            ff_net = get_ff()\n"}