{"BEFORE":"        self.pos_emb = nn.Parameter(torch.randn(n, m, dim_k, dim_u))\n","AFTER":"        self.local_contexts = exists(r)\n        if exists(r):\n            assert (r % 2) == 1, 'Receptive kernel size should be odd'\n            self.padding = r \/\/ 2\n            self.R = nn.Parameter(torch.randn(dim_k, dim_u, 1, r, r))\n        else:\n            assert exists(n), 'You must specify the total sequence length (h x w)'\n            self.pos_emb = nn.Parameter(torch.randn(n, n, dim_k, dim_u))\n\n\n    def forward(self, x):\n"}