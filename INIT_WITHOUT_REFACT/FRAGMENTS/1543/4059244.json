{"BEFORE":"        self.num_layers = num_layers\n        self.num_features = num_features\n        self.layers = nn.ModuleList()\n\n        for i in range(num_layers - 1):\n            self.layers.append(\n                GINConv(num_features[i], num_features[i + 1], activation=activation))\n        self.linear1 = nn.Linear(num_features[-2], num_features[-2])\n        self.linear2 = nn.Linear(num_features[-2], num_features[-1])\n","AFTER":"    def __init__(self, in_features, out_features, hidden_features, activation=F.relu, dropout=True):\n        super(GIN, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        if type(hidden_features) is int:\n            hidden_features = [hidden_features]\n        self.layers = nn.ModuleList()\n\n        self.layers.append(GINConv(in_features, hidden_features[0], activation=activation, dropout=dropout))\n        for i in range(len(hidden_features) - 1):\n            self.layers.append(\n                GINConv(hidden_features[i], hidden_features[i + 1], activation=activation))\n        self.linear1 = nn.Linear(hidden_features[-2], hidden_features[-1])\n        self.linear2 = nn.Linear(hidden_features[-1], out_features)\n"}