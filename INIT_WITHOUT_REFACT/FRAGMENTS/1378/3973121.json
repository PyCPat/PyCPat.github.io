{"BEFORE":"            attns\r\n        ) = predictions\r\n        src_masks = ~src_masks\r\n        mel_masks = ~mel_masks\r\n        mel_targets = mel_targets[:, : mel_masks.shape[1], :]\r\n        mel_masks = mel_masks[:, :mel_masks.shape[1]]\r\n\r\n        src_lens_targets.requires_grad = False\r\n        mel_targets.requires_grad = False\r\n        mel_lens_targets.requires_grad = False\r\n\r\n        # Iterative Loss Using Soft-DTW\r\n        mel_iter_loss = 0\r\n        mel_targets_comp = torch.sigmoid(mel_targets)\r\n        for mel_iter in mel_iters:\r\n            mel_iter_comp = torch.sigmoid(mel_iter)\r\n            mel_iter_loss += self.sdtw_loss(mel_iter_comp, mel_targets_comp).mean()\r\n        mel_loss = (mel_iter_loss \/ (self.L * mel_lens_targets)).mean()\r\n\r\n        # Duration Loss\r\n        duration_loss = self.lambda_ * (self.mae_loss((torch.exp(log_durations) - 1).sum(-1), mel_lens_targets) \/ src_lens_targets).mean()\r\n\r\n        # KL Divergence Loss\r\n        beta = torch.tensor(self.kl_anneal(step))\r\n        log_vars, mus = log_vars.masked_select(src_masks.unsqueeze(-1)), mus.masked_select(src_masks.unsqueeze(-1))\r\n        kl_loss = -0.5 * torch.sum(1 + log_vars - mus.pow(2) - log_vars.exp())\r\n\r\n        # Residual Attention Loss\r\n        attn_loss = self.guided_loss(attns.transpose(-2, -1), src_lens_targets, mel_lens_targets)\r\n\r\n        total_loss = (\r\n            mel_loss + duration_loss + beta * kl_loss + attn_loss\r\n        )\r\n\r\n        return (\r\n            total_loss,\r\n            mel_loss,\r\n            duration_loss,\r\n            kl_loss,\r\n            attn_loss,\r\n            beta,\r\n        )\r\n","AFTER":"            _,\r\n            _,\r\n        ) = predictions\r\n        src_masks = ~src_masks\r\n        mel_masks = ~mel_masks\r\n        mel_targets = mel_targets[:, : mel_masks.shape[1], :]\r\n        mel_masks = mel_masks[:, :mel_masks.shape[1]]\r\n\r\n        src_lens_targets.requires_grad = False\r\n        mel_targets.requires_grad = False\r\n        mel_lens_targets.requires_grad = False\r\n\r\n        # Iterative Loss Using Soft-DTW\r\n        mel_iter_loss = 0\r\n        mel_targets_comp = torch.sigmoid(mel_targets)\r\n        for mel_iter in mel_iters:\r\n            mel_iter_comp = torch.sigmoid(mel_iter)\r\n            mel_iter_loss += self.sdtw_loss(mel_iter_comp, mel_targets_comp).mean()\r\n            # mel_iter_loss += self.sdtw_loss(mel_iter, mel_targets).mean()\r\n        mel_loss = (mel_iter_loss \/ (len(mel_iters) * mel_lens_targets)).mean()\r\n\r\n        # Duration Loss\r\n        duration_loss = self.lambda_ * (self.mae_loss(durations.sum(-1), mel_lens_targets) \/ src_lens_targets).mean()\r\n\r\n        # KL Divergence Loss\r\n        beta = torch.tensor(self.kl_anneal(step))\r\n        kl_loss = -0.5 * torch.sum(1 + log_vars - mus.pow(2) - log_vars.exp())\r\n\r\n        total_loss = (\r\n            mel_loss + duration_loss + beta * kl_loss\r\n        )\r\n\r\n        return (\r\n            total_loss,\r\n            mel_loss,\r\n            duration_loss,\r\n            kl_loss,\r\n            beta,\r\n        )\r\n"}