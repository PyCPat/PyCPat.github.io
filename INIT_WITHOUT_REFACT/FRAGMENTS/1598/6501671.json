{"BEFORE":"        total_loss = torch.tensor(0., **to(x))\n\n        if has_local:\n            local_out = self.local_attn(lqk, lqk, lv, input_mask = input_mask)\n            out.append(local_out)\n\n        if has_global:\n            global_out, loss = self.global_attn(qk, v, input_mask = input_mask)\n            total_loss = total_loss + loss\n            out.append(global_out)\n\n        out = torch.cat(out, dim=1)\n        out = out.reshape(b, h, t, -1).transpose(1, 2).reshape(b, t, -1)\n        out = self.to_out(out)\n        return self.dropout(out), loss\n","AFTER":"        total_loss = torch.tensor(0., **to(x)).requires_grad_()\n\n        if has_local:\n            local_out = self.local_attn(lqk, lqk, lv, input_mask = input_mask)\n            out.append(local_out)\n\n        if has_global:\n            global_out, loss = self.global_attn(qk, v, input_mask = input_mask)\n            total_loss = total_loss + loss\n            out.append(global_out)\n\n        out = torch.cat(out, dim=1)\n        out = out.reshape(b, h, t, -1).transpose(1, 2).reshape(b, t, -1)\n        out = self.to_out(out)\n        return self.dropout(out), total_loss\n"}