{"BEFORE":"        encodings = self.encoder(smiles)\n        classifier_outputs = [self.classifiers[i](smiles) for i in range(self.num_sources)]\n\n        source_ids = range(self.num_sources)\n        source_alphas = [-self.mahalanobis_metric(encodings, \n                            self.domain_encs[i], i)\n                            for i in source_ids]\n        source_alphas = F.softmax(source_alphas)\n\n        output_moe = sum([ alpha.unsqueeze(1).repeat(1, 1) *\n                                F.softmax(classifier_outputs[id], dim=1)\n                        for alpha, id in zip(source_alphas, source_ids)])\n","AFTER":"        encodings = self.encoder(smiles)\n        classifier_outputs = [self.classifiers[i](encodings) for i in range(self.num_sources)]\n\n        source_ids = range(self.num_sources)\n        source_alphas = [-self.mahalanobis_metric(encodings, \n                            self.domain_encs[i], i).unsqueeze(0)\n                            for i in source_ids]\n        source_alphas = F.softmax(torch.cat(source_alphas, dim=0), dim=0) # n_source x bs\n        output_moe = sum([ source_alphas[j].unsqueeze(1).repeat(1, 1) *\n                            classifier_outputs[j] for j in source_ids])\n"}