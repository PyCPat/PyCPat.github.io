{"BEFORE":"        seq = torch.arange(n, device = device)\n        dist = seq[:, None] - seq[None, :]\n        dist = dist.clip(-max_pos_emb, max_pos_emb) + max_pos_emb\n        rel_pos_emb = self.rel_pos_emb(dist).to(q)\n        pos_attn = einsum('b h n d, n r d -> b h n r', q, rel_pos_emb) * self.scale\n        dots = dots + pos_attn\n\n        if exists(mask) or exists(context_mask):\n            mask = default(mask, lambda: torch.ones(*x.shape[:2], device = device))\n            context_mask = default(context_mask, mask) if not has_context else default(context_mask, lambda: torch.ones(*context.shape[:2], device = device))\n            mask_value = -torch.finfo(dots.dtype).max\n            mask = mask[:, None, :, None] * context_mask[:, None, None, :]\n","AFTER":"        seq = torch.arange(n, device = device)\n        dist = rearrange(seq, 'i -> i ()') - rearrange(seq, 'j -> () j')\n        dist = dist.clip(-max_pos_emb, max_pos_emb) + max_pos_emb\n        rel_pos_emb = self.rel_pos_emb(dist).to(q)\n        pos_attn = einsum('b h n d, n r d -> b h n r', q, rel_pos_emb) * self.scale\n        dots = dots + pos_attn\n\n        if exists(mask) or exists(context_mask):\n            mask = default(mask, lambda: torch.ones(*x.shape[:2], device = device))\n            context_mask = default(context_mask, mask) if not has_context else default(context_mask, lambda: torch.ones(*context.shape[:2], device = device))\n            mask_value = -torch.finfo(dots.dtype).max\n            mask = rearrange(mask, 'b i -> b () i ()') * rearrange(context_mask, 'b j -> b () () j')\n"}