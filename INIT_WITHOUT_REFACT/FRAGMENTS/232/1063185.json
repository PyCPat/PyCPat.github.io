{"BEFORE":"        super(Tokenizer, self).__init__()\n\n        n_filter_list = [n_input_channels] + \\\n                        [in_planes for _ in range(n_conv_layers - 1)] + \\\n                        [n_output_channels]\n\n        self.conv_layers = nn.Sequential(\n            *[nn.Sequential(\n                nn.Conv2d(n_filter_list[i], n_filter_list[i + 1],\n                          kernel_size=(kernel_size, kernel_size),\n                          stride=(stride, stride),\n                          padding=(padding, padding), bias=conv_bias),\n                nn.Identity() if activation is None else activation(),\n                nn.MaxPool2d(kernel_size=pooling_kernel_size,\n                             stride=pooling_stride,\n                             padding=pooling_padding) if max_pool else nn.Identity()\n            )\n                for i in range(n_conv_layers)\n            ])\n\n        self.flattener = nn.Flatten(2, 3)\n","AFTER":"        super().__init__()\n\n        n_filter_list = [n_input_channels] + \\\n                        [in_planes for _ in range(n_conv_layers - 1)] + \\\n                        [n_output_channels]\n\n        n_filter_list_pairs = zip(n_filter_list[:-1], n_filter_list[1:])\n\n        self.conv_layers = nn.Sequential(\n            *[nn.Sequential(\n                nn.Conv2d(chan_in, chan_out,\n                          kernel_size=(kernel_size, kernel_size),\n                          stride=(stride, stride),\n                          padding=(padding, padding), bias=conv_bias),\n                nn.Identity() if not exists(activation) else activation(),\n                nn.MaxPool2d(kernel_size=pooling_kernel_size,\n                             stride=pooling_stride,\n                             padding=pooling_padding) if max_pool else nn.Identity()\n            )\n                for chan_in, chan_out in n_filter_list_pairs\n"}