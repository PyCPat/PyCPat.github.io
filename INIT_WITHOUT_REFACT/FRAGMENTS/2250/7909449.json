{"BEFORE":"        past_key_values=None,\n    ):\n        hidden_states = self.wte(input_ids) + self.wpe(position_ids)\n        if self.tp_embeddings:\n            torch.distributed.all_reduce(hidden_states, group=self.process_group)\n\n        # Prefill\n        if past_key_values is None:\n            # Create past tensor\n            past_key_values = hidden_states.new_empty(\n                (\n                    len(self.h),\n                    len(hidden_states),\n                    2,\n                    1,\n                    self.head_size,\n                )\n            )\n            layer_past_present_indices = None\n            cu_seqlens_q = None\n        # Decode\n        else:\n            # Create indices from cumulative sequence lengths\n            layer_past_present_indices = cu_seqlens[1:] - 1\n            cu_seqlens_q = torch.arange(\n                cu_seqlens.shape[0], dtype=torch.int32, device=hidden_states.device\n            )\n\n        residual = None\n        for i, layer in enumerate(self.h):\n            hidden_states, residual = layer(\n                hidden_states,\n                residual,\n                cu_seqlens,\n                max_s,\n                past_key_values[i],\n                layer_past_present_indices,\n                cu_seqlens_q,\n            )\n","AFTER":"        past_key_values: Optional[torch.Tensor] = None,\n        pre_allocate_past_size: Optional[int] = None,\n    ):\n        hidden_states = self.wte(input_ids) + self.wpe(position_ids)\n        if self.tp_embeddings:\n            torch.distributed.all_reduce(hidden_states, group=self.process_group)\n\n        # Prefill\n        if past_key_values is None:\n            # Create past tensor\n            past_key_values = hidden_states.new_empty(\n                (\n                    len(self.h),\n                    len(hidden_states)\n                    if pre_allocate_past_size is None\n                    else pre_allocate_past_size,\n                    2,\n                    1,\n                    self.head_size,\n                )\n            )\n            layer_past_present_indices = None\n            cu_seqlens_q = None\n            slice_past_index = len(hidden_states)\n        # Decode\n        else:\n            # Create indices from cumulative sequence lengths\n            layer_past_present_indices = cu_seqlens[1:] - 1\n            cu_seqlens_q = torch.arange(\n                cu_seqlens.shape[0], dtype=torch.int32, device=hidden_states.device\n            )\n            slice_past_index = None\n\n        residual = None\n        for i, layer in enumerate(self.h):\n            # We added padding that now need to slice\n            layer_past_key_values = (\n                past_key_values[i]\n                if slice_past_index is None\n                else past_key_values[i, :slice_past_index]\n            )\n\n            hidden_states, residual = layer(\n                hidden_states,\n                residual,\n                cu_seqlens,\n                max_s,\n                layer_past_key_values,\n                layer_past_present_indices,\n                cu_seqlens_q,\n            )\n"}