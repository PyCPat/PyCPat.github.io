{"BEFORE":"        quantizeds = list()\n        codes = list()\n        # logits = list()\n        for i, (xRaw, k) in enumerate(zip(latents, self._k)):\n            n, c, h, w = xRaw.shape\n            # [n, c, h, w] -> [h, w, n, c]\n            \"\"\" *************** TODO: NEED DETACH? ******************* \"\"\"\n            # encoderIn = xRaw.detach().permute(2, 3, 0, 1)\n            encoderIn = xRaw.permute(2, 3, 0, 1)\n            # [h, w, n, c] -> [h*w, n, c]\n            x = encoderIn.reshape(-1, n ,c)\n            # similar to scaled dot-product attention\n            # [h*w, N, Cin],    M * [h*w, n, k]\n            quantized, samples, logits = self._attention(x, temp, True)\n            # [h*w, n, c] -> [n, c, h*w] -> [n, c, h, w]\n            deTransformed = quantized.reshape(h, w, n, c).permute(2, 3, 0, 1)\n\n            # mask = torch.rand_like(xRaw) > coeff\n            # mixed = mask * xRaw.detach() + torch.logical_not(mask) * deTransformed\n            # [n, c, h, w]\n            quantizeds.append(deTransformed)\n            samples = [s.argmax(-1).permute(1, 0).reshape(n, h, w) for s in samples]\n            logits = [l.permute(1, 0, 2).reshape(n, h, w, k) for l in logits]\n            # codes.append(samples.argmax(-1).permute(1, 0).reshape(n, h, w))\n            # logits.append(logit.permute(1, 0, 2).reshape(n, h, w, k))\n        return quantizeds, codes, logits\n","AFTER":"        quantizeds = list()\n        codes = list()\n        softQs = list()\n        # logits = list()\n        for _, (xRaw, k) in enumerate(zip(latents, self._k)):\n            n, c, h, w = xRaw.shape\n            # [n, c, h, w] -> [h, w, n, c]\n            \"\"\" *************** TODO: NEED DETACH? ******************* \"\"\"\n            # encoderIn = xRaw.detach().permute(2, 3, 0, 1)\n            encoderIn = xRaw.permute(2, 3, 0, 1)\n            # [h, w, n, c] -> [h*w, n, c]\n            x = encoderIn.reshape(-1, n ,c)\n            # similar to scaled dot-product attention\n            # [h*w, N, Cin],    M * [h*w, n, k]\n            quantized, samples, softQ, logits = self._attention(x, temp, True)\n            # [h*w, n, c] -> [n, c, h*w] -> [n, c, h, w]\n            deTransformed = quantized.reshape(h, w, n, c).permute(2, 3, 0, 1)\n\n            # mask = torch.rand_like(xRaw) > coeff\n            # mixed = mask * xRaw.detach() + torch.logical_not(mask) * deTransformed\n            # [n, c, h, w]\n            quantizeds.append(deTransformed)\n            softQs.append(softQ.reshape(h, w, n, c).permute(2, 3, 0, 1))\n            samples = [s.argmax(-1).permute(1, 0).reshape(n, h, w) for s in samples]\n            logits = [l.permute(1, 0, 2).reshape(n, h, w, k) for l in logits]\n            # codes.append(samples.argmax(-1).permute(1, 0).reshape(n, h, w))\n            # logits.append(logit.permute(1, 0, 2).reshape(n, h, w, k))\n        return quantizeds, softQs, codes, logits\n"}