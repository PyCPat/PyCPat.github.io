{"BEFORE":"        cap_len = len(tokenized_cap['input_ids'])\n\n        # Encode the video\n        # [N,T,H,W,C] -> [batch_size, channel, temporal_dim, height, width]\n        video = rearrange(video, 'n t h w c -> n c t h w')\n        enc_video = self.encoder(video)\n        enc_video = rearrange(enc_video, 'n c t h w -> t n h w c')\n        # enc_video :torch.Size([N, 1024, T\/2, H\/32, W\/32])\n        mylogger.debug(\"enc_video: {}\".format(str(enc_video.shape)))  # torch.Size([2, 20, 7, 7, 768])\n\n        # Decode\n        if mode == \"train\":\n            # embedding the caption\n            # enc_caption: torch.Size([N, max_len, 768])\n            enc_caption = self.embedding(**tokenized_cap).last_hidden_state\n            enc_caption = rearrange(enc_caption, 'n t c -> t n c')\n            _, tgt_mask, _, tgt_padding_mask = create_mask(enc_video,\n                                                           enc_caption,\n                                                           PAD_IDX=self.tokenizer.convert_tokens_to_ids('[PAD]'))\n            mylogger.debug(\"enc_caption: {}\".format(enc_caption.shape))  # torch.Size([2, 25, 768])\n            mylogger.debug(\"tgt_mask: {}\".format(tgt_mask.shape))\n            mylogger.debug(\"tgt_key_padding_mask: {}\".format(tgt_padding_mask.shape))\n\n            # dec_caption: (T, N, E)\n            dec_caption = self.decoder(tgt=enc_caption, memory=enc_video,\n                                       tgt_mask=tgt_mask,\n                                       tgt_key_padding_mask=tgt_padding_mask)\n            mylogger.debug(\"dec_caption: {}\".format(str(dec_caption.shape)))\n\n            # result: (T, N, 1)\n            prob = self.out_linear(self.out_drop(dec_caption))  # Tensor(N,T,vocab_size)\n            return prob\n        else:\n            pad_id = self.tokenizer.convert_tokens_to_ids('[PAD]')\n            cls_id = self.tokenizer.convert_tokens_to_ids('[CLS]')\n            current_word = torch.ones([batch_size, 1]) * cls_id  # [B, 1]\n\n            words = []\n            cls_count = 0\n            for i in range(self.max_out_len - 1):\n                # generate mask\n                cap_padding_mask = (current_word == pad_id)\n                # embedding current word\n                embed_current_word = self.embedding(current_word).last_hidden_state\n                # dec_caption: (1, N, E)\n                dec_caption = self.decoder(tgt=embed_current_word, memory=enc_video,\n                                           tgt_key_padding_mask=cap_padding_mask)\n                prob = self.out_linear(self.out_drop(dec_caption))  # prob(N,1,vocab_size)\n                _, current_word = torch.max(prob, dim=1)  # current_word[N,1]\n                words.append(current_word)\n                # break if all captions reach [CLS]\n                cls_count += torch.sum((current_word == cls_id).to(dtype=torch.int))\n                if cls_count >= batch_size:\n                    break\n            words = torch.stack(words)\n","AFTER":"        enc_video = self.encoder(video)\n        # mylogger.debug(\"enc_video: {}\".format(str(enc_video.shape)))  # enc_video: torch.Size([2, 768, 20, 7, 7])\n        enc_video = self.avg_pool(enc_video).squeeze_()\n        enc_video = rearrange(enc_video, 'n c t-> t n c')\n        mylogger.debug(\"enc_video: {}\".format(str(enc_video.shape)))  # torch.Size([20, 2, 7, 7, 768])\n\n        # Decode\n        if mode == \"train\":\n            # embedding the caption\n            # enc_caption: torch.Size([N, max_len, 768])\n            enc_caption = self.embedding(**tokenized_cap).last_hidden_state\n            enc_caption = rearrange(enc_caption, 'n t c -> t n c')\n            _, tgt_mask, _, tgt_padding_mask = create_mask(enc_video,\n                                                           tokenized_cap['input_ids'],\n                                                           PAD_IDX=self.tokenizer.convert_tokens_to_ids('[PAD]'))\n            mylogger.debug(\"enc_caption: {}\".format(enc_caption.shape))  # enc_caption: torch.Size([25, 2, 768])\n            mylogger.debug(\"tgt_mask: {}\".format(tgt_mask.shape))  # tgt_mask: torch.Size([25, 25])\n            mylogger.debug(\"tgt_key_padding_mask: {}\".format(tgt_padding_mask.shape))  # tgt_key_padding_mask: torch.Size([2, 25])\n\n            # dec_caption: (T, N, E)\n            dec_caption = self.decoder(tgt=enc_caption, memory=enc_video,\n                                       tgt_mask=tgt_mask,\n                                       tgt_key_padding_mask=tgt_padding_mask)\n            mylogger.debug(\"dec_caption: {}\".format(str(dec_caption.shape)))  # dec_caption: torch.Size([25, 2, 768])\n\n            # result: (T, N, 1)\n            prob = self.out_linear(self.out_drop(dec_caption))  # prob torch.Size([25, 2, 30522])\n            return prob\n        else:\n            pad_id = self.tokenizer.convert_tokens_to_ids('[PAD]')\n            cls_id = self.tokenizer.convert_tokens_to_ids('[CLS]')\n            current_word = torch.ones([batch_size, 1]) * cls_id  # [B, 1]\n\n            words = []\n            cls_count = 0\n            for i in range(self.max_out_len - 1):\n                # generate mask\n                cap_padding_mask = (current_word == pad_id).transpose(1, 0)\n                mylogger.debug(\"cap_padding_mask: {}\".format(str(cap_padding_mask.shape)))  # cap_padding_mask: torch.Size([2, 1])\n\n                # embedding current word\n                current_word = current_word.to(torch.int)\n                mylogger.debug(\"current_word: {}\".format(str(current_word.shape)))\n                embed_current_word = self.embedding(input_ids=current_word).last_hidden_state\n                mylogger.debug(\"embed_current_word: {}\".format(str(embed_current_word.shape)))  # embed_current_word: torch.Size([2, 1, 768])\n\n                # dec_caption: (1, N, E)\n                dec_caption = self.decoder(tgt=embed_current_word, memory=enc_video,\n                                           tgt_key_padding_mask=cap_padding_mask)\n                mylogger.debug(\"dec_caption: {}\".format(str(dec_caption.shape)))  # dec_caption: torch.Size([2, 1, 768])\n\n                prob = self.out_linear(self.out_drop(dec_caption))\n                mylogger.debug(\"prob: {}\".format(str(prob.shape)))  # prob torch.Size([25, 2, 30522])\n                _, current_word = torch.max(prob, dim=2)\n                mylogger.debug(\"current_word: {}\".format(str(current_word.shape)))  # current_word: torch.Size([2, 1])\n                words.append(current_word)\n\n                # break if all captions reach [CLS]\n                cls_count += torch.sum((current_word == cls_id).to(dtype=torch.int))\n                if cls_count >= batch_size:\n                    break\n            words = torch.stack(words).transpose(0, 1).squeeze_()\n"}