{"BEFORE":"    def forward(self, images, restored, c1, c2, predict, l1, l2):\n        device = images.device\n\n        _, _, h, w = c1.shape\n\n        l2Loss = 0.0 # F.mse_loss(restored, images)\n        l1Loss = 0.0 # F.l1_loss(restored, images)\n        ssimLoss = 1 - self._msssim(restored + 1, images + 1)\n        # [N, K, M, H, W], [N, M, H, W]\n        contextLoss = F.cross_entropy(predict, c1)\n        # [n, m, h, w, k] -> [n, k, m, h, w]\n        # l1, l2 = l1.permute(0, 4, 1, 2, 3), l2.permute(0, 4, 1, 2, 3)\n        # [N, K, M, H, W], [N, M, H, W]\n        # sum(-logP) \/ ()\n        # bppLoss = (F.cross_entropy(l1, c1, reduction=\"mean\") + F.cross_entropy(l2, c2, reduction=\"mean\")) \/ math.log(2)\n\n        l1 = l1.mean((2,3))\n        l2 = l2.mean((2,3))\n\n        posterior1 = torch.distributions.Categorical(logits=l1)\n        posterior2 = torch.distributions.Categorical(logits=l2)\n\n        prior1 = torch.distributions.Categorical(logits=torch.zeros_like(l1))\n        prior2 = torch.distributions.Categorical(logits=torch.zeros_like(l2))\n\n        reg = torch.distributions.kl_divergence(posterior1, prior1).mean() + torch.distributions.kl_divergence(posterior2, prior2).mean()\n\n        return ssimLoss, contextLoss, reg\n","AFTER":"        ssimLoss = 1 - self._msssim(restored + 1, images + 1)\n        # [N, K, M, H, W], [N, M, H, W]\n        contextLoss = 0.0 # F.cross_entropy(predict, c1)\n        # [n, m, h, w, k] -> [n, k, m, h, w]\n        # l1, l2 = l1.permute(0, 4, 1, 2, 3), l2.permute(0, 4, 1, 2, 3)\n        # [N, K, M, H, W], [N, M, H, W]\n        # sum(-logP) \/ ()\n        # bppLoss = (F.cross_entropy(l1, c1, reduction=\"mean\") + F.cross_entropy(l2, c2, reduction=\"mean\")) \/ math.log(2)\n\n        # l1 = l1.mean((2,3))\n        # l2 = l2.mean((2,3))\n        regs = list()\n        for l in allLogits:\n            posterior = torch.distributions.Categorical(logits=l)\n            prior = torch.distributions.Categorical(logits=torch.zeros_like(l))\n            reg = torch.distributions.kl_divergence(posterior, prior).mean()\n            regs.append(reg)\n\n        return ssimLoss, contextLoss, sum(regs)\n"}