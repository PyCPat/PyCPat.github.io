{"BEFORE":"            if exists(mask):\n                mask = F.pad(mask, (0, padding), value = False)\n\n        # group along sequence\n\n        quad_q, quad_k, lin_q, lin_k, v = map(lambda t: rearrange(t, 'b (g n) d -> b g n d', n = self.group_size), (quad_q, quad_k, lin_q, lin_k, v))\n","AFTER":"        b, n, device, g = x.shape[0], x.shape[-2], x.device, self.group_size\n\n        # prenorm\n\n        normed_x = self.norm(x)\n\n        # initial projections\n\n        v, gate = self.to_hidden(normed_x).chunk(2, dim = -1)\n        qk = self.to_qk(normed_x)\n\n        # offset and scale\n\n        quad_q, lin_q, quad_k, lin_k = self.qk_offset_scale(qk)\n\n        # mask out linear attention keys\n\n        if exists(mask):\n            lin_k = lin_k.masked_fill(~mask, 0.)\n\n        # rotate queries and keys\n\n        if exists(self.rotary_pos_emb):\n            quad_q, lin_q, quad_k, lin_k = map(self.rotary_pos_emb.rotate_queries_or_keys, (quad_q, lin_q, quad_k, lin_k))\n\n        # padding for groups\n\n        padding = padding_to_multiple_of(n, g)\n\n        if padding > 0:\n            quad_q, quad_k, lin_q, lin_k, v = map(lambda t: F.pad(t, (0, 0, 0, padding), value = 0.), (quad_q, quad_k, lin_q, lin_k, v))\n\n            mask = default(mask, torch.ones((b, n), device = device, dtype = torch.bool))\n            mask = F.pad(mask, (0, padding), value = False)\n"}