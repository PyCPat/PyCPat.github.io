{"BEFORE":"        x = self.token_emb(x)\n        b, t, d = x.shape\n\n        mem = default(mem, torch.empty(self.depth, b, 0, d))\n        hidden_states = []\n\n        for attn, ff, m in zip(self.attn_layers, self.ff_layers, mem):\n            hidden_states.append(x)\n            x = attn(x, mem = m)\n            x = ff(x)\n\n        out = self.to_logits(x)\n\n        # calculate new memory only if sequence length buffer is full\n        if self.seq_len == t:\n            hidden_states = torch.stack(hidden_states)\n            new_mem = torch.cat((mem, hidden_states), dim=2)[:, :, -self.mem_len:, :].detach()\n        else:\n            new_mem = mem\n\n        return out, new_mem\n","AFTER":"        x = self.token_emb(x)\n        b, t, d = x.shape\n\n        mem = default(mem, torch.empty(self.depth, b, 0, d))\n\n        next_mem = []\n        for attn, ff, m in zip(self.attn_layers, self.ff_layers, mem):\n            x, mem_out = attn(x, mem = m)\n            x, = ff(x)\n            next_mem.append(mem_out)\n\n        out = self.to_logits(x)\n        next_mem = torch.stack(next_mem)\n\n        return TransformerOutput(out = out, mem = next_mem)\n"}