{"BEFORE":"        enc_output, i_mask = None, None\n        if self.has_inputs:\n            i_mask = utils.create_pad_mask(inputs, self.src_pad_idx)\n            enc_output = self.encode(inputs, i_mask)\n\n        t_mask = utils.create_pad_mask(targets, self.trg_pad_idx)\n        target_size = targets.size()[1]\n        t_self_mask = utils.create_trg_self_mask(target_size,\n                                                 device=targets.device)\n        return self.decode(targets, enc_output, i_mask, t_self_mask, t_mask)\n","AFTER":"    def forward(self, padded_input, input_lengths, padded_target):\n        if self.feat_extractor == 'emb_cnn' or self.feat_extractor == 'vgg_cnn':\n            padded_input = self.conv(padded_input)\n\n        # Reshaping features\n        sizes = padded_input.size() # B x H_1 (channel?) x H_2 x T\n        padded_input = padded_input.view(sizes[0], sizes[1] * sizes[2], sizes[3])\n        padded_input = padded_input.transpose(1, 2).contiguous()  # BxTxH\n\n        encoder_padded_outputs, _ = self.encoder(padded_input, input_lengths)\n        pred, gold, *_ = self.decoder(padded_target, encoder_padded_outputs, input_lengths)\n        hyp_best_scores, hyp_best_ids = torch.topk(pred, 1, dim=2)\n\n        hyp_seq = hyp_best_ids.squeeze(2)\n        gold_seq = gold\n\n        return pred, gold, hyp_seq, gold_seq\n"}