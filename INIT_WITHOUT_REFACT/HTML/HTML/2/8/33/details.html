<html><h3>Pattern ID :33
</h3><img src='189250.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        get_attn = lambda: LSHSelfAttention(dim, heads, bucket_size, n_hashes, causal = causal, dropout = lsh_dropout, post_attn_dropout = post_attn_dropout, attn_chunks = attn_chunks, allow_duplicate_attention = lsh_allow_duplicate_attention, attend_across_buckets = lsh_attend_across_buckets, random_rotations_per_head = random_rotations_per_head, num_mem_kv = num_mem_kv, use_full_attn = use_full_attn, full_attn_thres = full_attn_thres, one_value_head = one_value_head, n_local_attn_heads = n_local_attn_heads)
        get_ff = lambda: Chunk(ff_chunks, FeedForward(dim, dropout = ff_dropout, activation = ff_activation, mult = ff_mult, glu = ff_glu), along_dim = -2)

        <a id="change">if weight_tie</a><a id="change">:
            </a>get_attn<a id="change"> = </a>cache_fn(get_attn)
            get_ff<a id="change"> = </a>cache_fn(get_ff)

        blocks = []
</code></pre><h3>After Change</h3><pre><code class='java'>
        get_ff = lambda: Chunk(ff_chunks, FeedForward(dim, dropout = ff_dropout, activation = ff_activation, mult = ff_mult, glu = ff_glu), along_dim = -2)
        get_pkm = lambda: PKM(dim, num_keys = pkm_num_keys)

        <a id="change">if </a>weight_tie<a id="change">:
            </a>get_attn<a id="change">, get_ff, get_pkm</a> = map(cache_fn, (get_attn<a id="change">, get_ff, get_pkm</a>))

        blocks = []
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/reformer-pytorch/commit/fbae34221f4e2c2d777551a5e92b8bba5ae2385c#diff-7e740488ff2ce47996cfdd5356981ed3e6ac2bb38739a71b400275ac91d1c3bfL751' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 189250</div><div id='project'> Project Name: lucidrains/reformer-pytorch</div><div id='commit'> Commit Name: fbae34221f4e2c2d777551a5e92b8bba5ae2385c</div><div id='time'> Time: 2020-06-06</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: reformer_pytorch/reformer_pytorch.py</div><div id='m_class'> M Class Name: Reformer</div><div id='n_method'> N Class Name: Reformer</div><div id='m_method'> M Method Name: __init__(32)</div><div id='n_method'> N Method Name: __init__(30)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: reformer_pytorch/reformer_pytorch.py</div><div id='n_file'> N File Name: reformer_pytorch/reformer_pytorch.py</div><div id='m_start'> M Start Line: 757</div><div id='m_end'> M End Line: 772</div><div id='n_start'> N Start Line: 751</div><div id='n_end'> N End Line: 789</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        get_attn = lambda: SinkhornSelfAttention(dim, causal = causal, heads = heads, buckets = buckets, kv_buckets = kv_buckets, non_permutative = non_permutative, sinkhorn_iter = sinkhorn_iter, n_sortcut = n_sortcut, temperature = temperature, attn_dropout = attn_dropout, dropout = attn_layer_dropout, attn_sort_net = attn_sort_net)
        get_ff = lambda: FeedForward(dim, dropout = ff_dropout, glu = ff_glu)

        <a id="change">if weight_tie</a><a id="change">:
            </a>get_attn<a id="change"> = </a>cache_fn(get_attn)
            get_ff<a id="change"> = </a>cache_fn(get_ff)

        for _ in range(depth):
            layers.append(nn.ModuleList([</code></pre><h3>After Change</h3><pre><code class='java'>
        get_attn_context = lambda: SinkhornSelfAttention(dim, context_only = True, heads = heads, buckets = buckets, kv_buckets = kv_buckets, non_permutative = non_permutative, sinkhorn_iter = sinkhorn_iter, n_sortcut = n_sortcut, temperature = temperature, attn_dropout = attn_dropout, dropout = attn_layer_dropout, attn_sort_net = attn_sort_net)
        get_ff_context = lambda: FeedForward(dim, dropout = ff_dropout, glu = ff_glu)

        <a id="change">if </a>weight_tie<a id="change">:
            </a>get_attn<a id="change">, get_attn_context, get_ff, get_ff_context</a> = map(cache_fn, (get_attn<a id="change">, get_attn_context, get_ff, get_ff_context</a>))

        for _ in range(depth):
            layers.append(nn.ModuleList([</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/sinkhorn-transformer/commit/c2662a20cb783efd3351936cfabc83131060a2a6#diff-2ed19ae7feb552dc52b4c7a7c89a078c0fd371c922789f894af09ff01f35eccbL543' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 189251</div><div id='project'> Project Name: lucidrains/sinkhorn-transformer</div><div id='commit'> Commit Name: c2662a20cb783efd3351936cfabc83131060a2a6</div><div id='time'> Time: 2020-04-10</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: sinkhorn_transformer/sinkhorn_transformer.py</div><div id='m_class'> M Class Name: SinkhornTransformer</div><div id='n_method'> N Class Name: SinkhornTransformer</div><div id='m_method'> M Method Name: __init__(20)</div><div id='n_method'> N Method Name: __init__(19)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: sinkhorn_transformer/sinkhorn_transformer.py</div><div id='n_file'> N File Name: sinkhorn_transformer/sinkhorn_transformer.py</div><div id='m_start'> M Start Line: 547</div><div id='m_end'> M End Line: 554</div><div id='n_start'> N Start Line: 546</div><div id='n_end'> N End Line: 576</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)

        <a id="change">if use_batch_norm</a><a id="change">:
            </a>self.bn1 = nn.BatchNorm2d(32)
            self.bn2<a id="change"> = </a>nn.BatchNorm2d(64)
            self.bn3<a id="change"> = </a>nn.BatchNorm2d(64)
            self.bn4 = nn.BatchNorm1d(512)

        self.fc = nn.Linear(self._get_linear_input_size(), 512)</code></pre><h3>After Change</h3><pre><code class='java'>
        super().__init__()

        &#47&#47 default architecture is based on Nature DQN paper.
        <a id="change">if </a>filters is None<a id="change">:
            </a>filters = [(32, 8, 4), (64<a id="change">, 4, 2</a>), (64<a id="change">, 3, 1</a>)]
        if feature_size is None:
            feature_size = 512
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/takuseno/d3rlpy/commit/557b11a8d5cf75edfc0a2928399d5192d1757ddb#diff-9039460d6f630d68f293a43c69c5f66cf03d3a614353b7c827a27367930ff0b0L6' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 189254</div><div id='project'> Project Name: takuseno/d3rlpy</div><div id='commit'> Commit Name: 557b11a8d5cf75edfc0a2928399d5192d1757ddb</div><div id='time'> Time: 2020-06-16</div><div id='author'> Author: takuma.seno@gmail.com</div><div id='file'> File Name: skbrl/models/torch/heads.py</div><div id='m_class'> M Class Name: PixelHead</div><div id='n_method'> N Class Name: PixelHead</div><div id='m_method'> M Method Name: __init__(5)</div><div id='n_method'> N Method Name: __init__(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: skbrl/models/torch/heads.py</div><div id='n_file'> N File Name: skbrl/models/torch/heads.py</div><div id='m_start'> M Start Line: 11</div><div id='m_end'> M End Line: 22</div><div id='n_start'> N Start Line: 8</div><div id='n_end'> N End Line: 43</div><BR>