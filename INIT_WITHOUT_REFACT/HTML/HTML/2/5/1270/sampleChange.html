<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.cls_token = nn.Parameter(torch.zeros(embed_dim))
        self.norm_1 = nn.LayerNorm(embed_dim)
        self.attn = nn.MultiheadAttention(embed_dim, 1, batch_first=True)
        self.layer_scale_1 = nn.Parameter(torch.full((<a id="change">1</a><a id="change">,1,embed_dim</a>), layer_scale_init))

        mlp_dim = embed_dim * mlp_ratio
        self.mlp = nn.Sequential(</code></pre><h3>After Change</h3><pre><code class='java'>
            nn.GELU(),
            nn.Linear(mlp_dim, embed_dim)
        )
        self.layer_scale_2 = nn.Parameter(<a id="change">torch.ones(</a>embed_dim<a id="change">) * </a>layer_scale_init)
        self.norm_3 = nn.LayerNorm(embed_dim)

    def forward(self, x: torch.Tensor):</code></pre>