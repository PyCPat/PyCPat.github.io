<html><h3>Pattern ID :1451
</h3><img src='3984158.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        if self.attention:
            self.W_a = nn.Linear(args.hidden_size, args.hidden_size, bias=False)
            self.W_b = nn.Linear(args.hidden_size, args.hidden_size)
        <a id="change">if </a>self.message_attention<a id="change">:
            </a>self.W_ma = nn.ModuleList([nn.Linear(args.hidden_size, args.hidden_size, bias=False)
                                       for _ in range(self.num_heads)])
            &#47&#47 uncomment this later if you want attention over binput + nei_message? or on atom incoming at end
            &#47&#47 self.W_ma2 = nn.Linear(hidden_size, 1, bias=False)</code></pre><h3>After Change</h3><pre><code class='java'>
            self.W_s2s_b = nn.Linear(args.hidden_size, args.hidden_size, bias=self.bias)

        if self.attention:
            self.W_a = <a id="change">nn.Linear(</a>args.hidden_size, args.hidden_size<a id="change">, bias=self.bias)</a>
            self.W_b = nn.Linear(args.hidden_size, args.hidden_size)

        &#47&#47 Dropout
        self.dropout_layer<a id="change"> = nn</a><a id="change">.Dropout(p=self.dropout)</a>

        &#47&#47 Activation
        if args.activation == &quotReLU&quot:
            self.act_func = nn.ReLU()</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/aamini/chemprop/commit/ddcdae2edb70f359d1d98863532cfe51709b1391#diff-91b4e1ef89fbd915183928818e07287d60ba2d013478779025955bb686f6410dL272' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 3984158</div><div id='project'> Project Name: aamini/chemprop</div><div id='commit'> Commit Name: ddcdae2edb70f359d1d98863532cfe51709b1391</div><div id='time'> Time: 2018-09-30</div><div id='author'> Author: swansonk.14@gmail.com</div><div id='file'> File Name: mpn.py</div><div id='m_class'> M Class Name: MPN</div><div id='n_method'> N Class Name: MPN</div><div id='m_method'> M Method Name: __init__(2)</div><div id='n_method'> N Method Name: __init__(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: mpn.py</div><div id='n_file'> N File Name: mpn.py</div><div id='m_start'> M Start Line: 275</div><div id='m_end'> M End Line: 301</div><div id='n_start'> N Start Line: 272</div><div id='n_end'> N End Line: 313</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        if self.attention:
            self.W_a = nn.Linear(args.hidden_size, args.hidden_size, bias=False)
            self.W_b = nn.Linear(args.hidden_size, args.hidden_size)
        <a id="change">if </a>self.message_attention<a id="change">:
            </a>self.W_ma = nn.ModuleList([nn.Linear(args.hidden_size, args.hidden_size, bias=False)
                                       for _ in range(self.num_heads)])
            &#47&#47 uncomment this later if you want attention over binput + nei_message? or on atom incoming at end
            &#47&#47 self.W_ma2 = nn.Linear(hidden_size, 1, bias=False)</code></pre><h3>After Change</h3><pre><code class='java'>

        if self.deepset:
            self.W_s2s_a = nn.Linear(args.hidden_size, args.hidden_size, bias=self.bias)
            self.W_s2s_b = <a id="change">nn.Linear(</a>args.hidden_size, args.hidden_size<a id="change">, bias=self.bias)</a>

        if self.attention:
            self.W_a = nn.Linear(args.hidden_size, args.hidden_size, bias=self.bias)
            self.W_b = nn.Linear(args.hidden_size, args.hidden_size)

        &#47&#47 Dropout
        self.dropout_layer<a id="change"> = </a><a id="change">nn.Dropout(p=self.dropout)</a>

        &#47&#47 Activation
        if args.activation == &quotReLU&quot:
            self.act_func = nn.ReLU()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/aamini/chemprop/commit/ddcdae2edb70f359d1d98863532cfe51709b1391#diff-91b4e1ef89fbd915183928818e07287d60ba2d013478779025955bb686f6410dL268' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 3984159</div><div id='project'> Project Name: aamini/chemprop</div><div id='commit'> Commit Name: ddcdae2edb70f359d1d98863532cfe51709b1391</div><div id='time'> Time: 2018-09-30</div><div id='author'> Author: swansonk.14@gmail.com</div><div id='file'> File Name: mpn.py</div><div id='m_class'> M Class Name: MPN</div><div id='n_method'> N Class Name: MPN</div><div id='m_method'> M Method Name: __init__(2)</div><div id='n_method'> N Method Name: __init__(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: mpn.py</div><div id='n_file'> N File Name: mpn.py</div><div id='m_start'> M Start Line: 275</div><div id='m_end'> M End Line: 301</div><div id='n_start'> N Start Line: 272</div><div id='n_end'> N End Line: 313</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.args = args
        self.pad_idx = numericalizer.pad_id

        <a id="change">if </a>sum(emb.dim for emb in encoder_embeddings) != args.dimension<a id="change">:
            </a>raise ValueError(&quotHidden dimension must be equal to the sum of the embedding sizes to use IdentityEncoder&quot)

        self.encoder_embeddings = CombinedEmbedding(numericalizer, encoder_embeddings, args.dimension,
                                                    trained_dimension=0,</code></pre><h3>After Change</h3><pre><code class='java'>
                                                    finetune_pretrained=args.train_encoder_embeddings)

        if self.args.rnn_layers &gt; 0 and self.args.rnn_dimension != self.args.dimension:
            self.dropout<a id="change"> = </a><a id="change">nn.Dropout(</a>args.dropout_ratio<a id="change">)</a>
            self.projection = <a id="change">nn.Linear(</a>self.encoder_embeddings.dimension, self.args.rnn_dimension<a id="change">, bias=False)</a>
        else:
            self.dropout = None
            self.projection = None
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/stanford-oval/genienlp/commit/ae1bf6a643f0c4ae8d6df12b291a9271e5fe4543#diff-2a9a18573b4512fdca0696a6ddb0ac7061b044f90150c1dde95c604b9ed45434L36' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 3984164</div><div id='project'> Project Name: stanford-oval/genienlp</div><div id='commit'> Commit Name: ae1bf6a643f0c4ae8d6df12b291a9271e5fe4543</div><div id='time'> Time: 2020-01-18</div><div id='author'> Author: gcampagn@cs.stanford.edu</div><div id='file'> File Name: decanlp/models/identity_encoder.py</div><div id='m_class'> M Class Name: IdentityEncoder</div><div id='n_method'> N Class Name: IdentityEncoder</div><div id='m_method'> M Method Name: __init__(4)</div><div id='n_method'> N Method Name: __init__(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: decanlp/models/identity_encoder.py</div><div id='n_file'> N File Name: decanlp/models/identity_encoder.py</div><div id='m_start'> M Start Line: 41</div><div id='m_end'> M End Line: 44</div><div id='n_start'> N Start Line: 38</div><div id='n_end'> N End Line: 53</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def __init__(self, backbone, num_classes, use_bottleneck=True, bottleneck_dim=1024, head_bottleneck_dim=1024):
        super(Classifier, self).__init__()
        self.backbone = backbone
        <a id="change">if </a>use_bottleneck<a id="change">:
            </a>self.bottleneck = nn.Sequential(
                nn.Linear(backbone.out_features, bottleneck_dim),
                nn.BatchNorm1d(bottleneck_dim),
                nn.ReLU(),</code></pre><h3>After Change</h3><pre><code class='java'>
            nn.Linear(bottleneck_dim, width),
            nn.ReLU(),
            nn.Dropout(0.5),
            <a id="change">nn.Linear(</a>width, num_classes<a id="change">)</a>
        )
        &#47&#47 The adversarial classifier head
        self.adv_head<a id="change"> = </a>nn.Sequential(
            nn.Linear(bottleneck_dim, width),
            nn.ReLU(),
            <a id="change">nn.Dropout(</a>0.5<a id="change">)</a>,
            nn.Linear(width, num_classes)
        )
        for dep in range(2):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/thuml/transfer-learning-library/commit/6715ce965493b31f0defccaf6c0b1a654ffd4a0a#diff-75629d4a6842bee5234184342a8805cc34f89995082f6ceea63da88c3fdf16c2L130' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 3984161</div><div id='project'> Project Name: thuml/transfer-learning-library</div><div id='commit'> Commit Name: 6715ce965493b31f0defccaf6c0b1a654ffd4a0a</div><div id='time'> Time: 2020-03-11</div><div id='author'> Author: JiangJunguang1123@outlook.com</div><div id='file'> File Name: dalib/adaptation/mdd.py</div><div id='m_class'> M Class Name: Classifier</div><div id='n_method'> N Class Name: Classifier</div><div id='m_method'> M Method Name: __init__(5)</div><div id='n_method'> N Method Name: __init__(6)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: dalib/adaptation/mdd.py</div><div id='n_file'> N File Name: dalib/adaptation/mdd.py</div><div id='m_start'> M Start Line: 130</div><div id='m_end'> M End Line: 148</div><div id='n_start'> N Start Line: 81</div><div id='n_end'> N End Line: 113</div><BR>