<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                   "{}/logits_fake".format(split): logits_fake.detach().mean()
                   }

            if <a id="change">self.training and disc_factor and global_step % 16 == 0</a>:
                gradients, = torch.autograd.grad(outputs=logits_real.sum(), inputs=inputs, create_graph=True)
                gradients = gradients.view(inputs.shape[0], -1)
</code></pre><h3>After Change</h3><pre><code class='java'>
        if optimizer_idx == 1:
            &#47&#47 second pass for discriminator update
            disc_factor = 1 if global_step &gt;= self.discriminator_iter_start else 0
            do_r1 = <a id="change">self.training and bool(disc_factor) and global_step % 16 == 0</a>

            logits_real = self.discriminator(inputs.detach().requires_grad_(do_r1))
            logits_fake = self.discriminator(reconstructions.detach())
            
            d_loss = disc_factor * self.disc_loss(logits_fake, logits_real)
            if do_r1:
                gradients, = torch.autograd.grad(outputs=logits_real.sum(), inputs=inputs, create_graph=True)
                gradients = gradients.view(inputs.shape[0], -1)

                gradients_norm = gradients.norm(2, dim=1).pow(2).mean()
                d_loss += 10 * gradients_norm/2

            log = {"{}/disc_loss".format(split): d_loss.detach(),
                   "{}/logits_real".format(split): logits_real.detach().mean(),
                   "{}/logits_fake".format(split): logits_fake.detach().mean()
                   }

            <a id="change">if </a>do_r1<a id="change">:
                </a>log["{}/r1_reg".format(split)] = gradients_norm.detach()
            
            return d_loss, log
</code></pre>