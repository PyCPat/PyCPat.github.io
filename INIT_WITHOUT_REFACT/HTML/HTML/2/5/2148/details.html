<html><h3>Pattern ID :2148
</h3><img src='7602016.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        
        &#47&#47 compute output
        att = (att_score @ v).view(batch_size, seg_len, -1)
        <a id="change">return </a>self.layer_norm(<a id="change">self.mlp(</a>att<a id="change">)</a> + x)
              
    def circulant_shift(self, x, shift):
        </code></pre><h3>After Change</h3><pre><code class='java'>
        
        &#47&#47 compute output
        att = (att_score @ v).view(batch_size, seg_len, -1)
        out<a id="change"> = </a>self.dropout(<a id="change">self.mlp(</a>att<a id="change">)</a>)
        <a id="change">return </a>self.layer_norm(out + x)
              
    def circulant_shift(self, x, shift):
        </code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/augustwester/transformer-xl/commit/cc7f32da8e71438812e757fbe2a131b14a5cfc2f#diff-eb7ae2070281ad9c6011e2b97c48b86fea6c48408aa6b0e9d20c8c520d3c7cf2L28' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 7602016</div><div id='project'> Project Name: augustwester/transformer-xl</div><div id='commit'> Commit Name: cc7f32da8e71438812e757fbe2a131b14a5cfc2f</div><div id='time'> Time: 2022-11-28</div><div id='author'> Author: august.wester@gmail.com</div><div id='file'> File Name: attention.py</div><div id='m_class'> M Class Name: MultiHeadAttention</div><div id='n_method'> N Class Name: MultiHeadAttention</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: attention.py</div><div id='n_file'> N File Name: attention.py</div><div id='m_start'> M Start Line: 28</div><div id='m_end'> M End Line: 51</div><div id='n_start'> N Start Line: 29</div><div id='n_end'> N End Line: 54</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            xs.append(normed_cont)

        x = torch.cat(xs, dim = -1)
        <a id="change">return </a><a id="change">self.mlp(</a>x<a id="change">)</a>
</code></pre><h3>After Change</h3><pre><code class='java'>
            xs.append(normed_cont)

        x = torch.cat(xs, dim = -1)
        logits<a id="change"> =</a><a id="change">self.mlp(</a>x<a id="change">)</a>

        if not return_attn:
            <a id="change">return </a>logits

        return logits, attns
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/tab-transformer-pytorch/commit/e2e8b58e7d1b453e47bc4638164e10c19fce271b#diff-d87dc12538c5ed85349330c1ea686aed85875355e3b485c6d2ab4ae1d54231c6L204' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 7602015</div><div id='project'> Project Name: lucidrains/tab-transformer-pytorch</div><div id='commit'> Commit Name: e2e8b58e7d1b453e47bc4638164e10c19fce271b</div><div id='time'> Time: 2023-04-05</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: tab_transformer_pytorch/tab_transformer_pytorch.py</div><div id='m_class'> M Class Name: TabTransformer</div><div id='n_method'> N Class Name: TabTransformer</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: tab_transformer_pytorch/tab_transformer_pytorch.py</div><div id='n_file'> N File Name: tab_transformer_pytorch/tab_transformer_pytorch.py</div><div id='m_start'> M Start Line: 212</div><div id='m_end'> M End Line: 228</div><div id='n_start'> N Start Line: 212</div><div id='n_end'> N End Line: 241</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        att = att.tril(mem_len) / embed_dim**0.5
        att = torch.softmax(att, dim=-1)
        
        out = self.layer_norm(<a id="change">self.mlp(</a>att @ v<a id="change">)</a>)
        <a id="change">return </a>self.pos_ff(out)
          
    def get_sinusoid_pos_encoding(self, mem_len, embed_dim):
        pos = torch.arange(mem_len).unsqueeze(1)</code></pre><h3>After Change</h3><pre><code class='java'>
        att = torch.softmax(att, dim=-1)
        
        &#47&#47 compute the output of the layer and save to memory
        out = self.layer_norm(<a id="change">self.mlp(</a>att @ v<a id="change">)</a> + x)
        out<a id="change"> = </a>self.pos_ff(out)
        self.save_to_memory(out)
        
        <a id="change">return </a>out
          
    def get_sinusoid_pos_encoding(self, mem_len, embed_dim):
        pos = torch.arange(mem_len).unsqueeze(1)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/augustwester/transformer-xl/commit/6cce5d3da48879cdef126940aa5529afcff03c9a#diff-eb7ae2070281ad9c6011e2b97c48b86fea6c48408aa6b0e9d20c8c520d3c7cf2L29' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 7602014</div><div id='project'> Project Name: augustwester/transformer-xl</div><div id='commit'> Commit Name: 6cce5d3da48879cdef126940aa5529afcff03c9a</div><div id='time'> Time: 2022-11-21</div><div id='author'> Author: august.wester@gmail.com</div><div id='file'> File Name: attention.py</div><div id='m_class'> M Class Name: Attention</div><div id='n_method'> N Class Name: Attention</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: attention.py</div><div id='n_file'> N File Name: attention.py</div><div id='m_start'> M Start Line: 30</div><div id='m_end'> M End Line: 46</div><div id='n_start'> N Start Line: 34</div><div id='n_end'> N End Line: 61</div><BR>