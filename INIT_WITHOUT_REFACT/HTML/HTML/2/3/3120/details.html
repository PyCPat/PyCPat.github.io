<html><h3>Pattern ID :3120
</h3><img src='10172682.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    def forward(self, x):
        x = self.norm(x)
        x = rearrange(x, &quotb n d -&gt; b d n&quot)
        x = F.pad(x, (2<a id="change">, 0</a>))

        q, k, v = self.to_q(x), self.to_k(x), self.to_v(x)
        q, k, v = rearrange_many((q, k, v), &quotb (h d) n -&gt; b h n d&quot, h = self.heads)</code></pre><h3>After Change</h3><pre><code class='java'>
        x = self.norm(x)
        x = rearrange(x, &quotb n d -&gt; b d n&quot)

        q, k, v = <a id="change">self.to_qkv(x).chunk(3</a><a id="change">, dim = 1)</a>

        q, k, v = rearrange_many((q, k, v), &quotb (h d) n -&gt; b h n d&quot, h = self.heads)

        &#47&#47 apply causal depthwise conv to queries, keys, values (a la Primer) with different kernel sizes across 4 groups of heads</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 5</div><BR><div id='size'>Non-data size: 2</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/tranception-pytorch/commit/b2eaf893294394093839a66effb621645d54cd6c#diff-18b0a7fd74d6ed16b8ebef089a2a63eb9b5ff5662fc464204330a63b2b4466c1L115' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 10172682</div><div id='project'> Project Name: lucidrains/tranception-pytorch</div><div id='commit'> Commit Name: b2eaf893294394093839a66effb621645d54cd6c</div><div id='time'> Time: 2022-06-12</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: tranception_pytorch/tranception_pytorch.py</div><div id='m_class'> M Class Name: CausalAttention</div><div id='n_method'> N Class Name: CausalAttention</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: tranception_pytorch/tranception_pytorch.py</div><div id='n_file'> N File Name: tranception_pytorch/tranception_pytorch.py</div><div id='m_start'> M Start Line: 115</div><div id='m_end'> M End Line: 125</div><div id='n_start'> N Start Line: 135</div><div id='n_end'> N End Line: 164</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def forward(self, x):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0]<a id="change">, qkv[1], qkv[2]</a>

        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)</code></pre><h3>After Change</h3><pre><code class='java'>
    def forward(self, x):
        B, N, C = x.shape

        qkv = <a id="change">self.qkv(x).chunk(3</a><a id="change">, dim = -1)</a>
        q, k, v = map(lambda t: rearrange(t, &quotb n (h d) -&gt; b h n d&quot, h = self.heads), qkv)

        q = q * self.scale
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/vit-pytorch/commit/cb6d749821bbf3b0bd17c9e8e64eb343f40b3f69#diff-d9cf888a006662bd788cc31712f154ca4e86227d51998c7d5bca6e17d1197c07L78' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 10172679</div><div id='project'> Project Name: lucidrains/vit-pytorch</div><div id='commit'> Commit Name: cb6d749821bbf3b0bd17c9e8e64eb343f40b3f69</div><div id='time'> Time: 2022-10-29</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: vit_pytorch/cct.py</div><div id='m_class'> M Class Name: Attention</div><div id='n_method'> N Class Name: Attention</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: vit_pytorch/cct.py</div><div id='n_file'> N File Name: vit_pytorch/cct.py</div><div id='m_start'> M Start Line: 80</div><div id='m_end'> M End Line: 90</div><div id='n_start'> N Start Line: 99</div><div id='n_end'> N End Line: 111</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        x,
        context = None
    ):
        b<a id="change">, c, h, w</a> = x.shape
        context = default(context, x)

        qkv = (self.to_q(x), *self.to_kv(context).chunk(2, dim = 1))</code></pre><h3>After Change</h3><pre><code class='java'>

        if exists(self.time_cond):
            assert exists(time)
            scale, shift = <a id="change">self.time_cond(time).chunk(2</a><a id="change">, dim = -1)</a>
            x = (x * (scale + 1)) + shift

        if has_context:
            context = self.norm_context(context)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/recurrent-interface-network-pytorch/commit/5cd08b2823cfe105785a525aea43a7396fea07e9#diff-c245517a15c6c34e71df6f08ed422324484176189f4795463b7e58278d35b9cdL144' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 10172678</div><div id='project'> Project Name: lucidrains/recurrent-interface-network-pytorch</div><div id='commit'> Commit Name: 5cd08b2823cfe105785a525aea43a7396fea07e9</div><div id='time'> Time: 2022-12-24</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: rin_pytorch/rin_pytorch.py</div><div id='m_class'> M Class Name: Attention</div><div id='n_method'> N Class Name: Attention</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: rin_pytorch/rin_pytorch.py</div><div id='n_file'> N File Name: rin_pytorch/rin_pytorch.py</div><div id='m_start'> M Start Line: 149</div><div id='m_end'> M End Line: 161</div><div id='n_start'> N Start Line: 199</div><div id='n_end'> N End Line: 225</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.blocks = nn.ModuleList([ReversibleBlock(f, g) for (f, g) in blocks])

    def forward(self, x, arg_route = (True, True), **kwargs):
        f_args<a id="change">, g_args</a> = map(lambda route: kwargs if route else {}, arg_route)
        block_kwargs = {&quotf_args&quot: f_args, &quotg_args&quot: g_args}
        return _ReversibleFunction.apply(x, self.blocks, block_kwargs)
</code></pre><h3>After Change</h3><pre><code class='java'>
            blocks, args = map(lambda ind: list(map(itemgetter(ind), layers_and_args)), (0, 1))

        out =  _ReversibleFunction.apply(x, blocks, args)
        return torch.stack(<a id="change">out.chunk(2</a><a id="change">, dim=-1)</a>).sum(dim=0)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/linear-attention-transformer/commit/fa23ce09a98a63d26116e3935ad5902cf705255d#diff-29d3c048298bdaa9a9670921a4026430e5b09b55bc1da20d65da18bd5c85f575L118' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 10172677</div><div id='project'> Project Name: lucidrains/linear-attention-transformer</div><div id='commit'> Commit Name: fa23ce09a98a63d26116e3935ad5902cf705255d</div><div id='time'> Time: 2020-06-04</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: linear_attention_transformer/reversible.py</div><div id='m_class'> M Class Name: ReversibleSequence</div><div id='n_method'> N Class Name: ReversibleSequence</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: linear_attention_transformer/reversible.py</div><div id='n_file'> N File Name: linear_attention_transformer/reversible.py</div><div id='m_start'> M Start Line: 118</div><div id='m_end'> M End Line: 121</div><div id='n_start'> N Start Line: 161</div><div id='n_end'> N End Line: 174</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        )

    def forward(self, x):
        b<a id="change">, c, h, w</a> = x.shape
        qkv = self.to_qkv(x).chunk(3, dim = 1)
        q, k, v = map(lambda t: rearrange(t, &quotb (h c) x y -&gt; b h c (x y)&quot, h = self.heads), qkv)
</code></pre><h3>After Change</h3><pre><code class='java'>

        if exists(self.time_cond):
            assert exists(time)
            scale, shift = <a id="change">self.time_cond(time).chunk(2</a><a id="change">, dim = -1)</a>
            x = (x * (scale + 1)) + shift

        qkv = self.to_qkv(x).chunk(3, dim = -1)
        q, k, v = map(lambda t: rearrange(t, &quotb n (h d) -&gt; b h n d&quot, h = h), qkv)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/recurrent-interface-network-pytorch/commit/5cd08b2823cfe105785a525aea43a7396fea07e9#diff-c245517a15c6c34e71df6f08ed422324484176189f4795463b7e58278d35b9cdL113' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 10172676</div><div id='project'> Project Name: lucidrains/recurrent-interface-network-pytorch</div><div id='commit'> Commit Name: 5cd08b2823cfe105785a525aea43a7396fea07e9</div><div id='time'> Time: 2022-12-24</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: rin_pytorch/rin_pytorch.py</div><div id='m_class'> M Class Name: LinearAttention</div><div id='n_method'> N Class Name: LinearAttention</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: rin_pytorch/rin_pytorch.py</div><div id='n_file'> N File Name: rin_pytorch/rin_pytorch.py</div><div id='m_start'> M Start Line: 114</div><div id='m_end'> M End Line: 126</div><div id='n_start'> N Start Line: 136</div><div id='n_end'> N End Line: 157</div><BR>