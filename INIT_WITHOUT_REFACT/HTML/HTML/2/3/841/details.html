<html><h3>Pattern ID :841
</h3><img src='2929407.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.positional_encoding = positional_encoding
        if positional_encoding:
            self.kh = torch.arange(1,int(np.ceil(np.log2(self.H))))
            self.kw = torch.arange(1,int(<a id="change">np.ceil(</a>np.log2(self.W)<a id="change">)</a>))
        else:
            self.kh, self.kw = [], []
            </code></pre><h3>After Change</h3><pre><code class='java'>
        if positional_encoding:
            assert positional_encoding&gt;1
            nh = int(np.ceil((np.log(self.H) - np.log(1))/np.log(positional_encoding)))
            nw = int(<a id="change">np.ceil(</a>(np.log(self.W) - np.log(1))/np.log(positional_encoding)<a id="change">)</a>)
            self.kh = nn.Parameter(positional_encoding**torch.arange(0,nh),requires_grad=False)
            self.kw = nn.Parameter(positional_encoding<a id="change">**</a>torch.arange(0,nw),requires_grad=False)
        else:
            self.kh, self.kw = [], []
            </code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/gerbenbeintema/deepsi/commit/9d76608949f76222a1c07304f901e4a94a01d47f#diff-c47ac71b5c790a760834d10dffdddbbe3e1104b431ea47d77b49064848c805afL328' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2929407</div><div id='project'> Project Name: gerbenbeintema/deepsi</div><div id='commit'> Commit Name: 9d76608949f76222a1c07304f901e4a94a01d47f</div><div id='time'> Time: 2021-10-26</div><div id='author'> Author: g.i.beintema@tue.nl</div><div id='file'> File Name: deepSI/utils/torch_nets.py</div><div id='m_class'> M Class Name: Shotgun_MLP</div><div id='n_method'> N Class Name: Shotgun_MLP</div><div id='m_method'> M Method Name: __init__(6)</div><div id='n_method'> N Method Name: __init__(6)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: deepSI/utils/torch_nets.py</div><div id='n_file'> N File Name: deepSI/utils/torch_nets.py</div><div id='m_start'> M Start Line: 328</div><div id='m_end'> M End Line: 329</div><div id='n_start'> N Start Line: 328</div><div id='n_end'> N End Line: 332</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.d_v = d_v
        self.n_heads = n_heads

        self.d_k_s = int(<a id="change">math.ceil(</a>d_k / n_heads<a id="change">)</a>)
        self.d_v_s = int(math.ceil(d_v / n_heads))
        d_k_p = self.d_k_s * n_heads
        d_v_p = self.d_v_s * n_heads</code></pre><h3>After Change</h3><pre><code class='java'>
        self.n_heads = n_heads

        &#47&#47 make sure that dimension of vectors is divisible by n_heads
        d_k_p = int(<a id="change">math.ceil(</a>d_k / n_heads<a id="change">)</a>)<a id="change"> * </a>n_heads
        d_v_p = int(math.ceil(d_v / n_heads)) * n_heads

        self.q_projector = nn.Linear(d_k, d_k_p)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/bdsaglam/torch-scae/commit/7083741a04de80cb3ed6c10ef12c8fc429d8ad1a#diff-279e3f60fb90a15cce8aad99c56549c814d3389376407c42447db94ffb3f8320L53' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2929400</div><div id='project'> Project Name: bdsaglam/torch-scae</div><div id='commit'> Commit Name: 7083741a04de80cb3ed6c10ef12c8fc429d8ad1a</div><div id='time'> Time: 2020-06-05</div><div id='author'> Author: bdsaglam@gmail.com</div><div id='file'> File Name: torch_scae/set_transformer.py</div><div id='m_class'> M Class Name: MultiHeadQKVAttention</div><div id='n_method'> N Class Name: MultiHeadQKVAttention</div><div id='m_method'> M Method Name: __init__(4)</div><div id='n_method'> N Method Name: __init__(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: torch_scae/set_transformer.py</div><div id='n_file'> N File Name: torch_scae/set_transformer.py</div><div id='m_start'> M Start Line: 59</div><div id='m_end'> M End Line: 62</div><div id='n_start'> N Start Line: 60</div><div id='n_end'> N End Line: 61</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        :param channels: The last dimension of the tensor you want to apply pos emb to.
        
        super(PositionalEncoding2D, self).__init__()
        channels = int(<a id="change">np.ceil(</a>channels/2<a id="change">)</a>)
        self.channels = channels
        inv_freq = 1. / (10000 ** (torch.arange(0, channels, 2).float() / channels))
        self.register_buffer(&quotinv_freq&quot, inv_freq)</code></pre><h3>After Change</h3><pre><code class='java'>
        :param channels: The last dimension of the tensor you want to apply pos emb to.
        
        super(PositionalEncoding2D, self).__init__()
        channels = int(<a id="change">np.ceil(</a>channels/4<a id="change">)*</a>2)
        self.channels = channels
        inv_freq = 1. / (10000 ** (torch.arange(0, channels, 2).float() / channels))
        self.register_buffer(&quotinv_freq&quot, inv_freq)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tatp22/multidim-positional-encoding/commit/b7f93964e60094a3d21a91b9b13c51e395b29df4#diff-a9a63c2407adfb9e442446bd9861217ddc9a827e0d376be56f8bcd241f658692L46' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2929401</div><div id='project'> Project Name: tatp22/multidim-positional-encoding</div><div id='commit'> Commit Name: b7f93964e60094a3d21a91b9b13c51e395b29df4</div><div id='time'> Time: 2021-05-25</div><div id='author'> Author: tatp22@gmail.com</div><div id='file'> File Name: positional_encodings/positional_encodings.py</div><div id='m_class'> M Class Name: PositionalEncoding2D</div><div id='n_method'> N Class Name: PositionalEncoding2D</div><div id='m_method'> M Method Name: __init__(2)</div><div id='n_method'> N Method Name: __init__(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: positional_encodings/positional_encodings.py</div><div id='n_file'> N File Name: positional_encodings/positional_encodings.py</div><div id='m_start'> M Start Line: 51</div><div id='m_end'> M End Line: 51</div><div id='n_start'> N Start Line: 51</div><div id='n_end'> N End Line: 51</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        :param channels: The last dimension of the tensor you want to apply pos emb to.
        
        super(PositionalEncoding3D, self).__init__()
        channels = int(<a id="change">np.ceil(</a>channels/3<a id="change">)</a>)
        if channels % 2:
            channels += 1
        self.channels = channels</code></pre><h3>After Change</h3><pre><code class='java'>
        :param channels: The last dimension of the tensor you want to apply pos emb to.
        
        super(PositionalEncoding3D, self).__init__()
        channels = int(<a id="change">np.ceil(</a>channels/6<a id="change">)*</a>2)
        if channels % 2:
            channels += 1
        self.channels = channels</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tatp22/multidim-positional-encoding/commit/b7f93964e60094a3d21a91b9b13c51e395b29df4#diff-a9a63c2407adfb9e442446bd9861217ddc9a827e0d376be56f8bcd241f658692L91' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2929402</div><div id='project'> Project Name: tatp22/multidim-positional-encoding</div><div id='commit'> Commit Name: b7f93964e60094a3d21a91b9b13c51e395b29df4</div><div id='time'> Time: 2021-05-25</div><div id='author'> Author: tatp22@gmail.com</div><div id='file'> File Name: positional_encodings/positional_encodings.py</div><div id='m_class'> M Class Name: PositionalEncoding3D</div><div id='n_method'> N Class Name: PositionalEncoding3D</div><div id='m_method'> M Method Name: __init__(2)</div><div id='n_method'> N Method Name: __init__(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: positional_encodings/positional_encodings.py</div><div id='n_file'> N File Name: positional_encodings/positional_encodings.py</div><div id='m_start'> M Start Line: 96</div><div id='m_end'> M End Line: 96</div><div id='n_start'> N Start Line: 96</div><div id='n_end'> N End Line: 96</div><BR>