<html><h3>Pattern ID :1506
</h3><img src='4400823.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        seq_idx = torch.arange(seq_len, device=x.device).type_as(self.theta)

        &#47&#47 Calculate the product of position index and $\theta_i$
        idx_theta = <a id="change">torch.einsum(&quotn,d-&gt;nd&quot</a>, seq_idx, self.theta<a id="change">)</a>

        &#47&#47 Concatenate so that for row $m$ we have
        &#47&#47 $[m \theta_0, m \theta_1, ..., m \theta_{\frac{d}{2}}, m \theta_0, m \theta_1, ..., m \theta_{\frac{d}{2}}]$
        idx_theta2 = torch.cat([idx_theta, idx_theta], dim=1)</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 \end{align}
        &#47&#47
        &#47&#47 for $i \in {1, 2, ..., \frac{d}{2}}$
        x_rope = (x_rope * self.cos_cached[:<a id="change">x.shape[0]</a>]) + (neg_half_x * self.sin_cached[:x.shape[0]])

        &#47&#47
        return torch.cat((x_rope, x_pass), dim=-1)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 2</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lab-ml/nn/commit/0ce65adf9e602321109528b05cf99fccb16cd2de#diff-637497eb531dcbf4c4fff534c48c9dc4ec23ef7b14699edc95d8b280b04de206L132' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4400823</div><div id='project'> Project Name: lab-ml/nn</div><div id='commit'> Commit Name: 0ce65adf9e602321109528b05cf99fccb16cd2de</div><div id='time'> Time: 2022-06-03</div><div id='author'> Author: vpjayasiri@gmail.com</div><div id='file'> File Name: labml_nn/transformers/rope/__init__.py</div><div id='m_class'> M Class Name: RotaryPositionalEmbeddings</div><div id='n_method'> N Class Name: RotaryPositionalEmbeddings</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: labml_nn/transformers/rope/__init__.py</div><div id='n_file'> N File Name: labml_nn/transformers/rope/__init__.py</div><div id='m_start'> M Start Line: 132</div><div id='m_end'> M End Line: 163</div><div id='n_start'> N Start Line: 171</div><div id='n_end'> N End Line: 193</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            supports = F.softmax(F.relu(torch.mm(node_embeddings, node_embeddings.transpose(0, 1))), dim=1)
        else:
            node_num = node_embeddings.shape[1]
            supports = F.softmax(F.relu(<a id="change">torch.einsum(&quotbnc,bmc-&gt;nm&quot</a>, node_embeddings, node_embeddings<a id="change">)</a>), dim=1)            
        support_set = [torch.eye(node_num).to(supports.device), supports]
        for k in range(2, self.cheb_k):
            support_set.append(torch.matmul(2 * supports, support_set[-1]) - support_set[-2]) </code></pre><h3>After Change</h3><pre><code class='java'>
        x_g = []        
        support_set = []
        for support in supports:
            support_ks = [torch.eye(<a id="change">support.shape[0]</a>).to(support.device), support]
            for k in range(2, self.cheb_k):
                support_ks.append(torch.matmul(2 * support, support_ks[-1]) - support_ks[-2]) 
            support_set.extend(support_ks)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/zezhishao/basicts/commit/4adec9a0403655584de5a4e97e41611c179b0f1e#diff-424cddf60b4a90b983c982297ee1c07b3ddf2079e710a5d89b564b5a3d9a4b78L15' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4400821</div><div id='project'> Project Name: zezhishao/basicts</div><div id='commit'> Commit Name: 4adec9a0403655584de5a4e97e41611c179b0f1e</div><div id='time'> Time: 2023-02-26</div><div id='author'> Author: 864453277@qq.com</div><div id='file'> File Name: basicts/archs/arch_zoo/megacrn/megacrn_arch.py</div><div id='m_class'> M Class Name: AGCN</div><div id='n_method'> N Class Name: AGCN</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: basicts/archs/arch_zoo/megacrn/megacrn_arch.py</div><div id='n_file'> N File Name: basicts/archs/arch_zoo/megacrn/megacrn_arch.py</div><div id='m_start'> M Start Line: 16</div><div id='m_end'> M End Line: 25</div><div id='n_start'> N Start Line: 18</div><div id='n_end'> N End Line: 24</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        v = self.v_proj(x_kv)

        q, k, v = (rearrange(x, "b n (h c) -&gt; (b h) n c", h=self.num_heads) for x in [q, k, v])
        attn = <a id="change">torch.einsum("b i c, b j c -&gt; b i j"</a>, q, k<a id="change">)</a> * self.dp_scale

        if pad_mask is not None:
            pad_mask = repeat(pad_mask, "b j -&gt; (b h) () j", h=self.num_heads)</code></pre><h3>After Change</h3><pre><code class='java'>

        if self.causal_attention:
            i = q.shape[2]
            j = <a id="change">k.shape[2]</a>

            causal_mask = torch.ones((i, j), device=x_q.device, dtype=torch.bool).triu(j - i + 1)
            attn.masked_fill_(causal_mask, attn_max_neg)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/krasserm/perceiver-io/commit/c2b9af32775fd28f693dd1b572142935efd31b99#diff-866d7079788f8c2d5430c75794ec5cca33c737ad6524b8b4f98feb4650471239L64' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4400820</div><div id='project'> Project Name: krasserm/perceiver-io</div><div id='commit'> Commit Name: c2b9af32775fd28f693dd1b572142935efd31b99</div><div id='time'> Time: 2022-09-25</div><div id='author'> Author: krasserm@googlemail.com</div><div id='file'> File Name: perceiver/model/core/modules.py</div><div id='m_class'> M Class Name: MultiHeadAttention</div><div id='n_method'> N Class Name: MultiHeadAttention</div><div id='m_method'> M Method Name: forward(6)</div><div id='n_method'> N Method Name: forward(5)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: perceiver/model/core/modules.py</div><div id='n_file'> N File Name: perceiver/model/core/modules.py</div><div id='m_start'> M Start Line: 64</div><div id='m_end'> M End Line: 93</div><div id='n_start'> N Start Line: 75</div><div id='n_end'> N End Line: 120</div><BR>