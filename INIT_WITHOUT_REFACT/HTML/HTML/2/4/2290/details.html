<html><h3>Pattern ID :2290
</h3><img src='7948387.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    def forward(self, labels: Union[str, Sequence[str]]) -&gt; torch.FloatTensor:
        Encode labels via the provided model and tokenizer.
        if isinstance(labels, str):
            labels = <a id="change">[</a>labels<a id="change"></a>]
        return self.model(
            **self.tokenizer(
                labels,</code></pre><h3>After Change</h3><pre><code class='java'>

    def forward(self, labels: Union[str, Sequence[str]]) -&gt; torch.FloatTensor:
        Encode labels via the provided model and tokenizer.
        labels<a id="change"> = </a>upgrade_to_sequence(labels)
        labels = <a id="change">list(</a><a id="change">map(</a>str, labels<a id="change">))</a>
        return self.model(
            **self.tokenizer(
                labels,
                return_tensors="pt",</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/pykeen/pykeen/commit/6e2219176c206f6e3e809e540abedce31c3446db#diff-816586822c329b965bdf9a03a9b5b9f0139fef86399d3ad5d85f380397436531L58' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 7948387</div><div id='project'> Project Name: pykeen/pykeen</div><div id='commit'> Commit Name: 6e2219176c206f6e3e809e540abedce31c3446db</div><div id='time'> Time: 2022-05-12</div><div id='author'> Author: berrendorf@dbs.ifi.lmu.de</div><div id='file'> File Name: src/pykeen/nn/utils.py</div><div id='m_class'> M Class Name: TransformerEncoder</div><div id='n_method'> N Class Name: TransformerEncoder</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: src/pykeen/nn/utils.py</div><div id='n_file'> N File Name: src/pykeen/nn/utils.py</div><div id='m_start'> M Start Line: 58</div><div id='m_end'> M End Line: 60</div><div id='n_start'> N Start Line: 92</div><div id='n_end'> N End Line: 93</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            blocks = self.blocks[:1] if len(blocks) == 0 else blocks

        f_args, g_args = map(lambda route: kwargs if route else {}, arg_route)
        block_kwargs = <a id="change">{</a>&quotf_args&quot: f_args, &quotg_args&quot: g_args<a id="change">}</a>        

        return _ReversibleFunction.apply(x, blocks, block_kwargs)
</code></pre><h3>After Change</h3><pre><code class='java'>

    def forward(self, x, **kwargs):
        blocks = self.blocks
        block_args<a id="change"> = </a>route_args(self.args_route, kwargs, len(self.blocks))

        if self.training and self.layer_dropout &gt; 0:
            to_drop = torch.empty(len(self.blocks)).uniform_(0, 1) &lt; self.layer_dropout
            blocks = [block for block, drop in zip(self.blocks, to_drop) if not drop]
            blocks = self.blocks[:1] if len(blocks) == 0 else blocks

        block_args = <a id="change">list(</a><a id="change">map(</a>lambda x: {&quotf_args&quot: x[0], &quotg_args&quot: x[1]}, block_args<a id="change">))</a>
        return _ReversibleFunction.apply(x, blocks, block_args)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/sinkhorn-transformer/commit/c2662a20cb783efd3351936cfabc83131060a2a6#diff-27d52caf7b4f725ff9fe7d9e57172c62a7c8744ebafb23663512e7990a5ed131L119' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 7948386</div><div id='project'> Project Name: lucidrains/sinkhorn-transformer</div><div id='commit'> Commit Name: c2662a20cb783efd3351936cfabc83131060a2a6</div><div id='time'> Time: 2020-04-10</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: sinkhorn_transformer/reversible.py</div><div id='m_class'> M Class Name: ReversibleSequence</div><div id='n_method'> N Class Name: ReversibleSequence</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: sinkhorn_transformer/reversible.py</div><div id='n_file'> N File Name: sinkhorn_transformer/reversible.py</div><div id='m_start'> M Start Line: 119</div><div id='m_end'> M End Line: 128</div><div id='n_start'> N Start Line: 134</div><div id='n_end'> N End Line: 141</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    def forward(self, x, arg_route = (True, True), **kwargs):
        f_args, g_args = map(lambda route: kwargs if route else {}, arg_route)
        block_kwargs = <a id="change">{</a>&quotf_args&quot: f_args, &quotg_args&quot: g_args<a id="change">}</a>
        return _ReversibleFunction.apply(x, self.blocks, block_kwargs)
</code></pre><h3>After Change</h3><pre><code class='java'>
        x = torch.cat([x, x], dim=-1)

        blocks = self.blocks
        args<a id="change"> = </a>route_args(self.args_route, kwargs, len(blocks))
        args = <a id="change">list(</a><a id="change">map(</a>lambda x: {&quotf_args&quot: x[0], &quotg_args&quot: x[1]}, args<a id="change">))</a>

        layers_and_args = list(zip(blocks, args))

        if self.training and self.layer_dropout &gt; 0:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/linear-attention-transformer/commit/fa23ce09a98a63d26116e3935ad5902cf705255d#diff-29d3c048298bdaa9a9670921a4026430e5b09b55bc1da20d65da18bd5c85f575L118' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 7948389</div><div id='project'> Project Name: lucidrains/linear-attention-transformer</div><div id='commit'> Commit Name: fa23ce09a98a63d26116e3935ad5902cf705255d</div><div id='time'> Time: 2020-06-04</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: linear_attention_transformer/reversible.py</div><div id='m_class'> M Class Name: ReversibleSequence</div><div id='n_method'> N Class Name: ReversibleSequence</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: linear_attention_transformer/reversible.py</div><div id='n_file'> N File Name: linear_attention_transformer/reversible.py</div><div id='m_start'> M Start Line: 118</div><div id='m_end'> M End Line: 121</div><div id='n_start'> N Start Line: 161</div><div id='n_end'> N End Line: 174</div><BR>