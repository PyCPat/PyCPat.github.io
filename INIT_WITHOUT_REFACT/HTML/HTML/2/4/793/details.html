<html><h3>Pattern ID :793
</h3><img src='2694937.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        get_norm = lambda: nn.LayerNorm(channels)

        for index in range(depth):
            self.layers.append(nn.ModuleList(<a id="change">[</a>get_attn(max(1, dim_k - index*k_reduce_by_layer)), get_norm()<a id="change"></a>]))
            if include_ff:
                self.layers.append(nn.ModuleList([get_ff(), get_norm()]))
</code></pre><h3>After Change</h3><pre><code class='java'>
            attn_layer = get_attn(max(1, dim_k - index*k_reduce_by_layer))
            ff_layer = get_ff()

            attn_layer<a id="change">, ff_layer = </a><a id="change">map(</a>lambda fn: Residual(PreNorm(channels, fn)), (attn_layer<a id="change">, ff_layer</a>)<a id="change">)</a>

            if include_ff:
                layers.extend([attn_layer, ff_layer])
            else:</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/tatp22/linformer-pytorch/commit/8f3208acf42096adf54ab8fca11b5a509d8b4eb9#diff-1ce692e4c944ce1a9eafe989300fabd5878d3e81dff51127062721b098ce8962L179' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2694937</div><div id='project'> Project Name: tatp22/linformer-pytorch</div><div id='commit'> Commit Name: 8f3208acf42096adf54ab8fca11b5a509d8b4eb9</div><div id='time'> Time: 2020-07-04</div><div id='author'> Author: tatp22@gmail.com</div><div id='file'> File Name: linformer_pytorch/linformer_pytorch.py</div><div id='m_class'> M Class Name: Linformer</div><div id='n_method'> N Class Name: Linformer</div><div id='m_method'> M Method Name: __init__(17)</div><div id='n_method'> N Method Name: __init__(18)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: linformer_pytorch/linformer_pytorch.py</div><div id='n_file'> N File Name: linformer_pytorch/linformer_pytorch.py</div><div id='m_start'> M Start Line: 179</div><div id='m_end'> M End Line: 207</div><div id='n_start'> N Start Line: 212</div><div id='n_end'> N End Line: 238</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.layers = nn.ModuleList([])
        for i in range(depth):
            should_cache = i &gt; 0 and weight_tie_layers
            cache_args = <a id="change">{</a>&quot_cache&quot: should_cache<a id="change">}</a>

            self_attns = nn.ModuleList([])

            for _ in range(self_per_cross_attn):</code></pre><h3>After Change</h3><pre><code class='java'>

        get_latent_attn = lambda: PreNorm(latent_dim, Attention(latent_dim, heads = latent_heads, dim_head = latent_dim_head))
        get_latent_ff = lambda: PreNorm(latent_dim, FeedForward(latent_dim))
        get_latent_attn<a id="change">, get_latent_ff = </a><a id="change">map(</a>cache_fn, (get_latent_attn<a id="change">, get_latent_ff</a>)<a id="change">)</a>

        self.layers = nn.ModuleList([])
        cache_args = {&quot_cache&quot: weight_tie_layers}
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/perceiver-pytorch/commit/dc530de88e6035a2f08d7e35ce23e57abe8371bd#diff-d109c4942e7a2f6d51cd8e55cbab114efc5eb0b0ce37d8e682449385df84ec13L106' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2694936</div><div id='project'> Project Name: lucidrains/perceiver-pytorch</div><div id='commit'> Commit Name: dc530de88e6035a2f08d7e35ce23e57abe8371bd</div><div id='time'> Time: 2021-08-30</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: perceiver_pytorch/perceiver_io.py</div><div id='m_class'> M Class Name: PerceiverIO</div><div id='n_method'> N Class Name: PerceiverIO</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: perceiver_pytorch/perceiver_io.py</div><div id='n_file'> N File Name: perceiver_pytorch/perceiver_io.py</div><div id='m_start'> M Start Line: 126</div><div id='m_end'> M End Line: 152</div><div id='n_start'> N Start Line: 125</div><div id='n_end'> N End Line: 143</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            shared_kv_proj = default(shared_kv_proj, attn.to_kv)
            attn.to_kv = shared_kv_proj

            self.layers.append(nn.ModuleList(<a id="change">[
                </a>Residual(PreNorm(dim, attn)),
                Residual(PreNorm(dim, ff))<a id="change"></a>
            ]))

        &#47&#47 memory parameters
</code></pre><h3>After Change</h3><pre><code class='java'>
            shared_kv_proj = default(shared_kv_proj, attn.to_kv)
            attn.to_kv = shared_kv_proj

            attn<a id="change">, ff = </a><a id="change">map(</a>lambda fn: Residual(PreNorm(dim, fn)), (attn<a id="change">, ff</a>)<a id="change">)</a>

            if seq_len == 1:
                memory_is_empty = lambda *args, **kwargs: not exists(kwargs[&quotmemory&quot])
                attn = SkipIf(memory_is_empty, attn)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/feedback-transformer-pytorch/commit/60f98187905b58c55f0a3061201d10a8f7d49122#diff-6b348c286a3e8b8e064ec9c64a1f61ec6d6b66e7f14bae4c1b91af070d30c72eL177' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2694938</div><div id='project'> Project Name: lucidrains/feedback-transformer-pytorch</div><div id='commit'> Commit Name: 60f98187905b58c55f0a3061201d10a8f7d49122</div><div id='time'> Time: 2021-02-03</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: feedback_transformer_pytorch/feedback_transformer_pytorch.py</div><div id='m_class'> M Class Name: FeedbackTransformer</div><div id='n_method'> N Class Name: FeedbackTransformer</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: feedback_transformer_pytorch/feedback_transformer_pytorch.py</div><div id='n_file'> N File Name: feedback_transformer_pytorch/feedback_transformer_pytorch.py</div><div id='m_start'> M Start Line: 204</div><div id='m_end'> M End Line: 213</div><div id='n_start'> N Start Line: 217</div><div id='n_end'> N End Line: 232</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        self.layers = nn.ModuleList([])
        for _ in range(depth):
            self.layers.append(nn.ModuleList(<a id="change">[
                </a>PreNorm(latent_dim, Attention(latent_dim, input_dim, dropout = attn_dropout), context_dim = input_dim),
                PreNorm(latent_dim, FeedForward(latent_dim, dropout = ff_dropout)),
                PreNorm(latent_dim, Attention(latent_dim, dropout = attn_dropout)),
                PreNorm(latent_dim, FeedForward(latent_dim, dropout = ff_dropout))<a id="change"></a>
            ]))

        self.to_logits = nn.Linear(latent_dim, num_classes)
</code></pre><h3>After Change</h3><pre><code class='java'>
        get_latent_ff = lambda: PreNorm(latent_dim, FeedForward(latent_dim, dropout = ff_dropout))

        if weight_tie_layers:
            get_cross_attn<a id="change">, get_cross_ff, get_latent_attn, get_latent_ff = </a><a id="change">map(</a>cache_fn, (get_cross_attn<a id="change">, get_cross_ff, get_latent_attn, get_latent_ff</a>)<a id="change">)</a>

        self.layers = nn.ModuleList([])
        for _ in range(depth):
            self.layers.append(nn.ModuleList([</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/perceiver-pytorch/commit/f0455b6ff59331de8151bf659b62ddf97ac802bd#diff-3a107568743779cad64d2e3582fceaafb7b32d2fd87d09209dce56be844a44a4L91' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2694940</div><div id='project'> Project Name: lucidrains/perceiver-pytorch</div><div id='commit'> Commit Name: f0455b6ff59331de8151bf659b62ddf97ac802bd</div><div id='time'> Time: 2021-03-04</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: perceiver_pytorch/perceiver_pytorch.py</div><div id='m_class'> M Class Name: Perceiver</div><div id='n_method'> N Class Name: Perceiver</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: perceiver_pytorch/perceiver_pytorch.py</div><div id='n_file'> N File Name: perceiver_pytorch/perceiver_pytorch.py</div><div id='m_start'> M Start Line: 115</div><div id='m_end'> M End Line: 120</div><div id='n_start'> N Start Line: 127</div><div id='n_end'> N End Line: 142</div><BR>