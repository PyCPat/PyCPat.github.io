<html><h3>Pattern ID :683
</h3><img src='2310339.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 Self-attention
        &#47&#47 TODO: 其他模型结构
        len_range = torch.from_numpy(np.arange(self.max_his)).to(history.device)
        position = (lengths[:, None] - <a id="change">len_range</a>[None, <a id="change">:</a>seq_len]) * valid_his
        pos_vectors = self.p_embeddings(position)
        his_vectors = his_vectors + pos_vectors
        attn_mask = valid_his.view(batch_size, 1, 1, seq_len)</code></pre><h3>After Change</h3><pre><code class='java'>
        his_vectors = self.i_embeddings(history)
        his_vector = self.encoder(his_vectors, lengths, valid_his, t_history, user_min_t)
        intent_pred = self.proj(his_vector)  &#47&#47 bsz, K
        <a id="change">return </a>his_vector<a id="change">, intent_pred</a>


class GRUEncoder(nn.Module):
    def __init__(self, emb_size):</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/thuwangcy/rechorus/commit/7c6b4cfaf0b7765452a8e750212a05ddf29aaae7#diff-cf7e7f7346927920e4d4df1c41793110b158e04cef7a9a1fb138ee2a0bedbbffL207' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2310339</div><div id='project'> Project Name: thuwangcy/rechorus</div><div id='commit'> Commit Name: 7c6b4cfaf0b7765452a8e750212a05ddf29aaae7</div><div id='time'> Time: 2021-08-16</div><div id='author'> Author: THUwangcy@gmail.com</div><div id='file'> File Name: src/models/developing/TiMiRec.py</div><div id='m_class'> M Class Name: IntentPredictor</div><div id='n_method'> N Class Name: IntentPredictor</div><div id='m_method'> M Method Name: forward(5)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: src/models/developing/TiMiRec.py</div><div id='n_file'> N File Name: src/models/developing/TiMiRec.py</div><div id='m_start'> M Start Line: 207</div><div id='m_end'> M End Line: 224</div><div id='n_start'> N Start Line: 217</div><div id='n_end'> N End Line: 222</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

            if flattened_dim:
                logits = rearrange(logits, &quotb ... n -&gt; b (...) n&quot)
                logits = <a id="change">logits</a>[:, <a id="change">:</a>seq_len]

            return logits
</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 bump space by one to account for a boundary case

        ids = F.pad(ids, (0<a id="change">, 0, 0, 1</a>))

        b, space, depth, device = *ids.shape, ids.device
        assert space &lt;= (self.max_spatial_seq_len + 1), &quotspatial dimension is greater than the max_spatial_seq_len set&quot
        assert depth == self.depth_seq_len, &quotdepth dimension must be equal to depth_seq_len&quot

        &#47&#47 get token embeddings

        tokens = self.token_emb(ids)

        spatial_pos = self.spatial_pos_emb(torch.arange(space, device = device))
        depth_pos = self.depth_pos_emb(torch.arange(depth, device = device))

        tokens_with_depth_pos = tokens + depth_pos

        &#47&#47 spatial tokens is tokens with depth pos reduced along depth dimension + spatial positions

        spatial_tokens = reduce(tokens_with_depth_pos, &quotb s d f -&gt; b s f&quot, &quotsum&quot) + spatial_pos

        spatial_tokens = torch.cat((
            repeat(self.spatial_start_token, &quotf -&gt; b 1 f&quot, b = b),
            spatial_tokens
        ), dim = -2)

        spatial_tokens = spatial_tokens[:, :-1]

        spatial_tokens = self.spatial_transformer(spatial_tokens)

        spatial_tokens = rearrange(spatial_tokens, &quotb s f -&gt; b s 1 f&quot)

        &#47&#47 spatial tokens become the start tokens of the depth dimension

        depth_tokens = torch.cat((spatial_tokens, tokens_with_depth_pos), dim = -2)

        depth_tokens = rearrange(depth_tokens, &quot... n d -&gt; (...) n d&quot)

        depth_tokens = self.depth_transformer(depth_tokens)

        depth_tokens = rearrange(depth_tokens, &quot(b s) d f -&gt; b s d f&quot, b = b)

        logits = self.to_logits(depth_tokens)

        if not return_loss:
            logits = logits[..., :-1, :]

            logits = rearrange(logits, &quotb ... n -&gt; b (...) n&quot)

            logits = logits[:, 1:(seq_len + 1)] &#47&#47 exclude first start token

            if flattened_dim:
                return logits

            <a id="change">return </a>rearrange(logits, &quotb (s d) n -&gt; b s d n&quot, d = depth)

        logits = logits[..., :-1, :]
        preds = rearrange(logits, &quotb ... c -&gt; b c (...)&quot)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/rq-transformer/commit/2a7448ae1a81a9da7b26d5e7e637098d2b829492#diff-0a4b6eb83a8d12dd7ed6dbd313edb989a9d2c9072f4b9f4bb83441f4996bffb7L196' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2310338</div><div id='project'> Project Name: lucidrains/rq-transformer</div><div id='commit'> Commit Name: 2a7448ae1a81a9da7b26d5e7e637098d2b829492</div><div id='time'> Time: 2022-04-12</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: rq_transformer/rq_transformer.py</div><div id='m_class'> M Class Name: RQTransformer</div><div id='n_method'> N Class Name: RQTransformer</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: rq_transformer/rq_transformer.py</div><div id='n_file'> N File Name: rq_transformer/rq_transformer.py</div><div id='m_start'> M Start Line: 198</div><div id='m_end'> M End Line: 257</div><div id='n_start'> N Start Line: 199</div><div id='n_end'> N End Line: 269</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        entity_embeddings, embedded_entity = self.entity_encoder(state.entity_state)   

        pos_index = SCHP.max_unit_type + AHP.entity_x_y_index  &#47&#47 13 + 1 + 5 + 5
        entity_x_y = <a id="change">state</a>.entity_state[:, :, <a id="change">pos_index</a>: pos_index + 8 * 2]

        &#47&#47map_skip, embedded_spatial = self.spatial_encoder(state.map_state, entity_embeddings, entity_x_y)
        map_skip, embedded_spatial = self.spatial_encoder(state.map_state)</code></pre><h3>After Change</h3><pre><code class='java'>
                            units=units, target_unit=target_unit, target_location=target_location)

        if multi_gpu_supvised_learning:
            <a id="change">return </a>action_type<a id="change">, action_type_logits, delay_logits, queue_logits, \
                units_logits, target_unit_logits, target_location_logits</a>

        if return_logits:
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/liuruoze/mini-alphastar/commit/b2d9d9a6dd94fc63f211c927f050c9857ef69d6d#diff-7978248a58b83c8226c5cf781b99297dbb195bb84c7db92daa76f7c45a310454L86' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2310337</div><div id='project'> Project Name: liuruoze/mini-alphastar</div><div id='commit'> Commit Name: b2d9d9a6dd94fc63f211c927f050c9857ef69d6d</div><div id='time'> Time: 2021-11-13</div><div id='author'> Author: liuruoze@163.com</div><div id='file'> File Name: alphastarmini/core/arch/arch_model.py</div><div id='m_class'> M Class Name: ArchModel</div><div id='n_method'> N Class Name: ArchModel</div><div id='m_method'> M Method Name: forward(10)</div><div id='n_method'> N Method Name: forward(9)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: alphastarmini/core/arch/arch_model.py</div><div id='n_file'> N File Name: alphastarmini/core/arch/arch_model.py</div><div id='m_start'> M Start Line: 91</div><div id='m_end'> M End Line: 123</div><div id='n_start'> N Start Line: 88</div><div id='n_end'> N End Line: 122</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        idx_theta2 = torch.cat([idx_theta, idx_theta], dim=1)

        &#47&#47 Calculate $[-x^{(\frac{d}{2} + 1)}, -x^{(\frac{d}{2} + 2)}, ..., -x^{(d)}, x^{(1)}, x^{(2)}, ..., -x^{(\frac{d}{2})}]$
        neg_half_x = torch.cat([-<a id="change">x</a>[:, :, :, <a id="change">d_2</a>:], x[:, :, :, :d_2]], dim=-1)

        &#47&#47 Calculate
        &#47&#47</code></pre><h3>After Change</h3><pre><code class='java'>
        x_rope = (x_rope * self.cos_cached[:x.shape[0]]) + (neg_half_x * self.sin_cached[:x.shape[0]])

        &#47&#47
        <a id="change">return </a>torch.cat((x_rope<a id="change">, x_pass</a>), dim=-1)


class RotaryPEMultiHeadAttention(MultiHeadAttention):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lab-ml/nn/commit/0ce65adf9e602321109528b05cf99fccb16cd2de#diff-637497eb531dcbf4c4fff534c48c9dc4ec23ef7b14699edc95d8b280b04de206L127' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2310336</div><div id='project'> Project Name: lab-ml/nn</div><div id='commit'> Commit Name: 0ce65adf9e602321109528b05cf99fccb16cd2de</div><div id='time'> Time: 2022-06-03</div><div id='author'> Author: vpjayasiri@gmail.com</div><div id='file'> File Name: labml_nn/transformers/rope/__init__.py</div><div id='m_class'> M Class Name: RotaryPositionalEmbeddings</div><div id='n_method'> N Class Name: RotaryPositionalEmbeddings</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: labml_nn/transformers/rope/__init__.py</div><div id='n_file'> N File Name: labml_nn/transformers/rope/__init__.py</div><div id='m_start'> M Start Line: 132</div><div id='m_end'> M End Line: 163</div><div id='n_start'> N Start Line: 171</div><div id='n_end'> N End Line: 193</div><BR>