<html><h3>Pattern ID :189
</h3><img src='835149.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        output_audio, _ = self.RNNs[1](X_audio)
        output_visual, _ = self.RNNs[2](X_visual)

        batch_size<a id="change"> = </a><a id="change">output_text.size(0</a><a id="change">)</a>

        &#47&#47 (batch, num_directions * hidden_size)
        output_text = output_text[:, -1, :]
        output_audio = output_audio[:, -1, :]
        output_visual = output_visual[:, -1, :]

        &#47&#47 (num_classes, 300)
        text_emo_vecs = self.textEmoEmbs(torch.LongTensor(list(range(self.num_classes))))
        visual_emo_vecs = self.affineVisual(text_emo_vecs)
        audio_emo_vecs = self.affineAudio(text_emo_vecs)

        text_emo_vecs = text_emo_vecs.unsqueeze(0).repeat(batch_size, 1, 1)
        visual_emo_vecs = visual_emo_vecs.unsqueeze(0).repeat(batch_size, 1, 1)
        audio_emo_vecs = audio_emo_vecs.unsqueeze(0).repeat(batch_size, 1, 1)

        text_attn_feature = self.attention(output_text, text_emo_vecs)
        visual_attn_feature = self.attention(output_visual, visual_emo_vecs)
        audio_attn_feature = self.attention(output_audio, audio_emo_vecs)

        &#47&#47 TODO: try residual connection

        logits<a id="change"> = </a>self.out(torch.cat((text_attn_feature, visual_attn_feature, audio_attn_feature), dim=1))
        return logits
</code></pre><h3>After Change</h3><pre><code class='java'>
            output_text, _ = self.RNNs[0](X_text)
            output_text = output_text[:, -1, :]
            text_emo_vecs_origin = self.textEmoEmbs(torch.LongTensor(list(range(self.num_classes))).to(self.device))
            text_emo_vecs = <a id="change">text_emo_vecs_origin.unsqueeze(0</a><a id="change">)</a>.repeat(batch_size, 1, 1)
            text_attn_weights = self.attention(output_text, text_emo_vecs)
            logits = text_attn_weights if logits is None else logits + text_attn_weights
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 5</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/wenliangdai/modality-transferable-mer/commit/b0e565d11d6b3bf9f65fb1dcbdc8c641a2bc8054#diff-9dca09e4df72ee7b751c74b7ea8c66a06c2d78f76fef6a3bdb604dc6680fd2efL53' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 835149</div><div id='project'> Project Name: wenliangdai/modality-transferable-mer</div><div id='commit'> Commit Name: b0e565d11d6b3bf9f65fb1dcbdc8c641a2bc8054</div><div id='time'> Time: 2020-06-10</div><div id='author'> Author: wenliang.dai.1995@gmail.com</div><div id='file'> File Name: src/models/temp.py</div><div id='m_class'> M Class Name: EmotionEmbAttnModel</div><div id='n_method'> N Class Name: EmotionEmbAttnModel</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: src/models/temp.py</div><div id='n_file'> N File Name: src/models/temp.py</div><div id='m_start'> M Start Line: 53</div><div id='m_end'> M End Line: 79</div><div id='n_start'> N Start Line: 70</div><div id='n_end'> N End Line: 96</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        if inputs is not None:
            &#47&#47 Grouping multiple frames if necessary
            if inputs.size(-1) == self.mel_dim:
                inputs<a id="change"> = </a>inputs.reshape(B, <a id="change">inputs.size(1</a><a id="change">)</a> // self.r, -1)
            assert inputs.size(-1) == self.mel_dim * self.r
            T_decoder<a id="change"> = </a>inputs.size(1)

        &#47&#47 Time first (T&quot, B, mel_dim*r)
        if inputs is not None:</code></pre><h3>After Change</h3><pre><code class='java'>

            &#47&#47 Store predictions
            mel_outputs.append(output)
            attn_scores.append(<a id="change">attention_score.unsqueeze(1</a><a id="change">)</a>)
            stop_tokens.extend([stop] * self.r)

            if greedy:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/thuhcsi/tacotron/commit/fea9ec535ec373aad564646f4f292fbee0217c29#diff-7ad5d3b60745b0b46860cc4003e6b1af82b421d3df25282bfacabd1db690e1b9L110' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 835140</div><div id='project'> Project Name: thuhcsi/tacotron</div><div id='commit'> Commit Name: fea9ec535ec373aad564646f4f292fbee0217c29</div><div id='time'> Time: 2021-03-18</div><div id='author'> Author: johnson.tsing@gmail.com</div><div id='file'> File Name: model/tacotron2.py</div><div id='m_class'> M Class Name: Decoder</div><div id='n_method'> N Class Name: Decoder</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: model/tacotron2.py</div><div id='n_file'> N File Name: model/tacotron2.py</div><div id='m_start'> M Start Line: 127</div><div id='m_end'> M End Line: 224</div><div id='n_start'> N Start Line: 127</div><div id='n_end'> N End Line: 216</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        if inputs is not None:
            &#47&#47 Grouping multiple frames if necessary
            if inputs.size(-1) == self.mel_dim:
                inputs<a id="change"> = </a>inputs.reshape(B, <a id="change">inputs.size(1</a><a id="change">)</a> // self.r, -1)
            assert inputs.size(-1) == self.mel_dim * self.r
            T_decoder<a id="change"> = </a>inputs.size(1)

        &#47&#47 Time first (T&quot, B, mel_dim*r)
        if inputs is not None:</code></pre><h3>After Change</h3><pre><code class='java'>

            &#47&#47 Store predictions
            mel_outputs.append(output)
            attn_scores.append(<a id="change">attention_score.unsqueeze(1</a><a id="change">)</a>)
            stop_tokens.extend([stop] * self.r)

            if greedy:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/thuhcsi/tacotron/commit/fea9ec535ec373aad564646f4f292fbee0217c29#diff-d9b8d279159e71117d405f97592a8727c16e635c5fc8b968d2c0b3bf1f1d9df1L70' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 835152</div><div id='project'> Project Name: thuhcsi/tacotron</div><div id='commit'> Commit Name: fea9ec535ec373aad564646f4f292fbee0217c29</div><div id='time'> Time: 2021-03-18</div><div id='author'> Author: johnson.tsing@gmail.com</div><div id='file'> File Name: model/tacotron.py</div><div id='m_class'> M Class Name: Decoder</div><div id='n_method'> N Class Name: Decoder</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: model/tacotron.py</div><div id='n_file'> N File Name: model/tacotron.py</div><div id='m_start'> M Start Line: 87</div><div id='m_end'> M End Line: 187</div><div id='n_start'> N Start Line: 88</div><div id='n_end'> N End Line: 180</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        g_t = torch.softmax(g_t, dim=-1) / sig_t + self.eps

        &#47&#47 each B x K x T_in
        g_t<a id="change"> = </a>g_t.unsqueeze(2).expand(g_t.size(0),
                                      <a id="change">g_t.size(1</a><a id="change">)</a>,
                                      inputs.size(1))
        sig_t<a id="change"> = </a>sig_t.unsqueeze(2).expand_as(g_t)
        mu_t_ = mu_t.unsqueeze(2).expand_as(g_t)
        j = self.J[:g_t.size(0), :, :inputs.size(1)]
</code></pre><h3>After Change</h3><pre><code class='java'>
        j = self.J[:inputs.size(1)]

        &#47&#47 attention weights
        phi_t = g_t.unsqueeze(-1) * torch.exp(-0.5 * (mu_t.unsqueeze(-1) - j)**2 / (<a id="change">sig_t.unsqueeze(-1</a><a id="change">)</a>**2))
        alpha_t = self.COEF * torch.sum(phi_t, 1)

        &#47&#47 apply masking</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/coqui-ai/tts/commit/0e8881114b7cd223a41a452ea7cf570b56c109a7#diff-1ca76b8dab5cda419aaf68884653245f0428af8e57cdb707fc538936bb59af29L144' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 835137</div><div id='project'> Project Name: coqui-ai/tts</div><div id='commit'> Commit Name: 0e8881114b7cd223a41a452ea7cf570b56c109a7</div><div id='time'> Time: 2020-01-10</div><div id='author'> Author: root@sp-mlc3-5423-0.mlc</div><div id='file'> File Name: layers/common_layers.py</div><div id='m_class'> M Class Name: GravesAttention</div><div id='n_method'> N Class Name: GravesAttention</div><div id='m_method'> M Method Name: forward(5)</div><div id='n_method'> N Method Name: forward(5)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: layers/common_layers.py</div><div id='n_file'> N File Name: layers/common_layers.py</div><div id='m_start'> M Start Line: 162</div><div id='m_end'> M End Line: 176</div><div id='n_start'> N Start Line: 346</div><div id='n_end'> N End Line: 355</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        k = k if torch.is_tensor(k) else q

        q = self.proj_q(q).view(-1, q.size(1), self._head_dims)
        k = self.proj_k(k).view(-1, <a id="change">k.size(1</a><a id="change">)</a>, self._head_dims)
        v = self.proj_v(v).view(-1, v.size(1), self._head_dims)

        att = torch.bmm(q, k.transpose(1, 2)) / self._h_dims**0.5

        if mask is not None:
            mask = torch.where(mask &gt; 0, .0, float(&quot-inf&quot))
            att += mask.repeat_interleave(self._heads, dim=0)

        att = att.softmax(-1)

        if self.dropout is not None:
            att = self.dropout(att)

        m<a id="change"> = </a>torch.bmm(att, v).view(-1, att.size(1), self._h_dims)
        m<a id="change"> = </a>self.proj_m(m)

        return m
</code></pre><h3>After Change</h3><pre><code class='java'>
        if mask is not None:
            mask = torch.where(mask &gt; 0, .0, float(&quot-inf&quot))
            mask = mask.repeat_interleave(self._heads, dim=0)
            att += <a id="change">mask.unsqueeze(1</a><a id="change">)</a>.expand(-1, att.size(1), -1)

        att = att.softmax(-1)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/yeliudev/nncore/commit/e58a22da4dce9778c38aae284b0c80d84937b04c#diff-b66764790ba7b310b71bcb990fc01ea802675de40cfbd1e8da361bbc4827b8c3L78' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 835138</div><div id='project'> Project Name: yeliudev/nncore</div><div id='commit'> Commit Name: e58a22da4dce9778c38aae284b0c80d84937b04c</div><div id='time'> Time: 2021-11-05</div><div id='author'> Author: yeliudev@outlook.com</div><div id='file'> File Name: nncore/nn/blocks/transformer.py</div><div id='m_class'> M Class Name: MultiHeadAttention</div><div id='n_method'> N Class Name: MultiHeadAttention</div><div id='m_method'> M Method Name: forward(5)</div><div id='n_method'> N Method Name: forward(5)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: nncore/nn/blocks/transformer.py</div><div id='n_file'> N File Name: nncore/nn/blocks/transformer.py</div><div id='m_start'> M Start Line: 79</div><div id='m_end'> M End Line: 98</div><div id='n_start'> N Start Line: 79</div><div id='n_end'> N End Line: 105</div><BR>