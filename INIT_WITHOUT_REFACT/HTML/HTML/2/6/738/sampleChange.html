<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            &#47&#47 todo: is clamp really necessary?
            log_std = self.std_clamp(log_std)
            std = log_std.exp()
            m = MultivariateNormal(mean.reshape(-1), <a id="change">torch.diag(</a>std.reshape(-1)<a id="change">)</a>)
            action = m.sample()
            action = action.reshape(mean.shape)
</code></pre><h3>After Change</h3><pre><code class='java'>
            action = action_base.tanh()

            &#47&#47 According to "Soft Actor-Critic" (Haarnoja et. al) Appendix C
            action_bound_compensation = torch.log(<a id="change">1.</a><a id="change"> - action.tanh().pow(2) + </a>1e-6)
            action_bound_compensation<a id="change"> = </a><a id="change">action_bound_compensation.sum(dim=-1, keepdim=True)</a>
            log_prob.sub_(action_bound_compensation)

        return action, log_prob
</code></pre>