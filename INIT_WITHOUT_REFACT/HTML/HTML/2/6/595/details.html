<html><h3>Pattern ID :595
</h3><img src='2037343.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 masked by the missing entries
        &#47&#47 note, different batch may contain different number of real entities
        tensor_list = <a id="change">[]</a>
        for i, batch in enumerate(out):
            mean_entity = 0.
            real_number = real_number_tensor[i]
            real_number = real_number if real_number != 0 else 1
            for j, entity in enumerate(batch):
                if j &gt;= real_number_tensor[i]:
                    break        
                mean_entity = mean_entity + entity
            mean_entity = mean_entity / (real_number)
            tensor_list.append(mean_entity.reshape(1, -1))
        tensor_mean<a id="change"> = </a>torch.cat(tensor_list, dim=0)
        print(&quottensor_mean:&quot, tensor_mean) if debug else None

        &#47&#47 print(&quotout.shape:&quot, out.shape) if debug else None</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 generate the mask for transformer
        mask = torch.arange(0, self.max_entities).float()
        mask = mask.repeat(batch_size, 1)
        mask<a id="change"> = </a>mask &lt; entity_num.unsqueeze(dim=1)

        print(&quotmask:&quot, mask) if debug else None
        print(&quotmask.shape:&quot, mask.shape) if debug else None

        &#47&#47 mask: [batch_size, max_entities]
        device = next(self.parameters()).device
        mask = mask.to(device)

        &#47&#47 assert the input shape is : batch_seq_size x entities_size x embeding_size
        &#47&#47 note: because the feature size of entity is not equal to 256, so it can not fed into transformer directly.
        &#47&#47 thus, we add a embedding layer to transfer it to right size.
        &#47&#47 x is batch_entities_tensor (dim = 3). Shape: batch_seq_size x entities_size x embeding_size
        x = self.embedd(x)
        print(&quotx.shape:&quot, x.shape) if debug else None

        &#47&#47 mask for transformer need a special format
        mask_seq_len = mask.shape[-1]
        tran_mask = mask.unsqueeze(1)

        &#47&#47 tran_mask: [batch_seq_size x max_entities x max_entities]
        tran_mask = tran_mask.repeat(1, mask_seq_len, 1)

        &#47&#47 out: [batch_seq_size x entities_size x embeding_size]
        out = self.transformer(x, mask=tran_mask)
        print(&quotout.shape:&quot, out.shape) if debug else None

        &#47&#47 entity_embeddings: [batch_seq_size x entities_size x conv1_output_size]
        entity_embeddings = F.relu(self.conv1(F.relu(out).transpose(1, 2))).transpose(1, 2)
        print(&quotentity_embeddings.shape:&quot, entity_embeddings.shape) if debug else None

        &#47&#47 AlphaStar: The mean of the transformer output across across the units (masked by the missing entries) 
        &#47&#47 is fed through a linear layer of size 256 and a ReLU to yield `embedded_entity`
        masked_out<a id="change"> = </a>out * <a id="change">mask.unsqueeze(2</a><a id="change">)</a>

        &#47&#47 sum over across the units
        &#47&#47 masked_out: [batch_seq_size x entities_size x embeding_size]</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/liuruoze/mini-alphastar/commit/612d42a7bf5ef827e1e919198d839fce106155cd#diff-71869e70f9cb835282d541d2d3529d25b6ed6795c79bc770e860e9318619f6b1L716' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2037343</div><div id='project'> Project Name: liuruoze/mini-alphastar</div><div id='commit'> Commit Name: 612d42a7bf5ef827e1e919198d839fce106155cd</div><div id='time'> Time: 2021-11-25</div><div id='author'> Author: liuruoze@163.com</div><div id='file'> File Name: alphastarmini/core/arch/entity_encoder.py</div><div id='m_class'> M Class Name: EntityEncoder</div><div id='n_method'> N Class Name: EntityEncoder</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: alphastarmini/core/arch/entity_encoder.py</div><div id='n_file'> N File Name: alphastarmini/core/arch/entity_encoder.py</div><div id='m_start'> M Start Line: 721</div><div id='m_end'> M End Line: 758</div><div id='n_start'> N Start Line: 716</div><div id='n_end'> N End Line: 778</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        if memory is not None:
            memory = memory.transpose(0, 1)
        outputs = []
        alignments = <a id="change">[]</a>
        stop_tokens = []
        t = 0
        memory_input = initial_memory
        while True:
            if t &gt; 0:
                if greedy:
                    memory_input = outputs[-1]
                else:
                    memory_input = memory[t-1]
            &#47&#47 Prenet
            processed_memory = self.prenet(memory_input)
            &#47&#47 Attention RNN
            attention_rnn_hidden, current_context_vec, alignment = self.attention_rnn(
                processed_memory, current_context_vec, attention_rnn_hidden, inputs)
            &#47&#47 Concat RNN output and attention context vector
            decoder_input = self.project_to_decoder_in(
                torch.cat((attention_rnn_hidden, current_context_vec), -1))
            &#47&#47 Pass through the decoder RNNs
            for idx in range(len(self.decoder_rnns)):
                decoder_rnn_hiddens[idx] = self.decoder_rnns[idx](
                    decoder_input, decoder_rnn_hiddens[idx])
                &#47&#47 Residual connectinon
                decoder_input = decoder_rnn_hiddens[idx] + decoder_input
            decoder_output = decoder_input
            &#47&#47 predict mel vectors from decoder vectors
            output = self.proj_to_mel(decoder_output)
            stop_input = output
            &#47&#47 predict stop token
            stop_token, stopnet_rnn_hidden = self.stopnet(stop_input, stopnet_rnn_hidden)
            outputs += [output]
            alignments<a id="change"> += </a>[alignment]
            stop_tokens += [stop_token]
            t += 1
            if (not greedy and self.training) or (greedy and memory is not None):</code></pre><h3>After Change</h3><pre><code class='java'>
            for _ in range(len(self.decoder_rnns))]
        current_context_vec = inputs.data.new(B, 256).zero_()
        stopnet_rnn_hidden = inputs.data.new(B, self.r * self.memory_dim).zero_()
        attention_vec<a id="change"> = </a>memory.data.new(B, T).zero_()
        attention_vec_cum = memory.data.new(B, T).zero_()
        &#47&#47 Time first (T_decoder, B, memory_dim)
        if memory is not None:
            memory = memory.transpose(0, 1)
        outputs = []
        attentions = []
        stop_tokens = []
        t = 0
        memory_input = initial_memory
        while True:
            if t &gt; 0:
                if greedy:
                    memory_input = outputs[-1]
                else:
                    memory_input = memory[t-1]
            &#47&#47 Prenet
            processed_memory = self.prenet(memory_input)
            &#47&#47 Attention RNN
            attention_vec_cat<a id="change"> = </a>torch.cat((<a id="change">attention_vec.unsqueeze(1</a><a id="change">)</a>,
                                               attention_vec_cum.unsqueeze(1)),
                                           dim=1)
            attention_rnn_hidden, current_context_vec, attention = self.attention_rnn(</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/coqui-ai/tts/commit/d43b4ccc1e1b6ba46125c8df97eec5711179edfe#diff-63415e5bdfe9c172be8a9cadb35d1f47d718fea5f15c6879f5715ebb9e331155L237' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2037342</div><div id='project'> Project Name: coqui-ai/tts</div><div id='commit'> Commit Name: d43b4ccc1e1b6ba46125c8df97eec5711179edfe</div><div id='time'> Time: 2018-05-17</div><div id='author'> Author: egolge@mozilla.com</div><div id='file'> File Name: layers/tacotron.py</div><div id='m_class'> M Class Name: Decoder</div><div id='n_method'> N Class Name: Decoder</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: layers/tacotron.py</div><div id='n_file'> N File Name: layers/tacotron.py</div><div id='m_start'> M Start Line: 276</div><div id='m_end'> M End Line: 325</div><div id='n_start'> N Start Line: 254</div><div id='n_end'> N End Line: 333</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                             (1 - self.beta) + res_attn * self.beta

        with g.local_scope():
            h_prime = <a id="change">[]</a>
            emb = emb.permute(1, 0, 2).contiguous()
            for i in range(self.num_heads):
                g.edata[&quotalpha&quot] = edge_attention[:, i]
                g.srcdata.update({&quotemb&quot: emb[i]})
                g.update_all(Fn.u_mul_e(&quotemb&quot, &quotalpha&quot, &quotm&quot),
                             Fn.sum(&quotm&quot, &quotemb&quot))
                h_prime.append(g.ndata[&quotemb&quot])
            h_output<a id="change"> = </a>torch.cat(h_prime, dim=1)

        g.edata[&quotalpha&quot] = edge_attention
        if self.residual:</code></pre><h3>After Change</h3><pre><code class='java'>
            edge_attention = edge_attention * \
                             (1 - self.beta) + res_attn * self.beta
        if self.num_heads == 1:
            edge_attention<a id="change"> = </a>edge_attention[:, 0]
            edge_attention<a id="change"> = </a><a id="change">edge_attention.unsqueeze(1</a><a id="change">)</a>

        with g.local_scope():
            emb = emb.permute(0, 2, 1).contiguous()
            g.edata[&quotalpha&quot] = edge_attention</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/bupt-gamma/openhgnn/commit/c2a6b5ad048e2d937b6642b0106dad5faa75a335#diff-59f1f6852baf41601055fbdbc029a635a162bb43b4244576cc8c6184f15c96d5L156' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2037333</div><div id='project'> Project Name: bupt-gamma/openhgnn</div><div id='commit'> Commit Name: c2a6b5ad048e2d937b6642b0106dad5faa75a335</div><div id='time'> Time: 2022-02-25</div><div id='author'> Author: 996179900@qq.com</div><div id='file'> File Name: openhgnn/models/SimpleHGN.py</div><div id='m_class'> M Class Name: SimpleHGNConv</div><div id='n_method'> N Class Name: SimpleHGNConv</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: openhgnn/models/SimpleHGN.py</div><div id='n_file'> N File Name: openhgnn/models/SimpleHGN.py</div><div id='m_start'> M Start Line: 159</div><div id='m_end'> M End Line: 190</div><div id='n_start'> N Start Line: 158</div><div id='n_end'> N End Line: 190</div><BR>