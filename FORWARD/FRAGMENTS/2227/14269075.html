<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        t_0 = self.l_4(t_0, attention_mask=decoder_attention_mask, position_bias=x1, encoder_hidden_states=x0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=x2)
        &#47&#47 returning:
        &#47&#47 T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]
        <a id="change">return </a>(t_0<a id="change"></a>,)

    def state_dict(self,*args,**kwargs):
        &#47&#47 we return the state dict of this part as it should be in the original model</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 moving inputs to current device no op if already on the correct device
        attention_mask, decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x1, attention_mask=attention_mask, position_bias=x0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        t_0<a id="change"> = </a>self.l_1(t_0, attention_mask=attention_mask, position_bias=x0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        t_0<a id="change"> = </a>self.l_2(t_0)
        t_0<a id="change"> = </a>self.l_3(t_0)
        t_1<a id="change"> = </a>self.l_4(x2)
        t_1 = self.l_5(t_1, attention_mask=decoder_attention_mask, position_bias=None, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=None)
        t_2 = t_1[0]
        t_3<a id="change"> = </a>t_1[1]
        t_1<a id="change"> = </a>t_1[2]
        t_2 = self.l_6(t_2, attention_mask=decoder_attention_mask, position_bias=t_3, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=t_1)
        t_2 = self.l_7(t_2, attention_mask=decoder_attention_mask, position_bias=t_3, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=t_1)
        t_2 = self.l_8(t_2, attention_mask=decoder_attention_mask, position_bias=t_3, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=t_1)
        t_2 = self.l_9(t_2, attention_mask=decoder_attention_mask, position_bias=t_3, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=t_1)
        t_2 = self.l_10(t_2, attention_mask=decoder_attention_mask, position_bias=t_3, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=t_1)
        t_2 = self.l_11(t_2, attention_mask=decoder_attention_mask, position_bias=t_3, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=t_1)
        &#47&#47 returning:
        &#47&#47 T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        &#47&#47 T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130
        &#47&#47 T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132
        &#47&#47 T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]
        <a id="change">return </a>list(flatten((t_0<a id="change">, t_3, t_1, t_2</a>)))

    def state_dict(self,*args,**kwargs):
        &#47&#47 we return the state dict of this part as it should be in the original model</code></pre>