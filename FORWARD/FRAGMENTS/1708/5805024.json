{"BEFORE":"            processed_memory = memory\n\n        if memory_lengths is not None and mask is None:\n            mask = get_mask_from_lengths(memory, memory_lengths)\n\n        # Concat input query and previous attention context\n        cell_input = torch.cat((query, attention), -1)\n\n        # Feed it to RNN\n        cell_output = self.rnn_cell(cell_input, cell_state)\n\n        # Alignment\n        # (batch, max_time)\n        alignment = self.attention_mechanism(cell_output, processed_memory)\n\n        if mask is not None:\n            mask = mask.view(query.size(0), -1)\n            alignment.data.masked_fill_(mask, self.score_mask_value)\n\n        # Normalize attention weight\n        alignment = F.softmax(alignment, dim=-1)\n\n        # Attention context vector\n        # (batch, 1, dim)\n        attention = torch.bmm(alignment.unsqueeze(1), memory)\n\n        # (batch, dim)\n        attention = attention.squeeze(1)\n\n        return cell_output, attention, alignment\n","AFTER":"            processed_inputs = memory\n\n        if memory_lengths is not None and mask is None:\n            mask = get_mask_from_lengths(memory, memory_lengths)\n\n        # Concat input query and previous context_vec context\n        import ipdb\n        ipdb.set_trace()\n        cell_input = torch.cat((query, context_vec), -1)\n\n        # Feed it to RNN\n        cell_output = self.rnn_cell(cell_input, cell_state)\n\n        # Alignment\n        # (batch, max_time)\n        alignment = self.alignment_model(cell_output, processed_inputs)\n\n        if mask is not None:\n            mask = mask.view(query.size(0), -1)\n            alignment.data.masked_fill_(mask, self.score_mask_value)\n\n        # Normalize context_vec weight\n        alignment = F.softmax(alignment, dim=-1)\n\n        # Attention context vector\n        # (batch, 1, dim)\n        context_vec = torch.bmm(alignment.unsqueeze(1), memory)\n\n        # (batch, dim)\n        context_vec = context_vec.squeeze(1)\n\n        return cell_output, context_vec, alignment\n"}