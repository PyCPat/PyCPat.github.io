{"BEFORE":"        return x\n","AFTER":"        batch, c, h, w, device, img_size, = *img.shape, img.device, self.image_size\n        assert h == img_size and w == img_size, f'height and width of image must be {img_size}'\n\n        # sample random times\n\n        times = torch.zeros((batch,), device = device).float().uniform_(0, 1.)\n\n        # noise sample\n\n        noise = torch.randn_like(img)\n\n        noise_level = self.log_snr(times)\n        padded_noise_level = right_pad_dims_to(img, noise_level)\n        alpha, sigma =  log_snr_to_alpha_sigma(padded_noise_level)\n\n        noised_img = alpha * img + sigma * noise\n\n        # if doing self-conditioning, 50% of the time, predict x_start from current set of times\n        # and condition with unet with that\n        # this technique will slow down training by 25%, but seems to lower FID significantly\n\n        self_cond = None\n        if random() < 0.5:\n            with torch.no_grad():\n                self_cond = self.model(noised_img, noise_level).detach_()\n\n        # predict and take gradient step\n\n        pred = self.model(noised_img, noise_level, self_cond)\n\n        return F.mse_loss(pred, img)\n"}