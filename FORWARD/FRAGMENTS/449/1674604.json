{"BEFORE":"                batch_size = context.size(0)\n\n                zero = torch.zeros(self.args.rnn_layers, batch_size, self.args.rnn_dimension,\n                                   dtype=torch.float, requires_grad=False, device=context.device)\n                context_rnn_state = (zero, zero)\n                question_rnn_state = (zero, zero)\n            else:\n                assert self.args.rnn_zero_state == 'average'\n                batch_size = context.size(0)\n\n                masked_final_context = context_embedded.last_layer.masked_fill(context_padding.unsqueeze(2), 0)\n                summed_context = torch.sum(masked_final_context, dim=1)\n                average_context = summed_context \/ context_lengths.unsqueeze(1)\n\n                packed_rnn_state = self.norm(self.pool(average_context))\n","AFTER":"        context_embedded = self.encoder_embeddings(context, padding=context_padding)\n        question_embedded = self.encoder_embeddings(question, padding=question_padding)\n\n        # pick the top-most N transformer layers to pass to the decoder for cross-attention\n        # (add 1 to account for the embedding layer - the decoder will drop it later)\n        self_attended_context = context_embedded.all_layers[-(self.args.transformer_layers + 1):]\n        final_context = context_embedded.last_layer\n        final_question = question_embedded.last_layer\n\n        if self.projection is not None:\n            final_context = self.dropout(final_context)\n            final_context = self.projection(final_context)\n\n            final_question = self.dropout(final_question)\n            final_question = self.projection(final_question)\n\n        context_rnn_state = None\n        question_rnn_state = None\n        if self.args.rnn_layers > 0:\n            batch_size = context.size(0)\n            if self.args.rnn_zero_state == 'zero':\n\n                zero = torch.zeros(self.args.rnn_layers, batch_size, self.args.rnn_dimension,\n                                   dtype=torch.float, requires_grad=False, device=context.device)\n                context_rnn_state = (zero, zero)\n                question_rnn_state = (zero, zero)\n            else:\n                if self.args.rnn_zero_state == 'cls':\n                    packed_rnn_state = self.norm(self.pool(context_embedded.last_layer[:, 0, :]))\n\n                elif self.args.rnn_zero_state == 'average':\n                    masked_final_context = context_embedded.last_layer.masked_fill(context_padding.unsqueeze(2), 0)\n                    summed_context = torch.sum(masked_final_context, dim=1)\n                    average_context = summed_context \/ context_lengths.unsqueeze(1)\n\n                    packed_rnn_state = self.norm(self.pool(average_context))\n\n                # packed_rnn_state is (batch, 2 * rnn_layers * rnn_dim)\n                packed_rnn_state = packed_rnn_state.reshape(batch_size, 2, self.args.rnn_layers,\n"}