{"BEFORE":"        tokens = self.token_emb(ids)\n\n        # get tokens for all hierarchical stages, reducing by appropriate dimensions\n        # and adding the absolute positional embeddings\n\n        tokens_at_stages = []\n        reduced_tokens = tokens.clone()\n\n        for ind, pos_emb in zip(range(len(prec_dims)), reversed(self.pos_embs)):\n            is_first = ind == 0\n\n            if not is_first:\n                reduced_tokens = reduce(reduced_tokens, 'b ... r d -> b ... d', 'sum')\n\n            positions = pos_emb(torch.arange(reduced_tokens.shape[-2], device = device))\n            tokens_with_position = reduced_tokens + positions\n            tokens_at_stages.insert(0, tokens_with_position)\n\n        # get start tokens and append to the coarsest stage\n\n        start_tokens = repeat(self.start_tokens, 'f -> b 1 f', b = b)\n\n        # spatial tokens is tokens with depth pos reduced along depth dimension + spatial positions        \n\n        for stage_tokens, transformer in zip(tokens_at_stages, self.transformers):\n            stage_tokens = torch.cat((\n                start_tokens,\n                stage_tokens,\n            ), dim = -2)            \n\n            *prec_dims, _, _ = stage_tokens.shape\n\n            stage_tokens = rearrange(stage_tokens, '... n d -> (...) n d')\n            attended = transformer(stage_tokens[:, :-1])\n            attended = rearrange_with_anon_dims(attended, '(...b) n d -> ...b n d', b = prec_dims)\n\n            start_tokens = rearrange(attended, '... n d -> ... n 1 d')\n\n        logits = self.to_logits(attended)\n\n        if flattened_dims:\n            logits = rearrange(logits, 'b ... n -> b (...) n')\n            logits = logits[:, :seq_len]\n\n        if not return_loss:\n            return logits\n\n        preds = rearrange(logits, 'b ... c -> b c (...)')\n\n        labels = rearrange(ids, 'b ... -> b (...)')\n        labels = labels[:, :preds.shape[-1]]\n","AFTER":"        tokens = self.token_emb(ids)\n\n        # get tokens for all hierarchical stages, reducing by appropriate dimensions\n        # and adding the absolute positional embeddings\n\n        tokens_at_stages = []\n        reduced_tokens = tokens\n\n        for ind, pos_emb in zip(range(len(prec_dims)), reversed(self.pos_embs)):\n            is_first = ind == 0\n\n            if not is_first:\n                reduced_tokens = reduce(reduced_tokens, 'b ... r d -> b ... d', 'sum')\n\n            positions = pos_emb(torch.arange(reduced_tokens.shape[-2], device = device))\n            tokens_with_position = reduced_tokens + positions\n            tokens_at_stages.insert(0, tokens_with_position)\n\n        # get start tokens and append to the coarsest stage\n\n        start_tokens = repeat(self.start_tokens, 'f -> b 1 f', b = b)\n\n        # spatial tokens is tokens with depth pos reduced along depth dimension + spatial positions        \n\n        for ind, (stage_tokens, transformer) in enumerate(zip(tokens_at_stages, self.transformers)):\n            is_last = ind == (self.stages - 1)\n\n            stage_tokens = torch.cat((\n                start_tokens,\n                stage_tokens,\n            ), dim = -2)\n\n            *prec_dims, _, _ = stage_tokens.shape\n\n            stage_tokens = rearrange(stage_tokens, '... n d -> (...) n d')\n            attended = transformer(stage_tokens)\n            attended = rearrange_with_anon_dims(attended, '(...b) n d -> ...b n d', b = prec_dims)\n\n            start_tokens = rearrange(attended[..., :-1, :], '... n d -> ... n 1 d')\n\n        logits = self.to_logits(attended)\n\n        if not return_loss:\n            logits = logits[..., 1:, :]\n\n            if flattened_dims:\n                logits = rearrange(logits, 'b ... n -> b (...) n')\n                logits = logits[:, :seq_len]\n\n            return logits\n\n        logits = logits[..., :-1, :]\n"}