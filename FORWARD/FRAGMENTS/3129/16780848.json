{"BEFORE":"        h = torch.cat((mem, x), dim=1)\n        \n        batch_size, seg_len, embed_dim = x.shape\n        mem_len = h.shape[1] - seg_len\n        total_len = h.shape[1]\n        \n        # compute projections of output from previous layer and the memory\n        q = self.w_q(x).reshape(batch_size, -1, seg_len, embed_dim)\n        k = self.w_ke(h).reshape(batch_size, -1, total_len, embed_dim)\n        v = self.w_v(h).reshape(batch_size, -1, total_len, embed_dim)\n        r = self.w_kr(self.pos).reshape(-1, total_len, embed_dim)\n        \n        # compute relative positional encodings\n        b = q @ r.transpose(1, 2)\n        b = self.circulant_shift(b, -seg_len+1)\n        \n        # this is the XL specific way of computing the attention score\n        k = k.transpose(2, 3)\n        att = q @ k + b + self.u @ k + self.v @ r.transpose(1, 2)\n        att = att.tril(mem_len) \/ embed_dim**0.5\n        att = torch.softmax(att, dim=-1)\n        \n        # compute the output of the layer and save to memory\n        out = self.layer_norm(self.mlp(att @ v) + x)\n        out = self.pos_ff(out)\n        self.save_to_memory(out)\n        \n        return out\n","AFTER":"    def forward(self, x, mem, att_mask):\n        # concat output from previous layer with \"memory\" from earlier segments\n        h = torch.cat((mem, x), dim=1)\n        \n        batch_size, seg_len, embed_dim = x.shape\n        mem_len = h.shape[1] - seg_len\n        total_len = h.shape[1]\n        \n        # compute projections of input and memory embeddings\n        q = self.w_q(x).view(batch_size, -1, seg_len, embed_dim)\n        kv = self.w_kv(h).view(2*batch_size, -1, total_len, embed_dim)\n        k, v = kv.chunk(2, dim=0)\n        k = k.transpose(2, 3) # only using transposed k below\n        \n        # relative distance between two tokens is max total_len\n        pos = self.pos[-total_len:]\n        r = self.w_r(pos).view(-1, total_len, embed_dim)\n        \n        # compute relative positional encodings\n        b = q @ r.transpose(1, 2)\n        b = self.circulant_shift(b, -seg_len+1)\n        \n        # u1 and u2 should be identical for all query vectors\n        u1 = self.u1.repeat(1, seg_len, 1)\n        u2 = self.u2.repeat(1, seg_len, 1)\n        \n        # this is the XL specific way of computing the attention score\n        #att_mask = att_mask.unsqueeze(1).unsqueeze(-1).repeat(1,10,1,total_len)\n        att_score = q @ k + b + u1 @ k + u2 @ r.transpose(1, 2)\n        #att_score = att_score * att_mask\n        att_score = att_score.tril(mem_len) \/ embed_dim**0.5\n        att_score[att_score == 0] = float(\"-inf\")\n        att_score = torch.softmax(att_score, dim=-1)\n        \n        # compute output and save to memory\n        att = (att_score @ v).view(batch_size, seg_len, -1)\n        return self.layer_norm(self.mlp(att) + x)\n"}