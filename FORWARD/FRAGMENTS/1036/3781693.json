{"BEFORE":"        z = z.permute(0, 2, 3, 1).contiguous()\n        z_flattened = z.view(-1, self.codebook_dim)\n        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n\n        d = torch.sum(z_flattened.pow(2), dim=1, keepdim=True) + \\\n            torch.sum(self.embedding.weight.pow(2), dim=1) - 2 * \\\n            torch.einsum('bd,dn->bn', z_flattened, self.embedding.weight.permute(1,0)) # 'n d -> d n'\n\n        encoding_indices = torch.argmin(d, dim=1)\n        z_q = self.embedding(encoding_indices).view(z.shape)\n        encodings = F.one_hot(encoding_indices, self.num_tokens).type(z.dtype)     \n        avg_probs = torch.mean(encodings, dim=0)\n        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n\n        if self.training:\n            encodings_sum = encodings.sum(0)\n            #EMA cluster size\n            self.cluster_size.mul_(self.decay).add_(encodings_sum, alpha=1 - self.decay)\n\n            embed_sum = torch.matmul(encodings.t(), z_flattened)\n            #EMA embedding average\n            self.embed_avg.mul_(self.decay).add_(embed_sum, alpha=1 - self.decay)\n\n            #cluster size Laplace smoothing \n            n = self.cluster_size.sum()\n            cluster_size = (\n                (self.cluster_size + self.eps) \/ (n + self.num_tokens * self.eps) * n\n            )\n            #normalize embedding average with smoothed cluster size\n            embed_normalized = self.embed_avg \/ cluster_size.unsqueeze(1)\n            #self.embedding.weight.data.copy_(embed_normalized.data)\n            self.embedding.weight = nn.Parameter(embed_normalized)\n        # compute loss for embedding\n        loss = self.beta * F.mse_loss(z_q.detach(), z) \n\n        # preserve gradients\n        z_q = z + (z_q - z).detach()\n\n        # reshape back to match original input shape\n        #z_q, 'b h w c -> b c h w'\n        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n","AFTER":"        z = rearrange(z, 'b c h w -> b h w c').contiguous()\n        z_flattened = z.view(-1, self.codebook_dim)\n        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n\n        d = torch.sum(z_flattened.pow(2), dim=1, keepdim=True) + \\\n            torch.sum(self.embedding.weight.pow(2), dim=1) - 2 * \\\n            torch.einsum('bd,dn->bn', z_flattened, self.embedding.weight.permute(1,0)) # 'n d -> d n'\n\n        encoding_indices = torch.argmin(d, dim=1)\n        z_q = self.embedding(encoding_indices).view(z.shape)\n        encodings = F.one_hot(encoding_indices, self.num_tokens).type(z.dtype)     \n        avg_probs = torch.mean(encodings, dim=0)\n        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n\n        if self.training:\n            encodings_sum = encodings.sum(0)\n            #EMA cluster size\n            self.cluster_size.mul_(self.decay).add_(encodings_sum, alpha=1 - self.decay)\n\n            embed_sum = torch.matmul(encodings.t(), z_flattened)\n            #EMA embedding average\n            self.embed_avg.mul_(self.decay).add_(embed_sum, alpha=1 - self.decay)\n\n            #cluster size Laplace smoothing \n            n = self.cluster_size.sum()\n            cluster_size = (\n                (self.cluster_size + self.eps) \/ (n + self.num_tokens * self.eps) * n\n            )\n            #normalize embedding average with smoothed cluster size\n            embed_normalized = self.embed_avg \/ cluster_size.unsqueeze(1)\n            #self.embedding.weight.data.copy_(embed_normalized.data)\n            self.embedding.weight = nn.Parameter(embed_normalized)\n        # compute loss for embedding\n        loss = self.beta * F.mse_loss(z_q.detach(), z) \n\n        # preserve gradients\n        z_q = z + (z_q - z).detach()\n\n        # reshape back to match original input shape\n        #z_q, 'b h w c -> b c h w'\n        z_q = rearrange(z_q, 'b h w c -> b c h w').contiguous()\n"}