{"BEFORE":"            quantized, sample, logit, wv = self._gumbelAttention(latent.permute(0, 2, 3, 1), self._codebook, self._codebook, None, temperature)\n        else:\n            quantized, sample, logit, wv = self._argmax(latent.permute(0, 2, 3, 1), self._codebook, self._codebook)\n        quantized = quantized.permute(0, 3, 1, 2)\n        if self._dropout is not None:\n            quantized = self._dropout(quantized)\n        # [n, c, h, w], [n, h, w], [n, h, w, k], [k, c]\n        return quantized, sample.argmax(-1).byte(), logit, wv\n","AFTER":"            quantized, sample, logit, (trueCode, frequency) = self._gumbelAttention(latent.permute(0, 2, 3, 1), self._codebook, self._codebook, None, temperature)\n        else:\n            quantized, sample, logit, (trueCode, frequency) = self._argmax(latent.permute(0, 2, 3, 1), self._codebook, self._codebook)\n        quantized = quantized.permute(0, 3, 1, 2)\n        # if self._dropout is not None:\n        #     quantized = self._dropout(quantized)\n        # [n, c, h, w], [n, h, w], [n, h, w, k], [k, c]\n        return quantized, trueCode.byte(), logit, (trueCode, frequency)\n"}