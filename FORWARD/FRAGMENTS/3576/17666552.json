{"BEFORE":"        batch_size = src.shape[1]\n        trg_len = trg.shape[0]\n        trg_vocab_size = self.decoder.output_dim\n        \n        #tensor to store decoder outputs\n        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n        \n        #encoder_outputs is all hidden states of the input sequence, back and forwards\n        #hidden is the final forward and backward hidden states, passed through a linear layer\n        encoder_outputs, hidden = self.encoder(src)\n                \n        #first input to the decoder is the <sos> tokens\n        input = trg[0,:]\n        \n        for t in range(1, trg_len):\n            \n            #insert input token embedding, previous hidden state and all encoder hidden states\n            #receive output tensor (predictions) and new hidden state\n            output, hidden = self.decoder(input, hidden, encoder_outputs)\n            \n            #place predictions in a tensor holding predictions for each token\n            outputs[t] = output\n            \n            #decide if we are going to use teacher forcing or not\n            teacher_force = random.random() < teacher_forcing_ratio\n            \n            #get the highest predicted token from our predictions\n            top1 = output.argmax(1) \n            \n            #if teacher forcing, use actual next token as next input\n            #if not, use predicted token\n            input = trg[t] if teacher_force else top1\n","AFTER":"        batch_size, x_dec_len, dec_in = x_dec.shape\n        #tensor to store decoder outputs\n        outputs = torch.zeros(batch_size, x_dec_len-1, dec_in).to(self.device)\n        \n        #last hidden state of the encoder is the context\n        encoder_outputs, hidden = self.encoder(x_enc)\n        \n        #first input to the decoder is the <sos> tokens\n        input = x_dec[:, 0, :].unsqueeze(dim=1)\n        \n        for t in range(1, x_dec_len):\n            \n            #insert input token embedding, previous hidden state and the context state\n            #receive output tensor (predictions) and new hidden state\n            output, hidden = self.decoder(input, hidden, encoder_outputs)\n            \n            #place predictions in a tensor holding predictions for each token\n            outputs[:, t-1, :] = output.squeeze()\n            \n            #decide if we are going to use teacher forcing or not\n            teacher_force = random.random() < teacher_forcing_ratio\n            \n            #if teacher forcing, use actual next token as next input\n            #if not, use predicted token\n            input = x_dec[:, t, :].unsqueeze(dim=1) if teacher_force else output\n"}