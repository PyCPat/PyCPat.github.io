{"BEFORE":"        hiddens = hiddens.detach()\n        batch, dim_head, mem_depth = lmem.shape[1], self.dim_head, self.num_memory_depth\n\n        if lmem.shape[2] == 0:\n            lmem = self.init_lmem.expand(mem_depth, batch, -1, -1)\n\n        # clone weights to avoid inplace error\n\n        w_q, w_kv, w_out, rezero_g = map(torch.clone, (self.to_q, self.to_kv, self.to_out, self.rezero_g))\n\n        # use efficient linear attention for updating long term memory\n\n        normed_lmem = self.norm(lmem)\n        q = torch.einsum('mbnd,de->mbne', normed_lmem, w_q)\n\n        kv_input = torch.cat((normed_lmem, smem, hiddens), dim=2)\n        k, v = torch.einsum('mbnd,de->mbne', kv_input, w_kv).chunk(2, dim=-1)\n\n        q, k, v = map(lambda t: reshape_dim(t, -1, (-1, dim_head)).transpose(2, 3), (q, k, v))\n        q, k = map(lambda t: t * dim_head ** -0.25, (q, k))\n\n        q = q.softmax(dim=-1)\n        k = k.softmax(dim=-2)\n\n        context = torch.einsum('mbhnd,mbhne->mbhde', k, v)\n        out = torch.einsum('mbhnd,mbhde->mbhne', q, context)\n\n        out = out.transpose(2, 3).reshape_as(lmem)\n        next_lmem = torch.einsum('mbnd,de->mbne', out, w_out)\n\n        # update the memory with rezero gating for now\n        # will update to use mogrifier\n\n        next_lmem = next_lmem * rezero_g + lmem\n","AFTER":"        hiddens, lmem = hiddens.detach(), lmem.detach()\n        batch, dim_head, mem_depth = lmem.shape[1], self.dim_head, self.num_memory_depth\n\n        if lmem.shape[2] == 0:\n            lmem = self.init_lmem.expand(mem_depth, batch, -1, -1).clone()\n\n        # clone weights to avoid inplace error\n\n        w_q, w_kv, w_out = map(torch.clone, (self.to_q, self.to_kv, self.to_out))\n\n        # use efficient linear attention for updating long term memory\n\n        normed_lmem = self.norm(lmem)\n        q = torch.einsum('mbnd,mde->mbne', normed_lmem, w_q)\n\n        kv_input = torch.cat((normed_lmem, smem, hiddens), dim=2)\n        k, v = torch.einsum('mbnd,mde->mbne', kv_input, w_kv).chunk(2, dim=-1)\n\n        q, k, v = map(lambda t: reshape_dim(t, -1, (-1, dim_head)).transpose(2, 3), (q, k, v))\n        q, k = map(lambda t: t * dim_head ** -0.25, (q, k))\n\n        q = q.softmax(dim=-1)\n        k = k.softmax(dim=-2)\n\n        context = torch.einsum('mbhnd,mbhne->mbhde', k, v)\n        out = torch.einsum('mbhnd,mbhde->mbhne', q, context)\n\n        out = out.transpose(2, 3).reshape_as(lmem)\n        next_lmem, w_gate = torch.einsum('mbnd,mde->mbne', out, w_out).chunk(2, dim=-1)\n\n        # GRU gating with rezero\n        next_lmem = w_gate.sigmoid() * lmem + next_lmem\n"}