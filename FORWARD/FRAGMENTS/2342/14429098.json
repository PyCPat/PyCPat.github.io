{"BEFORE":"        padding = 0 if bos_only else (tokens_per_frame - (n - 1) % tokens_per_frame)\n        num_frames = (n + padding) \/\/ tokens_per_frame\n\n        # pad for last token in video\n\n        if padding > 0:\n            x = F.pad(x, (0, 0, 0, padding), value = 0.)\n\n        # derive queries \/ keys \/ values\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        # early return if <bos>\n\n        if bos_only:\n            return self.to_out(v)\n\n        # split out heads\n\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n\n        # scale queries\n\n        q = q * self.scale\n\n        # take care of bos\n\n        q = q[:, 1:]\n        bos_value = v[:, :1]\n\n        # compute keys and values\n\n        (k_bos, k), (v_bos, v) = map(lambda t: (t[:, :1], t[:, 1:]), (k, v))\n        k, v = map(lambda t: rearrange(t, 'b (f h w) d -> b d f h w', f  = num_frames, h = fmap_size), (k, v))\n        k, v = map(lambda t: unfoldNd(t, kernel_size = kernel_size, padding = kernel_size \/\/ 2), (k, v))\n        k, v = map(lambda t: rearrange(t, 'b (d j) i -> b i j d', j = kernel_size ** 3), (k, v))\n\n        # append bos keys and values\n\n        k_bos, v_bos = map(lambda t: repeat(t, 'b 1 d -> b n 1 d', n = k.shape[1]), (k_bos, v_bos))\n        k = torch.cat((k_bos, k), dim = 2)\n        v = torch.cat((v_bos, v), dim = 2)\n\n        # calculate sim\n\n        sim = einsum('b i d, b i j d -> b i j', q, k)\n\n        # causal mask\n\n        i, j = sim.shape[-2:]\n        causal_mask = self.causal_mask[:i, :j]\n        causal_mask = repeat(causal_mask, 'i j -> b i j', b = b * h)\n\n        sim = sim.masked_fill(causal_mask, -torch.finfo(sim.dtype).max)\n\n        # attention\n\n        attn = sim.softmax(dim = -1)\n        attn = self.dropout(attn)\n\n        # aggregate values\n\n        out = einsum('b i j, b i j d -> b i d', attn, v)\n\n        # append bos value\n\n        out = torch.cat((bos_value, out), dim = 1)  # bos will always adopt its own value, since it pays attention only to itself\n\n        # merge heads\n\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n        return self.to_out(out[:, :n])\n","AFTER":"        padding = 0 if bos_only else (tokens_per_frame - (n - 1) % tokens_per_frame)\n        num_frames = (n + padding) \/\/ tokens_per_frame\n\n        # pad for last token in video\n\n        padded_x = F.pad(x, (0, 0, 0, padding), value = 0.) if padding > 0 else x\n\n        # derive queries \/ keys \/ values\n\n        q, k, v = (self.to_q(x), *self.to_kv(padded_x).chunk(2, dim = -1))\n\n        # early return if <bos>\n\n        if bos_only:\n            return self.to_out(v)\n\n        # split out heads\n\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n\n        # scale queries\n\n        q = q * self.scale\n\n        # take care of bos\n\n        q = q[:, 1:]\n        bos_value = v[:, :1]\n\n        # compute keys and values\n\n        (k_bos, k), (v_bos, v) = map(lambda t: (t[:, :1], t[:, 1:]), (k, v))\n        k, v = map(lambda t: rearrange(t, 'b (f h w) d -> b d f h w', f  = num_frames, h = fmap_size), (k, v))\n        k, v = map(lambda t: unfoldNd(t, kernel_size = kernel_size, padding = kernel_size \/\/ 2), (k, v))\n        k, v = map(lambda t: rearrange(t, 'b (d j) i -> b i j d', j = kernel_size ** 3), (k, v))\n        k, v = map(lambda t: t[:, :(n - 1)], (k, v))\n\n        # append bos keys and values\n\n        k_bos, v_bos = map(lambda t: repeat(t, 'b 1 d -> b n 1 d', n = k.shape[1]), (k_bos, v_bos))\n        k = torch.cat((k_bos, k), dim = 2)\n        v = torch.cat((v_bos, v), dim = 2)\n\n        # calculate sim\n\n        sim = einsum('b i d, b i j d -> b i j', q, k)\n\n        # causal mask\n\n        i, j = sim.shape[-2:]\n        causal_mask = self.causal_mask[:i, :j]\n        causal_mask = repeat(causal_mask, 'i j -> b i j', b = b * h)\n\n        sim = sim.masked_fill(causal_mask, -torch.finfo(sim.dtype).max)\n\n        # attention\n\n        attn = sim.softmax(dim = -1)\n        attn = self.dropout(attn)\n\n        # aggregate values\n\n        out = einsum('b i j, b i j d -> b i d', attn, v)\n\n        # append bos value\n\n        out = torch.cat((bos_value, out), dim = 1)  # bos will always adopt its own value, since it pays attention only to itself\n\n        # merge heads\n\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n        return self.to_out(out)\n"}