{"BEFORE":"        x = rearrange(x, 'b n d -> b d n')\n        x = F.pad(x, (2, 0))\n\n        q, k, v = self.to_q(x), self.to_k(x), self.to_v(x)\n        q, k, v = rearrange_many((q, k, v), 'b (h d) n -> b h n d', h = self.heads)\n\n        q = q * self.scale\n        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n\n        alibi_bias = self.alibi(sim)\n        sim = sim + alibi_bias\n","AFTER":"        x = rearrange(x, 'b n d -> b d n')\n\n        q, k, v = self.to_qkv(x).chunk(3, dim = 1)\n\n        q, k, v = rearrange_many((q, k, v), 'b (h d) n -> b h n d', h = self.heads)\n\n        # apply causal depthwise conv to queries, keys, values (a la Primer) with different kernel sizes across 4 groups of heads\n\n        def apply_causal_ds_conv_to_grouped_heads(args):\n            projs, ds_convs = args\n            batch = projs.shape[0]\n\n            projs = rearrange_many(projs.split(self.heads \/\/ 4, dim = 1), 'b h n d -> (b h) n d')\n            conv_out = [fn(t) for fn, t in zip(ds_convs, projs)]\n            conv_out = map(lambda t: rearrange(t, '(b h) d n -> b h d n', b = batch), conv_out)\n            return torch.cat(tuple(conv_out), dim = 1)\n\n        q, k, v = map(apply_causal_ds_conv_to_grouped_heads, zip((q, k, v), self.qkv_ds_convs))\n\n        # scale and similarity\n\n        q = q * self.scale\n        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n\n        # learned alibi pos bias across 4 groups of heads\n        # so heads specialize to looking at different distances of kmers\n\n        grouped_sims = sim.split(self.heads \/\/ 4, dim = 1)\n        grouped_sims = [(alibi(sim_group) + sim_group) for alibi, sim_group in zip(self.learned_alibi_pos_biases, grouped_sims)]\n        grouped_sims = torch.cat(grouped_sims, dim = 1)\n"}