{"BEFORE":"        logit_softmax = F.softmax(logit, dim = 1)\n        diff = -(logit.diag().view(-1, 1).expand_as(logit) - logit)\n        loss = torch.mean(logit_softmax * (torch.sigmoid(diff) + torch.sigmoid(logit ** 2)))\n        # add regularization\n        loss += self.lambda_ * torch.mean(logit_softmax * (logit ** 2))\n        return loss\n","AFTER":"    def forward(self, logits_scores, negative_mask):\n        \"\"\"\n        Args:\n            logits_scores: (#pos_target, #neg_samples):  scores of positive next items for all sessions\n                                                            + All negative samples (mini-batch + additional samples )\n            negative_mask: (#pos_target, #neg_samples) : specify the negative items for each positive target\n        \"\"\"\n        positive_mask = ~negative_mask\n        positives = logits_scores.diag().view(-1, 1).expand_as(logits_scores)\n        diff = positives - logits_scores\n        penalization = torch.sigmoid(logits_scores ** 2)\n        loss = torch.sigmoid(-diff) + penalization\n        # set to zeros the difference scores of positive targets of the same session\n        loss = loss.masked_fill(positive_mask, 0)\n        # Average over the nb of negative sample per\n        loss = loss.sum(1) \/ negative_mask.sum(1)\n        return torch.mean(loss)\n"}