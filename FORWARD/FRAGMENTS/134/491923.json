{"BEFORE":"                processed_inputs=None, mask=None, inputs_lengths=None):\n\n        if processed_inputs is None:\n            processed_inputs = inputs\n\n        if inputs_lengths is not None and mask is None:\n            mask = get_mask_from_lengths(inputs, inputs_lengths)\n\n        # Alignment\n        # (batch, max_time)\n        # e_{ij} = a(s_{i-1}, h_j)\n        # import ipdb\n        # ipdb.set_trace()\n        alignment = self.alignment_model(cell_state, processed_inputs)\n\n        if mask is not None:\n            mask = mask.view(query.size(0), -1)\n            alignment.data.masked_fill_(mask, self.score_mask_value)\n\n        # Normalize context_vec weight\n        alignment = F.softmax(alignment, dim=-1)\n\n        # Attention context vector\n        # (batch, 1, dim)\n        # c_i = \\sum_{j=1}^{T_x} \\alpha_{ij} h_j\n        context_vec = torch.bmm(alignment.unsqueeze(1), inputs)\n        context_vec = context_vec.squeeze(1)\n\n        # Concat input query and previous context_vec context\n        cell_input = torch.cat((query, context_vec), -1)\n        #cell_input = cell_input.unsqueeze(1)\n\n        # Feed it to RNN\n        # s_i = f(y_{i-1}, c_{i}, s_{i-1})\n        cell_output = self.rnn_cell(cell_input, cell_state)\n\n        context_vec = context_vec.squeeze(1)\n        return cell_output, context_vec, alignment\n","AFTER":"                mask=None, annotations_lengths=None):\n\n        if annotations_lengths is not None and mask is None:\n            mask = get_mask_from_lengths(annotations, annotations_lengths)\n\n        # Alignment\n        # (batch, max_time)\n        # e_{ij} = a(s_{i-1}, h_j)\n        alignment = self.alignment_model(annotations, rnn_state)\n\n        # TODO: needs recheck.\n        if mask is not None:\n            mask = mask.view(query.size(0), -1)\n            alignment.data.masked_fill_(mask, self.score_mask_value)\n\n        # Normalize context weight\n        alignment = F.softmax(alignment, dim=-1)\n\n        # Attention context vector\n        # (batch, 1, dim)\n        # c_i = \\sum_{j=1}^{T_x} \\alpha_{ij} h_j\n        context = torch.bmm(alignment.unsqueeze(1), annotations)\n        context = context.squeeze(1)\n\n        # Concat input query and previous context context\n        rnn_input = torch.cat((memory, context), -1)\n        #rnn_input = rnn_input.unsqueeze(1)\n\n        # Feed it to RNN\n        # s_i = f(y_{i-1}, c_{i}, s_{i-1})\n        rnn_output = self.rnn_cell(rnn_input, rnn_state)\n\n        context = context.squeeze(1)\n        return rnn_output, context, alignment\n"}