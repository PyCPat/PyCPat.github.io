{"BEFORE":"        semantic_embeddings = []\n\n        if self._cached_graph is None or self._cached_graph is not g:\n            self._cached_graph = g\n            self._cached_coalesced_graph.clear()\n            for meta_path in self.meta_paths:\n                self._cached_coalesced_graph[meta_path] = dgl.metapath_reachable_graph(\n                        g, meta_path)\n\n        for i, meta_path in enumerate(self.meta_paths):\n            new_g = self._cached_coalesced_graph[meta_path]\n            semantic_embeddings.append(self.gat_layers[i](new_g, h).flatten(1))\n        semantic_embeddings = torch.stack(semantic_embeddings, dim=1)                  # (N, M, D * K)\n\n        return self.semantic_attention(semantic_embeddings)                            # (N, D * K)\n","AFTER":"                self._cached_coalesced_graph[meta_path] = dgl.metapath_reachable_graph(\n                        g, meta_path)\n        h = self.model(self._cached_coalesced_graph, h)\n        return h\n"}