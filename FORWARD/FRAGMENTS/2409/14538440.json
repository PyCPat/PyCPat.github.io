{"BEFORE":"        flattened_dim = ids.ndim == 2\n        ids_orig_ndim = ids.ndim\n\n        if ids.numel() == 0:\n            return self.forward_empty(ids.shape[0])\n\n        if flattened_dim:\n            # allow for ids to be given in the shape of (batch, seq)\n            # in which case it will be auto-padded to the next nearest multiple of depth seq len\n            seq_len = ids.shape[-1]\n            padding = remainder_to_mult(seq_len, self.depth_seq_len)\n            ids = F.pad(ids, (0, padding), value = self.pad_id)\n            ids = rearrange(ids, 'b (s d) -> b s d', d = self.depth_seq_len)\n\n        b, space, depth, device = *ids.shape, ids.device\n        assert space <= self.max_spatial_seq_len, 'spatial dimension is greater than the max_spatial_seq_len set'\n        assert depth == self.depth_seq_len, 'depth dimension must be equal to depth_seq_len'\n\n        tokens = self.token_emb(ids)\n\n        spatial_pos = self.spatial_pos_emb(torch.arange(space, device = device))\n        depth_pos = self.depth_pos_emb(torch.arange(depth, device = device))\n\n        tokens_with_depth_pos = tokens + depth_pos\n\n        # spatial tokens is tokens with depth pos reduced along depth dimension + spatial positions\n\n        spatial_tokens = reduce(tokens_with_depth_pos, 'b s d f -> b s f', 'sum') + spatial_pos\n\n        spatial_tokens = torch.cat((\n            repeat(self.spatial_start_token, 'f -> b 1 f', b = b),\n            spatial_tokens\n        ), dim = -2)\n\n        spatial_tokens = spatial_tokens[:, :-1]\n\n        spatial_tokens = self.spatial_transformer(spatial_tokens)\n\n        spatial_tokens = rearrange(spatial_tokens, 'b s f -> b s 1 f')\n\n        # spatial tokens become the start tokens of the depth dimension\n\n        depth_tokens = torch.cat((spatial_tokens, tokens_with_depth_pos), dim = -2)\n\n        depth_tokens = rearrange(depth_tokens, '... n d -> (...) n d')\n\n        depth_tokens = self.depth_transformer(depth_tokens)\n\n        depth_tokens = rearrange(depth_tokens, '(b s) d f -> b s d f', b = b)\n\n        logits = self.to_logits(depth_tokens)\n\n        if not return_loss:\n            logits = logits[..., 1:, :]\n\n            if flattened_dim:\n                logits = rearrange(logits, 'b ... n -> b (...) n')\n                logits = logits[:, :seq_len]\n\n            return logits\n","AFTER":"        flattened_dim = ids.ndim == 2\n        ids_orig_ndim = ids.ndim\n\n        if ids.numel() == 0:\n            return self.forward_empty(ids.shape[0])\n\n        if flattened_dim:\n            # allow for ids to be given in the shape of (batch, seq)\n            # in which case it will be auto-padded to the next nearest multiple of depth seq len\n            seq_len = ids.shape[-1]\n            padding = remainder_to_mult(seq_len, self.depth_seq_len)\n            ids = F.pad(ids, (0, padding), value = self.pad_id)\n            ids = rearrange(ids, 'b (s d) -> b s d', d = self.depth_seq_len)\n        else:\n            seq_len = ids.shape[1] * ids.shape[2]\n\n        # bump space by one to account for a boundary case\n\n        ids = F.pad(ids, (0, 0, 0, 1))\n\n        b, space, depth, device = *ids.shape, ids.device\n        assert space <= (self.max_spatial_seq_len + 1), 'spatial dimension is greater than the max_spatial_seq_len set'\n        assert depth == self.depth_seq_len, 'depth dimension must be equal to depth_seq_len'\n\n        # get token embeddings\n\n        tokens = self.token_emb(ids)\n\n        spatial_pos = self.spatial_pos_emb(torch.arange(space, device = device))\n        depth_pos = self.depth_pos_emb(torch.arange(depth, device = device))\n\n        tokens_with_depth_pos = tokens + depth_pos\n\n        # spatial tokens is tokens with depth pos reduced along depth dimension + spatial positions\n\n        spatial_tokens = reduce(tokens_with_depth_pos, 'b s d f -> b s f', 'sum') + spatial_pos\n\n        spatial_tokens = torch.cat((\n            repeat(self.spatial_start_token, 'f -> b 1 f', b = b),\n            spatial_tokens\n        ), dim = -2)\n\n        spatial_tokens = spatial_tokens[:, :-1]\n\n        spatial_tokens = self.spatial_transformer(spatial_tokens)\n\n        spatial_tokens = rearrange(spatial_tokens, 'b s f -> b s 1 f')\n\n        # spatial tokens become the start tokens of the depth dimension\n\n        depth_tokens = torch.cat((spatial_tokens, tokens_with_depth_pos), dim = -2)\n\n        depth_tokens = rearrange(depth_tokens, '... n d -> (...) n d')\n\n        depth_tokens = self.depth_transformer(depth_tokens)\n\n        depth_tokens = rearrange(depth_tokens, '(b s) d f -> b s d f', b = b)\n\n        logits = self.to_logits(depth_tokens)\n\n        if not return_loss:\n            logits = logits[..., :-1, :]\n\n            logits = rearrange(logits, 'b ... n -> b (...) n')\n\n            logits = logits[:, 1:(seq_len + 1)] # exclude first start token\n\n            if flattened_dim:\n                return logits\n\n            return rearrange(logits, 'b (s d) n -> b s d n', d = depth)\n"}