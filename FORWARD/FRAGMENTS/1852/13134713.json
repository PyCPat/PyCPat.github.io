{"BEFORE":"        b, n, _, h = *x.shape, self.heads\n        qkv = self.to_qkv(x).chunk(3, dim=-1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), qkv)\n\n        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        attn = self.attend(dots)\n        # print(attn.shape)\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out), attn\n","AFTER":"    def forward(self, tokens, pos_embeddings=None):\n        \"\"\"\n        # demo1: no position_embedding --------------------------------------------\n        tokens = torch.ones([3, 5, 512])\n        dim = 512\n        heads = 8\n        dim_head = 32\n        dim_mlp = 64\n        attention_layer = TransformerEncoderLayer(dim, heads, dim_head, dim_mlp, AddPosEmb2Value=True)\n        tokens_, attention_maps = attention_layer(tokens)\n        print(tokens.shape, tokens_.shape, attention_maps.shape)\n\n        # demo2: qkv with position_embedding --------------------------------------------\n        tokens = torch.ones([3, 5, 512])\n        position_embedding = torch.ones([3, 5, 512])\n        dim = 512\n        heads = 8\n        dim_head = 32\n        dim_mlp = 64\n        attention_layer = TransformerEncoderLayer(dim, heads, dim_head, dim_mlp, AddPosEmb2Value=True)\n        tokens_, attention_maps = attention_layer(tokens, position_embedding) # way1 to add position_embedding\n        tokens_, attention_maps = attention_layer(tokens+position_embedding) # way2 to add position_embedding\n        print(tokens.shape, tokens_.shape, attention_maps.shape)\n\n        # demo3: qk with position_embedding, but v without position_embedding------------\n        tokens = torch.ones([3, 5, 512])\n        position_embedding = torch.ones([3, 5, 512])\n        dim = 512\n        heads = 8\n        dim_head = 32\n        dim_mlp = 64\n        attention_layer = TransformerEncoderLayer(dim, heads, dim_head, dim_mlp, AddPosEmb2Value=False)\n        tokens_, attention_maps = attention_layer(tokens, position_embedding)\n        print(tokens.shape, tokens_.shape, attention_maps.shape)\n\n        \"\"\"\n\n        # norm\n        q = k = v = self.norm1(tokens)\n\n        # add pos_embeddings\n        if pos_embeddings is not None:\n            print('add pos_embeddings to q and k')\n            q = q + pos_embeddings\n            k = k + pos_embeddings\n            if self.AddPosEmb2Value:\n                print('add pos_embeddings to v')\n                v = v + pos_embeddings\n\n        # self attention\n        attn_q, attn_map = self.Attention(q, k, v)  # attn_q has same shape with q\n\n        # residual\n        tokens = tokens + attn_q  # todo add token (which is not normed), not v !\n\n        # norm\n        q = self.norm2(tokens)\n\n        # FFN\n        ffn_q = self.FFN(q)\n\n        # residual\n        tokens = tokens + ffn_q  # todo add token (which is not normed), not v !\n\n        return tokens, attn_map\n"}