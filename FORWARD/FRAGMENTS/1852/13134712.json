{"BEFORE":"        out = out.transpose(1, 2).reshape(b, t, -1)\n        return self.to_out(out)\n","AFTER":"        mem = default(mem, torch.empty(b, 0, e))\n        mem_len = mem.shape[1]\n\n        q = self.to_q(x)\n\n        kv_input = torch.cat((mem, x), dim=1)\n        k, v = self.to_kv(kv_input).chunk(2, dim=-1)\n\n        q, k, v = map(lambda x: x.reshape(b, -1, h, dim_h).transpose(1, 2), (q, k, v))\n\n        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n\n        pos_attn = self.rel_pos_emb(q, mem_len)\n        dots = dots + pos_attn\n\n        mask = torch.ones(t, t + mem_len).triu_(diagonal = 1 + mem_len).bool()\n        dots.masked_fill_(mask[None, None, ...], float('-inf'))\n\n        attn = dots.softmax(dim=-1)\n\n        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n        out = out.transpose(1, 2).reshape(b, t, -1)\n\n        # calculate next memory - compressed memory calculations will be put here\n        mem_next = mem\n        if self.seq_len == t:\n            mem_next = torch.cat((mem, x), dim=1)[:, -self.mem_len:, :].detach()\n\n        return SelfAttentionOutput(out = self.to_out(out), mem = mem_next)\n"}