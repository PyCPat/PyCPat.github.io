{"BEFORE":"        x = self.to_inner(x)\n        x = rearrange(x, '... (h d) -> ... h d', h = self.heads)\n\n        # weights derived from alphas (learned exponential smoothing decay rate)\n\n        alphas = self.alphas.sigmoid()\n        dampen_factors = self.dampen_factors.sigmoid()\n\n        reversed_powers = torch.arange(seq_len - 1, -1, -1, device = device)\n        K = alphas * (((1 - alphas) * dampen_factors) ** rearrange(reversed_powers, '... l -> ... l 1'))\n\n        # conv1d fft O(nlog(n))\n\n        out = conv1d_fft(x, K, dim = -3, weight_dim = -2)\n\n        # combine heads and out\n\n        out = rearrange(out, '... h d -> ... (h d)')\n        return self.to_out(out)\n","AFTER":"        x = einsum('... d, h d -> ... h d', x, self.expansion)\n\n        # weights derived from alphas (learned exponential smoothing decay rate)\n\n        alphas = self.alphas.sigmoid()\n        dampen_factors = self.dampen_factors.sigmoid()\n\n        reversed_powers = torch.arange(seq_len - 1, -1, -1, device = device)\n        K = alphas * (((1 - alphas) * dampen_factors) ** rearrange(reversed_powers, '... l -> ... l 1'))\n\n        # conv1d fft O(nlog(n))\n\n        out = conv1d_fft(x, K, dim = -3, weight_dim = -2)\n\n        # combine heads and out\n\n        return einsum('... h d, h d -> ... d', out, self.reduction)\n"}