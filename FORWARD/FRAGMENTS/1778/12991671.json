{"BEFORE":"        batch_size = x.shape[0]\n        # x: (batch_size, num_route_nodes, in_channels)\n        # route_weights: (num_route_nodes, num_capsules, in_channels, out_channels)\n        # u_hat: (batch_size, num_capsules, num_route_nodes, out_channels)\n        u_hat = torch.einsum('ijk, jlkm -> iljm', x, self.route_weights)\n        # Detatch u_hat during routing iterations\n        u_hat_temp = u_hat.detach()\n\n        # Dynamic route\n        # b: (batch_size, num_capsules, num_route_nodes)\n        b = torch.zeros(batch_size, self.num_capsules, self.num_route_nodes).to(device)\n        for it in range(self.num_iterations - 1):\n            c = b.softmax(dim=1)\n\n            # c: (batch_size, num_capsules, num_route_nodes)\n            # u_hat: (batch_size, num_capsules, num_route_nodes, out_channels)\n            # s: (batch_size, num_capsules, out_channels)\n            s = torch.einsum('ijk, ijkl -> ijl', c, u_hat_temp)\n            v = squash(s)\n\n            # Update b\n            # u_hat: (batch_size, num_capsules, num_route_nodes, out_channels)\n            # v: (batch_size, num_capsules, out_channels)\n            # Shape of b: (batch_size, num_capsules, num_route_nodes)\n            uv = torch.einsum('ijkl, ijl -> ijk', u_hat_temp, v)\n            b += uv\n\n        # Last iteration with original u_hat to pass gradient\n        c = b.softmax(dim=1)\n        s = torch.einsum('ijk, ijkl -> ijl', c, u_hat_temp)\n","AFTER":"        batch_size = x.size(0)\n        # (batch_size, in_caps, in_dim) -> (batch_size, 1, in_caps, in_dim, 1)\n        x = x.unsqueeze(1).unsqueeze(4)\n        #\n        # W @ x =\n        # (1, num_caps, in_caps, dim_caps, in_dim) @ (batch_size, 1, in_caps, in_dim, 1) =\n        # (batch_size, num_caps, in_caps, dim_caps, 1)\n        u_hat = torch.matmul(self.W, x)\n        # (batch_size, num_caps, in_caps, dim_caps)\n        u_hat = u_hat.squeeze(-1)\n        # detach u_hat during routing iterations to prevent gradients from flowing\n        temp_u_hat = u_hat.detach()\n\n        b = torch.zeros(batch_size, self.num_caps, self.in_caps, 1).to(self.device)\n\n        for route_iter in range(self.num_routing - 1):\n            # (batch_size, num_caps, in_caps, 1) -> Softmax along num_caps\n            c = b.softmax(dim=1)\n\n            # element-wise multiplication\n            # (batch_size, num_caps, in_caps, 1) * (batch_size, in_caps, num_caps, dim_caps) ->\n            # (batch_size, num_caps, in_caps, dim_caps) sum across in_caps ->\n            # (batch_size, num_caps, dim_caps)\n            s = (c * temp_u_hat).sum(dim=2)\n            # apply \"squashing\" non-linearity along dim_caps\n            v = squash(s)\n            # dot product agreement between the current output vj and the prediction uj|i\n            # (batch_size, num_caps, in_caps, dim_caps) @ (batch_size, num_caps, dim_caps, 1)\n            # -> (batch_size, num_caps, in_caps, 1)\n            uv = torch.matmul(temp_u_hat, v.unsqueeze(-1))\n            b += uv\n\n        # last iteration is done on the original u_hat, without the routing weights update\n        c = b.softmax(dim=1)\n        s = (c * u_hat).sum(dim=2)\n"}