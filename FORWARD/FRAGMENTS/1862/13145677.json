{"BEFORE":"        keys = sparse_features.keys()\n        assert len(keys) == len(self.offsets), f\"keys len: {len(keys)}, offsets len: {len(self.offsets)}\"\n\n        sparse_dict = sparse_features.to_dict()\n        flattened_sparse_features = torch.cat(\n            [sparse_dict[key].values() + offset for key, offset in zip(keys, self.offsets)])\n        batch_offsets = sparse_features.offsets()\n\n        batch_size = len(sparse_features.lengths()) \/\/ len(keys)\n        feature_size = len(keys)\n        flattened_sparse_embeddings = self.embed(flattened_sparse_features,\n                                                 batch_offsets,\n                                                 send_shape=(batch_size, feature_size, -1))\n","AFTER":"        sparse_features = self.kjt_collector.all_to_all(sparse_features)\n        keys, batch_size = sparse_features.keys(), sparse_features.stride()\n\n        flattened_sparse_features = sparse_features.values()\n        batch_offsets = sparse_features.offsets()\n\n        flattened_sparse_embeddings = self.embed(\n            flattened_sparse_features,\n            batch_offsets,\n            shape_hook=lambda x: sparse_embedding_shape_hook(x, len(keys), batch_size))\n"}