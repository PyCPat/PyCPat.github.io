{"BEFORE":"            q, k, v = apply_pos_emb(rotary_pos_emb, (q, k, v))\n\n        q = q * self.scale\n\n        dots = torch.einsum(\"b h i d, b h j d -> b h i j\", q, k)\n        mask_value = max_neg_value(dots)\n\n        if exists(mask):\n            mask = rearrange(mask, \"b j -> b () () j\")\n            dots.masked_fill_(~mask, mask_value)\n            del mask\n\n        if self.causal == True:\n            i, j = dots.shape[-2:]\n            mask = torch.ones(i, j, device=device).triu_(j - i + 1).bool()\n            dots.masked_fill_(mask, mask_value)\n\n        attn = softmax(dots, dim=-1)\n\n        self.save_attn({\"attn\": attn, \"v\": v})\n","AFTER":"    def forward(self, x, mask=None, rotary_pos_emb=None, cache=None, cache_key=None):\n        b, n, _, h, device = *x.shape, self.heads, x.device\n        softmax = torch.softmax if not self.stable else stable_softmax\n\n        offset = cache.get(\"offset\", 0) if exists(cache) else 0\n\n        qkv = self.to_qkv(x).chunk(3, dim=-1)\n        q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=h), qkv)\n\n        if exists(rotary_pos_emb):\n            q, k, v = apply_pos_emb(rotary_pos_emb[..., offset:, :], (q, k, v))\n\n        q = q * self.scale\n\n        if offset > 0:\n            k_top, v_top = cache[cache_key]\n            k = torch.cat([k_top, k], dim=-2)\n            v = torch.cat([v_top, v], dim=-2)\n        if exists(cache):\n            cache[cache_key] = k, v\n\n        dots = torch.einsum(\"b h i d, b h j d -> b h i j\", q, k)\n        mask_value = max_neg_value(dots)\n\n        if exists(mask):\n            mask = rearrange(mask, \"b j -> b () () j\")\n            dots.masked_fill_(~mask, mask_value)\n            del mask\n\n        if (\n            self.causal and offset == 0\n        ):  # causality is naturally enforced for the cached inference\n            i, j = dots.shape[-2:]\n            mask = torch.ones(i, j, device=device).triu_(j - i + 1).bool()\n            dots.masked_fill_(mask, mask_value)\n\n        if exists(self.static_mask):\n            dots.masked_fill_(\n                ~self.static_mask[offset : offset + n, : offset + n], mask_value\n            )\n\n        attn = softmax(dots, dim=-1)\n\n        self.save_attn(attn)\n"}