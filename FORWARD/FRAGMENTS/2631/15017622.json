{"BEFORE":"        question, question_lengths = batch.question.value, batch.question.length\n\n        context_padding = context.data == self.pad_idx\n        question_padding = question.data == self.pad_idx\n\n        context_embedded = self.encoder_embeddings(context, padding=context_padding)\n        question_embedded = self.encoder_embeddings(question, padding=question_padding)\n\n        # pick the top-most N transformer layers to pass to the decoder for cross-attention\n        # (add 1 to account for the embedding layer - the decoder will drop it later)\n        self_attended_context = context_embedded.all_layers[-(self.args.transformer_layers + 1):]\n        final_context = context_embedded.last_layer\n        final_question = question_embedded.last_layer\n\n        if self.projection is not None:\n            final_context = self.dropout(final_context)\n            final_context = self.projection(final_context)\n\n            final_question = self.dropout(final_question)\n            final_question = self.projection(final_question)\n\n        context_rnn_state = None\n        question_rnn_state = None\n        if self.args.rnn_layers > 0:\n            batch_size = context.size(0)\n            if self.args.rnn_zero_state == 'zero':\n\n                zero = torch.zeros(self.args.rnn_layers, batch_size, self.args.rnn_dimension,\n                                   dtype=torch.float, requires_grad=False, device=context.device)\n                context_rnn_state = (zero, zero)\n                question_rnn_state = (zero, zero)\n            else:\n                if self.args.rnn_zero_state == 'cls':\n                    packed_rnn_state = self.norm(self.pool(context_embedded.last_layer[:, 0, :]))\n\n                elif self.args.rnn_zero_state == 'average':\n                    masked_final_context = context_embedded.last_layer.masked_fill(context_padding.unsqueeze(2), 0)\n                    summed_context = torch.sum(masked_final_context, dim=1)\n                    average_context = summed_context \/ context_lengths.unsqueeze(1)\n\n                    packed_rnn_state = self.norm(self.pool(average_context))\n\n                # packed_rnn_state is (batch, 2 * rnn_layers * rnn_dim)\n                packed_rnn_state = packed_rnn_state.reshape(batch_size, 2, self.args.rnn_layers,\n                                                            self.args.rnn_dimension)\n                # transpose to (2, batch, rnn_layers, rnn_dimension)\n                packed_rnn_state = packed_rnn_state.transpose(0, 1)\n                # transpose to (2, rnn_layers, batch, rnn_dimension)\n                packed_rnn_state = packed_rnn_state.transpose(1, 2)\n                # convert to a tuple of two (rnn_layers, batch, rnn_dimension) tensors\n                packed_rnn_state = packed_rnn_state.chunk(2, dim=0)\n                context_rnn_state = (packed_rnn_state[0].squeeze(0), packed_rnn_state[1].squeeze(0))\n\n        return self_attended_context, final_context, context_rnn_state, final_question, question_rnn_state\n","AFTER":"        self_attended_context = context_embedded.all_layers[-(self.args.transformer_layers + 1):]\n        final_context = context_embedded.last_layer\n\n        if self.projection is not None:\n            final_context = self.dropout(final_context)\n            final_context = self.projection(final_context)\n\n        context_rnn_state = None\n        if self.args.rnn_layers > 0:\n            batch_size = context.size(0)\n            if self.args.rnn_zero_state == 'zero':\n\n                zero = torch.zeros(self.args.rnn_layers, batch_size, self.args.rnn_dimension,\n                                   dtype=torch.float, requires_grad=False, device=context.device)\n                context_rnn_state = (zero, zero)\n            else:\n                if self.args.rnn_zero_state == 'cls':\n                    packed_rnn_state = self.norm(self.pool(context_embedded.last_layer[:, 0, :]))\n\n                elif self.args.rnn_zero_state == 'average':\n                    masked_final_context = context_embedded.last_layer.masked_fill(context_padding.unsqueeze(2), 0)\n                    summed_context = torch.sum(masked_final_context, dim=1)\n                    average_context = summed_context \/ context_lengths.unsqueeze(1)\n\n                    packed_rnn_state = self.norm(self.pool(average_context))\n\n                # packed_rnn_state is (batch, 2 * rnn_layers * rnn_dim)\n                packed_rnn_state = packed_rnn_state.reshape(batch_size, 2, self.args.rnn_layers,\n                                                            self.args.rnn_dimension)\n                # transpose to (2, batch, rnn_layers, rnn_dimension)\n                packed_rnn_state = packed_rnn_state.transpose(0, 1)\n                # transpose to (2, rnn_layers, batch, rnn_dimension)\n                packed_rnn_state = packed_rnn_state.transpose(1, 2)\n                # convert to a tuple of two (rnn_layers, batch, rnn_dimension) tensors\n                packed_rnn_state = packed_rnn_state.chunk(2, dim=0)\n                context_rnn_state = (packed_rnn_state[0].squeeze(0), packed_rnn_state[1].squeeze(0))\n\n        return self_attended_context, final_context, context_rnn_state\n"}