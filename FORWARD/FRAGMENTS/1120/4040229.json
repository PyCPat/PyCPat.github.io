{"BEFORE":"        a = torch.clamp_min(inp + self.m, min=0).detach()\n        src = torch.clamp_min(\n            - inp.gather(dim=1, index=label.unsqueeze(1)) + 1 + self.m,\n            min=0,\n        ).detach()\n        a.scatter_(dim=1, index=label.unsqueeze(1), src=src)\n\n        sigma = torch.ones_like(inp, device=inp.device, dtype=inp.dtype) * self.m\n        src = torch.ones_like(label.unsqueeze(1), dtype=inp.dtype, device=inp.device) - self.m\n        sigma.scatter_(dim=1, index=label.unsqueeze(1), src=src)\n\n        return self.loss(a * (inp - sigma) * self.gamma, label)\n","AFTER":"        ap = torch.clamp_min(- sp.detach() + 1 + self.m, min=0.)\n        an = torch.clamp_min(sn.detach() + self.m, min=0.)\n\n        sigma_p = 1 - self.m\n        sigma_n = self.m\n\n        logit_p = ap * (sp - sigma_p) * self.gamma\n        logit_n = an * (sn - sigma_n) * self.gamma\n\n        loss = torch.log(1 + torch.clamp_max(torch.exp(logit_n).sum() * torch.exp(- logit_p).sum(), max=1e38))\n        z = - torch.exp(- loss) + 1\n\n        sp.backward(gradient=z * ap * torch.softmax(logit_p, dim=0))\n        sn.backward(gradient=z * an * torch.softmax(logit_n, dim=0))\n\n        return loss.detach()\n"}