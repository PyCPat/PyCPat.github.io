{"BEFORE":"        context = torch.bmm(alignment.unsqueeze(1), inputs)\n        context = context.squeeze(1)\n        return context, alignment\n","AFTER":"        if self.forward_attn:\n            # forward attention\n            prev_alpha = F.pad(self.alpha[:, :-1].clone(), (1, 0, 0, 0)).to(inputs.device)\n            self.alpha = (((1-self.u) * self.alpha.clone().to(inputs.device) + self.u * prev_alpha) + 1e-7) * alignment\n            alpha_norm = self.alpha \/ self.alpha.sum(dim=1).unsqueeze(1)\n            # compute context\n            context = torch.bmm(alpha_norm.unsqueeze(1), inputs)\n            context = context.squeeze(1)\n            return context, alpha_norm, alignment\n        else:\n            context = torch.bmm(alignment.unsqueeze(1), inputs)\n            context = context.squeeze(1)\n            return context, alignment, alignment\n\n\nclass Postnet(nn.Module):\n"}