{"BEFORE":"        batch_size = encoder_out.size(0)\r\n        encoder_dim = encoder_out.size(-1)\r\n        vocab_size = self.vocab_size\r\n        num_pixels = encoder_out.size(1)\r\n\r\n        # Sort input data by decreasing lengths; why? For each of data in the batch, when len(prediction) = len(caption_lengths), Stop.\r\n        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\r\n        encoder_out = encoder_out[sort_ind]\r\n        encoded_captions = encoded_captions[sort_ind]\r\n        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\r\n        # So, decoding lengths are actual lengths - 1\r\n        decode_lengths = (caption_lengths - 1).tolist()\r\n\r\n        # dec_outputs: [batch_size, max_len=52, embed_dim=512]\r\n        # dec_self_attn_pad_mask: [batch_size, len_q=52, len_k=52], 1 if id=0(<pad>)\r\n        # dec_self_attn_subsequent_mask: [batch_size, 52, 52], Upper triangle of an array with 1.\r\n        # dec_self_attn_mask for self-decoder attention, the position whose val > 0 will be masked.\r\n        # dec_enc_attn_mask for encoder-decoder attention.\r\n        # e.g. 9488, 23, 53, 74, 0, 0  |  dec_self_attn_mask:\r\n        # 0 1 1 1 2 2\r\n        # 0 0 1 1 2 2\r\n        # 0 0 0 1 2 2\r\n        # 0 0 0 0 2 2\r\n        # 0 0 0 0 1 2\r\n        # 0 0 0 0 1 1\r\n        dec_outputs = self.tgt_emb(encoded_captions) + self.pos_emb(torch.LongTensor([list(range(52))]*batch_size).to(device))\r\n        dec_self_attn_pad_mask = self.get_attn_pad_mask(encoded_captions, encoded_captions)\r\n        dec_self_attn_subsequent_mask = self.get_attn_subsequent_mask(encoded_captions)\r\n        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequent_mask), 0)\r\n        dec_enc_attn_mask = (torch.tensor(np.zeros((batch_size, 52, 196))).to(device) == torch.tensor(np.ones((batch_size, 52, 196))).to(device))\r\n\r\n        dec_self_attns, dec_enc_attns = [], []\r\n        for layer in self.layers:\r\n            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, encoder_out, dec_self_attn_mask, dec_enc_attn_mask)\r\n            dec_self_attns.append(dec_self_attn)\r\n            dec_enc_attns.append(dec_enc_attn)\r\n        predictions = self.projection(dec_outputs)\r\n        # TODO: 暂时全部为0, return dec_self_attns, dec_enc_attns\r\n        alphas = torch.tensor(np.zeros((batch_size, 52, 196))).to(device)\r\n        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\r\n","AFTER":"        encoded_captions = encoded_captions[sort_ind]\r\n        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\r\n        # So, decoding lengths are actual lengths - 1\r\n        decode_lengths = (caption_lengths - 1).tolist()\r\n\r\n        # dec_outputs: [batch_size, max_len=52, embed_dim=512]\r\n        # dec_self_attn_pad_mask: [batch_size, len_q=52, len_k=52], 1 if id=0(<pad>)\r\n        # dec_self_attn_subsequent_mask: [batch_size, 52, 52], Upper triangle of an array with 1.\r\n        # dec_self_attn_mask for self-decoder attention, the position whose val > 0 will be masked.\r\n        # dec_enc_attn_mask for encoder-decoder attention.\r\n        # e.g. 9488, 23, 53, 74, 0, 0  |  dec_self_attn_mask:\r\n        # 0 1 1 1 2 2\r\n        # 0 0 1 1 2 2\r\n        # 0 0 0 1 2 2\r\n        # 0 0 0 0 2 2\r\n        # 0 0 0 0 1 2\r\n        # 0 0 0 0 1 1\r\n        dec_outputs = self.tgt_emb(encoded_captions) + self.pos_emb(torch.LongTensor([list(range(52))]*batch_size).to(device))\r\n        dec_outputs = self.dropout(dec_outputs)\r\n        dec_self_attn_pad_mask = self.get_attn_pad_mask(encoded_captions, encoded_captions)\r\n        dec_self_attn_subsequent_mask = self.get_attn_subsequent_mask(encoded_captions)\r\n        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequent_mask), 0)\r\n        dec_enc_attn_mask = (torch.tensor(np.zeros((batch_size, 52, 196))).to(device) == torch.tensor(np.ones((batch_size, 52, 196))).to(device))\r\n\r\n        dec_self_attns, dec_enc_attns = [], []\r\n        for layer in self.layers:\r\n            # attn: [batch_size, n_heads, len_q, len_k]\r\n            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, encoder_out, dec_self_attn_mask, dec_enc_attn_mask)\r\n            dec_self_attns.append(dec_self_attn)\r\n            dec_enc_attns.append(dec_enc_attn)\r\n        predictions = self.projection(dec_outputs)\r\n        return predictions, encoded_captions, decode_lengths, sort_ind, dec_self_attns, dec_enc_attns\r\n"}