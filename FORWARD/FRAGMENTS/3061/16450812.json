{"BEFORE":"        x = self.conv1(x)\n        x = nn.ReLU()(x)\n        x = nn.MaxPool2d(2, 1)(x)\n        x = self.dropout1(x)\n        x = self.conv2(x)\n        x = nn.ReLU()(x)\n        x = nn.MaxPool2d(2, 1)(x)\n        x = self.dropout2(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = nn.ReLU()(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n","AFTER":"        text, text_lengths = x\n        \n        embedded = self.embedding(text)\n        \n        #pack sequence\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n        out, (hidden, cell) = self.lstm(packed_embedded)\n        \n        # hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n        # output = self.fc1(hidden)\n        # output = self.dropout(self.fc2(output))\n                \n        #hidden = [batch size, hid dim * num directions]\n\n        # # Each sequence \"x\" is passed through an embedding layer\n        # out = self.embedding(x)\n\n        # Feed LSTMs\n        # out, (hidden, cell) = self.lstm(out)\n        out = self.dropout(out)\n\n        # The last hidden state is taken\n        out = torch.relu_(self.fc1(out[:,-1,:]))\n        out = self.dropout(out)\n        out = torch.sigmoid(self.fc2(out))\n            \n        return out\n"}