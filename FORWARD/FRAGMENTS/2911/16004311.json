{"BEFORE":"            current_sequence_length = hidden_states.shape[1]\n            if layer_past is not None:\n                current_sequence_length += layer_past[0].shape[1]\n            alibi = build_alibi_tensor(hidden_states.shape[1], n_head=self.num_heads, dtype=hidden_states.dtype)\n        # hidden_states: [batch_size, seq_length, hidden_size]\n        # repeat alibi tensor with the batch size\n        alibi = alibi.repeat(hidden_states.shape[0], 1, 1).to(hidden_states.device)  # TODO eliminate cpu-gpu transfer!\n\n        # apply preprocessing if the input is padded\n        if attention_mask is not None and 0 in attention_mask:  # TODO REMOVE CUDA SYNC\n            alibi = pre_process_alibi_for_pad(alibi, attention_mask, self.num_heads)\n\n        mixed_x_layer = self.query_key_value(hidden_states)\n\n        # [batch_size, seq_length, 3 x hidden_size] --> [batch_size, seq_length, num_heads, 3 x head_dim]\n        new_tensor_shape = mixed_x_layer.size()[:-1] + (self.num_heads, 3 * self.head_dim)\n        mixed_x_layer = mixed_x_layer.view(*new_tensor_shape)\n\n        # [batch_size, seq_length, num_heads, 3 x head_dim] --> 3  [batch_size, seq_length, num_heads, head_dim]\n        (query_layer, key_layer, value_layer) = split_tensor_along_last_dim(mixed_x_layer, 3)\n\n        if layer_past is not None:\n            past_key, past_value = layer_past\n            key_layer = torch.cat((past_key.type_as(key_layer), key_layer), dim=1)\n            value_layer = torch.cat((past_value.type_as(value_layer), value_layer), dim=1)\n\n        if use_cache is True:\n            present = (key_layer, value_layer)\n        else:\n            present = None\n\n        # [batch_size, head_dim, q_length, k_length]\n        output_size = (query_layer.size(0), query_layer.size(2), query_layer.size(1), key_layer.size(1))\n\n        # [batch_size, q_length, num_heads, head_dim] -> [q_length, batch_size * num_heads, head_dim]\n        query_layer = query_layer.transpose(1, 0).reshape(output_size[2], output_size[0] * output_size[1], -1)\n\n        # [batch_size, k_length, num_heads, head_dim] -> [k_length, batch_size * num_heads, head_dim]\n        key_layer = key_layer.transpose(1, 0).reshape(output_size[3], output_size[0] * output_size[1], -1)\n\n        # slice alibi tensor until the query length\n        sliced_alibi = alibi[: output_size[0] * output_size[1], :, : output_size[3]]\n\n        # Raw attention scores. [batch_size * num_heads, q_length, k_length]\n        beta = 1.0 \/ self.layer_number\n\n        matmul_result = torch.baddbmm(\n            sliced_alibi,\n","AFTER":"            current_sequence_length = hidden_states.shape[1] + (0 if layer_past is None else layer_past[0].shape[1])\n            alibi = build_alibi_tensor(\n                current_sequence_length, n_head=self.num_heads, dtype=hidden_states.dtype, device=hidden_states.device\n            )\n\n        # hidden_states: [batch_size, seq_length, hidden_size]\n        # apply preprocessing if the input is padded\n        if attention_mask is not None:\n            alibi = pre_process_alibi_for_pad(alibi, attention_mask)\n        # otherwise repeat alibi tensor with the batch size\n        else:\n            alibi = alibi.repeat(hidden_states.shape[0], 1, 1)\n\n        mixed_x_layer = self.query_key_value(hidden_states)\n\n        # [batch_size, seq_length, 3 x hidden_size] --> [batch_size, seq_length, num_heads, 3 x head_dim]\n        new_tensor_shape = mixed_x_layer.size()[:-1] + (self.num_heads, 3 * self.head_dim)\n        mixed_x_layer = mixed_x_layer.view(*new_tensor_shape)\n\n        # [batch_size, seq_length, num_heads, 3 x head_dim] --> 3  [batch_size, seq_length, num_heads, head_dim]\n        (query_layer, key_layer, value_layer) = split_tensor_along_last_dim(mixed_x_layer, 3)\n\n        if layer_past is not None:\n            past_key, past_value = layer_past\n            key_layer = torch.cat((past_key.type_as(key_layer), key_layer), dim=1)\n            value_layer = torch.cat((past_value.type_as(value_layer), value_layer), dim=1)\n\n        if use_cache is True:\n            present = (key_layer, value_layer)\n        else:\n            present = None\n\n        # [batch_size, head_dim, q_length, k_length]\n        output_size = (query_layer.size(0), query_layer.size(2), query_layer.size(1), key_layer.size(1))\n\n        # [batch_size, q_length, num_heads, head_dim] -> [q_length, batch_size * num_heads, head_dim]\n        query_layer = query_layer.transpose(1, 0).reshape(output_size[2], output_size[0] * output_size[1], -1)\n\n        # [batch_size, k_length, num_heads, head_dim] -> [k_length, batch_size * num_heads, head_dim]\n        key_layer = key_layer.transpose(1, 0).reshape(output_size[3], output_size[0] * output_size[1], -1)\n\n        # Raw attention scores. [batch_size * num_heads, q_length, k_length]\n        beta = 1.0 \/ self.layer_number\n\n        matmul_result = torch.baddbmm(\n            alibi,\n"}