{"BEFORE":"        attn = (q @ k.transpose(-2, -1))\n\n        attn = self.proj_l(attn.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n\n        attn = attn.softmax(dim=-1)\n\n        attn = self.proj_w(attn.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n","AFTER":"        attn = torch.matmul(q, k.transpose(-2, -1))\n\n        attn = self.proj_l(attn.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n\n        attn = attn.softmax(dim=-1)\n\n        attn = self.proj_w(attn.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n        attn = self.attn_drop(attn)\n\n        x = torch.matmul(attn, v).transpose(1, 2).reshape(B, N, C)\n"}