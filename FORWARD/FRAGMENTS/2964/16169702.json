{"BEFORE":"        features = []\n        x = self.conv1(x)  # 112x112\n        features.append(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)  # 56x56 ignore\n\n        x = self.layer1(x)  # 56x56\n        features.append(x)\n        x = self.layer2(x)  # 28x28\n        features.append(x)\n        x = self.layer3(x)  # 14x14 ignore (maybe not)\n        features.append(x)\n        x = self.layer4(x)  # 7x7\n        features.append(x)\n\n        if not self.include_top:\n            return features\n\n        x = self.avgpool(x)  # 1x1\n        features.append(x)\n\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n","AFTER":"    def forward(self, x, x_rec):\r\n        \"\"\"\r\n        Takes in original image and reconstructed image and feeds it through face network and takes the difference\r\n        between the different resolutions and scales by alpha_{i}.\r\n        Normalizing the features and applying spatial resolution was taken from LPIPS and wasn't mentioned in the paper.\r\n        \"\"\"\r\n        images = torch.concat([x, x_rec], dim=0)  # batch\r\n        features = self._forward(images)\r\n        features = [f.chunk(2) for f in features]\r\n        # diffs = [a * torch.abs(p[0] - p[1]).sum() for a, p in zip(self.alphas, features)]\r\n        diffs = [a * torch.abs(p[0] - p[1]).mean() for a, p in zip(self.alphas, features)]\r\n        # diffs = [a*torch.abs(self.norm_tensor(tf) - self.norm_tensor(rf)) for a, tf, rf in zip(self.alphas, true_features, rec_features)]\r\n\r\n        # diffs = [a * torch.mean(torch.abs(tf - rf)) for a, tf, rf in zip(self.alphas, features)]\r\n        return sum(diffs)\r\n"}