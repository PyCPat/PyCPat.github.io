{"BEFORE":"            - 2 * z_flattened @ self.embedding.weight.permute(1,0)\n            + self.embedding.weight.permute(1,0).pow(2).sum(0, keepdim=True)\n        )\n        _, encoding_indices = (-d).max(1)\n        encodings = F.one_hot(encoding_indices, self.num_tokens).type(z_flattened.dtype)\n        encoding_indices = encoding_indices.view(*z.shape[:-1])\n        z_q = self.embedding(encoding_indices)\n        avg_probs = torch.mean(encodings, dim=0)\n        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n\n        if self.training:\n            encodings_sum = encodings.sum(0)\n            embed_sum = z_flattened.transpose(0, 1) @ encodings\n\n            self.cluster_size.data.mul_(self.decay).add_(\n                encodings_sum, alpha=1 - self.decay\n            )\n            self.embed_avg.data.mul_(self.decay).add_(embed_sum, alpha=1 - self.decay)\n            n = self.cluster_size.sum()\n            cluster_size = (\n                (self.cluster_size + self.eps) \/ (n + self.num_tokens * self.eps) * n\n            )\n            embed_normalized = self.embed_avg \/ cluster_size.unsqueeze(0)\n            self.embedding.weight = nn.Parameter(embed_normalized.permute(1,0))\n","AFTER":"            - 2 * z_flattened @ self.embedding.weight\n            + self.embedding.weight.pow(2).sum(0, keepdim=True)\n        )\n        _, encoding_indices = (-d).max(1)\n        encodings = F.one_hot(encoding_indices, self.num_tokens).type(z_flattened.dtype)\n        encoding_indices = encoding_indices.view(*z.shape[:-1])\n        z_q = self.embedding(encoding_indices)\n        avg_probs = torch.mean(encodings, dim=0)\n        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n\n        if self.training:\n            encodings_sum = encodings.sum(0)\n            embed_sum = z_flattened.transpose(0, 1) @ encodings\n\n            self.embedding.cluster_size.data.mul_(self.decay).add_(\n                encodings_sum, alpha=1 - self.decay\n            )\n            self.embedding.embed_avg.data.mul_(self.decay).add_(embed_sum, alpha=1 - self.decay)\n            n = self.embedding.cluster_size.sum()\n            cluster_size = (\n                (self.embedding.cluster_size + self.eps) \/ (n + self.num_tokens * self.eps) * n\n            )\n            embed_normalized = self.embedding.embed_avg \/ cluster_size.unsqueeze(0)\n            self.embedding.weight.data.copy_(embed_normalized)\n"}