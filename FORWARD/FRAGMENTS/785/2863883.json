{"BEFORE":"        greedy = memory is None\n\n        if memory is not None:\n\n            # Grouping multiple frames if necessary\n            if memory.size(-1) == self.memory_dim:\n                memory = memory.view(B, memory.size(1) \/\/ self.r, -1)\n            assert memory.size(-1) == self.memory_dim * self.r,\\\n                \" !! Dimension mismatch {} vs {} * {}\".format(memory.size(-1),\n                                                         self.memory_dim, self.r)\n            T_decoder = memory.size(1)\n\n        # go frame - 0 frames tarting the sequence\n        initial_memory = Variable(\n            inputs.data.new(B, self.memory_dim * self.r).zero_())\n\n        # Init decoder states\n        attention_rnn_hidden = Variable(\n            inputs.data.new(B, 256).zero_())\n        decoder_rnn_hiddens = [Variable(\n            inputs.data.new(B, 256).zero_())\n            for _ in range(len(self.decoder_rnns))]\n        current_context_vec = Variable(\n            inputs.data.new(B, 256).zero_())\n\n        # Time first (T_decoder, B, memory_dim)\n        if memory is not None:\n            memory = memory.transpose(0, 1)\n\n        outputs = []\n        alignments = []\n\n        t = 0\n        memory_input = initial_memory\n        while True:\n            if t > 0:\n                if greedy:\n                    memory_input = outputs[-1]\n                else:\n                    # combine prev. model output and prev. real target\n                    memory_input = torch.div(outputs[-1] + memory[t-1], 2.0)\n                    # add a random noise\n                    noise = torch.autograd.Variable(\n                        memory_input.data.new(memory_input.size()).normal_(0.0, 0.5))\n                    memory_input = memory_input + noise\n\n            # Prenet\n            processed_memory = self.prenet(memory_input)\n\n            # Attention RNN\n            attention_rnn_hidden, current_context_vec, alignment = self.attention_rnn(\n                processed_memory, current_context_vec, attention_rnn_hidden,\n                inputs)\n\n            # Concat RNN output and attention context vector\n            decoder_input = self.project_to_decoder_in(\n                torch.cat((attention_rnn_hidden, current_context_vec), -1))\n\n            # Pass through the decoder RNNs\n            for idx in range(len(self.decoder_rnns)):\n                decoder_rnn_hiddens[idx] = self.decoder_rnns[idx](\n                    decoder_input, decoder_rnn_hiddens[idx])\n                # Residual connectinon\n                decoder_input = decoder_rnn_hiddens[idx] + decoder_input\n\n            output = decoder_input\n\n            # predict mel vectors from decoder vectors\n            output = self.proj_to_mel(output)\n\n            outputs += [output]\n            alignments += [alignment]\n\n            t += 1\n\n            if greedy:\n                if t > 1 and is_end_of_frames(output, self.eps):\n                    break\n                elif t > self.max_decoder_steps:\n                    print(\" !! Decoder stopped with 'max_decoder_steps'. \\\n                          Something is probably wrong.\")\n                    break\n            else:\n                if t >= T_decoder:\n                    break\n\n        assert greedy or len(outputs) == T_decoder\n\n        # Back to batch first\n        alignments = torch.stack(alignments).transpose(0, 1)\n        outputs = torch.stack(outputs).transpose(0, 1).contiguous()\n\n        return outputs, alignments\n","AFTER":"        greedy = ~self.training\n\n        if memory is not None:\n            # Grouping multiple frames if necessary\n            if memory.size(-1) == self.memory_dim:\n                memory = memory.view(B, memory.size(1) \/\/ self.r, -1)\n            assert memory.size(-1) == self.memory_dim * self.r,\\\n                \" !! Dimension mismatch {} vs {} * {}\".format(memory.size(-1),\n                                                         self.memory_dim, self.r)\n            T_decoder = memory.size(1)\n\n        # go frame - 0 frames tarting the sequence\n        initial_memory = Variable(\n            inputs.data.new(B, self.memory_dim * self.r).zero_())\n\n        # Init decoder states\n        attention_rnn_hidden = Variable(\n            inputs.data.new(B, 256).zero_())\n        decoder_rnn_hiddens = [Variable(\n            inputs.data.new(B, 256).zero_())\n            for _ in range(len(self.decoder_rnns))]\n        current_context_vec = Variable(\n            inputs.data.new(B, 256).zero_())\n\n        # Time first (T_decoder, B, memory_dim)\n        if memory is not None:\n            memory = memory.transpose(0, 1)\n\n        outputs = []\n        alignments = []\n        stop_outputs = []\n\n        t = 0\n        memory_input = initial_memory\n        while True:\n            if t > 0:\n                if greedy:\n                    memory_input = outputs[-1]\n                else:\n                    # combine prev. model output and prev. real target\n                    # memory_input = torch.div(outputs[-1] + memory[t-1], 2.0)\n                    # add a random noise\n                    # noise = torch.autograd.Variable(\n                        # memory_input.data.new(memory_input.size()).normal_(0.0, 0.5))\n                    # memory_input = memory_input + noise\n                    memory_input = memory[t-1]\n\n            # Prenet\n            processed_memory = self.prenet(memory_input)\n\n            # Attention RNN\n            attention_rnn_hidden, current_context_vec, alignment = self.attention_rnn(\n                processed_memory, current_context_vec, attention_rnn_hidden,\n                inputs)\n\n            # Concat RNN output and attention context vector\n            decoder_input = self.project_to_decoder_in(\n                torch.cat((attention_rnn_hidden, current_context_vec), -1))\n\n            # Pass through the decoder RNNs\n            for idx in range(len(self.decoder_rnns)):\n                decoder_rnn_hiddens[idx] = self.decoder_rnns[idx](\n                    decoder_input, decoder_rnn_hiddens[idx])\n                # Residual connectinon\n                decoder_input = decoder_rnn_hiddens[idx] + decoder_input\n            \n            output = decoder_input\n            stop_token_input = decoder_input\n            \n            # stop token prediction\n            stop_token_input = torch.cat((output, current_context_vec), -1)\n            stop_output = self.stop_token(stop_token_input)\n\n            # predict mel vectors from decoder vectors\n            output = self.proj_to_mel(output)\n\n            outputs += [output]\n            alignments += [alignment]\n            stop_outputs += [stop_output]\n\n            t += 1\n\n            if (not greedy and self.training) or (greedy and memory is not None):\n                if t >= T_decoder:\n                    break\n            else:\n                if t > 1 and is_end_of_frames(output, self.eps):\n                    break\n                elif t > self.max_decoder_steps:\n                    print(\" !! Decoder stopped with 'max_decoder_steps'. \\\n                          Something is probably wrong.\")\n                    break\n                           \n        assert greedy or len(outputs) == T_decoder\n\n        # Back to batch first\n        alignments = torch.stack(alignments).transpose(0, 1)\n        outputs = torch.stack(outputs).transpose(0, 1).contiguous()\n        stop_outputs = torch.stack(stop_outputs).transpose(0, 1).contiguous()\n\n        return outputs, alignments, stop_outputs\n"}