{"BEFORE":"    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size = x.shape[0]\n        x = x.permute(0, 2, 1).reshape(-1, x.shape[1])\n\n        codes = self.encode(x)\n        quantized = self.decode(codes)\n\n        diff = (x - quantized).pow(2).mean()\n        quantized = quantized + x - x.detach()\n\n        if self.training:\n            self.replace_dead_codes(x)\n\n            onehot = F.one_hot(codes, self.size).float()\n            usage = onehot.sum(0)\n            targets = (x.T @ onehot \/ usage).T\n\n            targets = torch.where(torch.isnan(targets), self.embedding,\n                                  targets)\n\n            ema_inplace(self.embedding, targets, self.ema)\n            ema_inplace(self.usage, usage, self.ema)\n\n        quantized = quantized.reshape(\n            batch_size,\n            -1,\n            quantized.shape[-1],\n        ).permute(0, 2, 1)\n        codes = codes.reshape(batch_size, -1)\n        return quantized, diff.mean(), codes[:, None]\n","AFTER":"    def forward(self, x):\n        shape, dtype = x.shape, x.dtype\n        x = self.preprocess(x)\n\n        self.init_embed_(x)\n\n        embed_ind = self.quantize(x)\n        embed_onehot = F.one_hot(embed_ind, self.codebook_size).type(dtype)\n        embed_ind = self.postprocess_emb(embed_ind, shape)\n        quantize = self.dequantize(embed_ind)\n\n        if self.training:\n            # We do the expiry of code at that point as buffers are in sync\n            # and all the workers will take the same decision.\n            self.expire_codes_(x)\n            ema_inplace(self.cluster_size, embed_onehot.sum(0), self.decay)\n            embed_sum = x.t() @ embed_onehot\n            ema_inplace(self.embed_avg, embed_sum.t(), self.decay)\n            cluster_size = (laplace_smoothing(\n                self.cluster_size, self.codebook_size, self.epsilon) *\n                            self.cluster_size.sum())\n            embed_normalized = self.embed_avg \/ cluster_size.unsqueeze(1)\n            self.embed.data.copy_(embed_normalized)\n\n        return quantize, embed_ind\n"}