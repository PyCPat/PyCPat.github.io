{"BEFORE":"        if offsets is not None:\n            x = x + x.new_tensor(offsets).unsqueeze(0)\n\n        # Get the quotient index.\n        quotient_index = torch.div(x, self.num_buckets, rounding_mode='floor')\n\n        # Get the reminder index.\n        remainder_index = torch.remainder(x, self.num_buckets)\n\n        # Lookup the quotient_embedding using the quotient_index.\n        quotient_embedding = self.q_embeddings(quotient_index)\n\n        # Lookup the remainder_embedding using the remainder_index.\n        remainder_embedding = self.r_embeddings(remainder_index)\n\n        # Use multiplication as a combiner operation\n        return quotient_embedding * remainder_embedding\n","AFTER":"        assert offsets is None, \"offsets have been handled internally.\\n Users shouldn't manually input offsets\"\n        x_parallel = self._lbmgr.put_tensor_on_rank(x, self.rank)\n        output_parallel = self.embed(x_parallel)\n        output_gather = self.comm_func(output_parallel, self.parallel_mode, reduce_op=self.mode)\n\n        if self.mode == 'mean':\n            output_gather = output_gather \/ self.num_groups\n\n        assert output_gather.shape == (x.size(0), self.embedding_dim)\n        return output_gather\n"}