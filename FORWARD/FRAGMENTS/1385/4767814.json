{"BEFORE":"    def forward(self, q, k, v, mask=None, attention_mask=None, head_mask=None):\n        # calculate attention\n        matmul_qk = torch.matmul(q, k.permute(0, 1, 3, 2))\n\n        dk = k.shape[-1]\n        scaled_attention_logits = matmul_qk \/ np.sqrt(dk)\n\n        if mask is not None:\n            nd = scaled_attention_logits.size(-2)\n            ns = scaled_attention_logits.size(-1)\n            scaled_attention_logits += mask[ns - nd: ns, :ns] * -1e4\n\n        if attention_mask is not None:\n            # Apply the attention mask\n            scaled_attention_logits = scaled_attention_logits + attention_mask\n\n        attention_weights = torch.softmax(scaled_attention_logits, dim=-1)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_weights = attention_weights * head_mask\n\n        # output = torch.matmul(attention_weights, v)\n        # return output, attention_weights\n        return v, attention_weights\n","AFTER":"        nd = scaled_attention_logits.size(-2)\n        ns = scaled_attention_logits.size(-1)\n        scaled_attention_logits += mask[ns - nd: ns, :ns] * -1e4\n\n        # if attention_mask is not None:\n        #     # Apply the attention mask\n        #     scaled_attention_logits = scaled_attention_logits + attention_mask\n\n        attention_weights = torch.softmax(scaled_attention_logits, dim=-1)\n\n        # # Mask heads if we want to\n        # if head_mask is not None:\n        #     attention_weights = attention_weights * head_mask\n\n        output = torch.matmul(attention_weights, v)\n        # return output, attention_weights\n        return output\n"}