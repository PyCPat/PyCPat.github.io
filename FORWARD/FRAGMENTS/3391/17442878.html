<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        action = action.tanh()

        <a id="change">return </a>action<a id="change">, std</a>


if __name__ == &quot__main__&quot:
    use_cuda = torch.cuda.is_available()</code></pre><h3>After Change</h3><pre><code class='java'>
            log_prob = m.log_prob(action_base)
            log_prob.unsqueeze_(-1)

            action<a id="change"> = </a><a id="change">action_base.tanh()</a>

            &#47&#47 According to "Soft Actor-Critic" (Haarnoja et. al) Appendix C
            action_bound_compensation = torch.log(1. - action.tanh().pow(2) + 1e-6)
            action_bound_compensation = action_bound_compensation.sum(dim=-1, keepdim=True)</code></pre>