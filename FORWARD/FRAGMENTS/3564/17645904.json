{"BEFORE":"        device = x.device\n        b, t, e, m = *x.shape, self.num_mem_kv\n\n        mem = self.mem_kv.expand(m, b, e)\n        keys = default(keys, torch.empty(b, 0, e, device=device))\n\n        x, keys = x.transpose(0, 1), keys.transpose(0, 1)\n\n        kv = torch.cat((x, mem, keys))\n\n        attn_shape = (t, kv.shape[0])\n        attn_mask = torch.zeros(*attn_shape, device=x.device)\n        if self.causal:\n            i, j = torch.triu_indices(t, t, 1)\n            attn_mask[i, j] = float('-inf')\n\n        output, _ = self.attn(x, kv, kv, attn_mask = attn_mask)\n        return self.to_out(output.transpose(0, 1))\n","AFTER":"    def forward(self, qk, v, query_len = None):\n        query_len = default(query_len, qk.shape[1])\n        t = query_len\n\n        q = qk[:, 0:query_len]\n        qk = F.normalize(qk, 2, dim=-1)\n\n        dot = torch.einsum('bie,bje->bij', q, qk)\n\n        # qk attention requires tokens not attend to self\n        i = torch.arange(t)\n        dot[:, i, i] = -1e-5 \n\n        if self.causal:\n            i, j = torch.triu_indices(t, t, 1)\n            dot[:, i, j] = float('-inf')\n\n        dot = dot.softmax(dim=-1)\n        out = torch.einsum('bij,bje->bie', dot, v)\n        return out, dot\n"}