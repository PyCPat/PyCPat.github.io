{"BEFORE":"        coor_weights = self.coors_mlp(m_ij)\n\n        if exists(mask):\n            mask_value = -torch.finfo(coor_weights.dtype).max if self.coor_attention else 0.\n            coor_weights.masked_fill_(~mask, mask_value)\n\n        if self.coor_attention:\n            coor_weights = coor_weights.softmax(dim = -1)\n\n        rel_coors = self.rel_coors_norm(rel_coors)\n\n        coors_out = einsum('b h i j, b i j c -> b i c h', coor_weights, rel_coors)\n","AFTER":"        coor_weights = self.coors_mlp(m_ij)\n        coors_gate_input = rearrange(coor_weights, 'b h i j -> b i j h')\n\n        if exists(mask):\n            mask_value = -torch.finfo(coor_weights.dtype).max if self.coor_attention else 0.\n            coor_weights.masked_fill_(~mask, mask_value)\n\n        if self.coor_attention:\n            coor_weights = coor_weights.softmax(dim = -1)\n\n        rel_coors = self.rel_coors_norm(rel_coors)\n        rel_coors = repeat(rel_coors, 'b i j c -> b i j c h', h = h)\n\n        if exists(self.coors_gate):\n            rel_coors_signs = self.coors_gate(coors_gate_input)\n            rel_coors_signs = rearrange(rel_coors_signs, 'b i j h -> b i j () h')\n            rel_coors = rel_coors * rel_coors_signs\n\n        coors_out = einsum('b h i j, b i j c h -> b i c h', coor_weights, rel_coors)\n"}