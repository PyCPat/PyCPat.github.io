{"BEFORE":"        reshape_with_window = lambda x: x.reshape(b, h, nc, -1, d)\n\n        if self.shared_qk:\n            wsz = min(wsz, t)\n            indices, aux_loss = self.kmeans(k, wsz)\n            k = F.normalize(k, 2, dim=-1).type(dtype)\n            key_mask = query_mask\n            kv_indices = indices\n        else:\n            wsz = min(wsz, t)\n            c_wsz = min(c_wsz, kv_t)\n\n            if wsz < self.window_size:\n                c_wsz = (self.window_size * self.context_window_size) \/\/ wsz\n            indices, loss_1 = self.kmeans(q, wsz)\n            kv_indices, loss_2 = self.kmeans(k, c_wsz)\n            aux_loss = loss_1 + loss_2\n","AFTER":"        is_reverse = kwargs.pop('_reverse', False)\n\n        out = torch.zeros_like(q, dtype=dtype)\n\n        update_kmeans = self.training and not is_reverse\n\n        if self.shared_qk:\n            wsz = min(wsz, t)\n            dists, aux_loss = self.kmeans(k, update_kmeans)\n            indices = distribution(dists, wsz)\n            k = F.normalize(k, 2, dim=-1).type(dtype)\n            key_mask = query_mask\n            kv_indices = indices\n        else:\n            wsz = min(wsz, t)\n            c_wsz = min(c_wsz, kv_t)\n\n            dists, aux_loss = self.kmeans(torch.cat((q, k), dim=2), update_kmeans)\n            q_dists, k_dists = split_at_index(2, t, dists)\n            indices = distribution(q_dists, wsz)\n            kv_indices = distribution(k_dists, c_wsz)\n\n        q = batched_index_select(q, indices)\n        k = batched_index_select(k, kv_indices)\n        v = batched_index_select(v, kv_indices)\n\n        reshape_with_window = lambda x: x.reshape(b, h, nc, -1, d)\n"}