<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        valid_mask = len_range[None, :] &lt; lengths[:, None]

        &#47&#47 Position embedding
        position = (<a id="change">lengths</a>[:, None] - len_range[None, :]) * valid_mask.long()
        pos_vectors = self.p_embeddings(position)
        seq = seq + pos_vectors

        &#47&#47 Self-attention
        causality_mask = np.tril(np.ones((1, 1, seq_len, seq_len), dtype=np.int))
        attn_mask = torch.from_numpy(causality_mask).to(self.device)
        &#47&#47 attn_mask = valid_his.view(batch_size, 1, 1, seq_len)
        for block in self.transformer_block:
            seq = block(seq, attn_mask)
        seq = seq * valid_mask[:, :, None].float()

        his_vector<a id="change"> = </a><a id="change">(seq * (position == 1).float()[:, :, None]).sum(</a>1<a id="change">)</a>
        return his_vector
</code></pre><h3>After Change</h3><pre><code class='java'>
        seq = seq + pos_vectors

        &#47&#47 Self-attention
        attn_mask = <a id="change">valid_mask.view(</a>batch_size, <a id="change">1</a>, <a id="change">1</a>, seq_len<a id="change">)</a>
        for block in self.transformer_block:
            seq = block(seq, attn_mask)
        seq = seq * valid_mask[:, :, None].float()

        his_vector<a id="change"> = </a>seq[torch.arange(batch_size), lengths - 1]
        return his_vector
</code></pre>