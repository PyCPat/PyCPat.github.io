{"BEFORE":"        seq_len = seq.size(1)\n        len_range = torch.from_numpy(np.arange(seq_len)).to(self.device)\n        valid_mask = len_range[None, :] < lengths[:, None]\n\n        # Position embedding\n        position = (lengths[:, None] - len_range[None, :]) * valid_mask.long()\n        pos_vectors = self.p_embeddings(position)\n        seq = seq + pos_vectors\n\n        # Self-attention\n        causality_mask = np.tril(np.ones((1, 1, seq_len, seq_len), dtype=np.int))\n        attn_mask = torch.from_numpy(causality_mask).to(self.device)\n        # attn_mask = valid_his.view(batch_size, 1, 1, seq_len)\n        for block in self.transformer_block:\n            seq = block(seq, attn_mask)\n        seq = seq * valid_mask[:, :, None].float()\n\n        his_vector = (seq * (position == 1).float()[:, :, None]).sum(1)\n","AFTER":"        batch_size, seq_len = seq.size(0), seq.size(1)\n        len_range = torch.from_numpy(np.arange(seq_len)).to(self.device)\n        valid_mask = len_range[None, :] < lengths[:, None]\n\n        # Position embedding\n        position = len_range[None, :] * valid_mask.long()\n        pos_vectors = self.p_embeddings(position)\n        seq = seq + pos_vectors\n\n        # Self-attention\n        attn_mask = valid_mask.view(batch_size, 1, 1, seq_len)\n        for block in self.transformer_block:\n            seq = block(seq, attn_mask)\n        seq = seq * valid_mask[:, :, None].float()\n\n        his_vector = seq[torch.arange(batch_size), lengths - 1]\n"}