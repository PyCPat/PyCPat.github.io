{"BEFORE":"    def forward(self, x, H, W, rel_pos_bias=None):\n        B, N, C = x.shape\n        # qkv_bias = None\n        # if self.q_bias is not None:\n        #     qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n        # qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C \/\/ self.num_heads).permute(2, 0, 3, 1, 4)\n        qkv = self.qkv(x)\n        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[\n            2]  # make torchscript happy (cannot use tensor as tuple)\n\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))\n        attn = calc_rel_pos_spatial(attn, q, self.window_size,\n                                    self.window_size, self.rel_pos_h,\n                                    self.rel_pos_w)\n        # if self.relative_position_bias_table is not None:\n        #     relative_position_bias = \\\n        #         self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n        #             self.window_size[0] * self.window_size[1] + 1,\n        #             self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n        #     relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n        #     attn = attn + relative_position_bias.unsqueeze(0)\n\n        # if rel_pos_bias is not None:\n        #     attn = attn + rel_pos_bias\n\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n","AFTER":"    def forward(self, x):\n        B, H, W, _ = x.shape\n        # qkv with shape (3, B, nHead, H * W, C)\n        qkv = self.qkv(x).reshape(B, H * W, 3, self.num_heads,\n                                  -1).permute(2, 0, 3, 1, 4)\n        # q, k, v with shape (B * nHead, H * W, C)\n        q, k, v = qkv.reshape(3, B * self.num_heads, H * W, -1).unbind(0)\n\n        attn = (q * self.scale) @ k.transpose(-2, -1)\n\n        if self.use_rel_pos:\n            attn = add_decomposed_rel_pos(attn, q, self.rel_pos_h,\n                                          self.rel_pos_w, (H, W), (H, W))\n\n        attn = attn.softmax(dim=-1)\n        x = (attn @ v).view(B, self.num_heads, H, W,\n                            -1).permute(0, 2, 3, 1, 4).reshape(B, H, W, -1)\n"}