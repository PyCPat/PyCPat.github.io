{"BEFORE":"        sparse_embeddings = [self.linear_embedding_dict[feat.embedding_name](X[\n            :, self.feature_name_to_index[feat.name][0]:self.feature_name_to_index[feat.name][1]\n        ].long()) for feat in self.sparse_feature_columns]\n        dense_values = [X[\n            :, self.feature_name_to_index[feat.name][0]:self.feature_name_to_index[feat.name][1]\n        ] for feat in self.dense_feature_columns]\n\n        linear_logit = torch.zeros(\n            [X.shape[0], 1]).to(sparse_embeddings[0].device)\n","AFTER":"        dense_values, sparse_embeddings = collect_inputs_and_embeddings(\n            X, sparse_feature_columns=self.sparse_feature_columns,\n            dense_feature_columns=self.dense_feature_columns,\n            feature_name_to_index=self.feature_name_to_index,\n            embedding_layer_def=self.linear_embedding_dict)\n\n        linear_logit = torch.zeros([X.shape[0], 1]).to(self.device)\n        if len(sparse_embeddings) > 0:\n            linear_logit = linear_logit.to(sparse_embeddings[0].device)\n"}