{"BEFORE":"        x_cross_attn_mask = rearrange(mask[:, :, None] * mask[:, None, :], 'b i j -> b (i j)')\n\n        # embed multiple sequence alignment\n\n        m = self.token_emb(msa)\n        m += self.pos_emb(torch.arange(msa.shape[-1], device = device))[None, None, ...]\n        m = rearrange(m, 'b m n d -> b (m n) d')\n\n        if exists(msa_mask):\n            msa_mask = rearrange(msa_mask, 'b m n -> b (m n)')\n\n        # trunk\n\n        for ((attn, cross_attn, ff), (msa_attn, msa_cross_attn, msa_ff)) in zip(self.layers, self.msa_layers):\n            # self attention\n\n            x = attn(x, mask = mask) + x\n            m = msa_attn(m, mask = msa_mask) + m\n\n            # cross attention\n\n            x = rearrange(x, 'b i j d -> b (i j) d')\n\n            m = msa_cross_attn(\n                m,\n                mask = msa_mask,\n                context = x,\n                context_mask = x_cross_attn_mask\n            ) + m\n\n            x = cross_attn(\n                x,\n                mask = x_cross_attn_mask,\n                context = m,\n                context_mask = msa_mask\n            ) + x\n\n            x = rearrange(x, 'b (i j) d -> b i j d', i = n)\n","AFTER":"        x_mask = mask[:, :, None] * mask[:, None, :]\n\n        # embed multiple sequence alignment\n\n        m = self.token_emb(msa)\n        m += self.msa_pos_emb(torch.arange(msa.shape[-1], device = device))[None, None, ...]\n        m += self.msa_num_pos_emb(torch.arange(msa.shape[1], device = device))[None, :, None, :]\n"}