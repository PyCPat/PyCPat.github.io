{"BEFORE":"        rand_times = torch.empty(b, device = device).uniform_(0, 1)\n        batched_randperm = torch.rand((b, n), device = device).argsort(dim = -1).float()\n\n        def get_mask_from_times(rand_times, randperm):\n            rand_probs = self.schedule_fn(rand_times)\n            num_tokens_mask = (rand_probs * n).clamp(min = 1.)\n            return randperm < rearrange(num_tokens_mask, 'b -> b 1')\n\n        mask = get_mask_from_times(rand_times, batched_randperm)\n        masked = torch.where(mask, self.mask_id, x)\n\n        if self.self_cond:\n            if random() > self.self_cond_train_prob:\n                self_cond = self.null_embed\n            else:\n                with torch.no_grad():\n                    self_cond = self.net(masked, return_embeddings = True, **kwargs).detach()\n\n            kwargs.update(sum_embeds = self.to_self_cond(self_cond))\n\n        logits = self.net(masked, **kwargs)\n\n        loss = F.cross_entropy(\n            logits[mask],\n            x[mask]\n","AFTER":"        batched_randperm = torch.rand((b, n), device = device).argsort(dim = -1).float()\n\n        rand_probs = self.schedule_fn(rand_times)\n        num_tokens_mask = (rand_probs * n).clamp(min = 1.)\n        mask = batched_randperm < rearrange(num_tokens_mask, 'b -> b 1')\n\n        labels = x[mask]\n\n        # to ensure all tokens produce embeddings, instead of just the ones with [mask] input, as done in seminal BERT MLM paper\n        # potentially needed for self-conditioning (on embedding) to work well\n\n        replace_mask_id_mask = mask.clone()\n        frac_seq_left = 1.\n\n        if self.no_replace_prob > 0. and coin_flip():\n            frac_seq_left -= self.no_replace_prob\n\n            no_replace_prob_mask = get_mask_subset_prob(mask, self.no_replace_prob)\n            replace_mask_id_mask &= ~no_replace_prob_mask\n\n        if self.random_token_prob > 0. and coin_flip():\n            random_token_prob_mask = get_mask_subset_prob(replace_mask_id_mask, self.random_token_prob * frac_seq_left)\n            random_tokens = torch.randint(0, self.num_tokens, (b, n), device = device)\n\n            x = torch.where(random_token_prob_mask, random_tokens, x)\n            replace_mask_id_mask &= ~random_token_prob_mask\n\n        masked = torch.where(replace_mask_id_mask, self.mask_id, x)\n\n        # self conditioning\n\n        if self.self_cond:\n            self_cond = self.null_embed\n\n            if sample_prob(self.self_cond_train_prob):\n                with torch.no_grad():\n                    self_cond = self.net(masked, return_embeddings = True, **kwargs).detach()\n\n            kwargs.update(sum_embeds = self.to_self_cond(self_cond))\n\n        # logits\n\n        logits = self.net(masked, **kwargs)\n\n        # cross entropy loss\n\n        loss = F.cross_entropy(\n            logits[mask],\n            labels\n"}