{"BEFORE":"        conf_data = conf_data.view(-1, num_priors, self.num_classes)\n        obj_data = obj_data.view(-1, num_priors, 2)\n\n        # match priors (default boxes) and ground truth boxes\n        if GPU:\n            loc_t = torch.Tensor(num, num_priors, 4).cuda()\n            conf_t = torch.CharTensor(num, num_priors).cuda()\n            obj_t = torch.ByteTensor(num, num_priors).cuda()\n        else:\n            loc_t = torch.Tensor(num, num_priors, 4)\n            conf_t = torch.CharTensor(num, num_priors)\n            obj_t = torch.ByteTensor(num, num_priors)\n\n        # match priors with gt\n        for idx in range(n_way):\n            for idy in range(n_shot):\n                truths = targets[idx][idy][:, :-1].data  # [obj_num, 4]\n                labels = targets[idx][idy][:, -1].data   # [obj_num]\n                defaults = priors.data                   # [num_priors,4]\n                match(self.threshold,truths,defaults,self.variance,labels,loc_t,conf_t,obj_t,idx*n_shot+idy)\n\n        pos = obj_t.byte() # [num, num_priors]\n\n        # Localization Loss (Smooth L1)\n        # Shape: [batch,num_priors,4]\n        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)\n        loc_p = loc_data[pos_idx].view(-1, 4) #整个batch的正样本priors\n        loc_t = loc_t[pos_idx].view(-1, 4)    #对应的target\n        loss_l = F.smooth_l1_loss(loc_p, loc_t, reduction='sum') # size_average=False等价于reduction='sum'\n\n\n\n        # Confidence Loss(cosine distance to classes center)\n        # pos [num, num_priors]\n        # conf_data [num, num_priors, feature_dim]\n        conf_data = conf_data \/ torch.norm(conf_data, dim=2, keepdim=True)  # [num, num_priors, feature_dim]\n        batch_conf = conf_data.view(-1, self.num_classes).mm(self.imprinted_matrix.t()) * self.scale  # [n_way, num_classes]\n","AFTER":"        features = [conf_data.view(-1, self.num_classes)]\n        for i in range(3):\n            new_features = (self.denselayer1, self.denselayer2, self.denselayer3)[i](*features)\n            features.append(new_features)\n        batch_conf = new_features * self.scale  # [n_way, num_classes]\n"}