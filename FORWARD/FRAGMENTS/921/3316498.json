{"BEFORE":"        q = q.view(q.shape[:-1] + (self.no_heads, -1))\n        k = k.view(k.shape[:-1] + (self.no_heads, -1))\n        v = v.view(v.shape[:-1] + (self.no_heads, -1))\n\n        # [*, H, Q, K]\n        a = torch.matmul(\n            permute_final_dims(q, (1, 0, 2)),  # [*, H, Q, C_hidden]\n            permute_final_dims(k, (1, 2, 0)),  # [*, H, C_hidden, K]\n        )\n\n        del q, k\n\n        norm = 1 \/ math.sqrt(self.c_hidden)  # [1]\n        a *= norm\n        if biases is not None:\n            for b in biases:\n                a = a + b\n        a = self.softmax(a)\n\n        # [*, H, Q, C_hidden]\n        o = torch.matmul(\n            a,\n            permute_final_dims(v, (1, 0, 2)),  # [*, H, V, C_hidden]\n        )\n","AFTER":"        q = q.view(q.shape[:-1] + (self.no_heads, -1))\n        k = k.view(k.shape[:-1] + (self.no_heads, -1))\n        v = v.view(v.shape[:-1] + (self.no_heads, -1))\n\n        # [*, H, Q, C_hidden]\n        q = permute_final_dims(q, (1, 0, 2))\n\n        # [*, H, C_hidden, K]\n        k = permute_final_dims(k, (1, 2, 0))\n\n        # [*, H, Q, K]\n        a = torch.matmul(q, k)\n\n        del q, k\n\n        norm = 1 \/ math.sqrt(self.c_hidden)  # [1]\n        a *= norm\n        if biases is not None:\n            for b in biases:\n                a = a + b\n        a = self.softmax(a)\n\n        # [*, H, V, C_hidden]\n        v = permute_final_dims(v, (1, 0, 2))\n\n        # [*, H, Q, C_hidden]\n        o = torch.matmul(a, v)\n"}