{"BEFORE":"    def forward(self, x):\n        outs_b = [block(x_) for x_, block in zip(x, self.blocks)]\n        # only take the cls token out\n        proj_cls_token = [proj(x[:, 0:1]) for x, proj in zip(outs_b, self.projs)]\n        # cross attention\n        outs = []\n        for i in range(self.num_branches):\n            tmp = torch.cat((proj_cls_token[i], outs_b[(i + 1) % self.num_branches][:, 1:, ...]), dim=1)\n            tmp = self.fusion[i](tmp)\n            reverted_proj_cls_token = self.revert_projs[i](tmp[:, 0:1, ...])\n","AFTER":"    def forward(self, x: List[torch.Tensor]) -> List[torch.Tensor]:\n\n        outs_b = []\n        for i, block in enumerate(self.blocks):\n            outs_b.append(block(x[i]))\n\n        # only take the cls token out\n        proj_cls_token = torch.jit.annotate(List[torch.Tensor], [])\n        for i, proj in enumerate(self.projs):\n            proj_cls_token.append(proj(outs_b[i][:, 0:1, ...]))\n\n        # cross attention\n        outs = []\n        for i, (fusion, revert_proj) in enumerate(zip(self.fusion, self.revert_projs)):\n            tmp = torch.cat((proj_cls_token[i], outs_b[(i + 1) % self.num_branches][:, 1:, ...]), dim=1)\n            tmp = fusion(tmp)\n"}