{"BEFORE":"        xi = x[:, :-1]\n        xo = x[:, 1:]\n\n        # help auto-solve a frequent area of confusion around input masks in auto-regressive\n        # if user supplies a mask that is only off by one from the source sequence, resolve it for them\n        mask = kwargs.get('mask', None)\n        if mask is not None and mask.shape[1] == x.shape[1]:\n            mask = mask[:, :-1]\n            kwargs['mask'] = mask\n\n        out = self.net(xi, **kwargs)\n        loss = F.cross_entropy(out.transpose(1, 2), xo, ignore_index = self.ignore_index)\n","AFTER":"        seq, ignore_index = x.shape[1], self.ignore_index\n\n        inp, target = x[:, :-1], x[:, 1:]\n\n        if self.mask_prob > 0.:\n            rand = torch.randn(inp.shape, device = x.device)\n            rand[:, 0] = -torch.finfo(rand.dtype).max # first token should not be masked out\n            num_mask = min(int(seq * self.mask_prob), seq - 1)\n            indices = rand.topk(num_mask, dim = -1).indices\n            mask = ~torch.zeros_like(inp).scatter(1, indices, 1.).bool()\n            kwargs.update(context_mask = mask)\n\n        out = self.net(inp, **kwargs)\n\n        out = out.transpose(1, 2)\n\n        loss = F.cross_entropy(\n            out,\n            target,\n            ignore_index = ignore_index\n        )\n"}