{"BEFORE":"        batched_output_per_clip = []\n        hr_per_clip = []\n\n        # Need to have so as to reflect a batch_size = 1 \/\/ if batched then comment out\n        st_maps = st_maps.unsqueeze(0)\n        for t in range(st_maps.size(1)):\n            # with torch.no_grad():\n            x = self.resnet18(st_maps[:, t, :, :, :])\n            # collapse dimensions to BSx512 (resnet o\/p)\n            x = x.view(x.size(0), -1)\n            # Unsqueeze for sequence length\n            # if t == 0:\n            #     gru_output, h_n = self.rnn(x.unsqueeze(1))\n            # else:\n            #     gru_output, h_n = self.rnn(x.unsqueeze(1), h_n)\n            # output dim: BSx1 and Squeeze sequence length after completing GRU step\n            # x = self.fc_resnet(gru_output.squeeze(1))\n            # normalize by frame-rate: 25.0 for VIPL\n            # x = x*25.0\n            batched_output_per_clip.append(x.squeeze(0))\n            # input should be (seq_len, batch, input_size)\n\n        # the features extracted from the backbone CNN are fed to a one-layer GRU structure.\n        output_seq = torch.stack(batched_output_per_clip, dim=0)\n        gru_output, h_n = self.rnn(output_seq.unsqueeze(1))\n        gru_output = gru_output.squeeze(1)\n        for i in range(gru_output.size(0)):\n            hr = self.fc_resnet(gru_output[i, :])\n            # hr = hr * 25.0\n            hr_per_clip.append(hr)\n\n        output_seq = torch.stack(hr_per_clip, dim=0).permute(1,0)\n","AFTER":"        batched_output_per_clip = []\n        hr_per_clip = []\n\n        # Need to have so as to reflect a batch_size = 1 \/\/ if batched then comment out\n        st_maps = st_maps.unsqueeze(0)\n        for t in range(st_maps.size(1)):\n            # with torch.no_grad():\n            x = self.resnet18(st_maps[:, t, :, :, :])\n            # collapse dimensions to BSx512 (resnet o\/p)\n            x = x.view(x.size(0), -1)\n            # Unsqueeze for sequence length\n            if t == 0:\n                gru_output, h_n = self.rnn(x.unsqueeze(1))\n            else:\n                gru_output, h_n = self.rnn(x.unsqueeze(1), h_n)\n            # output dim: BSx1 and Squeeze sequence length after completing GRU step\n            x = self.fc_resnet(gru_output.squeeze(1))\n            # normalize by frame-rate: 25.0 for VIPL\n            # x = x*25.0\n            batched_output_per_clip.append(x.squeeze(0))\n            # input should be (seq_len, batch, input_size)\n\n        # the features extracted from the backbone CNN are fed to a one-layer GRU structure.\n        output_seq = torch.stack(batched_output_per_clip, dim=0).permute(1,0)\n"}