{"BEFORE":"        new_input_ids, embeddings = self.embed_input_ids(\n            input_ids=input_ids, prefix_ids=prefix_ids\n        )\n\n        # Automatically set attention mask and position-ids\n        attention_mask = ~(new_input_ids == self.tokenizer.pad_token_id)\n\n        assert new_input_ids.shape == embeddings.shape[0:2]\n        return new_input_ids, self.model(\n            inputs_embeds=embeddings,\n            attention_mask=attention_mask,\n        )\n","AFTER":"        raise NotImplementedError()\n"}