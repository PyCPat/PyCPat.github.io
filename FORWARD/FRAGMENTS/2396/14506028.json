{"BEFORE":"        iid_embeddings = self.trial_net(x.view(batch * permutation_dim, -1)).view(\n            batch, permutation_dim, -1\n        )\n\n        if self.combining_operation == \"mean\":\n            e = iid_embeddings.mean(dim=1)\n        elif self.combining_operation == \"sum\":\n            e = iid_embeddings.sum(dim=1)\n        else:\n            raise ValueError(\"combining_operation must be in ['sum', 'mean'].\")\n\n        embedding = self.fc_subnet(e)\n\n        return embedding\n","AFTER":"        if not torch.isnan(x).any():\n            trial_embeddings = self.trial_net(x.view(batch * permutation_dim, -1)).view(\n                batch, permutation_dim, -1\n            )\n            combined_embedding = self.combining_function(trial_embeddings, dim=1)\n            trial_counts = torch.ones(batch, 1, dtype=torch.float32) * permutation_dim\n\n        # otherwise we need to loop over the batch to account for varying trial lengths\n        else:\n            combined_embedding = []\n            trial_counts = torch.zeros(batch, 1)\n            for i in range(batch):\n                # remove NaNs\n                valid_x = x[i, ~torch.isnan(x[i, :, 0]), :]\n                trial_counts[i] = valid_x.shape[0]\n                trial_embeddings = self.trial_net(valid_x)\n                # apply combining operation over permutation dimension\n                combined_embedding.append(\n                    self.combining_function(trial_embeddings, dim=0)\n                )\n\n            combined_embedding = torch.stack(combined_embedding, dim=0)\n\n        assert not torch.isnan(combined_embedding).any(), \"NaNs in embedding.\"\n\n        # add number of trials as additional input\n        # print(torch.cat([combined_embedding, trial_counts], dim=1))\n        return self.fc_subnet(torch.cat([combined_embedding, trial_counts], dim=1))\n"}