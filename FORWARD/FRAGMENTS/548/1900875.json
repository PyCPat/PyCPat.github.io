{"BEFORE":"            return self.W_v(atom_hiddens)[1:]  # num_atoms x vocab\/output size (leave out atom padding)\n\n        # Readout\n        if self.set2set:\n            # Set up sizes\n            batch_size = len(a_scope)\n            lengths = [length for _, length in a_scope]\n            max_num_atoms = max(lengths)\n\n            # Set up memory from atom features\n            memory = torch.zeros(batch_size, max_num_atoms, self.hidden_size)  # (batch_size, max_num_atoms, hidden_size)\n            for i, (start, size) in enumerate(a_scope):\n                memory[i, :size] = atom_hiddens.narrow(0, start, size)\n            memory_transposed = memory.transpose(2, 1)  # (batch_size, hidden_size, max_num_atoms)\n\n            # Create mask (1s for atoms, 0s for not atoms)\n            mask = create_mask(lengths, cuda=next(self.parameters()).is_cuda)  # (max_num_atoms, batch_size)\n            mask = mask.t().unsqueeze(2)  # (batch_size, max_num_atoms, 1)\n\n            # Set up query\n            query = torch.ones(1, batch_size, self.hidden_size)  # (1, batch_size, hidden_size)\n\n            # Move to cuda\n            if next(self.parameters()).is_cuda:\n                memory, memory_transposed, query = memory.cuda(), memory_transposed.cuda(), query.cuda()\n\n            # Run RNN\n            for _ in range(self.set2set_iters):\n                # Compute attention weights over atoms in each molecule\n                query = query.squeeze(0).unsqueeze(2)  # (batch_size,  hidden_size, 1)\n                dot = torch.bmm(memory, query)  # (batch_size, max_num_atoms, 1)\n                dot = dot * mask + (1 - mask) * (-1e+20)  # (batch_size, max_num_atoms, 1)\n                attention = F.softmax(dot, dim=1)  # (batch_size, max_num_atoms, 1)\n\n                # Construct next input as attention over memory\n                attended = torch.bmm(memory_transposed, attention)  # (batch_size, hidden_size, 1)\n                attended = attended.view(1, batch_size, self.hidden_size)  # (1, batch_size, hidden_size)\n\n                # Run RNN for one step\n                query, _ = self.set2set_rnn(attended)  # (1, batch_size, hidden_size)\n\n            # Final RNN output is the molecule encodings\n            mol_vecs = query.squeeze(0)  # (batch_size, hidden_size)\n        else:\n            mol_vecs = []\n            # TODO: Maybe do this in parallel with masking rather than looping\n            for i, (a_start, a_size) in enumerate(a_scope):\n                if a_size == 0:\n                    mol_vecs.append(self.cached_zero_vector)\n                else:\n                    cur_hiddens = atom_hiddens.narrow(0, a_start, a_size)\n\n                    if self.attention:\n                        att_w = torch.matmul(self.W_a(cur_hiddens), cur_hiddens.t())\n                        att_w = F.softmax(att_w, dim=1)\n                        att_hiddens = torch.matmul(att_w, cur_hiddens)\n                        att_hiddens = self.act_func(self.W_b(att_hiddens))\n                        att_hiddens = self.dropout_layer(att_hiddens)\n                        mol_vec = (cur_hiddens + att_hiddens)\n\n                        if viz_dir is not None:\n                            visualize_atom_attention(viz_dir, mol_graph.smiles_batch[i], a_size, att_w)\n                    else:\n                        mol_vec = cur_hiddens  # (num_atoms, hidden_size)\n\n                    mol_vec = mol_vec.sum(dim=0) \/ a_size\n                    mol_vecs.append(mol_vec)\n\n            mol_vecs = torch.stack(mol_vecs, dim=0)  # (num_molecules, hidden_size)\n        \n        if self.args.use_input_features:\n            features_batch = features_batch.to(mol_vecs)\n            if len(features_batch.shape) == 1:\n                features_batch = features_batch.view([1,features_batch.shape[0]])\n            return torch.cat([mol_vecs, features_batch], dim=1)  # (num_molecules, hidden_size)\n","AFTER":"            atom_preds = self.W_v(atom_hiddens)[1:]  # num_atoms x vocab\/output size (leave out atom padding)\n\n        # Readout\n        if self.set2set:\n            # Set up sizes\n            batch_size = len(a_scope)\n            lengths = [length for _, length in a_scope]\n            max_num_atoms = max(lengths)\n\n            # Set up memory from atom features\n            memory = torch.zeros(batch_size, max_num_atoms, self.hidden_size)  # (batch_size, max_num_atoms, hidden_size)\n            for i, (start, size) in enumerate(a_scope):\n                memory[i, :size] = atom_hiddens.narrow(0, start, size)\n            memory_transposed = memory.transpose(2, 1)  # (batch_size, hidden_size, max_num_atoms)\n\n            # Create mask (1s for atoms, 0s for not atoms)\n            mask = create_mask(lengths, cuda=next(self.parameters()).is_cuda)  # (max_num_atoms, batch_size)\n            mask = mask.t().unsqueeze(2)  # (batch_size, max_num_atoms, 1)\n\n            # Set up query\n            query = torch.ones(1, batch_size, self.hidden_size)  # (1, batch_size, hidden_size)\n\n            # Move to cuda\n            if next(self.parameters()).is_cuda:\n                memory, memory_transposed, query = memory.cuda(), memory_transposed.cuda(), query.cuda()\n\n            # Run RNN\n            for _ in range(self.set2set_iters):\n                # Compute attention weights over atoms in each molecule\n                query = query.squeeze(0).unsqueeze(2)  # (batch_size,  hidden_size, 1)\n                dot = torch.bmm(memory, query)  # (batch_size, max_num_atoms, 1)\n                dot = dot * mask + (1 - mask) * (-1e+20)  # (batch_size, max_num_atoms, 1)\n                attention = F.softmax(dot, dim=1)  # (batch_size, max_num_atoms, 1)\n\n                # Construct next input as attention over memory\n                attended = torch.bmm(memory_transposed, attention)  # (batch_size, hidden_size, 1)\n                attended = attended.view(1, batch_size, self.hidden_size)  # (1, batch_size, hidden_size)\n\n                # Run RNN for one step\n                query, _ = self.set2set_rnn(attended)  # (1, batch_size, hidden_size)\n\n            # Final RNN output is the molecule encodings\n            mol_vecs = query.squeeze(0)  # (batch_size, hidden_size)\n        else:\n            mol_vecs = []\n            # TODO: Maybe do this in parallel with masking rather than looping\n            for i, (a_start, a_size) in enumerate(a_scope):\n                if a_size == 0:\n                    mol_vecs.append(self.cached_zero_vector)\n                else:\n                    cur_hiddens = atom_hiddens.narrow(0, a_start, a_size)\n\n                    if self.attention:\n                        att_w = torch.matmul(self.W_a(cur_hiddens), cur_hiddens.t())\n                        att_w = F.softmax(att_w, dim=1)\n                        att_hiddens = torch.matmul(att_w, cur_hiddens)\n                        att_hiddens = self.act_func(self.W_b(att_hiddens))\n                        att_hiddens = self.dropout_layer(att_hiddens)\n                        mol_vec = (cur_hiddens + att_hiddens)\n\n                        if viz_dir is not None:\n                            visualize_atom_attention(viz_dir, mol_graph.smiles_batch[i], a_size, att_w)\n                    else:\n                        mol_vec = cur_hiddens  # (num_atoms, hidden_size)\n\n                    mol_vec = mol_vec.sum(dim=0) \/ a_size\n                    mol_vecs.append(mol_vec)\n\n            mol_vecs = torch.stack(mol_vecs, dim=0)  # (num_molecules, hidden_size)\n        \n        if self.args.use_input_features:\n            features_batch = features_batch.to(mol_vecs)\n            if len(features_batch.shape) == 1:\n                features_batch = features_batch.view([1,features_batch.shape[0]])\n            mol_vecs = torch.cat([mol_vecs, features_batch], dim=1)  # (num_molecules, hidden_size)\n\n        if self.bert_pretraining:\n            features_preds = self.W_f(mol_vecs)\n            return {\n                'features': features_preds,\n                'vocab': atom_preds\n            }\n\n        return mol_vecs  # num_molecules x hidden\n"}