{"BEFORE":"    def forward(self, x_q, x_kv, pad_mask=None, attn_mask=None):\n        \"\"\"\n        :param x_q: Query input of shape (B, N, D) where B is the batch size, N the query sequence length\n            and D the number of query input channels (= `num_q_input_channels`)\n        :param x_kv: Key\/value input of shape (B, L, C) where B is the batch size, L the key\/value sequence\n            length and C are the number of key\/value input channels (= `num_kv_input_channels`)\n        :param pad_mask: Boolean key padding mask. `True` values indicate padding tokens.\n        :param attn_mask: Boolean attention mask. Not needed\/supported yet.\n        :return: attention result of shape (B, N, F) where B is the batch size, N the query sequence length\n            and F the number of output channels (= `num_output_channels`)\n        \"\"\"\n        if attn_mask is not None:\n            raise NotImplementedError(\"attention masks not supported yet\")\n\n        q = self.q_proj(x_q)\n        k = self.k_proj(x_kv)\n        v = self.v_proj(x_kv)\n\n        q, k, v = (rearrange(x, \"b n (h c) -> (b h) n c\", h=self.num_heads) for x in [q, k, v])\n        attn = torch.einsum(\"b i c, b j c -> b i j\", q, k) * self.dp_scale\n\n        if pad_mask is not None:\n            pad_mask = repeat(pad_mask, \"b j -> (b h) () j\", h=self.num_heads)\n            attn_max_neg = -torch.finfo(attn.dtype).max\n            attn.masked_fill_(pad_mask, attn_max_neg)\n\n        attn = attn.softmax(dim=-1)\n        attn = self.dropout(attn)\n\n        o = torch.einsum(\"b i j, b j c -> b i c\", attn, v)\n","AFTER":"        rot_pos_emb_q: Optional[RotaryPositionEmbedding] = None,\n        rot_pos_emb_k: Optional[RotaryPositionEmbedding] = None,\n    ):\n        \"\"\"\n        :param x_q: Query input of shape (B, N, D) where B is the batch size, N the query sequence length\n            and D the number of query input channels (= `num_q_input_channels`)\n        :param x_kv: Key\/value input of shape (B, L, C) where B is the batch size, L the key\/value sequence\n            length and C are the number of key\/value input channels (= `num_kv_input_channels`)\n        :param pad_mask: Boolean key padding mask. `True` values indicate padding tokens.\n        :param rot_pos_emb_q: Applies a rotary position embedding to query i.e. if defined, rotates the query.\n        :param rot_pos_emb_k: Applies a rotary position embedding to key i.e. if defined, rotates the key.\n        :return: attention result of shape (B, N, F) where B is the batch size, N the query sequence length\n            and F the number of output channels (= `num_output_channels`)\n        \"\"\"\n\n        q = self.q_proj(x_q)\n        k = self.k_proj(x_kv)\n        v = self.v_proj(x_kv)\n\n        q, k, v = (rearrange(x, \"b n (h c) -> b h n c\", h=self.num_heads) for x in [q, k, v])\n        q = q * self.dp_scale\n\n        if rot_pos_emb_q is not None:\n            q = rot_pos_emb_q.rotate(q)\n\n        if rot_pos_emb_k is not None:\n            k = rot_pos_emb_k.rotate(k)\n\n        attn = torch.einsum(\"b h i c, b h j c -> b h i j\", q, k)\n        attn_max_neg = -torch.finfo(attn.dtype).max\n\n        if pad_mask is not None:\n            pad_mask = rearrange(pad_mask, \"b j -> b 1 1 j\")\n            attn.masked_fill_(pad_mask, attn_max_neg)\n\n        if self.causal_attention:\n            i = q.shape[2]\n            j = k.shape[2]\n\n            causal_mask = torch.ones((i, j), device=x_q.device, dtype=torch.bool).triu(j - i + 1)\n            attn.masked_fill_(causal_mask, attn_max_neg)\n\n        attn = attn.softmax(dim=-1)\n        attn = self.dropout(attn)\n\n        o = torch.einsum(\"b h i j, b h j c -> b h i c\", attn, v)\n"}