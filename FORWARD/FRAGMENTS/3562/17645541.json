{"BEFORE":"        flatten = rearrange(x, '... d -> (...) d')\n\n        self.init_embed_(flatten)\n\n        embed = self.embed if not self.learnable_codebook else self.embed.detach()\n        embed = self.embed.t()\n\n        dist = -(\n            flatten.pow(2).sum(1, keepdim=True)\n            - 2 * flatten @ embed\n            + embed.pow(2).sum(0, keepdim=True)\n        )\n\n        embed_ind = gumbel_sample(dist, dim = -1, temperature = self.sample_codebook_temp)\n        embed_onehot = F.one_hot(embed_ind, self.codebook_size).type(dtype)\n        embed_ind = embed_ind.view(*shape[:-1])\n        quantize = F.embedding(embed_ind, self.embed)\n\n        if self.training:\n            cluster_size = embed_onehot.sum(0)\n            self.all_reduce_fn(cluster_size)\n\n            ema_inplace(self.cluster_size, cluster_size, self.decay)\n\n            embed_sum = flatten.t() @ embed_onehot\n            self.all_reduce_fn(embed_sum)\n\n            ema_inplace(self.embed_avg, embed_sum.t(), self.decay)\n            cluster_size = laplace_smoothing(self.cluster_size, self.codebook_size, self.eps) * self.cluster_size.sum()\n            embed_normalized = self.embed_avg \/ cluster_size.unsqueeze(1)\n","AFTER":"        needs_codebook_dim = x.ndim < 4\n\n        x = x.float()\n\n        if needs_codebook_dim:\n            x = rearrange(x, '... -> 1 ...')\n\n        shape, dtype = x.shape, x.dtype\n        flatten = rearrange(x, 'h ... d -> h (...) d')\n\n        self.init_embed_(flatten)\n\n        embed = self.embed if not self.learnable_codebook else self.embed.detach()\n\n        embed = rearrange(embed, '... n d -> ... d n')\n\n        dist = -(\n            (flatten ** 2).sum(dim = -1, keepdim=True)\n            - 2 * flatten @ embed\n            + (embed ** 2).sum(dim = -2, keepdim=True)\n        )\n\n        embed_ind = gumbel_sample(dist, dim = -1, temperature = self.sample_codebook_temp)\n        embed_onehot = F.one_hot(embed_ind, self.codebook_size).type(dtype)\n        embed_ind = embed_ind.view(*shape[:-1])\n\n        quantize = batched_embedding(embed_ind, self.embed)\n\n        if self.training:\n            cluster_size = embed_onehot.sum(dim = 1)\n\n            self.all_reduce_fn(cluster_size)\n            ema_inplace(self.cluster_size, cluster_size, self.decay)\n\n            embed_sum = einsum('h n d, h n c -> h c d', flatten, embed_onehot)\n            self.all_reduce_fn(embed_sum)\n\n            cluster_size = laplace_smoothing(self.cluster_size, self.codebook_size, self.eps) * self.cluster_size.sum()\n\n            embed_normalized = self.embed_avg \/ rearrange(cluster_size, '... -> ... 1')\n            self.embed.data.copy_(embed_normalized)\n            self.expire_codes_(x)\n\n        if needs_codebook_dim:\n            quantize, embed_ind = map(lambda t: rearrange(t, '1 ... -> ...'), (quantize, embed_ind))\n\n        return quantize, embed_ind\n"}