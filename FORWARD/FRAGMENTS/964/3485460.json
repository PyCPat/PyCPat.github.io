{"BEFORE":"        device = f.device\n        # get shapes\n        raw_int_fs = list(f.size())   # b*c*h*w\n        raw_int_bs = list(b.size())   # b*c*h*w\n\n        # extract patches from background with stride and rate\n        kernel = 2 * self.rate\n        # raw_w is extracted for reconstruction\n        raw_w = extract_image_patches(b,\n                                      ksize=kernel,\n                                      stride=self.rate*self.stride,\n                                      rate=1, padding='auto')  # [N, C*k*k, L]\n        # raw_shape: [N, C, k, k, L]\n        raw_w = raw_w.view(raw_int_bs[0], raw_int_bs[1], kernel, kernel, -1)\n        raw_w = raw_w.permute(0, 4, 1, 2, 3)    # raw_shape: [N, L, C, k, k]\n        raw_w_groups = torch.split(raw_w, 1, dim=0)\n\n        # downscaling foreground option: downscaling both foreground and\n        # background for matching and use original background for reconstruction.\n        f = F.interpolate(f, scale_factor=1.\/self.rate,\n                          mode='nearest', recompute_scale_factor=False)\n        b = F.interpolate(b, scale_factor=1.\/self.rate,\n                          mode='nearest', recompute_scale_factor=False)\n        int_fs = list(f.size())     # b*c*h*w\n        int_bs = list(b.size())\n        # split tensors along the batch dimension\n        f_groups = torch.split(f, 1, dim=0)\n        # w shape: [N, C*k*k, L]\n        w = extract_image_patches(b, ksize=self.ksize,\n                                  stride=self.stride,\n                                  rate=1, padding='auto')\n        # w shape: [N, C, k, k, L]\n        w = w.view(int_bs[0], int_bs[1], self.ksize, self.ksize, -1)\n        w = w.permute(0, 4, 1, 2, 3)    # w shape: [N, L, C, k, k]\n        w_groups = torch.split(w, 1, dim=0)\n\n        # process mask\n        if mask is None:\n            mask = torch.zeros([int_bs[0], 1, int_bs[2], int_bs[3]])\n            mask = mask.to(device)\n        else:\n            mask = F.interpolate(\n                mask, scale_factor=1.\/((2**self.n_down)*self.rate), mode='nearest', recompute_scale_factor=False)\n        int_ms = list(mask.size())\n        # m shape: [N, C*k*k, L]\n        m = extract_image_patches(mask, ksize=self.ksize,\n                                  stride=self.stride,\n                                  rate=1, padding='auto')\n        # m shape: [N, C, k, k, L]\n        m = m.view(int_ms[0], int_ms[1], self.ksize, self.ksize, -1)\n        m = m.permute(0, 4, 1, 2, 3)    # m shape: [N, L, C, k, k]\n        m = m[0]    # m shape: [L, C, k, k]\n        # mm shape: [L, 1, 1, 1]\n\n        mm = (torch.mean(m, dim=[1, 2, 3], keepdim=True) == 0.).to(\n            torch.float32)\n\n        #mm = (torch.mean(m, dim=[1, 2, 3], keepdim=True) == 0.).to(torch.float32)\n        mm = mm.permute(1, 0, 2, 3)  # mm shape: [1, L, 1, 1]\n\n        y = []\n        offsets = []\n        k = self.fuse_k\n        scale = self.softmax_scale    # to fit the PyTorch tensor image value range\n        fuse_weight = torch.eye(k).view(1, 1, k, k)  # 1*1*k*k\n        fuse_weight = fuse_weight.to(device)\n\n        for xi, wi, raw_wi in zip(f_groups, w_groups, raw_w_groups):\n            '''\n            O => output channel as a conv filter\n            I => input channel as a conv filter\n            xi : separated tensor along batch dimension of front; (B=1, C=128, H=32, W=32)\n            wi : separated patch tensor along batch dimension of back; (B=1, O=32*32, I=128, KH=3, KW=3)\n            raw_wi : separated tensor along batch dimension of back; (B=1, I=32*32, O=128, KH=4, KW=4)\n            '''\n            # conv for compare\n            escape_NaN = torch.Tensor([1e-4])\n            escape_NaN = escape_NaN.to(device)\n            wi = wi[0]  # [L, C, k, k]\n          \n            #max_wi = torch.max(torch.sqrt(torch.sum(wi**2, dim=[1, 2, 3], keepdim=True)), escape_NaN)\n            max_wi = torch.sqrt(torch.sum(torch.pow(wi, 2) + escape_NaN, dim=[1, 2, 3], keepdim=True))\n\n            wi_normed = wi \/ max_wi\n            # xi shape: [1, C, H, W], yi shape: [1, L, H, W]\n            yi = F.conv2d(xi, wi_normed, stride=1, padding=(self.ksize-1)\/\/2)   # [1, L, H, W]\n            # conv implementation for fuse scores to encourage large patches\n            if self.fuse:\n                # make all of depth to spatial resolution\n                # (B=1, I=1, H=32*32, W=32*32)\n                yi = yi.view(1, 1, int_bs[2]*int_bs[3], int_fs[2]*int_fs[3])\n                # (B=1, C=1, H=32*32, W=32*32)\n                yi = F.conv2d(yi, fuse_weight, stride=1, padding=(k-1)\/\/2)\n                # (B=1, 32, 32, 32, 32)\n                yi = yi.contiguous().view(1, int_bs[2], int_bs[3], int_fs[2], int_fs[3])\n                yi = yi.permute(0, 2, 1, 4, 3)\n                \n                yi = yi.contiguous().view(1, 1, int_bs[2]*int_bs[3], int_fs[2]*int_fs[3])\n                yi = F.conv2d(yi, fuse_weight, stride=1, padding=(k-1)\/\/2)\n                yi = yi.contiguous().view(1, int_bs[3], int_bs[2], int_fs[3], int_fs[2])\n                yi = yi.permute(0, 2, 1, 4, 3).contiguous()\n\n            # (B=1, C=32*32, H=32, W=32)\n            yi = yi.view(1, int_bs[2] * int_bs[3], int_fs[2], int_fs[3])\n            # softmax to match\n            yi = yi * mm\n            yi = F.softmax(yi*scale, dim=1)\n            yi = yi * mm  # [1, L, H, W]\n\n            offset = torch.argmax(yi, dim=1, keepdim=True)  # 1*1*H*W\n\n            if int_bs != int_fs:\n                # Normalize the offset value to match foreground dimension\n                times = float(int_fs[2] * int_fs[3]) \/ \\\n                    float(int_bs[2] * int_bs[3])\n                offset = ((offset + 1).float() * times - 1).to(torch.int64)\n            offset = torch.cat([torch.div(offset, int_fs[3], rounding_mode='trunc'),\n                                offset % int_fs[3]], dim=1)  # 1*2*H*W\n\n            # deconv for patch pasting\n            wi_center = raw_wi[0]\n            yi = F.conv_transpose2d(\n                yi, wi_center, stride=self.rate, padding=1) \/ 4.  # (B=1, C=128, H=64, W=64)\n            y.append(yi)\n            offsets.append(offset)\n\n        y = torch.cat(y, dim=0)  # back to the mini-batch\n        y = y.contiguous().view(raw_int_fs)\n\n        if not self.return_flow:\n            return y, None\n\n        offsets = torch.cat(offsets, dim=0)\n        offsets = offsets.view(int_fs[0], 2, *int_fs[2:])\n\n        # case1: visualize optical flow: minus current position\n        h_add = torch.arange(int_fs[2]).view(\n            [1, 1, int_fs[2], 1]).expand(int_fs[0], -1, -1, int_fs[3])\n        w_add = torch.arange(int_fs[3]).view(\n            [1, 1, 1, int_fs[3]]).expand(int_fs[0], -1, int_fs[2], -1)\n        ref_coordinate = torch.cat([h_add, w_add], dim=1)\n        ref_coordinate = ref_coordinate.to(device)\n\n        offsets = offsets - ref_coordinate\n        # flow = pt_flow_to_image(offsets)\n\n        flow = torch.from_numpy(flow_to_image(\n            offsets.permute(0, 2, 3, 1).cpu().data.numpy())) \/ 255.\n        flow = flow.permute(0, 3, 1, 2)\n        flow = flow.to(device)\n","AFTER":"        raw_int_fs, raw_int_bs = list(f.size()), list(b.size())   # b*c*h*w\n\n        # extract patches from background with stride and rate\n        kernel = 2 * self.rate\n        # raw_w is extracted for reconstruction\n        raw_w = extract_image_patches(b, ksize=kernel,\n                                         stride=self.rate*self.stride,\n                                         rate=1, padding='auto')  # [N, C*k*k, L]\n        # raw_shape: [N, C, k, k, L]\n        raw_w = raw_w.view(raw_int_bs[0], raw_int_bs[1], kernel, kernel, -1)\n        raw_w = raw_w.permute(0, 4, 1, 2, 3)    # raw_shape: [N, L, C, k, k]\n        raw_w_groups = torch.split(raw_w, 1, dim=0)\n\n        # downscaling foreground option: downscaling both foreground and\n        # background for matching and use original background for reconstruction.\n        f = F.interpolate(f, scale_factor=1.\/self.rate,\n                          mode='nearest', recompute_scale_factor=False)\n        b = F.interpolate(b, scale_factor=1.\/self.rate,\n                          mode='nearest', recompute_scale_factor=False)\n        int_fs, int_bs = list(f.size()), list(b.size())   # b*c*h*w\n        # split tensors along the batch dimension\n        f_groups = torch.split(f, 1, dim=0)\n        # w shape: [N, C*k*k, L]\n        w = extract_image_patches(b, ksize=self.ksize,\n                                     stride=self.stride,\n                                     rate=1, padding='auto')\n        # w shape: [N, C, k, k, L]\n        w = w.view(int_bs[0], int_bs[1], self.ksize, self.ksize, -1)\n        w = w.permute(0, 4, 1, 2, 3)    # w shape: [N, L, C, k, k]\n        w_groups = torch.split(w, 1, dim=0)\n\n        # process mask\n        if mask is None:\n            mask = torch.zeros([int_bs[0], 1, int_bs[2], int_bs[3]], device=device)\n        else:\n            mask = F.interpolate(\n                mask, scale_factor=1.\/((2**self.n_down)*self.rate), mode='nearest', recompute_scale_factor=False)\n        int_ms = list(mask.size())\n        # m shape: [N, C*k*k, L]\n        m = extract_image_patches(mask, ksize=self.ksize,\n                                        stride=self.stride,\n                                        rate=1, padding='auto')\n        # m shape: [N, C, k, k, L]\n        m = m.view(int_ms[0], int_ms[1], self.ksize, self.ksize, -1)\n        m = m.permute(0, 4, 1, 2, 3)    # m shape: [N, L, C, k, k]\n        m = m[0]    # m shape: [L, C, k, k]\n        # mm shape: [L, 1, 1, 1]\n\n        mm = (torch.mean(m, dim=[1, 2, 3], keepdim=True) == 0.).to(torch.float32)\n        mm = mm.permute(1, 0, 2, 3)  # mm shape: [1, L, 1, 1]\n\n        y = []\n        offsets = []\n        k = self.fuse_k\n        scale = self.softmax_scale    # to fit the PyTorch tensor image value range\n        fuse_weight = torch.eye(k, device=device).view(1, 1, k, k)  # 1*1*k*k\n\n        for xi, wi, raw_wi in zip(f_groups, w_groups, raw_w_groups):\n            '''\n            O => output channel as a conv filter\n            I => input channel as a conv filter\n            xi : separated tensor along batch dimension of front; (B=1, C=128, H=32, W=32)\n            wi : separated patch tensor along batch dimension of back; (B=1, O=32*32, I=128, KH=3, KW=3)\n            raw_wi : separated tensor along batch dimension of back; (B=1, I=32*32, O=128, KH=4, KW=4)\n            '''\n            # conv for compare\n            wi = wi[0]  # [L, C, k, k]   \n            max_wi = torch.sqrt(torch.sum(torch.pow(wi, 2), dim=[1, 2, 3], keepdim=True)).clamp_min(1e-4)\n            wi_normed = wi \/ max_wi\n            # xi shape: [1, C, H, W], yi shape: [1, L, H, W]\n            yi = F.conv2d(xi, wi_normed, stride=1, padding=(self.ksize-1)\/\/2)   # [1, L, H, W]\n            # conv implementation for fuse scores to encourage large patches\n            if self.fuse:\n                # make all of depth to spatial resolution\n                # (B=1, I=1, H=32*32, W=32*32)\n                yi = yi.view(1, 1, int_bs[2]*int_bs[3], int_fs[2]*int_fs[3])\n                # (B=1, C=1, H=32*32, W=32*32)\n                yi = F.conv2d(yi, fuse_weight, stride=1, padding=(k-1)\/\/2)\n                # (B=1, 32, 32, 32, 32)\n                yi = yi.contiguous().view(1, int_bs[2], int_bs[3], int_fs[2], int_fs[3])\n                yi = yi.permute(0, 2, 1, 4, 3)\n                \n                yi = yi.contiguous().view(1, 1, int_bs[2]*int_bs[3], int_fs[2]*int_fs[3])\n                yi = F.conv2d(yi, fuse_weight, stride=1, padding=(k-1)\/\/2)\n                yi = yi.contiguous().view(1, int_bs[3], int_bs[2], int_fs[3], int_fs[2])\n                yi = yi.permute(0, 2, 1, 4, 3).contiguous()\n\n            # (B=1, C=32*32, H=32, W=32)\n            yi = yi.view(1, int_bs[2] * int_bs[3], int_fs[2], int_fs[3])\n            # softmax to match\n            yi = yi * mm\n            yi = F.softmax(yi*scale, dim=1)\n            yi = yi * mm  # [1, L, H, W]\n\n            if self.return_flow:\n                offset = torch.argmax(yi, dim=1, keepdim=True)  # 1*1*H*W\n\n                if int_bs != int_fs:\n                    # Normalize the offset value to match foreground dimension\n                    times = (int_fs[2]*int_fs[3])\/(int_bs[2]*int_bs[3])\n                    offset = ((offset + 1).float() * times - 1).to(torch.int64)\n                offset = torch.cat([torch.div(offset, int_fs[3], rounding_mode='trunc'),\n                                    offset % int_fs[3]], dim=1)  # 1*2*H*W\n                offsets.append(offset)\n\n            # deconv for patch pasting\n            wi_center = raw_wi[0]\n            yi = F.conv_transpose2d(yi, wi_center, stride=self.rate, padding=1) \/ 4.  # (B=1, C=128, H=64, W=64)\n            y.append(yi)\n\n        y = torch.cat(y, dim=0)  # back to the mini-batch\n        y = y.contiguous().view(raw_int_fs)\n\n        if not self.return_flow:\n            return y, None\n\n        offsets = torch.cat(offsets, dim=0)\n        offsets = offsets.view(int_fs[0], 2, *int_fs[2:])\n\n        # case1: visualize optical flow: minus current position\n        h_add = torch.arange(int_fs[2], device=device).view([1, 1, int_fs[2], 1]).expand(int_fs[0], -1, -1, int_fs[3])\n        w_add = torch.arange(int_fs[3], device=device).view([1, 1, 1, int_fs[3]]).expand(int_fs[0], -1, int_fs[2], -1)\n        offsets = offsets - torch.cat([h_add, w_add], dim=1)\n"}