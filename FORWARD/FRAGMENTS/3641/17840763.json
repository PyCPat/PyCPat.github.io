{"BEFORE":"        x = self.emb(x) * math.sqrt(self.hidden_channels) # [b, t, h]\n        #x.size() : torch.Size([batch_size, phoneme_length, self.hidden_channels])\n        x = torch.transpose(x, 1, -1) # [b, h, t]\n        #x.size() : torch.Size([batch_size, self.hidden_channels, phoneme_length])\n        x_mask = torch.unsqueeze(sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n        #x_mask.size() : torch.Size([batch_size, 1, phoneme_length])\n\n        x = self.encoder(x * x_mask, x_mask)\n        #x.size() : torch.Size([batch_size, self.hidden_channels, phoneme_length])\n        stats = self.proj(x) * x_mask\n\n        m, logs = torch.split(stats, self.out_channels, dim=1)\n        return x, m, logs, x_mask\n","AFTER":"        text_padded_embedded = self.emb(text_padded) * math.sqrt(self.hidden_channels)\n        #text_padded_embedded.size() : torch.Size([batch_size, text_length, self.hidden_channels])\n        text_padded_embedded = torch.transpose(text_padded_embedded, 1, -1)\n        #text_padded_embedded.size() : torch.Size([batch_size, self.hidden_channels, text_length])\n        #マスクの作成\n        max_text_length = text_padded_embedded.size(2)\n        progression = torch.arange(max_text_length, dtype=text_lengths.dtype, device=text_lengths.device)\n        text_mask = (progression.unsqueeze(0) < text_lengths.unsqueeze(1))\n        text_mask = torch.unsqueeze(text_mask, 1).to(text_padded_embedded.dtype)\n        #text_mask.size() : torch.Size([batch_size, 1, text_length])\n\n        text_encoded = self.encoder(text_padded_embedded * text_mask, text_mask)\n        #text_encoded.size() : torch.Size([batch_size, self.hidden_channels, text_length])\n        stats = self.proj(text_encoded) * text_mask\n\n        m, logs = torch.split(stats, self.out_channels, dim=1)\n        return text_encoded, m, logs, text_mask\n"}