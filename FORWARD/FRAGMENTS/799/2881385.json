{"BEFORE":"        oC, iC, kH, kW = self.weight.size()\n\n        affined_style = self.style_fc(style)\n        weight = self.elr_scale * self.weight.view(1, oC, iC, kH, kW) * affined_style.view(B, 1, iC, 1, 1)\n    \n        if self.demod:\n            norm = 1 \/ ((weight**2).sum([2, 3, 4]) + 1.e-8)**0.5\n            weight = weight * norm.view(B, oC, 1, 1, 1)\n\n        out = F.conv2d(\n            x.contiguous().view(1, B*iC, H, W), weight.view(B*oC, iC, kH, kW),\n            stride=self.stride, padding=self.padding, groups=B\n        )\n\n        _, _, H, W = out.size()\n        out = out.view(B, -1, H, W)\n        return out\n","AFTER":"        y = self.affine(y) + 1 # init bias with 1\n                               # a little bit forcible method but still works\n                               # for init weights with .apply()\n\n        # modulate\n        weight = self.weight[None, :, :, :, :] * y[:, None, :, None, None]\n\n        # demodulate\n        if self.demod:\n            d = torch.rsqrt(weight.pow(2).sum([2, 3, 4], keepdim=True) + 1e-4)\n            weight = weight * d\n\n        # reshaping for conv input\n        x = x.reshape(1, -1, H, W)\n        _, _, *ws = weight.size()\n        weight = weight.reshape(B*self.out_channels, *ws)\n        pad = self._get_same_padding(H)\n\n        # conv\n        x = F.conv2d(x, weight, padding=pad, groups=B)\n\n        # return with bias\n        return x.reshape(B, self.out_channels, H, W) + self.bias\n"}