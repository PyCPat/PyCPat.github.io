{"BEFORE":"        offsets_h, offsets_w = offsets.shape[-2:]\n\n        grid = torch.stack(torch.meshgrid(\n            torch.arange(offsets_h, device = device),\n            torch.arange(offsets_w, device = device),\n        indexing = 'ij'))\n\n        grid.requires_grad = False\n        grid = grid.type_as(x)\n\n        vgrid = grid + offsets\n\n        vgrid_h, vgrid_w = vgrid.unbind(dim = 1)\n\n        vgrid_h = 2.0 * vgrid_h \/ max(offsets_h - 1, 1) - 1.0\n        vgrid_w = 2.0 * vgrid_w \/ max(offsets_w - 1, 1) - 1.0\n\n        vgrid_scaled = torch.stack((vgrid_h, vgrid_w), dim = -1)\n\n        kv_feats = F.grid_sample(\n            offsets_input,\n            vgrid_scaled,\n        mode = 'bilinear', padding_mode = 'zeros', align_corners = False)\n\n        kv_feats = rearrange(kv_feats, '(b g) d ... -> b (g d) ...', b = b)\n\n        # derive key \/ values\n\n        k, v = self.to_kv(kv_feats).chunk(2, dim = 1)\n\n        # scale queries\n\n        q = q * self.scale\n\n        # split out heads\n\n        q, k, v = map(lambda t: rearrange(t, 'b (h d) ... -> b h (...) d', h = heads), (q, k, v))\n\n        # query \/ key similarity\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n        sim = sim - sim.amax(dim = -1, keepdim = True).detach()\n\n        # attention\n\n        attn = sim.softmax(dim = -1)\n        attn = self.dropout(attn)\n\n        # aggregate and combine heads\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h (x y) d -> b (h d) x y', x = h, y = w)\n        return self.to_out(out)\n","AFTER":"    def forward(self, x, return_vgrid = False):\n        \"\"\"\n        b - batch\n        h - heads\n        x - height\n        y - width\n        d - dimension\n        g - offset groups\n        \"\"\"\n\n        heads, b, h, w, downsample_factor, device = self.heads, x.shape[0], *x.shape[-2:], self.downsample_factor, x.device\n\n        # queries\n\n        q = self.to_q(x)\n\n        # calculate offsets - offset MLP shared across all groups\n\n        grouped_feats = rearrange(q, 'b (g d) ... -> (b g) d ...', g = self.offset_groups)\n        offsets = self.to_offsets(grouped_feats)\n\n        # calculate grid + offsets\n\n        grid =create_grid_like(offsets)\n        vgrid = grid + offsets\n\n        vgrid_h, vgrid_w = vgrid.unbind(dim = 1)\n\n        vgrid_h = 2.0 * vgrid_h \/ max(offsets.shape[-2] - 1, 1) - 1.0\n        vgrid_w = 2.0 * vgrid_w \/ max(offsets.shape[-1] - 1, 1) - 1.0\n\n        vgrid_scaled = torch.stack((vgrid_h, vgrid_w), dim = -1)\n\n        kv_feats = F.grid_sample(\n            grouped_feats,\n            vgrid_scaled,\n        mode = 'bilinear', padding_mode = 'zeros', align_corners = False)\n\n        kv_feats = rearrange(kv_feats, '(b g) d ... -> b (g d) ...', b = b)\n\n        # derive key \/ values\n\n        k, v = self.to_kv(kv_feats).chunk(2, dim = 1)\n\n        # scale queries\n\n        q = q * self.scale\n\n        # split out heads\n\n        q, k, v = map(lambda t: rearrange(t, 'b (h d) ... -> b h (...) d', h = heads), (q, k, v))\n\n        # query \/ key similarity\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n\n        # relative positional bias\n\n        grid = create_grid_like(x)\n        rel_pos_bias = self.rel_pos_bias(grid, vgrid)\n        sim = sim + rel_pos_bias\n\n        # numerical stability\n\n        sim = sim - sim.amax(dim = -1, keepdim = True).detach()\n\n        # attention\n\n        attn = sim.softmax(dim = -1)\n        attn = self.dropout(attn)\n\n        # aggregate and combine heads\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h (x y) d -> b (h d) x y', x = h, y = w)\n        out = self.to_out(out)\n\n        if return_vgrid:\n            return out, vgrid\n\n        return out\n"}