{"BEFORE":"        pos_mask = targets.eq(1).float()\n        neg_mask = targets.lt(1).float()\n\n        probs = torch.sigmoid(inputs)   # convert logits to probabilities\n\n        # use logsigmoid for numerical stability\n        pos_loss = -F.logsigmoid(inputs) * (1-probs)**self.alpha * pos_mask                         # loss at Gaussian peak\n        neg_loss = -F.logsigmoid(-inputs) * probs**self.alpha * (1-targets)**self.beta * neg_mask   # loss at everywhere else\n\n        pos_loss = pos_loss.sum()\n        neg_loss = neg_loss.sum()\n\n        N = pos_mask.sum()  # number of peaks = number of ground-truth detections\n        # use N + eps instead of 2 cases?\n        if N == 0:\n            loss = neg_loss\n        else:\n            loss = (pos_loss + neg_loss) \/ N\n\n        return loss\n","AFTER":"        pos_weight = targets.eq(1).float()              # gaussian peaks are positive samples\n        neg_weight = torch.pow(1-targets, self.beta)    # when target = 1, this will become 0\n\n        probs = torch.sigmoid(inputs)   # convert logits to probabilities\n\n        # use logsigmoid for numerical stability\n        # NOTE: log(1 - sigmoid(x)) = log(sigmoid(-x))\n        pos_loss = -(1-probs)**self.alpha * F.logsigmoid(inputs) * pos_weight\n        neg_loss = -probs**self.alpha * F.logsigmoid(-inputs) * neg_weight\n\n        loss = pos_loss + neg_loss\n\n        if self.reduction == \"sum\":\n            return torch.sum(loss)\n        \n        if self.reduction == \"mean\":\n            return torch.sum(loss) \/ torch.sum(pos_weight)\n\n        return loss\n"}