{"BEFORE":"    def forward(self, fea_t, fea_s, conf_t, conf_s, priors, targets, var=None):\n\n        if self.mode == 'mse':\n            return ((fea_s-fea_t)**2).mean() * self.factor\n\n        if self.mode == 'pdf':\n            with torch.no_grad():\n                x1 = conf_t.sigmoid()\n                x2 = conf_s.sigmoid()\n                disagree = (x1 - x2) ** 2\n                disagree = disagree.sum(-1).unsqueeze(1).sqrt()\n                if self.multi_anchor:\n                    disagree = F.avg_pool1d(disagree, kernel_size=6, stride=6, padding=0)\n                disagree = disagree.permute(0,2,1).expand_as(fea_t)\n                weight = disagree \/ disagree.sum()\n            return (weight*((fea_s-fea_t)**2)).sum() * self.factor\n\n        raise NotImplementedError\n","AFTER":"        loc_t, conf_t, fea_t = pred_t['loc'], pred_t['conf'], pred_t['feature']\n        loc_t, conf_t, fea_t = loc_t.detach(), conf_t.detach(), fea_t.detach()\n        loc_s, conf_s, fea_s = pred_s['loc'], pred_s['conf'], pred_s['feature']\n\n        if self.mode == 'mse':\n            return ((fea_s-fea_t)**2).mean() * self.factor\n\n        if self.mode == 'pdf':\n            with torch.no_grad():\n                x1 = conf_t.sigmoid()\n                x2 = conf_s.sigmoid()\n                disagree = (x1 - x2) ** 2\n                weight = disagree.sum(-1).unsqueeze(1).sqrt()\n                if self.multi_anchor:\n                    weight = F.avg_pool1d(weight, kernel_size=6, stride=6, padding=0)\n                weight = weight.permute(0,2,1).expand_as(fea_t)\n                weight = weight \/ weight.sum()\n            loss_pdf = (weight*((fea_s-fea_t)**2)).sum() * self.factor\n                    \n            loss_cls = F.binary_cross_entropy_with_logits(conf_s, x1, reduction='none') * disagree\n            loss_cls = loss_cls.sum() \/ (x1>0.5).float().sum()\n\n            loss_reg = F.mse_loss(loc_s, loc_t)\n\n            return loss_pdf + loss_cls + loss_reg\n            \n        raise NotImplementedError\n"}