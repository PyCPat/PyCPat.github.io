{"BEFORE":"        embed_x = self.embedding(x)\n        # print('### embed x',embed_x.size()) # [16384, 128]\n        # print('### linear',self.linear(x).squeeze(1).size())\n        # print('### fm',self.fm(embed_x).size())\n        # print('### mlp',torch.sum(self.mlp(embed_x).squeeze(-1),dim=1).size())\n        if self.enable_qr:\n            x = self.linear(x) + self.fm(embed_x) + self.mlp(embed_x).squeeze(1)\n        else:\n            x = self.linear(x).squeeze(1) + torch.sum(self.fm(embed_x),dim=1) + torch.sum(self.mlp(embed_x).squeeze(-1),dim=1)\n        return torch.sigmoid(x)\n","AFTER":"        embed_x = self.embedding(x)\n        # print('[DEBUG] embed x',embed_x.size()) # [16384, 128]\n        # print('[DEBUG] linear',self.linear(x).squeeze(1).size())\n        # print('[DEBUG] fm',self.fm(embed_x).size())\n        # print('[DEBUG] mlp',self.mlp(embed_x).squeeze(-1).size())\n        x = self.linear(x).squeeze(1) + self.fm(embed_x) + self.mlp(embed_x).squeeze(-1)\n"}