{"BEFORE":"            b, n, d, h = *x[0].shape, self.heads\n            q, k, v = map(lambda proj_token_pair: proj_token_pair[0](proj_token_pair[1]), zip((self.Q, self.K, self.V), x))\n        else:\n            b, n, d, h = *x.shape, self.heads\n","AFTER":"        h = self.heads\n        \n        if isinstance(x, tuple):\n            q, k, v = map(lambda proj_token_pair: proj_token_pair[0](proj_token_pair[1]), zip((self.Q, self.K, self.V), x))\n        else:\n            q, k, v = map(lambda proj: proj(x), (self.Q, self.K, self.V))\n\n        q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=h), (q, k, v))\n        k = k*self.scale\n\n        attention = einsum(\"b h n d, b h m d -> b h n m\", q, k)\n        print(attention.shape)\n        if attention_mask is not None:\n            attention_mask = repeat(attention_mask, \"b 1 n m -> b h n m\", h=h)\n            attention.masked_fill_(attention_mask, self.mask_value)\n\n        attention = attention.softmax(dim=-1)\n        attention = self.attention_dropout(attention)\n        \n        out = einsum(\"b h n m, b h m d -> b h n d\", attention, v)\n        out = rearrange(out, \"b h n d -> b n (h d)\")\n        out = self.out_linear(out)\n        print(\"out:\", out.shape)\n"}