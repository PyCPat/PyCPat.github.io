{"BEFORE":"        h = self.GroupNorm_0(x)\n        q = self.NIN_0(h)\n        k = self.NIN_1(h)\n        v = self.NIN_2(h)\n\n        w = torch.einsum('bchw,bcij->bhwij', q, k) * (int(C) ** (-0.5))\n        w = torch.reshape(w, (B, H, W, H * W))\n        w = F.softmax(w, dim=-1)\n        w = torch.reshape(w, (B, H, W, H, W))\n        h = torch.einsum('bhwij,bcij->bchw', w, v)\n        h = self.NIN_3(h)\n        return x + h\n","AFTER":"        n_head = self.n_head\n        head_dim = channel \/\/ n_head\n\n        norm = self.norm(input)\n        qkv = self.qkv(norm).view(batch, n_head, head_dim * 3, height, width)\n        query, key, value = qkv.chunk(3, dim=2)  # bhdyx\n\n        attn = torch.einsum(\n            \"bnchw, bncyx -> bnhwyx\", query, key\n        ).contiguous() \/ math.sqrt(channel)\n        attn = attn.view(batch, n_head, height, width, -1)\n        attn = torch.softmax(attn, -1)\n        attn = attn.view(batch, n_head, height, width, height, width)\n\n        out = torch.einsum(\"bnhwyx, bncyx -> bnchw\", attn, value).contiguous()\n        out = self.out(out.view(batch, channel, height, width))\n\n        return out + input\n"}