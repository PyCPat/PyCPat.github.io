<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        mem = self.mem_kv.expand(m, b, e)
        keys = default(keys, torch.empty(b, 0, e, device=device))

        x<a id="change">, keys</a> = x.transpose(0, 1), keys.transpose(0, 1)

        kv = torch.cat((x<a id="change">, mem, keys</a>))

        attn_shape = (t, kv.shape[0])
        attn_mask = torch.zeros(*attn_shape, device=x.device)</code></pre><h3>After Change</h3><pre><code class='java'>
        self.causal = causal

    def forward(self, qk, v, query_len = None):
        query_len = <a id="change">default(</a>query_len, qk.shape[1]<a id="change">)</a>
        t = query_len

        q = qk[:, 0:query_len]
        qk = F.normalize(qk, 2, dim=-1)

        dot = torch.einsum(&quotbie,bje-&gt;bij&quot, q, qk)

        &#47&#47 qk attention requires tokens not attend to self
        i = torch.arange(t)
        dot[:, i, i] = -1e-5 

        if self.causal:
            i, j = torch.triu_indices(t, t, 1)
            dot[:, i, j] = float(&quot-inf&quot)

        dot<a id="change"> = </a>dot.softmax(dim=-1)
        out = torch.einsum(&quotbij,bje-&gt;bie&quot, dot, v)
        return out<a id="change">, dot</a>

&#47&#47 Shared qk attention, using either full or LSH attention

class LSHSelfAttention(nn.Module):</code></pre>