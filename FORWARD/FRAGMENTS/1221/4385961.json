{"BEFORE":"        value=None,\n        identity=None,\n        query_pos=None,\n        key_pos=None,\n        attn_mask=None,\n        key_padding_mask=None,\n        **kwargs,\n    ):\n        if key is None:\n            key = query\n        if value is None:\n            value = key\n        if identity is None:\n            identity = query\n        if key_pos is None:\n            if query_pos is not None:\n                # use query_pos if key_pos is not available\n                if query_pos.shape == key.shape:\n                    key_pos = query_pos\n                else:\n                    warnings.warn(\n                        f\"position encoding of key is\" f\"missing in {self.__class__.__name__}.\"\n                    )\n        if query_pos is not None:\n            query = query + query_pos\n        if key_pos is not None:\n            key = key + key_pos\n\n        out = self.attn(\n            query=query,\n            key=key,\n            value=value,\n            attn_mask=attn_mask,\n            key_padding_mask=key_padding_mask,\n        )[0]\n\n        return identity + self.proj_drop(out)\n","AFTER":"        tgt_mask: Optional[Tensor] = None,\n        memory_mask: Optional[Tensor] = None,\n        tgt_key_padding_mask: Optional[Tensor] = None,\n        memory_key_padding_mask: Optional[Tensor] = None,\n        pos: Optional[Tensor] = None,\n        query_pos: Optional[Tensor] = None,\n    ):\n        output = tgt\n\n        intermediate = []\n\n        for layer in self.layers:\n            output = layer(\n                output,\n                memory,\n                tgt_mask=tgt_mask,\n                memory_mask=memory_mask,\n                tgt_key_padding_mask=tgt_key_padding_mask,\n                memory_key_padding_mask=memory_key_padding_mask,\n                pos=pos,\n                query_pos=query_pos,\n            )\n            if self.return_intermediate:\n                intermediate.append(self.norm(output))\n\n        if self.norm is not None:\n            output = self.norm(output)\n            if self.return_intermediate:\n                intermediate.pop()\n                intermediate.append(output)\n\n        if self.return_intermediate:\n            return torch.stack(intermediate)\n\n        return output.unsqueeze(0)\n"}