{"BEFORE":"        logits = None\n        if 't' in self.modalities:\n            output_text, _ = self.RNNs[0](X_text)\n            output_text = output_text[:, -1, :]\n            text_emo_vecs_origin = self.textEmoEmbs(torch.LongTensor(list(range(self.num_classes))).to(self.device))\n            text_emo_vecs = text_emo_vecs_origin.unsqueeze(0).repeat(batch_size, 1, 1)\n            text_attn_weights = self.attention(output_text, text_emo_vecs)\n            logits = text_attn_weights if logits is None else logits + text_attn_weights\n\n        if 'a' in self.modalities:\n            output_audio, _ = self.RNNs[1](X_audio)\n            output_audio = output_audio[:, -1, :]\n            audio_emo_vecs = self.affineAudio(text_emo_vecs_origin)\n            audio_emo_vecs = audio_emo_vecs.unsqueeze(0).repeat(batch_size, 1, 1)\n            audio_attn_weights = self.attention(output_audio, audio_emo_vecs)\n            logits = audio_attn_weights if logits is None else logits + audio_attn_weights\n\n        if 'v' in self.modalities:\n            output_visual, _ = self.RNNs[2](X_visual)\n            output_visual = output_visual[:, -1, :]\n            visual_emo_vecs = self.affineVisual(text_emo_vecs_origin)\n            visual_emo_vecs = visual_emo_vecs.unsqueeze(0).repeat(batch_size, 1, 1)\n            visual_attn_weights = self.attention(output_visual, visual_emo_vecs)\n            logits = visual_attn_weights if logits is None else logits + visual_attn_weights\n","AFTER":"        text_emo_vecs_origin = self.textEmoEmbs(torch.LongTensor(list(range(self.num_classes))).to(self.device))\n        logits = None\n        scores = []\n        if 't' in self.modalities:\n            output_text, _ = self.RNNs[0](X_text)\n            output_text = output_text[:, -1, :]\n            text_emo_vecs = text_emo_vecs_origin.unsqueeze(0).repeat(batch_size, 1, 1)\n            text_attn_weights = self.attention(output_text, text_emo_vecs)\n            # logits = text_attn_weights if logits is None else logits + text_attn_weights\n            scores.append(text_attn_weights.unsqueeze(0))\n\n        if 'a' in self.modalities:\n            output_audio, _ = self.RNNs[1](X_audio)\n            output_audio = output_audio[:, -1, :]\n            audio_emo_vecs = self.affineAudio(text_emo_vecs_origin)\n            audio_emo_vecs = audio_emo_vecs.unsqueeze(0).repeat(batch_size, 1, 1)\n            audio_attn_weights = self.attention(output_audio, audio_emo_vecs)\n            # logits = audio_attn_weights if logits is None else logits + audio_attn_weights\n            scores.append(audio_attn_weights.unsqueeze(0))\n\n        if 'v' in self.modalities:\n            output_visual, _ = self.RNNs[2](X_visual)\n            output_visual = output_visual[:, -1, :]\n            visual_emo_vecs = self.affineVisual(text_emo_vecs_origin)\n            visual_emo_vecs = visual_emo_vecs.unsqueeze(0).repeat(batch_size, 1, 1)\n            visual_attn_weights = self.attention(output_visual, visual_emo_vecs)\n            # logits = visual_attn_weights if logits is None else logits + visual_attn_weights\n            scores.append(visual_attn_weights.unsqueeze(0))\n\n        scores = torch.cat(tuple(scores), dim=0).transpose(0, 2)\n        logits = self.modality_weights(scores)\n        logits = logits.squeeze().t()\n"}