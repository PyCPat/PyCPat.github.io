{"BEFORE":"        after_masked_attn = self.masked_multihead_attention(x, x, x, mask=masked_attn_mask) # (B, L, d_model) => (B, L, d_model)\r\n        after_norm_1 = self.layer_norm1(after_masked_attn + x) # (B, L, d_model) => (B, L, d_model)\r\n        after_attn = self.multihead_attention(after_norm_1, encoder_output, encoder_output, mask=attn_mask) # (B, L, d_model) => (B, L, d_model)\r\n        after_norm_2 = self.layer_norm2(after_attn + after_norm_1) # (B, L, d_model) => (B, L, d_model)\r\n        after_ff = self.feed_forward(after_norm_2) # (B, L, d_model) => (B, L, d_ff) => (B, L, d_model)\r\n        after_norm_3 = self.layer_norm3(after_ff + after_norm_2) # (B, L, d_model) => (B, L, d_model)\r\n\r\n        return after_norm_3\r\n","AFTER":"        after_norm_1 = self.layer_norm_1(x) # (B, L, d_model)\r\n        x += self.drop_out_1(self.masked_multihead_attention(after_norm_1, after_norm_1, after_norm_1, mask=decoder_mask)) # (B, L, d_model)\r\n        after_norm_2 = self.layer_norm_2(x) # (B, L, d_model)\r\n        x += self.drop_out_2(self.multihead_attention(after_norm_2, encoder_output, encoder_output, mask=encoder_mask)) # (B, L, d_model)\r\n        after_norm_3 = self.layer_norm_3(x) # (B, L, d_model)\r\n        x += self.drop_out_3(self.feed_forward(after_norm_3)) # (B, L, d_model)\r\n\r\n        return self.layer_norm_4(x) # (B, L, d_model)\r\n"}