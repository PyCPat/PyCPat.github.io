{"BEFORE":"        mem = lmem = None\n        if memories is not None:\n            mem, lmem = memories\n\n        mem = default(mem, lambda: torch.empty(b, 0, e, **to(x)))\n        lmem = default(lmem, lambda: torch.empty(b, 0, e, **to(x)))\n\n        mem_len = mem.shape[1]\n        lmem_len = lmem.shape[1]\n\n        q = self.to_q(x)\n\n        kv_input = torch.cat((lmem, mem, x), dim=1)\n        kv_len = kv_input.shape[1]\n        k, v = self.to_kv(kv_input).chunk(2, dim=-1)\n\n        merge_heads = lambda x: x.reshape(*x.shape[:2], -1, dim_h).transpose(1, 2)\n        q, k, v = map(merge_heads, (q, k, v))\n\n        k, v = map(lambda x: x.expand(-1, h, -1, -1), (k, v))\n\n        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n        mask_value = max_neg_value(dots)\n\n        if pos_emb is not None:\n            pos_emb = pos_emb[:, -kv_len:].type(q.dtype)\n            pos_dots = torch.einsum('bhid,hjd->bhij', q, pos_emb) * self.scale\n            pos_dots = shift(pos_dots)\n            dots = dots + pos_dots\n\n        if input_mask is not None:\n            mask = input_mask[:, None, :, None] * input_mask[:, None, None, :]\n            mask = F.pad(mask, (mem_len + lmem_len, 0), value = True)\n            dots.masked_fill_(~mask, mask_value)\n\n        total_mem_len = mem_len + lmem_len\n        mask = torch.ones(t, t + total_mem_len, **to(x)).triu_(diagonal = 1 + total_mem_len).bool()\n        dots.masked_fill_(mask[None, None, ...], mask_value)\n\n        attn = dots.softmax(dim=-1)\n        attn = self.attn_dropout(attn)\n\n        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n        out = out.transpose(1, 2).reshape(b, t, -1)\n        logits = self.to_out(out)\n        logits = self.dropout(logits)\n\n        new_mem = mem\n        new_lmem = lmem\n\n        if self.seq_len > t or not calc_memory:\n            return logits, Memory(new_mem, new_lmem)\n\n        # calculate memory and compressed memory\n\n        old_mem, new_mem = split_at_index(1, -self.mem_len, torch.cat((mem, x), dim=1))\n\n        if old_mem.shape[1] != 0 and self.lmem_len > 0:\n            pass\n\n        return logits, Memory(new_mem, new_lmem)\n","AFTER":"        memories = default(memories, (None, None))\n        mem, lmem = memories\n\n        init_mem = lambda: torch.empty(b, 0, e, **to(x))\n        mem = default(mem, init_mem)\n        lmem = default(lmem, init_mem)\n\n        mem_len = mem.shape[1]\n        lmem_len = lmem.shape[1]\n\n        q = self.to_q(x)\n\n        kv_input = torch.cat((lmem, mem, x), dim=1)\n        kv_len = kv_input.shape[1]\n        k, v = self.to_kv(kv_input).chunk(2, dim=-1)\n\n        merge_heads = lambda x: reshape_dim(x, -1, (-1, dim_h)).transpose(1, 2)\n        q, k, v = map(merge_heads, (q, k, v))\n\n        k, v = map(lambda x: x.expand(-1, h, -1, -1), (k, v))\n\n        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n        mask_value = max_neg_value(dots)\n\n        if pos_emb is not None:\n            pos_emb = pos_emb[:, -kv_len:].type(q.dtype)\n            pos_dots = torch.einsum('bhid,hjd->bhij', q, pos_emb) * self.scale\n            pos_dots = shift(pos_dots)\n            dots = dots + pos_dots\n\n        if input_mask is not None:\n            mask = input_mask[:, None, :, None] * input_mask[:, None, None, :]\n            mask = F.pad(mask, (mem_len + lmem_len, 0), value = True)\n            dots.masked_fill_(~mask, mask_value)\n\n        total_mem_len = mem_len + lmem_len\n        mask = torch.ones(t, t + total_mem_len, **to(x)).triu_(diagonal = 1 + total_mem_len).bool()\n        dots.masked_fill_(mask[None, None, ...], mask_value)\n\n        attn = dots.softmax(dim=-1)\n        attn = self.attn_dropout(attn)\n\n        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n        out = out.transpose(1, 2).reshape(b, t, -1)\n        out = self.to_out(out)\n\n        return self.dropout(out)\n"}