{"BEFORE":"        out = self.transformer(tokens, mask = mask)\n        return self.to_logits(out)\n","AFTER":"        return_loss = False\n    ):\n        device = text.device\n\n        text_emb = self.text_emb(text)\n        text_emb += self.text_pos_emb(torch.arange(text.shape[1], device = device))\n\n        image_emb = self.image_emb(image)\n        image_emb += self.image_pos_emb(torch.arange(image.shape[1], device = device))\n\n        tokens = torch.cat((text_emb, image_emb), dim = 1)\n\n        if exists(mask):\n            mask = F.pad(mask, (0, self.image_seq_len), value = True)\n\n        out = self.transformer(tokens, mask = mask)\n        out = self.to_logits(out)\n\n        if not return_loss:\n            return out\n\n        offsetted_image = image + self.num_text_tokens\n        labels = torch.cat((text, offsetted_image), dim = 1)\n        labels = F.pad(labels, (0, 1), value = (self.total_tokens - 1)) # last token predicts EOS\n        loss = F.cross_entropy(out.transpose(1, 2), labels[:, 1:])\n        return loss\n"}