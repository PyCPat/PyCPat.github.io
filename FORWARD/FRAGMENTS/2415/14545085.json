{"BEFORE":"    def forward(self, src, tgt, src_sz):\n        '''\n        src: (batch_size, sequence_len.padded)\n        tgt: (batch_size, sequence_len.padded)\n        src_sz: [batch_size, 1] -  Unpadded sequence lengths -> unused\n        '''\n\n        batch_size = src.shape[0]\n        # x: batch_size, max_length, embed_dim\n        x = self.embedding(src)\n\n        #output :shp: batch_size, mx_seq_length, voc_dim\n        output = self.ffnn(x)\n\n        #output :shp: batch_size, voc_dim, mx_seq_length\n        output = output.permute(0,2,1)\n\n        #predict_vecs: batch_size, voc_dim, max_length\n        predict_vecs = torch.zeros(batch_size, self.voc_dim, tgt.size(1) ).to(self.device)\n        curr_sz = tgt.size(1)\n        predict_vecs[:,:,:curr_sz] = output[:,:,:curr_sz]\n\n        return predict_vecs\n","AFTER":"        x = self.embedding(src)\n\n        ## pack the padded data\n        # x: batch_size, max_length, embed_dim -> for pack_pad\n        x = nn.utils.rnn.pack_padded_sequence(x, src_sz, enforce_sorted=False,\n                                                        batch_first= True) # unpad\n\n        # output_: batch_size, packed_size, embed_dim\n        # hidden:  n_layer*num_directions, batch_size, hidden_dim | if LSTM (h_n, c_n)\n        output_, hidden = self.corr_rnn(x)\n\n        # hidden: n_layer*num_directions, batch_size, hidden_dim | if LSTM h_n\n        hidden = hidden if self.rnn_type != \"lstm\" \\\n                        else hidden[0] #h_n\n\n        # hidden: 1, batch_size, hidden_dim * directions ->tking only last two layers\n        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = -1) if self.directions == 2 \\\n                        else hidden[:,-1,:]\n\n        #output :shp: batch_size, word_voc_dim\n        output = self.ffnn(hidden.squeeze(0))\n\n        return output\n"}