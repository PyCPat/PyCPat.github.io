{"BEFORE":"        qkv = self.qkv_dwconv(self.qkv(x))\n        q, k, v = qkv.chunk(3, dim=1)\n\n        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n\n        q = torch.nn.functional.normalize(q, dim=-1)\n        k = torch.nn.functional.normalize(k, dim=-1)\n\n        attn = (q @ k.transpose(-2, -1)) * self.temperature\n        attn = attn.softmax(dim=-1)\n\n        out = (attn @ v)\n\n        out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)\n\n        out = self.project_out(out)\n","AFTER":"        q, k, v = self.qkv_conv(self.qkv(x)).chunk(3, dim=1)\n\n        q = q.view(b, c, -1).view(b, self.num_heads, -1, h * w)\n        k = k.view(b, c, -1).view(b, self.num_heads, -1, h * w)\n        v = v.view(b, c, -1).view(b, self.num_heads, -1, h * w)\n        q, k = F.normalize(q, dim=-1), F.normalize(k, dim=-1)\n\n        attn = torch.softmax(torch.matmul(q, k.transpose(-2, -1)) * self.temperature, dim=-1)\n        out = self.project_out(torch.matmul(attn, v).view(b, -1, h * w).view(b, -1, h, w))\n"}