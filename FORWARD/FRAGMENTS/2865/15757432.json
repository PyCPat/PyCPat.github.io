{"BEFORE":"\t\timg: torch.FloatTensor,\n\t\tsource_mask: torch.BoolTensor\n\t\t) :\n\t\tfeats = self.backbone(img)\n\t\tfeats = torch.einsum('n e h s -> s n e', feats)\n\t\tfeats = self.pe(feats)\n\t\tfeats = self.encoders(feats, src_key_padding_mask = source_mask)\n\t\tfeats = torch.einsum('s n e -> n s e', feats)\n\t\tpred_char_logits = self.char_pred(feats)\n\t\tpred_color_values = self.color_pred1(feats)\n\t\treturn pred_char_logits, pred_color_values\n","AFTER":"\tdef forward(self, input, lengths):\r\n\t\t\"\"\"\r\n\t\tinput : visual feature [batch_size x T x input_size]\r\n\t\toutput : contextual feature [batch_size x T x output_size]\r\n\t\t\"\"\"\r\n\t\tself.rnn.flatten_parameters()\r\n\t\tinput = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True, enforce_sorted=False)\r\n\t\trecurrent, _ = self.rnn(input)  # batch_size x T x input_size -> batch_size x T x (2*hidden_size)\r\n\t\trecurrent, _ = torch.nn.utils.rnn.pad_packed_sequence(recurrent, batch_first=True)\r\n\t\toutput = self.linear(recurrent)  # batch_size x T x output_size\r\n\t\treturn output\r\n"}