{"BEFORE":"        mask_value = max_neg_value(dots_text)\n\n        i, j = dots_text.shape[-2:]\n        text_causal_mask = torch.ones(i, j, device = device).triu_(j - i + 1).bool()\n        dots_text.masked_fill_(text_causal_mask, mask_value)\n\n        attn_text = dots_text.softmax(dim = -1)\n        out_text = einsum('b i j, b j d -> b i d', attn_text, v_text)\n\n        # image attention\n\n        split_axis_einops = 'b (h w) c -> (b h) w c' if axis == 0 else 'b (h w) c -> (b w) h c'\n        merge_axis_einops = '(b ax) n d -> b (ax n) d' if axis == 0 else '(b ax) n d -> b (n ax) d'\n\n        # split out axis\n\n        q_img, k_img, v_img = map(lambda t: rearrange(t, split_axis_einops, h = img_size), (q_img, k_img, v_img))\n\n        # prepare text key \/ values for the image tokens to attend to\n\n        k_text, v_text = map(lambda t: repeat(t, 'b n d -> (b ax) n d', ax = img_size), (k_text, v_text))\n        k_img = torch.cat((k_text, k_img), dim = 1)\n        v_img = torch.cat((v_text, v_img), dim = 1)\n\n        # similarity\n\n        dots_image = einsum('b i d, b j d -> b i j', q_img, k_img)\n\n        # mask so image has full attention to text, but causal along axis\n\n        bh, i, j = dots_image.shape\n        causal_mask = torch.ones(i, img_size, device = device).triu_(img_size - i + 1).bool()\n        causal_mask = repeat(causal_mask, 'i j -> b i j', b = bh)\n\n        mask = repeat(mask, 'b j -> (b r) i j', r = (bh \/\/ b), i = i)\n        mask = torch.cat((~mask, causal_mask), dim = -1)\n\n        dots_image.masked_fill_(mask, mask_value)\n\n        # attention.\n\n        attn_image = dots_image.softmax(dim = -1)\n\n        # aggregate\n\n        out_image = einsum('b i j, b j d -> b i d', attn_image, v_img)\n\n        # merge back axis\n\n        out_image = rearrange(out_image, merge_axis_einops, ax = img_size)\n","AFTER":"        text_len = seq_len + 1 - img_seq_len\n\n        # padding\n\n        padding = seq_len - n + 1\n        mask = default(mask, lambda: torch.ones(b, text_len, device = device).bool())\n\n        x = F.pad(x, (0, 0, 0, padding), value = 0)\n        mask = mask[:, :text_len]\n\n        # derive queries \/ keys \/ values\n\n        qkv = self.to_qkv(x).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), qkv)\n\n        q *= self.scale\n\n        ((q_text, q_img), (k_text, k_img), (v_text, v_img)) = map(lambda t: (t[:, :-img_seq_len], t[:, -img_seq_len:]), (q, k, v))\n\n        # text attention\n\n        dots_text = einsum('b i d, b j d -> b i j', q_text, k_text)\n        mask_value = max_neg_value(dots_text)\n\n        i, j = dots_text.shape[-2:]\n        text_causal_mask = torch.ones(i, j, device = device).triu_(j - i + 1).bool()\n        dots_text.masked_fill_(text_causal_mask, mask_value)\n\n        attn_text = dots_text.softmax(dim = -1)\n        out_text = einsum('b i j, b j d -> b i d', attn_text, v_text)\n\n        # image attention\n\n        split_axis_einops = 'b (h w) c -> b h w c' if axis == 0 else 'b (h w) c -> b w h c'\n        merge_axis_einops = 'b x n d -> b (x n) d' if axis == 0 else 'b x n d -> b (n x) d'\n\n        # split out axis\n\n        q_img, k_img, v_img = map(lambda t: rearrange(t, split_axis_einops, h = img_size), (q_img, k_img, v_img))\n\n        # similarity\n\n        dots_image_to_image = einsum('b x i d, b x j d -> b x i j', q_img, k_img)\n        dots_image_to_text = einsum('b x i d, b j d -> b x i j', q_img, k_text)\n\n        dots = torch.cat((dots_image_to_text, dots_image_to_image), dim = -1)\n\n        # mask so image has full attention to text, but causal along axis\n\n        bh, x, i, j = dots.shape\n        causal_mask = torch.ones(i, img_size, device = device).triu_(img_size - i + 1).bool()\n        causal_mask = repeat(causal_mask, 'i j -> b x i j', b = bh, x = x)\n\n        mask = repeat(mask, 'b j -> (b h) x i j', h = h, x = x, i = i)\n        mask = torch.cat((~mask, causal_mask), dim = -1)\n\n        dots.masked_fill_(mask, mask_value)\n\n        # attention.\n\n        attn = dots.softmax(dim = -1)\n\n        # aggregate\n\n        attn_image_to_text, attn_image_to_image = attn[..., :text_len], attn[..., text_len:]\n\n        out_image_to_image = einsum('b x i j, b x j d -> b x i d', attn_image_to_image, v_img)\n        out_image_to_text = einsum('b x i j, b j d -> b x i d', attn_image_to_text, v_text)\n\n        out_image = out_image_to_image + out_image_to_text\n\n        # merge back axis\n\n        out_image = rearrange(out_image, merge_axis_einops, x = img_size)\n"}