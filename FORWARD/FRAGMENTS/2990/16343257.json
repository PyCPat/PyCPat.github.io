{"BEFORE":"        logit_softmax = F.softmax(logit, dim = 1)\n        diff = logit.diag().view(-1, 1).expand_as(logit) - logit\n        loss = -torch.log(torch.mean(logit_softmax * torch.sigmoid(diff)))\n        # add regularization\n        loss += self.lambda_ * torch.mean(logit_softmax * (logit ** 2))\n        return loss\n","AFTER":"    def forward(self, logits_scores, negative_mask):\n        \"\"\"\n        Args:\n            logits_scores: (#pos_target, #neg_samples):  scores of positive next items for all sessions\n                                                            + All negative samples (mini-batch + additional samples )\n            negative_mask: (#pos_target, #neg_samples) : specify the negative items for each positive target\n        \"\"\"\n        positive_mask = ~negative_mask\n        positives = logits_scores.diag().view(-1, 1).expand_as(logits_scores)\n        diff = positives - logits_scores\n        loss = F.sigmoid(diff)\n        logit_softmax = torch.softmax(logits_scores, dim = 1)\n        loss = logit_softmax * loss\n        reg = logit_softmax * (logits_scores**2)\n        # set to zeros the diff scores of positive targets present in the same session\n        loss = loss.masked_fill(positive_mask, 0)\n        reg = reg.masked_fill(positive_mask, 0)\n        # Average over the nb of negative sample per\n        loss = -torch.log(loss.sum(1)) + self.lambda_ * reg.sum(1)\n        return torch.mean(loss)\n"}