{"BEFORE":"    def forward(self, x):\n        b, n, device = *x.shape, x.device\n\n        x = self.token_emb(x)\n\n        memory_keys = None\n        memory_values = None\n\n        outputs = []\n\n        # calculate weighting of layers for storing to memory\n\n        layer_weight = self.layer_weight.softmax(dim = -1)\n        layer_weight = rearrange(self.layer_weight, 'd -> d () () ()')\n\n        for x in x.split(self.seq_len, dim = 1):\n            hiddens = []\n\n            # prepare memory for attention, if it exists\n\n            memory = None\n            if exists(memory_keys):\n                memory = (memory_keys, memory_values)\n\n            for attn, ff in self.layers:\n                hiddens.append(x)\n\n                x = attn(x, memory = memory, pos_emb = self.pos_emb)\n                x = ff(x)\n\n            outputs.append(x)\n\n            # calculate new memory key \/ values and store to FIFO queue\n\n            hiddens = torch.stack(hiddens)\n            agg_hiddens = (hiddens * layer_weight).sum(dim = 0)\n\n            # pre-calculate memory key \/ values and store to buffer\n\n            mem_k, mem_v = self.shared_kv_proj(agg_hiddens).chunk(2, dim = -1)\n            memory_keys = safe_cat(memory_keys, mem_k, dim = 1)\n            memory_values = safe_cat(memory_values, mem_v, dim = 1)\n\n            # enforce max length on memory buffer\n\n            memory_keys = memory_keys[-self.mem_len:]\n            memory_values = memory_values[-self.mem_len:]\n\n        x = torch.cat((outputs), dim = 1)\n        return self.to_logits(x)\n","AFTER":"    def forward(self, x, memory = None, return_memory = False):\n        b, n, device = *x.shape, x.device\n\n        x = self.token_emb(x)\n\n        memory_keys = None\n        memory_values = None\n\n        if exists(memory):\n            memory_keys, memory_values = memory\n\n        outputs = []\n\n        # calculate weighting of layers for storing to memory\n\n        layer_weight = self.layer_weight.softmax(dim = -1)\n        layer_weight = rearrange(self.layer_weight, 'd -> d () () ()')\n\n        for x in x.split(self.seq_len, dim = 1):\n            hiddens = []\n\n            # prepare memory for attention, if it exists\n\n            memory = None\n            if exists(memory_keys):\n                memory = (memory_keys, memory_values)\n\n            for attn, ff in self.layers:\n                hiddens.append(x)\n\n                x = attn(x, memory = memory, pos_emb = self.pos_emb)\n                x = ff(x)\n\n            outputs.append(x)\n\n            # calculate new memory key \/ values and store to FIFO queue\n\n            hiddens = torch.stack(hiddens)\n            agg_hiddens = (hiddens * layer_weight).sum(dim = 0)\n\n            # pre-calculate memory key \/ values and store to buffer\n\n            mem_k, mem_v = self.shared_kv_proj(agg_hiddens).chunk(2, dim = -1)\n            memory_keys = safe_cat(memory_keys, mem_k, dim = 1)\n            memory_values = safe_cat(memory_values, mem_v, dim = 1)\n\n            # enforce max length on memory buffer\n\n            memory_keys = memory_keys[-self.mem_len:]\n            memory_values = memory_values[-self.mem_len:]\n\n        x = torch.cat((outputs), dim = 1)\n        out = self.to_logits(x)\n\n        if not return_memory:\n            return out\n\n        return out, Memory(memory_keys, memory_values)\n"}