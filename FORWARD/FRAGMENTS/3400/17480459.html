<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.embedding.weight.data.uniform_(1./self.K, 1./self.K)

    def forward(self, input: Tensor):
        input = <a id="change">input.permute(0</a>, <a id="change">2</a>, <a id="change">3</a>, <a id="change">1</a><a id="change">)</a> &#47&#47 [B x D x H x W] -&gt; [B x H x W x D]
        input_shape = input.shape
        flat_input = input.contiguous().view(-1, self.D)    &#47&#47 [BHW x D]

        &#47&#47 Compute L2 distance between input and embedding weights
        dist = torch.sum(flat_input**2, dim = 1, keepdim=True) + \
               torch.sum(self.embedding.weight ** 2, dim = 1) - \
               2 * torch.matmul(flat_input, self.embedding.weight.t()) &#47&#47 [BHW x K]

        &#47&#47 Get the encoding that has the min distance
        encoding_inds = torch.argmin(dist, dim = 1).view(-1, 1) &#47&#47 [BHW, 1]

        &#47&#47 Convert to one-hot encodings
        device = input.device
        encoding_one_hot = torch.zeros(encoding_inds.size(0), self.K, device=device)
        encoding_one_hot.scatter_(1, encoding_inds, 1.) &#47&#47 [BHW x K]

        &#47&#47 Quantize the input
        quantized_input = torch.matmul(encoding_one_hot, self.embedding.weight) &#47&#47 [BHW, D]
        quantized_input = quantized_input.view(input_shape) &#47&#47 [B x H x W x D]

        <a id="change">return </a>quantized_input.permute(0, 3, 1, 2) &#47&#47 [B x D x H x W]


class VQVAE(BaseVAE):</code></pre><h3>After Change</h3><pre><code class='java'>
        self.embedding.weight.data.uniform_(-1 / self.K, 1 / self.K)

    def forward(self, latents: Tensor) -&gt; Tensor:
        latents = <a id="change">latents.permute(0</a>, <a id="change">2</a>, <a id="change">3</a>, <a id="change">1</a><a id="change">)</a>.contiguous()  &#47&#47 [B x D x H x W] -&gt; [B x H x W x D]
        latents_shape = latents.shape
        flat_latents = latents.view(-1, self.D)  &#47&#47 [BHW x D]

        &#47&#47 Compute L2 distance between latents and embedding weights
        dist = torch.sum(flat_latents ** 2, dim=1, keepdim=True) + \
               torch.sum(self.embedding.weight ** 2, dim=1) - \
               2 * torch.matmul(flat_latents, self.embedding.weight.t())  &#47&#47 [BHW x K]

        &#47&#47 Get the encoding that has the min distance
        encoding_inds = torch.argmin(dist, dim=1).unsqueeze(1)  &#47&#47 [BHW, 1]

        &#47&#47 Convert to one-hot encodings
        device = latents.device
        encoding_one_hot = torch.zeros(encoding_inds.size(0), self.K, device=device)
        encoding_one_hot.scatter_(1, encoding_inds, 1)  &#47&#47 [BHW x K]

        &#47&#47 Quantize the latents
        quantized_latents = torch.matmul(encoding_one_hot, self.embedding.weight)  &#47&#47 [BHW, D]
        quantized_latents = quantized_latents.view(latents_shape)  &#47&#47 [B x H x W x D]

        &#47&#47 Compute the VQ Losses
        commitment_loss<a id="change"> = </a>F.mse_loss(quantized_latents.detach(), latents)
        embedding_loss = F.mse_loss(quantized_latents, latents.detach())

        vq_loss<a id="change"> = </a>commitment_loss<a id="change"> * </a>self.beta + embedding_loss

        &#47&#47 Add the residue back to the latents
        quantized_latents = latents + (quantized_latents - latents).detach()

        <a id="change">return </a>quantized_latents.permute(0, 3, 1, 2).contiguous(), vq_loss  &#47&#47 [B x D x H x W]


&#47&#47 class VectorQuantizer(nn.Module):</code></pre>