{"BEFORE":"            dots.masked_fill_(~mask, float('-inf'))\n            del mask\n\n        # Causal masking\n        if self.causal:\n            mask = bq_t[:, :, :, None] < bkv_t[:, :, None, :].clamp(max=query_len - 1)\n            dots.masked_fill_(mask, float('-inf'))\n            del mask\n\n        # Mask out attention to self except when no other targets are available.\n        self_mask = bq_t[:, :, :, None] == bkv_t[:, :, None, :]\n        dots.masked_fill_(self_mask, TOKEN_SELF_ATTN_VALUE)\n        del self_mask\n\n        # Mask out attention to other hash buckets.\n        if not self._attend_across_buckets:\n            bucket_mask = bq_buckets[:, :, :, None] != bkv_buckets[:, :, None, :]\n            dots.masked_fill_(bucket_mask, float('-inf'))\n","AFTER":"        dots = torch.einsum('bhie,bhje->bhij', bq, bk) * (dim ** -0.5)\n        masked_value = max_neg_value(dots)\n\n        if input_mask is not None:\n            input_mask = F.pad(input_mask, (0, seqlen - input_mask.shape[1]), 'constant', True)\n            mq = input_mask.gather(1, st).reshape((batch_size, chunk_size, -1))\n            mkv = look_one_back(mq)\n            mask = mq[:, :, :, None] * mkv[:, :, None, :]\n            dots.masked_fill_(~mask, masked_value)\n            del mask\n\n        # Causal masking\n        if self.causal:\n            mask = bq_t[:, :, :, None] < bkv_t[:, :, None, :].clamp(max=query_len - 1)\n            dots.masked_fill_(mask, masked_value)\n            del mask\n\n        # Mask out attention to self except when no other targets are available.\n        self_mask = bq_t[:, :, :, None] == bkv_t[:, :, None, :]\n        dots.masked_fill_(self_mask, TOKEN_SELF_ATTN_VALUE)\n        del self_mask\n\n        # Mask out attention to other hash buckets.\n        if not self._attend_across_buckets:\n            bucket_mask = bq_buckets[:, :, :, None] != bkv_buckets[:, :, None, :]\n            dots.masked_fill_(bucket_mask, masked_value)\n"}