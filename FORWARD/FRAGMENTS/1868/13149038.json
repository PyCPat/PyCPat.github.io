{"BEFORE":"        mask_value = float('-inf')\n\n        if shared_qk:\n            mask = bq_t[:, :, :, None] == bq_k[:, :, None, :]\n            dots.masked_fill_(mask, TOKEN_SELF_ATTN_VALUE)\n            del mask\n\n        if causal:\n            mask = bq_t[:, :, :, None] < bq_k[:, :, None, :]\n            dots.masked_fill_(mask, mask_value)\n            del mask\n\n        mask = bq_k[:, :, None, :] == -1\n        dots.masked_fill_(mask, mask_value)\n        del mask\n\n        if input_mask is not None:\n            h = b \/\/ input_mask.shape[0]\n            input_mask = input_mask.reshape(-1, buckets, bucket_size)\n            mq = mk = input_mask\n            mk = look_around(mk, pad_value=False, **look_around_kwargs)\n            mask = (mq[:, None, :, :, None] * mk[:, None, :, None, :])\n            mask = merge_dims(0, 1, mask.expand(-1, h, -1, -1, -1))\n            dots.masked_fill_(~mask, mask_value)\n            del mask\n\n        attn = dots.softmax(dim=-1)\n        attn = self.dropout(attn)\n\n        out = torch.einsum('bhij,bhje->bhie', attn, bv)\n        out = out.reshape(b, t, e)\n","AFTER":"        shape = q.shape\n\n        merge_into_batch = lambda t: t.reshape(-1, *t.shape[-2:])\n        q, k, v = map(merge_into_batch, (q, k, v))\n\n        b, t, e, h, device, dtype = *q.shape, self.heads, q.device, q.dtype\n        bucket_size, causal, look_backward, look_forward, shared_qk = self.bucket_size, self.causal, self.look_backward, self.look_forward, self.shared_qk\n\n        buckets = t \/\/ bucket_size\n\n        if shared_qk:\n            k = F.normalize(k, 2, dim=-1).type(q.type())\n\n        ticker = torch.arange(t, device=device, dtype=dtype)[None, :]\n        b_t = ticker.reshape(1, buckets, bucket_size)\n\n        bucket_fn = lambda t: t.reshape(b, buckets, bucket_size, -1)\n        bq, bk, bv = map(bucket_fn, (q, k, v))\n\n        look_around_kwargs = {'backward': look_backward, 'forward': look_forward}\n        bk = look_around(bk, **look_around_kwargs)\n        bv = look_around(bv, **look_around_kwargs)\n\n        bq_t = b_t\n        bq_k = look_around(b_t, **look_around_kwargs)\n\n        dots = torch.einsum('bhie,bhje->bhij', bq, bk) * (e ** -0.5)\n\n        mask_value = max_neg_value(dots)\n\n        if shared_qk:\n            mask = bq_t[:, :, :, None] == bq_k[:, :, None, :]\n            dots.masked_fill_(mask, TOKEN_SELF_ATTN_VALUE)\n            del mask\n\n        if causal:\n            mask = bq_t[:, :, :, None] < bq_k[:, :, None, :]\n            dots.masked_fill_(mask, mask_value)\n            del mask\n\n        mask = bq_k[:, :, None, :] == -1\n        dots.masked_fill_(mask, mask_value)\n        del mask\n\n        if input_mask is not None:\n            h = b \/\/ input_mask.shape[0]\n            input_mask = input_mask.reshape(-1, buckets, bucket_size)\n            mq = mk = input_mask\n            mk = look_around(mk, pad_value=False, **look_around_kwargs)\n            mask = (mq[:, :, :, None] * mk[:, :, None, :])\n            mask = merge_dims(0, 1, expand_dim(mask, 1, h))\n            dots.masked_fill_(~mask, mask_value)\n            del mask\n\n        attn = dots.softmax(dim=-1)\n        attn = self.dropout(attn)\n\n        out = torch.einsum('bhij,bhje->bhie', attn, bv)\n        out = out.reshape(*shape)\n"}