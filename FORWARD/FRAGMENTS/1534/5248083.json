{"BEFORE":"        seq_len, device = text.shape[1], text.device\n        assert seq_len <= self.text_max_seq_len, 'your input text has a greater length than what was designated on initialization'\n\n        tokens = self.text_embedding(text)\n        pos_emb = self.text_pos_embedding(torch.arange(seq_len, device = device))\n        tokens = tokens + rearrange(pos_emb, 'n d -> 1 n d')\n\n        text_embeds = self.text_transformer(tokens, mask = text_mask)\n        return text_embeds\n","AFTER":"        batch, seq_len, device = *text.shape, text.device\n        assert seq_len <= self.text_max_seq_len, 'your input text has a greater length than what was designated on initialization'\n\n        tokens = self.text_embedding(text)\n        pos_emb = self.text_pos_embedding(torch.arange(seq_len, device = device))\n        tokens = tokens + rearrange(pos_emb, 'n d -> 1 n d')\n\n        text_embeds = self.text_transformer(tokens, mask = text_mask)\n\n        frame_indices = self.vae.get_video_indices(video)\n        frame_indices = rearrange(frame_indices, 'b ... -> b (...)')\n        frame_indices_input = frame_indices[:, :-1] if return_loss else frame_indices\n\n        frame_embeddings = self.image_embedding(frame_indices_input)\n        frame_embeddings = self.video_pos_emb(frame_embeddings) + frame_embeddings\n\n        bos = repeat(self.video_bos, 'd -> b 1 d', b = batch)\n        frame_embeddings = torch.cat((bos, frame_embeddings), dim = 1)\n        frame_embeddings = self.video_transformer(frame_embeddings)\n\n        logits = self.to_logits(frame_embeddings)\n\n        if not return_loss:\n            return logits\n\n        loss = F.cross_entropy(rearrange(logits, 'b n c -> b c n'), frame_indices)\n        return loss\n"}