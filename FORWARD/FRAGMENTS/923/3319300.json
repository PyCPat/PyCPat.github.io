{"BEFORE":"        b_q, n_q, _, h_q = *q_tokens.shape, self.heads\n        b_kv, n_kv, _, h_kv = *kv_tokens.shape, self.heads\n        q, k, v = [\n            self.to_q(q_tokens),\n            self.to_k(kv_tokens),\n            self.to_v(kv_tokens),\n        ]\n        q = rearrange(q, 'b n (h d) -> b h n d', h=h_q)\n        k = rearrange(k, 'b n (h d) -> b h n d', h=h_kv)\n        v = rearrange(v, 'b n (h d) -> b h n d', h=h_kv)\n","AFTER":"    def forward(self, q_tokens, k_tokens=None, v_tokens=None):\n        \"\"\"\n        # demo1: self attention\n        tokens = torch.ones([3, 5, 512])\n        dim = 512\n        heads = 8\n        dim_head = 32\n        dim = 512\n        attention_layer = MultiHeadAttention(dim, heads, dim_head)\n        tokens_, attention_maps = attention_layer(tokens)\n        print(tokens.shape, tokens_.shape, attention_maps.shape)\n\n\n        # demo2: cross attention\n        q_tokens = torch.ones([3, 5, 512])\n        k_tokens = torch.ones([3, 15, 512])\n        v_tokens = torch.ones([3, 15, 512]) # the same size as k_tokens\n        dim = 512\n        heads = 8\n        dim_head = 32\n        dim = 512\n        attention_layer = MultiHeadAttention(dim, heads, dim_head)\n        tokens_, attention_maps = attention_layer(q_tokens, k_tokens, v_tokens)\n        print(tokens.shape, tokens_.shape, attention_maps.shape)\n\n        \"\"\"\n\n        if k_tokens is None and v_tokens is None:\n            v_tokens = k_tokens = q_tokens\n        elif k_tokens is not None and v_tokens is not None:\n            pass\n        else:\n            raise ValueError('k_tokens and v_tokens should be None or not simultaneously')\n\n        q, k, v = [\n            self.to_q(q_tokens),\n            self.to_k(k_tokens),\n            self.to_v(v_tokens),\n        ]\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h=self.heads)\n        k = rearrange(k, 'b n (h d) -> b h n d', h=self.heads)\n        v = rearrange(v, 'b n (h d) -> b h n d', h=self.heads)\n"}