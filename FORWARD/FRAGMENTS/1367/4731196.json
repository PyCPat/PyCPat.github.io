{"BEFORE":"        sim = einsum('b i d, b j d -> b i j', qk, qk)\n\n        sim = sim * self.scale\n","AFTER":"        seq_len, dim = x.shape[-2:]\n\n        is_softmax_attn = not self.laplacian_attn_fn\n\n        v_input = default(v_input, x)\n\n        qk, v = self.to_qk(x), self.to_v(v_input)\n        q, k = self.offsetscale(qk)\n\n        scale = (seq_len ** -1) if self.laplacian_attn_fn else (dim ** -0.5)\n\n        sim = einsum('b i d, b j d -> b i j', q, k) * scale\n"}