{"BEFORE":"        n, device = x.shape[1], x.device\n        l = n + context_len\n        t = torch.arange(l - 1, -1, -1, device = device).type_as(self.inv_freq)\n        sinusoid_inp = einsum('i , j -> i j', t, self.inv_freq)\n        emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim = -1)\n        return emb\n","AFTER":"        device = self.relative_attention_bias.weight.device\n        q_pos = torch.arange(qlen, dtype = torch.long, device = device)\n        k_pos = torch.arange(klen, dtype = torch.long, device = device)\n        rel_pos = k_pos[None, :] - q_pos[:, None]\n        rp_bucket = self._relative_position_bucket(rel_pos, causal = self.causal, num_buckets = self.num_buckets)\n        values = self.relative_attention_bias(rp_bucket)\n        return rearrange(values, 'i j h -> () h i j')\n"}