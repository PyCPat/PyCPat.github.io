{"BEFORE":"        weight, bias = weight[:, :n, :n], bias[:, :n]\n\n        mask = torch.ones(weight.shape[:2], device = device).triu_(1).bool()\n        weight = weight.masked_fill(mask[..., None], 0.)\n\n        gate = rearrange(gate, 'b n (h d) -> b h n d', h = h)\n        gate = einsum('b h n d, h n m -> b h m d', gate, weight)\n        gate = gate + rearrange(bias, 'h n -> () h n ()')\n        gate = rearrange(gate, 'b h n d -> b n (h d)')\n\n        return gate * res\n","AFTER":"        device, n, h, w = x.device, x.shape[1], self.heads, self.window\n\n        x = pad_to_multiple(x, w, dim = -2)\n        x = rearrange(x, 'b (w n) d -> b w n d', n = w)\n\n        res, gate = x.chunk(2, dim = -1)\n        gate = self.norm(gate)\n\n        gate = F.pad(gate, (0, 0, 0, 0, 1, 0), value = 0.)\n        gate = torch.cat((gate[:, :-1], gate[:, 1:]), dim = 2)\n\n        weight, bias = self.weight, self.bias\n\n        mask = torch.ones(weight.shape[-2:], device = device).triu_(1 + w).bool()\n        weight = weight.masked_fill(mask[None, ...], 0.)\n\n        gate = rearrange(gate, 'b w n (h d) -> b w h n d', h = h)\n        gate = einsum('b w h n d, h m n -> b w h m d', gate, weight)\n        gate = gate + rearrange(bias, 'h n -> () () h n ()')\n\n        gate = rearrange(gate, 'b w h n d -> b w n (h d)')\n\n        out = gate * res\n        out = rearrange(out, 'b w n d -> b (w n) d')\n        return out[:, :n]\n"}