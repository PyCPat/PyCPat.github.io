{"BEFORE":"        g_t = gbk_t[:, 0, :]\n        b_t = gbk_t[:, 1, :]\n        k_t = gbk_t[:, 2, :]\n\n        # attention GMM parameters\n        sig_t = torch.nn.functional.softplus(b_t) + self.eps\n\n        mu_t = self.mu_prev + torch.nn.functional.softplus(k_t)\n        g_t = torch.softmax(g_t, dim=-1) \/ sig_t + self.eps\n\n        # each B x K x T_in\n        j = self.J[:inputs.size(1)]\n\n        # attention weights\n        phi_t = g_t.unsqueeze(-1) * torch.exp(-0.5 * (mu_t.unsqueeze(-1) - j)**2 \/ (sig_t.unsqueeze(-1)**2))\n        alpha_t = self.COEF * torch.sum(phi_t, 1)\n","AFTER":"        g_t = gbk_t[:, 0, :]\n        b_t = gbk_t[:, 1, :]\n        k_t = gbk_t[:, 2, :]\n\n        # attention GMM parameters\n        sig_t = torch.nn.functional.softplus(b_t) + self.eps\n\n        mu_t = self.mu_prev + torch.nn.functional.softplus(k_t)\n        g_t = torch.softmax(g_t, dim=-1) + self.eps\n\n        j = self.J[:inputs.size(1)+1]\n\n        # attention weights\n        phi_t = g_t.unsqueeze(-1) * (1 \/ (1 + torch.sigmoid((mu_t.unsqueeze(-1) - j) \/ sig_t.unsqueeze(-1))))\n\n        # discritize attention weights\n        alpha_t = torch.sum(phi_t, 1)\n        alpha_t = alpha_t[:, 1:] - alpha_t[:, :-1]\n        alpha_t[alpha_t == 0] = 1e-8\n"}