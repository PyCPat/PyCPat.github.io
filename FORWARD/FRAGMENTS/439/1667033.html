<link rel="stylesheet" href="../default.css">
<script src="../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        sig_t = torch.nn.functional.softplus(b_t) + self.eps

        mu_t = self.mu_prev + torch.nn.functional.softplus(k_t)
        g_t = <a id="change">torch.softmax(</a>g_t<a id="change">, dim=-1)</a><a id="change"> / sig_t</a> + self.eps

        &#47&#47 each B x K x T_in
        j = self.J[:<a id="change">inputs.size(1</a><a id="change">)</a>]

        &#47&#47 attention weights
        phi_t = g_t.unsqueeze(-1) * <a id="change">torch.exp(</a>-<a id="change">0.5 * (mu_t.unsqueeze(-1) - j)**2 / </a>(<a id="change">sig_t.unsqueeze(-1</a><a id="change">)</a><a id="change">**2</a>)<a id="change">)</a>
        alpha_t = self.COEF<a id="change"> * torch.sum(</a>phi_t, <a id="change">1</a><a id="change">)</a>

        &#47&#47 apply masking
        if mask is not None:
            alpha_t.data.masked_fill_(~mask, self._mask_value)</code></pre><h3>After Change</h3><pre><code class='java'>
        sig_t = torch.nn.functional.softplus(b_t) + self.eps

        mu_t = self.mu_prev + torch.nn.functional.softplus(k_t)
        g_t = <a id="change">torch.softmax(</a>g_t<a id="change">, dim=-1)</a> + self.eps

        j = self.J[:<a id="change">inputs.size(1</a><a id="change">)</a><a id="change">+1</a>]

        &#47&#47 attention weights
        phi_t = g_t.unsqueeze(-1) * (<a id="change">1</a><a id="change"> / </a>(1<a id="change"> + </a><a id="change">torch.sigmoid(</a>(<a id="change">mu_t.unsqueeze(</a>-1<a id="change">)</a><a id="change"> - </a>j)<a id="change"> / sig_t.unsqueeze(-1</a><a id="change">)</a><a id="change">)</a>))

        &#47&#47 discritize attention weights
        <a id="change">alpha_t</a> = <a id="change">torch.sum(</a>phi_t, <a id="change">1</a><a id="change">)</a>
        <a id="change">alpha_t = alpha_t[:, 1:]</a><a id="change"> - alpha_t[:, :-1]</a>
        <a id="change">alpha_t[alpha_t == 0] = 1e-8</a>

        &#47&#47 apply masking
        if mask is not None:
            alpha_t.data.masked_fill_(~mask, self._mask_value)</code></pre>