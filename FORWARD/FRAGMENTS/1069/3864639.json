{"BEFORE":"        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        x = F.log_softmax(x, dim=1)\n        return x\n","AFTER":"        text, text_lengths = x\n        \n        embedded = self.embedding(text)\n        \n        #pack sequence\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True, enforce_sorted = False)\n        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n\n        #unpack sequence\n        out, out_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n        \n        # hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n        # output = self.fc1(hidden)\n        # output = self.dropout(self.fc2(output))\n                \n        #hidden = [batch size, hid dim * num directions]\n\n        # # Each sequence \"x\" is passed through an embedding layer\n        # out = self.embedding(x) \n\n        # Feed LSTMs\n        # out, (hidden, cell) = self.lstm(out)\n        out = self.dropout(out)\n\n        # The last hidden state is taken\n        out = torch.relu_(self.fc1(out[:,-1,:]))\n        out = self.dropout(out)\n        out = torch.sigmoid(self.fc2(out))\n            \n        return out\n"}