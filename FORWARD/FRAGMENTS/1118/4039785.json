{"BEFORE":"        x = x[:, :, 1:] - x[:, :, :-1]\n\n        # prepare exponential alpha\n\n        alpha = self.alpha.sigmoid()\n        alpha = rearrange(alpha, 'h -> h 1')\n\n        # arange == powers\n\n        arange = torch.arange(n, device = device)\n        weights = alpha * (1 - alpha) ** torch.flip(arange, dims = (0,))\n\n        weights = repeat(weights, '... l -> ... t l', t = n)\n        indices = repeat(arange, 'l -> h t l', h = h, t = n)\n\n        indices = (indices - rearrange(arange + 1, 't -> 1 t 1')) % n\n\n        weights = weights.gather(-1, indices)\n        weights = self.dropout(weights)\n\n        # causal\n\n        weights = weights.tril()\n\n        # multiply\n\n        output = einsum('b h n d, h m n -> b h m d', x, weights)\n","AFTER":"    def forward(self, x, naive = False):\n        b, n, d, h, device = *x.shape, self.heads, x.device\n\n        # linear project in\n\n        x = self.project_in(x)\n\n        # split out heads\n\n        x = rearrange(x, 'b n (h d) -> b h n d', h = h)\n\n        # temporal difference\n\n        x = torch.cat((\n            repeat(self.initial_state, 'h d -> b h 1 d', b = b),\n            x\n        ), dim = -2)\n\n        x = x[:, :, 1:] - x[:, :, :-1]\n\n        # prepare exponential alpha\n\n        alpha = self.alpha.sigmoid()\n        alpha = rearrange(alpha, 'h -> h 1')\n\n        # arange == powers\n\n        arange = torch.arange(n, device = device)\n        weights = alpha * (1 - alpha) ** torch.flip(arange, dims = (0,))\n\n        if naive:\n            output = self.naive_Aes(x, weights)\n        else:\n            output = conv1d_fft(x, weights)\n\n        # get initial state contribution\n\n        init_weight = (1 - alpha) ** (arange + 1)\n"}