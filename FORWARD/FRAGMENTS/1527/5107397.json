{"BEFORE":"        A = x[:, ::2, :, :]\n        B = x[:, 1::2, :, :]\n\n        # A = A.reshape(N, 1, ord_num * H * W)\n        # B = B.reshape(N, 1, ord_num * H * W)\n        A = A.unsqueeze(dim=1)\n        B = B.unsqueeze(dim=1)\n        concat_feats = torch.cat((A, B), dim=1)\n\n        if self.training:\n            prob = F.log_softmax(concat_feats, dim=1)\n            ord_prob = x.clone()\n            ord_prob[:, 0::2, :, :] = prob[:, 0, :, :, :]\n            ord_prob[:, 1::2, :, :] = prob[:, 1, :, :, :]\n            return ord_prob\n\n        ord_prob = F.softmax(concat_feats, dim=1)[:, 0, ::]\n        ord_label = torch.sum((ord_prob > 0.5), dim=1).reshape((N, 1, H, W))\n","AFTER":"        ord_num = C \/\/ 2\n\n        # implementation according to the paper\n        # A = x[:, ::2, :, :]\n        # B = x[:, 1::2, :, :]\n        #\n        # # A = A.reshape(N, 1, ord_num * H * W)\n        # # B = B.reshape(N, 1, ord_num * H * W)\n        # A = A.unsqueeze(dim=1)\n        # B = B.unsqueeze(dim=1)\n        # concat_feats = torch.cat((A, B), dim=1)\n        #\n        # if self.training:\n        #     prob = F.log_softmax(concat_feats, dim=1)\n        #     ord_prob = x.clone()\n        #     ord_prob[:, 0::2, :, :] = prob[:, 0, :, :, :]\n        #     ord_prob[:, 1::2, :, :] = prob[:, 1, :, :, :]\n        #     return ord_prob\n        #\n        # ord_prob = F.softmax(concat_feats, dim=1)[:, 0, ::]\n        # ord_label = torch.sum((ord_prob > 0.5), dim=1).reshape((N, 1, H, W))\n        # return ord_prob, ord_label\n\n        # reimplementation for fast speed.\n\n        x = x.view(-1, 2, ord_num, H, W)\n        if self.training:\n            prob = F.log_softmax(x, dim=1).view(N, C, H, W)\n            return prob\n\n        ord_prob = F.softmax(x, dim=1)[:, 0, :, :, :]\n        ord_label = torch.sum((ord_prob > 0.5), dim=1)\n"}