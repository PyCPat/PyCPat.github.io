<html><h3>Pattern ID :439
</h3><img src='1667033.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        sig_t = torch.nn.functional.softplus(b_t) + self.eps

        mu_t = self.mu_prev + torch.nn.functional.softplus(k_t)
        g_t = <a id="change">torch.softmax(</a>g_t<a id="change">, dim=-1)</a><a id="change"> / sig_t</a> + self.eps

        &#47&#47 each B x K x T_in
        j = self.J[:<a id="change">inputs.size(1</a><a id="change">)</a>]

        &#47&#47 attention weights
        phi_t = g_t.unsqueeze(-1) * <a id="change">torch.exp(</a>-<a id="change">0.5 * (mu_t.unsqueeze(-1) - j)**2 / </a>(<a id="change">sig_t.unsqueeze(-1</a><a id="change">)</a><a id="change">**2</a>)<a id="change">)</a>
        alpha_t = self.COEF<a id="change"> * torch.sum(</a>phi_t, <a id="change">1</a><a id="change">)</a>

        &#47&#47 apply masking
        if mask is not None:
            alpha_t.data.masked_fill_(~mask, self._mask_value)</code></pre><h3>After Change</h3><pre><code class='java'>
        sig_t = torch.nn.functional.softplus(b_t) + self.eps

        mu_t = self.mu_prev + torch.nn.functional.softplus(k_t)
        g_t = <a id="change">torch.softmax(</a>g_t<a id="change">, dim=-1)</a> + self.eps

        j = self.J[:<a id="change">inputs.size(1</a><a id="change">)</a><a id="change">+1</a>]

        &#47&#47 attention weights
        phi_t = g_t.unsqueeze(-1) * (<a id="change">1</a><a id="change"> / </a>(1<a id="change"> + </a><a id="change">torch.sigmoid(</a>(<a id="change">mu_t.unsqueeze(</a>-1<a id="change">)</a><a id="change"> - </a>j)<a id="change"> / sig_t.unsqueeze(-1</a><a id="change">)</a><a id="change">)</a>))

        &#47&#47 discritize attention weights
        <a id="change">alpha_t</a> = <a id="change">torch.sum(</a>phi_t, <a id="change">1</a><a id="change">)</a>
        <a id="change">alpha_t = alpha_t[:, 1:]</a><a id="change"> - alpha_t[:, :-1]</a>
        <a id="change">alpha_t[alpha_t == 0] = 1e-8</a>

        &#47&#47 apply masking
        if mask is not None:
            alpha_t.data.masked_fill_(~mask, self._mask_value)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 37</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/coqui-ai/tts/commit/0d17019d224c7db47c2370088e35986e2e8c69af#diff-1ca76b8dab5cda419aaf68884653245f0428af8e57cdb707fc538936bb59af29L163' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 1667033</div><div id='project'> Project Name: coqui-ai/tts</div><div id='commit'> Commit Name: 0d17019d224c7db47c2370088e35986e2e8c69af</div><div id='time'> Time: 2020-02-19</div><div id='author'> Author: root@sp-mlc3-5423-0.mlc</div><div id='file'> File Name: layers/common_layers.py</div><div id='m_class'> M Class Name: GravesAttention</div><div id='n_method'> N Class Name: GravesAttention</div><div id='m_method'> M Method Name: forward(5)</div><div id='n_method'> N Method Name: forward(5)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: layers/common_layers.py</div><div id='n_file'> N File Name: layers/common_layers.py</div><div id='m_start'> M Start Line: 420</div><div id='m_end'> M End Line: 435</div><div id='n_start'> N Start Line: 163</div><div id='n_end'> N End Line: 181</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        k_t = gbk_t[:, 2, :]

        &#47&#47 attention GMM parameters
        <a id="change">sig_t</a> = torch.nn.functional.softplus(b_t) + self.eps

        mu_t = self.mu_prev + torch.nn.functional.softplus(k_t)
        g_t = <a id="change">torch.softmax(</a>g_t<a id="change">, dim=-1)</a><a id="change"> / </a>sig_t + self.eps

        &#47&#47 each B x K x T_in
        j = self.J[:<a id="change">inputs.size(1</a><a id="change">)</a>]

        &#47&#47 attention weights
        phi_t = g_t.unsqueeze(-1) * <a id="change">torch.exp(</a>-<a id="change">0.5 * (mu_t.unsqueeze(-1) - j)**2 / </a>(<a id="change">sig_t.unsqueeze(-1</a><a id="change">)</a><a id="change">**2</a>)<a id="change">)</a>
        alpha_t = self.COEF<a id="change"> * torch.sum(</a>phi_t, <a id="change">1</a><a id="change">)</a>

        &#47&#47 apply masking
        if mask is not None:
            alpha_t.data.masked_fill_(~mask, self._mask_value)</code></pre><h3>After Change</h3><pre><code class='java'>
        sig_t = torch.nn.functional.softplus(b_t) + self.eps

        mu_t = self.mu_prev + torch.nn.functional.softplus(k_t)
        g_t = <a id="change">torch.softmax(</a>g_t<a id="change">, dim=-1)</a> + self.eps

        j = self.J[:<a id="change">inputs.size(1</a><a id="change">)</a><a id="change">+1</a>]

        &#47&#47 attention weights
        phi_t = g_t.unsqueeze(-1) * (<a id="change">1</a><a id="change"> / </a>(1<a id="change"> + </a><a id="change">torch.sigmoid(</a>(<a id="change">mu_t.unsqueeze(</a>-1<a id="change">)</a><a id="change"> - </a>j)<a id="change"> / sig_t.unsqueeze(-1</a><a id="change">)</a><a id="change">)</a>))

        &#47&#47 discritize attention weights
        <a id="change">alpha_t</a> = <a id="change">torch.sum(</a>phi_t, <a id="change">1</a><a id="change">)</a>
        <a id="change">alpha_t = </a><a id="change">alpha_t[:, 1:]</a><a id="change"> - alpha_t[:, :-1]</a>
        <a id="change">alpha_t[alpha_t == 0] = 1e-8</a>

        &#47&#47 apply masking
        if mask is not None:
            alpha_t.data.masked_fill_(~mask, self._mask_value)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/coqui-ai/tts/commit/0d17019d224c7db47c2370088e35986e2e8c69af#diff-1ca76b8dab5cda419aaf68884653245f0428af8e57cdb707fc538936bb59af29L407' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 1667065</div><div id='project'> Project Name: coqui-ai/tts</div><div id='commit'> Commit Name: 0d17019d224c7db47c2370088e35986e2e8c69af</div><div id='time'> Time: 2020-02-19</div><div id='author'> Author: root@sp-mlc3-5423-0.mlc</div><div id='file'> File Name: layers/common_layers.py</div><div id='m_class'> M Class Name: GravesAttention</div><div id='n_method'> N Class Name: GravesAttention</div><div id='m_method'> M Method Name: forward(5)</div><div id='n_method'> N Method Name: forward(5)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: layers/common_layers.py</div><div id='n_file'> N File Name: layers/common_layers.py</div><div id='m_start'> M Start Line: 420</div><div id='m_end'> M End Line: 435</div><div id='n_start'> N Start Line: 163</div><div id='n_end'> N End Line: 181</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        k_t = gbk_t[:, 2, :]

        &#47&#47 attention GMM parameters
        <a id="change">sig_t</a> = torch.nn.functional.softplus(b_t) + self.eps

        mu_t = self.mu_prev + torch.nn.functional.softplus(k_t)
        g_t = <a id="change">torch.softmax(</a>g_t<a id="change">, dim=-1)</a><a id="change"> / </a>sig_t + self.eps

        &#47&#47 each B x K x T_in
        j = self.J[:<a id="change">inputs.size(1</a><a id="change">)</a>]

        &#47&#47 attention weights
        phi_t = g_t.unsqueeze(-1) * <a id="change">torch.exp(</a>-<a id="change">0.5 * (mu_t.unsqueeze(-1) - j)**2 / </a>(<a id="change">sig_t.unsqueeze(-1</a><a id="change">)</a><a id="change">**2</a>)<a id="change">)</a>
        alpha_t = self.COEF<a id="change"> * torch.sum(</a>phi_t, <a id="change">1</a><a id="change">)</a>

        &#47&#47 apply masking
        if mask is not None:
            alpha_t.data.masked_fill_(~mask, self._mask_value)</code></pre><h3>After Change</h3><pre><code class='java'>
        sig_t = torch.nn.functional.softplus(b_t) + self.eps

        mu_t = self.mu_prev + torch.nn.functional.softplus(k_t)
        g_t = <a id="change">torch.softmax(</a>g_t<a id="change">, dim=-1)</a> + self.eps

        j = self.J[:<a id="change">inputs.size(1</a><a id="change">)</a><a id="change">+1</a>]

        &#47&#47 attention weights
        phi_t = g_t.unsqueeze(-1) * (<a id="change">1</a><a id="change"> / </a>(1<a id="change"> + </a><a id="change">torch.sigmoid(</a>(<a id="change">mu_t.unsqueeze(</a>-1<a id="change">)</a><a id="change"> - </a>j)<a id="change"> / sig_t.unsqueeze(-1</a><a id="change">)</a><a id="change">)</a>))

        &#47&#47 discritize attention weights
        <a id="change">alpha_t</a> = <a id="change">torch.sum(</a>phi_t, <a id="change">1</a><a id="change">)</a>
        <a id="change">alpha_t = </a><a id="change">alpha_t[:, 1:]</a><a id="change"> - alpha_t[:, :-1]</a>
        <a id="change">alpha_t[alpha_t == 0] = 1e-8</a>

        &#47&#47 apply masking
        if mask is not None:
            alpha_t.data.masked_fill_(~mask, self._mask_value)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/coqui-ai/tts/commit/7a616aa9ef85fa4833eff6b78fd8e155a152a002#diff-1ca76b8dab5cda419aaf68884653245f0428af8e57cdb707fc538936bb59af29L407' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 1667064</div><div id='project'> Project Name: coqui-ai/tts</div><div id='commit'> Commit Name: 7a616aa9ef85fa4833eff6b78fd8e155a152a002</div><div id='time'> Time: 2020-01-27</div><div id='author'> Author: root@sp-mlc3-5423-0.mlc</div><div id='file'> File Name: layers/common_layers.py</div><div id='m_class'> M Class Name: GravesAttention</div><div id='n_method'> N Class Name: GravesAttention</div><div id='m_method'> M Method Name: forward(5)</div><div id='n_method'> N Method Name: forward(5)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: layers/common_layers.py</div><div id='n_file'> N File Name: layers/common_layers.py</div><div id='m_start'> M Start Line: 420</div><div id='m_end'> M End Line: 435</div><div id='n_start'> N Start Line: 163</div><div id='n_end'> N End Line: 181</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        k_t = gbk_t[:, 2, :]

        &#47&#47 attention GMM parameters
        <a id="change">sig_t</a> = torch.nn.functional.softplus(b_t) + self.eps

        mu_t = self.mu_prev + torch.nn.functional.softplus(k_t)
        g_t = <a id="change">torch.softmax(</a>g_t<a id="change">, dim=-1)</a><a id="change"> / </a>sig_t + self.eps

        &#47&#47 each B x K x T_in
        j = self.J[:<a id="change">inputs.size(1</a><a id="change">)</a>]

        &#47&#47 attention weights
        phi_t = g_t.unsqueeze(-1) * <a id="change">torch.exp(</a>-<a id="change">0.5 * (mu_t.unsqueeze(-1) - j)**2 / </a>(<a id="change">sig_t.unsqueeze(-1</a><a id="change">)</a><a id="change">**2</a>)<a id="change">)</a>
        alpha_t = self.COEF<a id="change"> * torch.sum(</a>phi_t, <a id="change">1</a><a id="change">)</a>

        &#47&#47 apply masking
        if mask is not None:
            alpha_t.data.masked_fill_(~mask, self._mask_value)</code></pre><h3>After Change</h3><pre><code class='java'>
        sig_t = torch.nn.functional.softplus(b_t) + self.eps

        mu_t = self.mu_prev + torch.nn.functional.softplus(k_t)
        g_t = <a id="change">torch.softmax(</a>g_t<a id="change">, dim=-1)</a> + self.eps

        j = self.J[:<a id="change">inputs.size(1</a><a id="change">)</a><a id="change">+1</a>]

        &#47&#47 attention weights
        phi_t = g_t.unsqueeze(-1) * (<a id="change">1</a><a id="change"> / </a>(1<a id="change"> + </a><a id="change">torch.sigmoid(</a>(<a id="change">mu_t.unsqueeze(</a>-1<a id="change">)</a><a id="change"> - </a>j)<a id="change"> / sig_t.unsqueeze(-1</a><a id="change">)</a><a id="change">)</a>))

        &#47&#47 discritize attention weights
        <a id="change">alpha_t</a> = <a id="change">torch.sum(</a>phi_t, <a id="change">1</a><a id="change">)</a>
        <a id="change">alpha_t = </a><a id="change">alpha_t[:, 1:]</a><a id="change"> - alpha_t[:, :-1]</a>
        <a id="change">alpha_t[alpha_t == 0] = 1e-8</a>

        &#47&#47 apply masking
        if mask is not None:
            alpha_t.data.masked_fill_(~mask, self._mask_value)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/coqui-ai/tts/commit/bb1117ff32d91a9ba32710810391e062596f62b7#diff-1ca76b8dab5cda419aaf68884653245f0428af8e57cdb707fc538936bb59af29L407' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 1667038</div><div id='project'> Project Name: coqui-ai/tts</div><div id='commit'> Commit Name: bb1117ff32d91a9ba32710810391e062596f62b7</div><div id='time'> Time: 2020-02-19</div><div id='author'> Author: root@sp-mlc3-5423-0.mlc</div><div id='file'> File Name: layers/common_layers.py</div><div id='m_class'> M Class Name: GravesAttention</div><div id='n_method'> N Class Name: GravesAttention</div><div id='m_method'> M Method Name: forward(5)</div><div id='n_method'> N Method Name: forward(5)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: layers/common_layers.py</div><div id='n_file'> N File Name: layers/common_layers.py</div><div id='m_start'> M Start Line: 420</div><div id='m_end'> M End Line: 435</div><div id='n_start'> N Start Line: 162</div><div id='n_end'> N End Line: 180</div><BR>