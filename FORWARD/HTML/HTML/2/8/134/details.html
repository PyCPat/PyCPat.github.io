<html><h3>Pattern ID :134
</h3><img src='491926.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            )
        )

        <a id="change">return </a>tight_loss<a id="change">, logs</a>

    def get_distortion(self, Y_hat, targets):
        raise NotImplementedError()
</code></pre><h3>After Change</h3><pre><code class='java'>
            distortion = torch.logsumexp(distortion, 0) - math.log(n_z)
        else:
            distortion = distortion.squeeze(0)
            rate<a id="change"> = </a><a id="change">rate.squeeze(0</a><a id="change">)</a>

        &#47&#47 E_x[...]. shape: shape: []
        rate = rate.mean(0)
        distortion<a id="change"> = </a>distortion.mean(0)
        loss = distortion + self.beta * rate

        logs.update(
            dict(
                loose_loss=loose_loss / math.log(BASE_LOG),
                loss=loss / math.log(BASE_LOG),
                rate=rate / math.log(BASE_LOG),
                distortion=distortion / math.log(BASE_LOG),
            )
        )

        <a id="change">return </a>loss<a id="change">, logs</a>

    def get_distortion(self, Y_hat, targets):
        raise NotImplementedError()
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/yanndubs/lossyless/commit/0a24755494712f360cfbc0e8b8ac9f6907157997#diff-a23e2e04059e40be395b9c191a83e67f186f919edfe19507cc3f4a8876a7a250L30' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 491926</div><div id='project'> Project Name: yanndubs/lossyless</div><div id='commit'> Commit Name: 0a24755494712f360cfbc0e8b8ac9f6907157997</div><div id='time'> Time: 2020-12-04</div><div id='author'> Author: yanndubois96@gmail.com</div><div id='file'> File Name: lossyless/losses.py</div><div id='m_class'> M Class Name: Loss</div><div id='n_method'> N Class Name: Loss</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: lossyless/losses.py</div><div id='n_file'> N File Name: lossyless/losses.py</div><div id='m_start'> M Start Line: 35</div><div id='m_end'> M End Line: 53</div><div id='n_start'> N Start Line: 30</div><div id='n_end'> N End Line: 59</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                processed_inputs=None, mask=None, inputs_lengths=None):

        if processed_inputs is None:
            processed_inputs<a id="change"> = </a>inputs

        if inputs_lengths is not None and mask is None:
            mask = get_mask_from_lengths(inputs, inputs_lengths)

        &#47&#47 Alignment
        &#47&#47 (batch, max_time)
        &#47&#47 e_{ij} = a(s_{i-1}, h_j)
        &#47&#47 import ipdb
        &#47&#47 ipdb.set_trace()
        alignment = self.alignment_model(cell_state, processed_inputs)

        if mask is not None:
            mask = mask.view(query.size(0), -1)
            alignment.data.masked_fill_(mask, self.score_mask_value)

        &#47&#47 Normalize context_vec weight
        alignment = F.softmax(alignment, dim=-1)

        &#47&#47 Attention context vector
        &#47&#47 (batch, 1, dim)
        &#47&#47 c_i = \sum_{j=1}^{T_x} \alpha_{ij} h_j
        context_vec = torch.bmm(alignment.unsqueeze(1), inputs)
        context_vec = context_vec.squeeze(1)

        &#47&#47 Concat input query and previous context_vec context
        cell_input = torch.cat((query, context_vec), -1)
        &#47&#47cell_input = cell_input.unsqueeze(1)

        &#47&#47 Feed it to RNN
        &#47&#47 s_i = f(y_{i-1}, c_{i}, s_{i-1})
        cell_output = self.rnn_cell(cell_input, cell_state)

        context_vec = context_vec.squeeze(1)
        <a id="change">return </a>cell_output<a id="change">, context_vec, alignment</a>


</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 s_i = f(y_{i-1}, c_{i}, s_{i-1})
        rnn_output = self.rnn_cell(rnn_input, rnn_state)

        context<a id="change"> = </a><a id="change">context.squeeze(1</a><a id="change">)</a>
        <a id="change">return </a>rnn_output<a id="change">, context, alignment</a>


</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/coqui-ai/tts/commit/b4032e8dffc2addf976468826129edea2b459ead#diff-d280da42b761c3f70383c398535500cfb7dcb7e40c79196cc732c1f2bd12bf39L54' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 491923</div><div id='project'> Project Name: coqui-ai/tts</div><div id='commit'> Commit Name: b4032e8dffc2addf976468826129edea2b459ead</div><div id='time'> Time: 2018-03-07</div><div id='author'> Author: egolge@mozilla.com</div><div id='file'> File Name: layers/attention.py</div><div id='m_class'> M Class Name: AttentionWrapper</div><div id='n_method'> N Class Name: AttentionRNN</div><div id='m_method'> M Method Name: forward(7)</div><div id='n_method'> N Method Name: forward(8)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: layers/attention.py</div><div id='n_file'> N File Name: layers/attention.py</div><div id='m_start'> M Start Line: 55</div><div id='m_end'> M End Line: 92</div><div id='n_start'> N Start Line: 56</div><div id='n_end'> N End Line: 89</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 Alignment
        &#47&#47 (batch, max_time)
        alignment<a id="change"> = </a>self.attention_mechanism(cell_output, processed_memory)

        if mask is not None:
            mask = mask.view(query.size(0), -1)
            alignment.data.masked_fill_(mask, self.score_mask_value)

        &#47&#47 Normalize attention weight
        alignment = F.softmax(alignment, dim=-1)

        &#47&#47 Attention context vector
        &#47&#47 (batch, 1, dim)
        attention = torch.bmm(alignment.unsqueeze(1), memory)

        &#47&#47 (batch, dim)
        attention = attention.squeeze(1)

        <a id="change">return </a>cell_output<a id="change">, attention, alignment</a>

</code></pre><h3>After Change</h3><pre><code class='java'>
        context_vec = torch.bmm(alignment.unsqueeze(1), memory)

        &#47&#47 (batch, dim)
        context_vec<a id="change"> = </a><a id="change">context_vec.squeeze(1</a><a id="change">)</a>

        <a id="change">return </a>cell_output<a id="change">, context_vec, alignment</a>


</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/coqui-ai/tts/commit/2fd37a5bad9374002873db26927694dd0d329e90#diff-d280da42b761c3f70383c398535500cfb7dcb7e40c79196cc732c1f2bd12bf39L54' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 491930</div><div id='project'> Project Name: coqui-ai/tts</div><div id='commit'> Commit Name: 2fd37a5bad9374002873db26927694dd0d329e90</div><div id='time'> Time: 2018-02-05</div><div id='author'> Author: egolge@mozilla.com</div><div id='file'> File Name: layers/attention.py</div><div id='m_class'> M Class Name: AttentionWrapper</div><div id='n_method'> N Class Name: AttentionWrapper</div><div id='m_method'> M Method Name: forward(8)</div><div id='n_method'> N Method Name: forward(8)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: layers/attention.py</div><div id='n_file'> N File Name: layers/attention.py</div><div id='m_start'> M Start Line: 57</div><div id='m_end'> M End Line: 86</div><div id='n_start'> N Start Line: 58</div><div id='n_end'> N End Line: 89</div><BR>