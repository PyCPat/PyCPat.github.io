<html><h3>Pattern ID :1089
</h3><img src='3956796.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 masked by the missing entries
        &#47&#47 note, different batch may contain different number of real entities
        tensor_list = <a id="change">[]</a>
        for i, batch in enumerate(out):
            mean_entity = 0.
            real_number = real_number_tensor[i]
            real_number = real_number if real_number != 0 else 1
            for j, entity in enumerate(batch):
                if j &gt;= real_number_tensor[i]:
                    break        
                mean_entity<a id="change"> = </a>mean_entity + entity
            mean_entity = mean_entity / (real_number)
            <a id="change">tensor_list.append(</a>mean_entity.reshape(1, -1)<a id="change">)</a>
        tensor_mean = torch.cat(tensor_list, dim=0)
        print(&quottensor_mean:&quot, tensor_mean) if debug else None
</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 generate the mask for transformer
        mask = torch.arange(0, self.max_entities).float()
        mask<a id="change"> = </a>mask.repeat(batch_size, 1)
        mask = mask &lt; entity_num.unsqueeze(dim=1)

        print(&quotmask:&quot, mask) if debug else None
        print(&quotmask.shape:&quot, mask.shape) if debug else None

        &#47&#47 mask: [batch_size, max_entities]
        device = next(self.parameters()).device
        mask = mask.to(device)

        &#47&#47 assert the input shape is : batch_seq_size x entities_size x embeding_size
        &#47&#47 note: because the feature size of entity is not equal to 256, so it can not fed into transformer directly.
        &#47&#47 thus, we add a embedding layer to transfer it to right size.
        &#47&#47 x is batch_entities_tensor (dim = 3). Shape: batch_seq_size x entities_size x embeding_size
        x = self.embedd(x)
        print(&quotx.shape:&quot, x.shape) if debug else None

        &#47&#47 mask for transformer need a special format
        mask_seq_len = <a id="change">mask.shape[-1]</a>
        tran_mask<a id="change"> = </a>mask.unsqueeze(1)

        &#47&#47 tran_mask: [batch_seq_size x max_entities x max_entities]
        tran_mask<a id="change"> = </a>tran_mask.repeat(1, mask_seq_len, 1)

        &#47&#47 out: [batch_seq_size x entities_size x embeding_size]
        out = self.transformer(x, mask=tran_mask)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/liuruoze/mini-alphastar/commit/612d42a7bf5ef827e1e919198d839fce106155cd#diff-71869e70f9cb835282d541d2d3529d25b6ed6795c79bc770e860e9318619f6b1L716' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 3956796</div><div id='project'> Project Name: liuruoze/mini-alphastar</div><div id='commit'> Commit Name: 612d42a7bf5ef827e1e919198d839fce106155cd</div><div id='time'> Time: 2021-11-25</div><div id='author'> Author: liuruoze@163.com</div><div id='file'> File Name: alphastarmini/core/arch/entity_encoder.py</div><div id='m_class'> M Class Name: EntityEncoder</div><div id='n_method'> N Class Name: EntityEncoder</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: alphastarmini/core/arch/entity_encoder.py</div><div id='n_file'> N File Name: alphastarmini/core/arch/entity_encoder.py</div><div id='m_start'> M Start Line: 721</div><div id='m_end'> M End Line: 758</div><div id='n_start'> N Start Line: 716</div><div id='n_end'> N End Line: 778</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        return nn.Sequential(*layers)

    def forward(self, x):  &#47&#47 224x224
        features = <a id="change">[]</a>
        x = self.conv1(x)  &#47&#47 112x112
        features.append(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)  &#47&#47 56x56 ignore

        x<a id="change"> = </a>self.layer1(x)  &#47&#47 56x56
        <a id="change">features.append(</a>x<a id="change">)</a>
        x = self.layer2(x)  &#47&#47 28x28
        features.append(x)
        x<a id="change"> = </a>self.layer3(x)  &#47&#47 14x14 ignore (maybe not)
        features.append(x)
        x = self.layer4(x)  &#47&#47 7x7
        features.append(x)

        if not self.include_top:
            return features

        x = self.avgpool(x)  &#47&#47 1x1
        features.append(x)

        x<a id="change"> = </a>x.view(x.size(0), -1)
        x = self.fc(x)
        return x
</code></pre><h3>After Change</h3><pre><code class='java'>
        Normalizing the features and applying spatial resolution was taken from LPIPS and wasn&quott mentioned in the paper.
        
        images = torch.concat([x, x_rec], dim=0)  &#47&#47 batch
        features<a id="change"> = </a>self._forward(images)
        features = [f.chunk(2) for f in features]
        &#47&#47 diffs = [a * torch.abs(p[0] - p[1]).sum() for a, p in zip(self.alphas, features)]
        diffs = [a * torch.abs(<a id="change">p[0]</a> - p[1]).mean() for a, p in zip(self.alphas, features)]
        &#47&#47 diffs = [a*torch.abs(self.norm_tensor(tf) - self.norm_tensor(rf)) for a, tf, rf in zip(self.alphas, true_features, rec_features)]

        &#47&#47 diffs = [a * torch.mean(torch.abs(tf - rf)) for a, tf, rf in zip(self.alphas, features)]</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/casualganpapers/make-a-scene/commit/89ba77e885ac1c12ac2d5df5a6b3da842e30bfe0#diff-1ccb21e596d4cb9b26cee32d3e64f968bb2dc277d3d4514cc0cfe9ff65e9ef08L126' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 3956792</div><div id='project'> Project Name: casualganpapers/make-a-scene</div><div id='commit'> Commit Name: 89ba77e885ac1c12ac2d5df5a6b3da842e30bfe0</div><div id='time'> Time: 2022-05-26</div><div id='author'> Author: 61938694+dome272@users.noreply.github.com</div><div id='file'> File Name: losses/face_loss.py</div><div id='m_class'> M Class Name: ResNet</div><div id='n_method'> N Class Name: ResNet</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: losses/face_loss.py</div><div id='n_file'> N File Name: losses/face_loss.py</div><div id='m_start'> M Start Line: 127</div><div id='m_end'> M End Line: 151</div><div id='n_start'> N Start Line: 163</div><div id='n_end'> N End Line: 177</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        )

    def forward(self, input, hidden_state=None):
        output = <a id="change">[]</a>
        for step in range(input.size(1)):
            &#47&#47 Compute current time-step
            hidden_state<a id="change"> = </a>self.rnn_cell(input[:, step, :, :, :], hidden_state)
            <a id="change">output.append(</a>hidden_state<a id="change">)</a>
        &#47&#47 Stack the list of output hidden states into a tensor
        output = torch.stack(output, 0)
        return output
</code></pre><h3>After Change</h3><pre><code class='java'>
        last_state_list, layer_output
        
        input = self.input_dp(input)
        cur_layer_input<a id="change"> = </a>torch.unbind(input, dim=int(self.batch_first))

        if hidden_state is None:
            hidden_state<a id="change"> = </a>self.get_init_states(<a id="change">cur_layer_input[0]</a>)

        seq_len = len(cur_layer_input)

        layer_output_list = []
        last_state_list = []

        for l, (gru_cell, hid_dp) in enumerate(zip(self.cell_list, self.hidden_dps)):
            h<a id="change"> = </a>hidden_state[l]
            output_inner = []
            for t in range(seq_len):
                h = gru_cell(input=cur_layer_input[t], h_prev=h)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/openclimatefix/skillful_nowcasting/commit/02c5ceadd01484d6ac8bce848ff76446fe7a6917#diff-cee00a565d888c684ec40bd3eac27669aa3223b223a80ae9f871383a9fb64917L268' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 3955387</div><div id='project'> Project Name: openclimatefix/skillful_nowcasting</div><div id='commit'> Commit Name: 02c5ceadd01484d6ac8bce848ff76446fe7a6917</div><div id='time'> Time: 2021-10-18</div><div id='author'> Author: jacob@bieker.tech</div><div id='file'> File Name: nowcasting_gan/layers/ConvGRU.py</div><div id='m_class'> M Class Name: ConvGRU</div><div id='n_method'> N Class Name: ConvGRU</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: nowcasting_gan/layers/ConvGRU.py</div><div id='n_file'> N File Name: nowcasting_gan/layers/ConvGRU.py</div><div id='m_start'> M Start Line: 269</div><div id='m_end'> M End Line: 276</div><div id='n_start'> N Start Line: 196</div><div id='n_end'> N End Line: 221</div><BR>