<html><h3>Pattern ID :1217
</h3><img src='4382594.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        )

        if self.combining_operation == "mean":
            e = <a id="change">iid_embeddings.mean(dim=1)</a>
        elif self.combining_operation == "sum":
            e = iid_embeddings.sum(dim=1)
        else:
            raise ValueError("combining_operation must be in [&quotsum&quot, &quotmean&quot].")</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 otherwise we need to loop over the batch to account for varying trial lengths
        else:
            combined_embedding<a id="change"> = </a><a id="change">[]</a>
            trial_counts = torch.zeros(batch, 1)
            <a id="change">for </a>i in range(batch)<a id="change">:
                &#47&#47 remove NaNs
                </a>valid_x = x[i, ~torch.isnan(x[i, :, 0]), :]
                trial_counts[i]<a id="change"> = </a>valid_x.shape[0]
                trial_embeddings = self.trial_net(valid_x)
                &#47&#47 apply combining operation over permutation dimension
                <a id="change">combined_embedding.append(
                    </a>self.combining_function(trial_embeddings, dim=0)<a id="change">
                )</a>

            combined_embedding<a id="change"> = </a>torch.stack(combined_embedding, dim=0)

        assert not torch.isnan(combined_embedding).any(), "NaNs in embedding."
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/mackelab/sbi/commit/1352e77bdbc47aa4a4130679903b57672e48218c#diff-672ba10e6c3065a6f5554033b9b173c4fe88f4c05ffc31c70a9198691e6c6659L271' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4382594</div><div id='project'> Project Name: mackelab/sbi</div><div id='commit'> Commit Name: 1352e77bdbc47aa4a4130679903b57672e48218c</div><div id='time'> Time: 2023-03-01</div><div id='author'> Author: jan.boelts@tum.de</div><div id='file'> File Name: sbi/neural_nets/embedding_nets.py</div><div id='m_class'> M Class Name: PermutationInvariantEmbedding</div><div id='n_method'> N Class Name: PermutationInvariantEmbedding</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: sbi/neural_nets/embedding_nets.py</div><div id='n_file'> N File Name: sbi/neural_nets/embedding_nets.py</div><div id='m_start'> M Start Line: 271</div><div id='m_end'> M End Line: 284</div><div id='n_start'> N Start Line: 277</div><div id='n_end'> N End Line: 304</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            xq = xq + _xq
            codes.append(_codes)
        codes = torch.cat(codes, 1)
        return xq, <a id="change">(x - xq).pow(2).mean()</a>, codes

    def encode(self, x):
        return self.forward(x)[-1]</code></pre><h3>After Change</h3><pre><code class='java'>
        residual = x

        all_losses = []
        all_indices<a id="change"> = </a><a id="change">[]</a>

        n_q = n_q or len(self.layers)

        <a id="change">for </a><a id="change">layer</a> in self.layers[:n_q]<a id="change">:
            </a>quantized, indices, loss = layer(residual)
            residual<a id="change"> = </a>residual - quantized
            quantized_out = quantized_out + quantized

            <a id="change">all_indices.append(</a>indices<a id="change">)</a>
            all_losses.append(loss)

        out_losses<a id="change">, out_indices = </a>map(torch.stack, (all_losses, all_indices))
        return quantized_out, sum(out_losses), out_indices.permute(1, 0, 2)

    def encode(self,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/acids-ircam/rave/commit/b58cba5d330c227f2122bc07fcbf7ed068eb91be#diff-0642a8a2acbceb830eb07caddb975242233188021b9f16fec4b7168271bf7749L106' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4382595</div><div id='project'> Project Name: acids-ircam/rave</div><div id='commit'> Commit Name: b58cba5d330c227f2122bc07fcbf7ed068eb91be</div><div id='time'> Time: 2023-01-24</div><div id='author'> Author: caillon@ircam.fr</div><div id='file'> File Name: rave/quantization.py</div><div id='m_class'> M Class Name: ResidualVQ</div><div id='n_method'> N Class Name: ResidualVectorQuantization</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: rave/quantization.py</div><div id='n_file'> N File Name: rave/quantization.py</div><div id='m_start'> M Start Line: 106</div><div id='m_end'> M End Line: 116</div><div id='n_start'> N Start Line: 289</div><div id='n_end'> N End Line: 307</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        )

        if self.combining_operation == "mean":
            e = <a id="change">iid_embeddings.mean(dim=1)</a>
        elif self.combining_operation == "sum":
            e = iid_embeddings.sum(dim=1)
        else:
            raise ValueError("combining_operation must be in [&quotsum&quot, &quotmean&quot].")</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 otherwise we need to loop over the batch to account for varying trial lengths
        else:
            combined_embedding<a id="change"> = </a><a id="change">[]</a>
            trial_counts = torch.zeros(batch, 1)
            <a id="change">for </a><a id="change">i</a> in range(batch)<a id="change">:
                &#47&#47 remove NaNs
                </a>valid_x = x[i, ~torch.isnan(x[i, :, 0]), :]
                trial_counts[i]<a id="change"> = </a>valid_x.shape[0]
                trial_embeddings = self.trial_net(valid_x)
                &#47&#47 apply combining operation over permutation dimension
                <a id="change">combined_embedding.append(
                    </a>self.combining_function(trial_embeddings, dim=0)<a id="change">
                )</a>

            combined_embedding<a id="change"> = </a>torch.stack(combined_embedding, dim=0)

        assert not torch.isnan(combined_embedding).any(), "NaNs in embedding."
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/mackelab/sbi/commit/1352e77bdbc47aa4a4130679903b57672e48218c#diff-672ba10e6c3065a6f5554033b9b173c4fe88f4c05ffc31c70a9198691e6c6659L262' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4382585</div><div id='project'> Project Name: mackelab/sbi</div><div id='commit'> Commit Name: 1352e77bdbc47aa4a4130679903b57672e48218c</div><div id='time'> Time: 2023-03-01</div><div id='author'> Author: jan.boelts@tum.de</div><div id='file'> File Name: sbi/neural_nets/embedding_nets.py</div><div id='m_class'> M Class Name: PermutationInvariantEmbedding</div><div id='n_method'> N Class Name: PermutationInvariantEmbedding</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: sbi/neural_nets/embedding_nets.py</div><div id='n_file'> N File Name: sbi/neural_nets/embedding_nets.py</div><div id='m_start'> M Start Line: 271</div><div id='m_end'> M End Line: 284</div><div id='n_start'> N Start Line: 277</div><div id='n_end'> N End Line: 304</div><BR>