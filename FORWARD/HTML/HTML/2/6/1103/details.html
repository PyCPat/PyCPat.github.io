<html><h3>Pattern ID :1103
</h3><img src='4028031.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        idx_theta2 = torch.cat([idx_theta, idx_theta], dim=1)

        &#47&#47 Calculate $[-x^{(\frac{d}{2} + 1)}, -x^{(\frac{d}{2} + 2)}, ..., -x^{(d)}, x^{(1)}, x^{(2)}, ..., -x^{(\frac{d}{2})}]$
        neg_half_x = torch.cat([-<a id="change">x[:, :, :, d_2:]</a>, x[:, :, :, :d_2]], dim=-1)

        &#47&#47 Calculate
        &#47&#47</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 \end{align}
        &#47&#47
        &#47&#47 for $i \in {1, 2, ..., \frac{d}{2}}$
        x_rope = (x_rope<a id="change"> * </a>self.cos_cached[:x.shape[0]])<a id="change"> + </a>(neg_half_x * self.sin_cached[:x.shape[0]])

        &#47&#47
        <a id="change">return </a>torch.cat((x_rope, x_pass), dim=-1)


class RotaryPEMultiHeadAttention(MultiHeadAttention):</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lab-ml/nn/commit/0ce65adf9e602321109528b05cf99fccb16cd2de#diff-637497eb531dcbf4c4fff534c48c9dc4ec23ef7b14699edc95d8b280b04de206L132' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4028031</div><div id='project'> Project Name: lab-ml/nn</div><div id='commit'> Commit Name: 0ce65adf9e602321109528b05cf99fccb16cd2de</div><div id='time'> Time: 2022-06-03</div><div id='author'> Author: vpjayasiri@gmail.com</div><div id='file'> File Name: labml_nn/transformers/rope/__init__.py</div><div id='m_class'> M Class Name: RotaryPositionalEmbeddings</div><div id='n_method'> N Class Name: RotaryPositionalEmbeddings</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: labml_nn/transformers/rope/__init__.py</div><div id='n_file'> N File Name: labml_nn/transformers/rope/__init__.py</div><div id='m_start'> M Start Line: 132</div><div id='m_end'> M End Line: 163</div><div id='n_start'> N Start Line: 171</div><div id='n_end'> N End Line: 193</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        x = self.embedding(x)  &#47&#47 (batch_size, target_seq_len, d_model)
        x *= math.sqrt(self.d_model)
        x += <a id="change">self.pos_encoding[:, :seq_len, :]</a>
        x = self.dropout(x)

        &#47&#47 Batch first = True in decoder
        for i in range(self.num_layers):</code></pre><h3>After Change</h3><pre><code class='java'>
                self.attention[i](normed_output, normed_output, normed_output, target_mask)
            )
            normed_output = self.layer_norm(output)
            output = output<a id="change"> + </a>self.dropout(
                self.source_attention[i](normed_output, memory, memory, source_mask)
            )
            normed_output = self.layer_norm(output)
            output = output<a id="change"> + </a>self.dropout(self.position_feed_forward[i](normed_output))

        <a id="change">return </a>self.layer_norm(output)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/mindee/doctr/commit/fddceba7bee5098b4219b7ba6a0bdf4f4a98adfe#diff-5eb4b6b7a215017ac91351a0c5e665af04fa644ddf1ecd5037545ba096ec1337L69' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4028012</div><div id='project'> Project Name: mindee/doctr</div><div id='commit'> Commit Name: fddceba7bee5098b4219b7ba6a0bdf4f4a98adfe</div><div id='time'> Time: 2022-06-09</div><div id='author'> Author: felixdittrich92@gmail.com</div><div id='file'> File Name: doctr/models/recognition/transformer/pytorch.py</div><div id='m_class'> M Class Name: Decoder</div><div id='n_method'> N Class Name: Decoder</div><div id='m_method'> M Method Name: forward(5)</div><div id='n_method'> N Method Name: forward(5)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: doctr/models/recognition/transformer/pytorch.py</div><div id='n_file'> N File Name: doctr/models/recognition/transformer/pytorch.py</div><div id='m_start'> M Start Line: 74</div><div id='m_end'> M End Line: 91</div><div id='n_start'> N Start Line: 147</div><div id='n_end'> N End Line: 167</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        gate = self.norm(gate)

        weight, bias = self.weight, self.bias
        weight, bias = <a id="change">weight[:, :n, :n]</a>, bias[:, :n]

        mask = torch.ones(weight.shape[:2], device = device).triu_(1).bool()
        weight = weight.masked_fill(mask[..., None], 0.)</code></pre><h3>After Change</h3><pre><code class='java'>

        weight, bias = self.weight, self.bias

        mask = torch.ones(weight.shape[-2:], device = device).triu_(1<a id="change"> + </a>w).bool()
        weight = weight.masked_fill(mask[None, ...], 0.)

        gate = rearrange(gate, &quotb w n (h d) -&gt; b w h n d&quot, h = h)
        gate = einsum(&quotb w h n d, h m n -&gt; b w h m d&quot, gate, weight)
        gate = gate<a id="change"> + </a>rearrange(bias, &quoth n -&gt; () () h n ()&quot)

        gate = rearrange(gate, &quotb w h n d -&gt; b w n (h d)&quot)

        out = gate * res
        out = rearrange(out, &quotb w n d -&gt; b (w n) d&quot)
        <a id="change">return </a>out[:, :n]

def gMLPBlock(
    *,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/g-mlp-gpt/commit/a2f065dfcf7be1d1e4b205ac1a55de4ad1b3327d#diff-9e1c762f97dc7e52cf231d6bec8dbb8412bc9383faaf89eadd9b1c02e46fea80L83' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4028020</div><div id='project'> Project Name: lucidrains/g-mlp-gpt</div><div id='commit'> Commit Name: a2f065dfcf7be1d1e4b205ac1a55de4ad1b3327d</div><div id='time'> Time: 2021-05-20</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: g_mlp_gpt/g_mlp_gpt.py</div><div id='m_class'> M Class Name: CausalSpatialGatingUnit</div><div id='n_method'> N Class Name: CausalLocalSGU</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: g_mlp_gpt/g_mlp_gpt.py</div><div id='n_file'> N File Name: g_mlp_gpt/g_mlp_gpt.py</div><div id='m_start'> M Start Line: 90</div><div id='m_end'> M End Line: 100</div><div id='n_start'> N Start Line: 130</div><div id='n_end'> N End Line: 154</div><BR>