<html><h3>Pattern ID :1819
</h3><img src='13086087.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        super().__init__()

    def forward(self, x):
        <a id="change">return </a>x
</code></pre><h3>After Change</h3><pre><code class='java'>
        h = self.heads
        context = default(context, x)

        q<a id="change"> = </a>self.to_q(x)
        k, v = self.to_kv(context).chunk(2, dim = -1)

        q = q * self.scale

        q<a id="change">, k, v = </a><a id="change">map(</a>lambda t: rearrange(t, &quotb n (h d) -&gt; b h n d&quot, h = h), (q<a id="change">, k, v</a>)<a id="change">)</a>

        out = attention(q, k, v, mask = mask, causal = self.causal)

        out = rearrange(out, &quotb h n d -&gt; b n (h d)&quot)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/memory-efficient-attention-pytorch/commit/b5a38e377b7dc5bab5407cce852eddc69f45de82#diff-7de1bf8844a9aafc0b616bb3f7f2bc3701cbdab6390a8d8096e5dfea36da1708L16' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 13086087</div><div id='project'> Project Name: lucidrains/memory-efficient-attention-pytorch</div><div id='commit'> Commit Name: b5a38e377b7dc5bab5407cce852eddc69f45de82</div><div id='time'> Time: 2022-03-03</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: memory_efficient_attention_pytorch/memory_efficient_attention.py</div><div id='m_class'> M Class Name: Attention</div><div id='n_method'> N Class Name: Attention</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: memory_efficient_attention_pytorch/memory_efficient_attention.py</div><div id='n_file'> N File Name: memory_efficient_attention_pytorch/memory_efficient_attention.py</div><div id='m_start'> M Start Line: 16</div><div id='m_end'> M End Line: 17</div><div id='n_start'> N Start Line: 64</div><div id='n_end'> N End Line: 78</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        next_mem, next_lmem = map(torch.stack, (next_mem, next_lmem))
        next_mem, next_lmem = map(torch.detach, (next_mem, next_lmem))

        <a id="change">return </a>out, Memory(short = next_mem, long = next_lmem)
</code></pre><h3>After Change</h3><pre><code class='java'>
        num_memory_layers = len(self.memory_layers)
        init_mem = lambda: torch.empty(num_memory_layers, b, 0, d, **to(x))

        mem<a id="change"> = </a>default(mem, init_mem)
        lmem = default(lmem, init_mem)

        mem_len<a id="change">, lmem_len = </a><a id="change">map(</a>lambda t: t.shape[2], (mem<a id="change">, lmem</a>)<a id="change">)</a>
        total_len = mem_len + lmem_len + self.seq_len

        pos_emb = self.pos_emb[:, (self.seq_len - t):total_len]
        mem_iter, lmem_iter = map(iterate_tensor, (mem, lmem))

        hiddens = []

        for ind, (attn, ff) in enumerate(zip(self.attn_layers, self.ff_layers)):
            layer_num = ind + 1
            use_memory = layer_num in self.memory_layers
            memories = map(next, (mem_iter, lmem_iter)) if use_memory else None

            if use_memory:
                hiddens.append(x)

            x = attn(x, memories = memories, calc_memory = use_memory, input_mask = mask, pos_emb = pos_emb)
            x = ff(x)

        hiddens = torch.stack(hiddens)
        out = self.to_logits(x)

        &#47&#47 calculate next memory state

        next_memory = self.memory_network(lmem, mem, hiddens)
        return out<a id="change">, next_memory</a>
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/memory-transformer-xl/commit/cbabe1ae6fa311092a9d0a88116c079a5ad8d790#diff-a0bf60cdef5ad628f9a83f0454c9c66a535268529835e839bb54088805dba051L253' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 13086092</div><div id='project'> Project Name: lucidrains/memory-transformer-xl</div><div id='commit'> Commit Name: cbabe1ae6fa311092a9d0a88116c079a5ad8d790</div><div id='time'> Time: 2020-07-22</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: memory_transformer_xl/memory_transformer_xl.py</div><div id='m_class'> M Class Name: MemoryTransformerXL</div><div id='n_method'> N Class Name: MemoryTransformerXL</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: memory_transformer_xl/memory_transformer_xl.py</div><div id='n_file'> N File Name: memory_transformer_xl/memory_transformer_xl.py</div><div id='m_start'> M Start Line: 255</div><div id='m_end'> M End Line: 296</div><div id='n_start'> N Start Line: 306</div><div id='n_end'> N End Line: 345</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        position_logits = self.to_position_logits(embed)
        value_logits = self.to_value_logits(embed)

        <a id="change">return </a>channel_logits
</code></pre><h3>After Change</h3><pre><code class='java'>
        embed = torch.cat((start_token, embed), dim = 1)

        if return_loss:
            embed<a id="change"> = </a>embed[:, :-1]

        embed = self.postemb_norm(embed)

        &#47&#47 layers of attention + cross attention

        for attn, cross_attn, ff in self.layers:
            embed = attn(embed) + embed
            embed = cross_attn(embed, encoded) + embed
            embed = ff(embed) + embed

        &#47&#47 to logits

        embed = self.final_norm(embed)

        channel_logits = self.to_channel_logits(embed)
        position_logits = self.to_position_logits(embed)
        value_logits = self.to_value_logits(embed)

        if not return_loss:
            return channel_logits<a id="change">, position_logits, value_logits</a>

        channel_logits<a id="change">, position_logits, value_logits = </a><a id="change">map(</a>lambda t: rearrange(t, &quotb n c -&gt; b c n&quot), (channel_logits<a id="change">, position_logits, value_logits</a>)<a id="change">)</a>

        channel_loss = F.cross_entropy(channel_logits, channels)
        position_loss = F.cross_entropy(channel_logits, channels)
        value_loss = F.cross_entropy(channel_logits, channels)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/transframer-pytorch/commit/0ccad402b8e5bb5e46393b16e72048567d983ed3#diff-8297de4bcdca0c16a036a8cea9b0742314816cee6103a4872f00abb1bf4234f4L150' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 13086089</div><div id='project'> Project Name: lucidrains/transframer-pytorch</div><div id='commit'> Commit Name: 0ccad402b8e5bb5e46393b16e72048567d983ed3</div><div id='time'> Time: 2022-08-20</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: transframer_pytorch/transframer_pytorch.py</div><div id='m_class'> M Class Name: Transframer</div><div id='n_method'> N Class Name: Transframer</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: transframer_pytorch/transframer_pytorch.py</div><div id='n_file'> N File Name: transframer_pytorch/transframer_pytorch.py</div><div id='m_start'> M Start Line: 179</div><div id='m_end'> M End Line: 183</div><div id='n_start'> N Start Line: 154</div><div id='n_end'> N End Line: 200</div><BR>