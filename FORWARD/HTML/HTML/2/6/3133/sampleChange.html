<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    def forward(self, x, keys = None):
        device = x.device
        b<a id="change">, t, e, m</a> = *x.shape, self.num_mem_kv

        mem = self.mem_kv.expand(m, b, e)
        keys = default(keys, torch.empty(b, 0, e, device=device))

        x, keys = x.transpose(0, 1), keys.transpose(0, 1)

        kv = torch.cat((x, mem, keys))

        attn_shape = (<a id="change">t</a><a id="change">, kv.shape[0]</a>)
        attn_mask = torch.zeros(*attn_shape, device=x.device)
        if self.causal:
            i, j = torch.triu_indices(t, t, 1)</code></pre><h3>After Change</h3><pre><code class='java'>
        self.causal = causal

    def forward(self, qk, v, query_len = None):
        query_len = <a id="change">default(</a>query_len, qk.shape[1]<a id="change">)</a>
        t = query_len

        q = qk[:, 0:query_len]
        qk = F.normalize(qk, 2, dim=-1)

        dot = torch.einsum(&quotbie,bje-&gt;bij&quot, q, qk)

        &#47&#47 qk attention requires tokens not attend to self
        i = torch.arange(t)
        dot[:, i, i] = -1e-5 

        if self.causal:
            i, j = torch.triu_indices(t, t, 1)
            dot[:, i, j] = float(&quot-inf&quot)

        dot<a id="change"> = </a>dot.softmax(dim=-1)
        out = torch.einsum(&quotbij,bje-&gt;bie&quot, dot, v)
        return out<a id="change">, dot</a>

&#47&#47 Shared qk attention, using either full or LSH attention

class LSHSelfAttention(nn.Module):</code></pre>