<html><h3>Pattern ID :3540
</h3><img src='17620172.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        total_len = mem.shape[2] + lmem.shape[2] + self.seq_len
        pos_emb = self.pos_emb[:, (self.seq_len - t):total_len]

        next_mem<a id="change"> = </a>[]
        next_lmem = []

        mem_iter, lmem_iter = map(iterate_tensor, (mem, lmem))

        for ind, (attn, ff) in enumerate(zip(self.attn_layers, self.ff_layers)):
            layer_num = ind + 1
            use_memory = layer_num in self.memory_layers

            memories = None
            if use_memory:
                memories = (next(mem_iter), next(lmem_iter))

            x, (mem_out, lmem_out) = attn(x, memories = memories, calc_memory = use_memory, input_mask = mask, pos_emb = pos_emb)
            x, = ff(x)

            if use_memory:
                next_mem.append(mem_out)
                next_lmem.append(lmem_out)

        out = self.to_logits(x)

        next_mem, next_lmem = map(torch.stack, (next_mem, next_lmem))
        next_mem, next_lmem = <a id="change">map(</a>torch.detach, (next_mem, next_lmem)<a id="change">)</a>

        return out, Memory(short = next_mem, long = next_lmem)
</code></pre><h3>After Change</h3><pre><code class='java'>
        mem = default(mem, init_mem)
        lmem = default(lmem, init_mem)

        mem_len, lmem_len = <a id="change">map(</a>lambda t: t.shape[2], (mem, lmem)<a id="change">)</a>
        total_len = mem_len + lmem_len + self.seq_len

        pos_emb = self.pos_emb[:, (self.seq_len - t):total_len]
        mem_iter, lmem_iter = map(iterate_tensor, (mem, lmem))</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/memory-transformer-xl/commit/cbabe1ae6fa311092a9d0a88116c079a5ad8d790#diff-a0bf60cdef5ad628f9a83f0454c9c66a535268529835e839bb54088805dba051L255' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 17620172</div><div id='project'> Project Name: lucidrains/memory-transformer-xl</div><div id='commit'> Commit Name: cbabe1ae6fa311092a9d0a88116c079a5ad8d790</div><div id='time'> Time: 2020-07-22</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: memory_transformer_xl/memory_transformer_xl.py</div><div id='m_class'> M Class Name: MemoryTransformerXL</div><div id='n_method'> N Class Name: MemoryTransformerXL</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: memory_transformer_xl/memory_transformer_xl.py</div><div id='n_file'> N File Name: memory_transformer_xl/memory_transformer_xl.py</div><div id='m_start'> M Start Line: 255</div><div id='m_end'> M End Line: 296</div><div id='n_start'> N Start Line: 306</div><div id='n_end'> N End Line: 345</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            blocks = [block for block, drop in zip(self.blocks, to_drop) if not drop]
            blocks = self.blocks[:1] if len(blocks) == 0 else blocks

        f_args, g_args = <a id="change">map(</a>lambda route: kwargs if route else {}, arg_route<a id="change">)</a>
        block_kwargs = {&quotf_args&quot: f_args, &quotg_args&quot: g_args}        

        return _ReversibleFunction.apply(x, blocks, block_kwargs)
</code></pre><h3>After Change</h3><pre><code class='java'>

    def forward(self, x, **kwargs):
        blocks = self.blocks
        block_args<a id="change"> = </a>route_args(self.args_route, kwargs, len(self.blocks))

        if self.training and self.layer_dropout &gt; 0:
            to_drop = torch.empty(len(self.blocks)).uniform_(0, 1) &lt; self.layer_dropout
            blocks = [block for block, drop in zip(self.blocks, to_drop) if not drop]
            blocks = self.blocks[:1] if len(blocks) == 0 else blocks

        block_args = list(<a id="change">map(</a>lambda x: {&quotf_args&quot: x[0], &quotg_args&quot: x[1]}, block_args<a id="change">)</a>)
        return _ReversibleFunction.apply(x, blocks, block_args)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/sinkhorn-transformer/commit/c2662a20cb783efd3351936cfabc83131060a2a6#diff-27d52caf7b4f725ff9fe7d9e57172c62a7c8744ebafb23663512e7990a5ed131L119' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 17620158</div><div id='project'> Project Name: lucidrains/sinkhorn-transformer</div><div id='commit'> Commit Name: c2662a20cb783efd3351936cfabc83131060a2a6</div><div id='time'> Time: 2020-04-10</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: sinkhorn_transformer/reversible.py</div><div id='m_class'> M Class Name: ReversibleSequence</div><div id='n_method'> N Class Name: ReversibleSequence</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: sinkhorn_transformer/reversible.py</div><div id='n_file'> N File Name: sinkhorn_transformer/reversible.py</div><div id='m_start'> M Start Line: 119</div><div id='m_end'> M End Line: 128</div><div id='n_start'> N Start Line: 134</div><div id='n_end'> N End Line: 141</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.blocks = nn.ModuleList([ReversibleBlock(f, g) for (f, g) in blocks])

    def forward(self, x, arg_route = (True, True), **kwargs):
        f_args, g_args = <a id="change">map(</a>lambda route: kwargs if route else {}, arg_route<a id="change">)</a>
        block_kwargs = {&quotf_args&quot: f_args, &quotg_args&quot: g_args}
        return _ReversibleFunction.apply(x, self.blocks, block_kwargs)
</code></pre><h3>After Change</h3><pre><code class='java'>
        x = torch.cat([x, x], dim=-1)

        blocks = self.blocks
        args<a id="change"> = </a>route_args(self.args_route, kwargs, len(blocks))
        args = list(<a id="change">map(</a>lambda x: {&quotf_args&quot: x[0], &quotg_args&quot: x[1]}, args<a id="change">)</a>)

        layers_and_args = list(zip(blocks, args))
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/linear-attention-transformer/commit/fa23ce09a98a63d26116e3935ad5902cf705255d#diff-29d3c048298bdaa9a9670921a4026430e5b09b55bc1da20d65da18bd5c85f575L118' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 17620163</div><div id='project'> Project Name: lucidrains/linear-attention-transformer</div><div id='commit'> Commit Name: fa23ce09a98a63d26116e3935ad5902cf705255d</div><div id='time'> Time: 2020-06-04</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: linear_attention_transformer/reversible.py</div><div id='m_class'> M Class Name: ReversibleSequence</div><div id='n_method'> N Class Name: ReversibleSequence</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: linear_attention_transformer/reversible.py</div><div id='n_file'> N File Name: linear_attention_transformer/reversible.py</div><div id='m_start'> M Start Line: 118</div><div id='m_end'> M End Line: 121</div><div id='n_start'> N Start Line: 161</div><div id='n_end'> N End Line: 174</div><BR>