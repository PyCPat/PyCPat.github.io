<html><h3>Pattern ID :1489
</h3><img src='5064212.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 returing:
        &#47&#47 WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[0]/Conv2d[conv2]
        &#47&#47 WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[0]/ReLU[relu1]
        return (self.l_7(self.l_6(self.l_5(self.l_4(self.l_3(t_0)))))<a id="change">, t_0</a>)

    def state_dict(self,device):
        &#47&#47 we return the state dict of this part as it should be in the original model</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 calling torch.add with arguments:
        &#47&#47 WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[0]/aten::add286
        &#47&#47 WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[1]/Conv2d[conv2]
        t_2 = <a id="change">torch.add(input=t_1, other=self.l_15(self.l_14(self.l_13(self.l_12(self.l_11(self.l_10(self.l_9(t_1))))))))</a>
        &#47&#47 returing:
        &#47&#47 WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[1]/aten::add379
        &#47&#47 WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[2]/ReLU[relu1]
        <a id="change">return </a>(t_2, self.l_17(self.l_16(t_2)))

    def state_dict(self,device):
        &#47&#47 we return the state dict of this part as it should be in the original model</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/7e9b5e2b5fad1b4c475cd5f52cc6049fa8958c52#diff-a814c88399d815440945eb657b90182147320894f4d0d33b05af09d2d09a32a5L248' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 5064212</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: 7e9b5e2b5fad1b4c475cd5f52cc6049fa8958c52</div><div id='time'> Time: 2019-12-31</div><div id='author'> Author: saareliad@campus.technion.ac.il</div><div id='file'> File Name: models/partitioned/wrn_28x10_c100_dr03_p4.py</div><div id='m_class'> M Class Name: WideResNetPartition0</div><div id='n_method'> N Class Name: WideResNetPartition0</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: models/partitioned/wrn_28x10_c100_dr03_p4.py</div><div id='n_file'> N File Name: models/partitioned/wrn_28x10_c100_dr03_p4.py</div><div id='m_start'> M Start Line: 248</div><div id='m_end'> M End Line: 252</div><div id='n_start'> N Start Line: 312</div><div id='n_end'> N End Line: 320</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 returing:
        &#47&#47 GPT2Model/Block[2]/MLP[mlp]/Dropout[dropout]
        &#47&#47 GPT2Model/Block[2]/aten::add5567
        return (self.l_29(self.l_28(torch.mul(input=torch.mul(input=t_33, other=0.5), other=torch.add(input=Tensor.tanh(torch.mul(input=torch.add(input=t_33, other=torch.mul(input=Tensor.pow(t_33, exponent=3), other=0.044715)), other=0.7978845608028654)), other=1))))<a id="change">, t_32</a>)

    def state_dict(self,device=None):
        &#47&#47 we return the state dict of this part as it should be in the original model</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 calling torch.add with arguments:
        &#47&#47 GPT2LMHeadModel/GPT2Model[transformer]/Block[2]/aten::add5739
        &#47&#47 GPT2LMHeadModel/GPT2Model[transformer]/Block[3]/Attention[attn]/Dropout[resid_dropout]
        t_43 = <a id="change">torch.add(input=t_34, other=self.l_34(self.l_33(Tensor.view(t_42, size=[Tensor.size(t_42, dim=0), Tensor.size(t_42, dim=1), torch.mul(input=Tensor.size(t_42, dim=-2), other=Tensor.size(t_42, dim=-1))]))))</a>
        &#47&#47 returing:
        &#47&#47 GPT2LMHeadModel/GPT2Model[transformer]/Block[3]/LayerNorm[ln_2]
        &#47&#47 GPT2LMHeadModel/GPT2Model[transformer]/Block[3]/aten::add5953
        <a id="change">return </a>(self.l_35(t_43), t_43)

    def state_dict(self,device=None):
        &#47&#47 we return the state dict of this part as it should be in the original model</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/9ddf98456ca57cac3a4a982fc519c5a71642dc25#diff-4d93af50bd9b6dc7752538156bde96fadf361ac3f37eb0019e2bbf01206492abL311' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 5064215</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: 9ddf98456ca57cac3a4a982fc519c5a71642dc25</div><div id='time'> Time: 2020-03-10</div><div id='author'> Author: saareliad@campus.technion.ac.il</div><div id='file'> File Name: models/partitioned/gpt2.py</div><div id='m_class'> M Class Name: Partition0</div><div id='n_method'> N Class Name: Partition0</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: models/partitioned/gpt2.py</div><div id='n_file'> N File Name: models/partitioned/gpt2.py</div><div id='m_start'> M Start Line: 460</div><div id='m_end'> M End Line: 467</div><div id='n_start'> N Start Line: 396</div><div id='n_end'> N End Line: 430</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            init.zeros_(self.bias)

    def forward(self, x):
        batch_size<a id="change">, c_in, T, n_vertex</a> = x.shape

        &#47&#47 Using recurrence relation to reduce time complexity from O(n^2) to O(K|E|),
        &#47&#47 where K = Ks - 1</code></pre><h3>After Change</h3><pre><code class='java'>
        cheb_graph_conv = torch.einsum(&quotbtkhi,kij-&gt;bthj&quot, x, self.weight)

        if self.bias is not None:
            cheb_graph_conv = <a id="change">torch.add(</a>cheb_graph_conv, self.bias<a id="change">)</a>
        else:
            cheb_graph_conv = cheb_graph_conv
        
        <a id="change">return </a>cheb_graph_conv

class GraphConv(nn.Module):
    def __init__(self, c_in, c_out, gso, bias):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/hazdzz/stgcn/commit/de050cc05a36453eafe1bf7bac60401c6561e947#diff-db69bbef9e404fa5af08d5bc209ae131c092e0ce285d496efe5f46170b20add9L182' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 5064211</div><div id='project'> Project Name: hazdzz/stgcn</div><div id='commit'> Commit Name: de050cc05a36453eafe1bf7bac60401c6561e947</div><div id='time'> Time: 2022-02-07</div><div id='author'> Author: raphaelpeo@gmail.com</div><div id='file'> File Name: model/layers.py</div><div id='m_class'> M Class Name: ChebConv</div><div id='n_method'> N Class Name: ChebGraphConv</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: model/layers.py</div><div id='n_file'> N File Name: model/layers.py</div><div id='m_start'> M Start Line: 183</div><div id='m_end'> M End Line: 209</div><div id='n_start'> N Start Line: 153</div><div id='n_end'> N End Line: 180</div><BR>