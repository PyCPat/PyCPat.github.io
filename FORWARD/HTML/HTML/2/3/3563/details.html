<html><h3>Pattern ID :3563
</h3><img src='17645631.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        
        
        if isinstance(x, tuple):
            b<a id="change">, n, d, h</a> = *x[0].shape, self.heads
            q, k, v = map(lambda proj_token_pair: proj_token_pair[0](proj_token_pair[1]), zip((self.Q, self.K, self.V), x))
        else:
            b, n, d, h = *x.shape, self.heads</code></pre><h3>After Change</h3><pre><code class='java'>
        out = einsum("b h n m, b h m d -&gt; b h n d", attention, v)
        out = rearrange(out, "b h n d -&gt; b n (h d)")
        out = self.out_linear(out)
        <a id="change">print("out:"</a>, out.shape<a id="change">)</a>
        return self.out_dropout(out)


class TalkingHeadAttention(nn.Module):</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 2</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/blakechi/comvex/commit/eb0dba924b3de0a0f9a4a956c9d9be3eb7733de9#diff-ca14ccd7b7581cec594a814cc72eced700051b9de55ba4d8236d3e8664aeb6f0L57' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 17645631</div><div id='project'> Project Name: blakechi/comvex</div><div id='commit'> Commit Name: eb0dba924b3de0a0f9a4a956c9d9be3eb7733de9</div><div id='time'> Time: 2021-06-27</div><div id='author'> Author: pwchi@ucdavis.edu</div><div id='file'> File Name: comvex/utils/multihead_attention.py</div><div id='m_class'> M Class Name: MultiheadAttention</div><div id='n_method'> N Class Name: MultiheadAttention</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: comvex/utils/multihead_attention.py</div><div id='n_file'> N File Name: comvex/utils/multihead_attention.py</div><div id='m_start'> M Start Line: 59</div><div id='m_end'> M End Line: 62</div><div id='n_start'> N Start Line: 57</div><div id='n_end'> N End Line: 79</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        last_output = torch.zeros((batch_size, self.d_output), device=device)

        if timespans is None:
            timespans = x.new_ones(x.shape[:-1]+(1<a id="change"></a>,)) / x.shape[1]

        for t in range(seq_len):
            inputs = x[:, t]</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 def forward(self, x, state=None,lengths=None,timespans=None):
    def forward(self, u, state=None, rate=1.0, lengths=None, **kwargs):
        if state is not None:
            <a id="change">print("state is not None -&gt; breakpoint"</a><a id="change">)</a>
            breakpoint()
        &#47&#47 if lengths is not None:
        &#47&#47     print("lengths is not None -&gt; breakpoint")
        &#47&#47     breakpoint()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/raminmh/liquid-s4/commit/52f2ec0442e4b1472915480269dff07788ed7f97#diff-ab2372bdc1906ad2a23c39f4d39f3f048c08b9f7b91891862db640d26d4916afL83' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 17645640</div><div id='project'> Project Name: raminmh/liquid-s4</div><div id='commit'> Commit Name: 52f2ec0442e4b1472915480269dff07788ed7f97</div><div id='time'> Time: 2022-08-30</div><div id='author'> Author: mlech26l@gmail.com</div><div id='file'> File Name: src/models/sequence/mm.py</div><div id='m_class'> M Class Name: mmRNN</div><div id='n_method'> N Class Name: mmRNN</div><div id='m_method'> M Method Name: forward(5)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: src/models/sequence/mm.py</div><div id='n_file'> N File Name: src/models/sequence/mm.py</div><div id='m_start'> M Start Line: 83</div><div id='m_end'> M End Line: 108</div><div id='n_start'> N Start Line: 89</div><div id='n_end'> N End Line: 123</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        x_, attention_maps = attention_layer(x)
        print(x.shape, x_.shape)
        
        b, n, _, h = *x.shape<a id="change">, self.heads</a>
        qkv = self.to_qkv(x).chunk(3, dim=-1)
        q, k, v = map(lambda t: rearrange(t, &quotb n (h d) -&gt; b h n d&quot, h=h), qkv)

        dots = einsum(&quotb h i d, b h j d -&gt; b h i j&quot, q, k) * self.scale</code></pre><h3>After Change</h3><pre><code class='java'>
            q = q + pos_embeddings
            k = k + pos_embeddings
            if self.AddPosEmb2Value:
                <a id="change">print(&quotadd pos_embeddings to v&quot</a><a id="change">)</a>
                v = v + pos_embeddings

        &#47&#47 self attention
        attn_q, attn_map = self.Attention(q, k, v)  &#47&#47 attn_q has same shape with q</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/wamawama/wama_modules/commit/f6865257f4bea4ed62e0170644d99dcdb1d20c94#diff-f5d4e504fb1ef29f758d7f77376318f372efbfb4498293f8d8797c46106d1d7aL67' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 17645637</div><div id='project'> Project Name: wamawama/wama_modules</div><div id='commit'> Commit Name: f6865257f4bea4ed62e0170644d99dcdb1d20c94</div><div id='time'> Time: 2022-10-28</div><div id='author'> Author: wmy19970215@gmail.com</div><div id='file'> File Name: wama_modules/Transformer.py</div><div id='m_class'> M Class Name: SelfAttention</div><div id='n_method'> N Class Name: TransformerEncoderLayer</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: wama_modules/Transformer.py</div><div id='n_file'> N File Name: wama_modules/Transformer.py</div><div id='m_start'> M Start Line: 78</div><div id='m_end'> M End Line: 88</div><div id='n_start'> N Start Line: 135</div><div id='n_end'> N End Line: 199</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            x = encoder(x)
            encoder_outputs.append(x)
        
        x<a id="change">,_</a> = self.de_lstm(x)

        for decoder in self.decoder:
            skip_connection = encoder_outputs.pop(-1)</code></pre><h3>After Change</h3><pre><code class='java'>
        if self.resample&gt;1:
            x = audio.pt_resample_audio(audio=x, sr=self.sampling_rate,
                        target_sr=int(self.sampling_rate * self.resample))
        <a id="change">print("resampled-&gt;"</a>,x.shape<a id="change">)</a>
        encoder_outputs = []
        for encoder in self.encoder:
            x = encoder(x)
            print(x.shape)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/shahules786/mayavoz/commit/b42ca28851f172797b28d83dc6f9c26e34b2d4a1#diff-ae222fe663e9b184f8fdc35f842fee011444775d4757dbff087f2e43e1eedf0fL87' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 17645632</div><div id='project'> Project Name: shahules786/mayavoz</div><div id='commit'> Commit Name: b42ca28851f172797b28d83dc6f9c26e34b2d4a1</div><div id='time'> Time: 2022-09-06</div><div id='author'> Author: shahules786@gmail.com</div><div id='file'> File Name: enhancer/models/demucs.py</div><div id='m_class'> M Class Name: Demus</div><div id='n_method'> N Class Name: Demucs</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: enhancer/models/demucs.py</div><div id='n_file'> N File Name: enhancer/models/demucs.py</div><div id='m_start'> M Start Line: 97</div><div id='m_end'> M End Line: 100</div><div id='n_start'> N Start Line: 90</div><div id='n_end'> N End Line: 103</div><BR>