<html><h3>Pattern ID :2858
</h3><img src='15713418.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        u_hat = torch.einsum(&quotijk, jkl -&gt; ijl&quot, x, self.route_weights)

        &#47&#47 Dynamic route
        b = torch.zeros(<a id="change">x.shape[1]</a>, self.num_capsules, requires_grad=True)
        for it in range(self.num_iterations):
            c = b.softmax(dim=-1)
</code></pre><h3>After Change</h3><pre><code class='java'>
            b += uv

        &#47&#47 Last iteration with original u_hat to pass gradient
        c = <a id="change">b.softmax(dim=1)</a>
        s = torch.einsum(&quotijk, ijkl -&gt; ijl&quot, c, u_hat_temp)
        v = squash(s)

        return v</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 2</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/riroaki/capsnet/commit/408b1e77f4e40589def9c313c0b11beaa88f2108#diff-903964c7bdce50a4b995b4520374f243c677f45b2f8c7ed7b15abeeb6f26af12L47' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 15713418</div><div id='project'> Project Name: riroaki/capsnet</div><div id='commit'> Commit Name: 408b1e77f4e40589def9c313c0b11beaa88f2108</div><div id='time'> Time: 2020-03-08</div><div id='author'> Author: aki@akideMacBook-Pro.local</div><div id='file'> File Name: capsnet.py</div><div id='m_class'> M Class Name: DigitCaps</div><div id='n_method'> N Class Name: DigitCaps</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: capsnet.py</div><div id='n_file'> N File Name: capsnet.py</div><div id='m_start'> M Start Line: 47</div><div id='m_end'> M End Line: 65</div><div id='n_start'> N Start Line: 52</div><div id='n_end'> N End Line: 84</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        attention_mask = source_ids.ne(self.tokenizer.pad_token_id)
        outputs = self.encoder(input_ids=source_ids, attention_mask=attention_mask,
                               labels=source_ids, decoder_attention_mask=attention_mask, output_hidden_states=True)
        hidden_states = <a id="change">outputs[&quotdecoder_hidden_states&quot][-1]</a>
        eos_mask = source_ids.eq(self.config.eos_token_id)

        if len(torch.unique(eos_mask.sum(1))) &gt; 1:
            raise ValueError("All examples must have the same number of &lt;eos&gt; tokens.")</code></pre><h3>After Change</h3><pre><code class='java'>
            vec = self.get_roberta_vec(source_ids)

        logits = self.classifier(vec)
        prob = <a id="change">nn.functional.softmax(</a>logits<a id="change">)</a>

        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits, labels)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/salesforce/codet5/commit/0bf3c0c43e92fcf54d9df68c793ac22f2b60aad4#diff-dcfeaa5ac3dffd54264034760914588d92fc7395c81eaa9274715840dc28100bL110' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 15713417</div><div id='project'> Project Name: salesforce/codet5</div><div id='commit'> Commit Name: 0bf3c0c43e92fcf54d9df68c793ac22f2b60aad4</div><div id='time'> Time: 2021-10-29</div><div id='author'> Author: 337111657@qq.com</div><div id='file'> File Name: models.py</div><div id='m_class'> M Class Name: CloneModel</div><div id='n_method'> N Class Name: CloneModel</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: models.py</div><div id='n_file'> N File Name: models.py</div><div id='m_start'> M Start Line: 111</div><div id='m_end'> M End Line: 125</div><div id='n_start'> N Start Line: 103</div><div id='n_end'> N End Line: 116</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        kv = torch.cat((x, mem, keys))

        attn_shape = (t, <a id="change">kv.shape[0]</a>)
        attn_mask = torch.zeros(*attn_shape, device=x.device)
        if self.causal:
            i, j = torch.triu_indices(t, t, 1)</code></pre><h3>After Change</h3><pre><code class='java'>
            i, j = torch.triu_indices(t, t, 1)
            dot[:, i, j] = float(&quot-inf&quot)

        dot = <a id="change">dot.softmax(dim=-1)</a>
        out = torch.einsum(&quotbij,bje-&gt;bie&quot, dot, v)
        return out, dot

&#47&#47 Shared qk attention, using either full or LSH attention</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/reformer-pytorch/commit/d2462f9b33944e20f5fbeaf19efaa9591378ba65#diff-7e740488ff2ce47996cfdd5356981ed3e6ac2bb38739a71b400275ac91d1c3bfL385' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 15713415</div><div id='project'> Project Name: lucidrains/reformer-pytorch</div><div id='commit'> Commit Name: d2462f9b33944e20f5fbeaf19efaa9591378ba65</div><div id='time'> Time: 2020-01-28</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: reformer_pytorch/reformer_pytorch.py</div><div id='m_class'> M Class Name: SelfAttention</div><div id='n_method'> N Class Name: FullQKAttention</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: reformer_pytorch/reformer_pytorch.py</div><div id='n_file'> N File Name: reformer_pytorch/reformer_pytorch.py</div><div id='m_start'> M Start Line: 386</div><div id='m_end'> M End Line: 403</div><div id='n_start'> N Start Line: 330</div><div id='n_end'> N End Line: 349</div><BR>