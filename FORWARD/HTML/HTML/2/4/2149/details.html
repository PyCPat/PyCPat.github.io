<html><h3>Pattern ID :2149
</h3><img src='14003429.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 compute projections of output from previous layer and the memory
        q = self.w_q(x).reshape(batch_size, -1, seg_len, embed_dim)
        k = self.w_ke(h).reshape(batch_size, -1, total_len, embed_dim)
        v = <a id="change">self.w_v(h).reshape(</a>batch_size, -1, total_len, embed_dim<a id="change">)</a>
        r = self.w_kr(self.pos).reshape(-1, total_len, embed_dim)
        
        &#47&#47 compute relative positional encodings
        b = q @ r.transpose(1, 2)
        b = self.circulant_shift(b, -seg_len+1)
        
        &#47&#47 this is the XL specific way of computing the attention score
        k = k.transpose(2, 3)
        att = q @ k + b + self.u @ k + self.v @ r.transpose(1, 2)
        att = att.tril(mem_len) / embed_dim**0.5
        att = torch.softmax(att, dim=-1)
        
        &#47&#47 compute the output of the layer and save to memory
        out<a id="change"> = </a>self.layer_norm(self.mlp(att @ v) + x)
        out = self.pos_ff(out)
        self.save_to_memory(out)
        </code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 compute projections of input and memory embeddings
        q = self.w_q(x).view(batch_size, -1, seg_len, embed_dim)
        kv = self.w_kv(h).view(2*batch_size, -1, total_len, embed_dim)
        k, v = <a id="change">kv.chunk(2</a><a id="change">, dim=0)</a>
        k = k.transpose(2, 3) &#47&#47 only using transposed k below
        
        &#47&#47 relative distance between two tokens is max total_len
        pos = self.pos[-total_len:]</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/augustwester/transformer-xl/commit/57fbeff47839f3e9f29dfaf34449752b2d4807e6#diff-eb7ae2070281ad9c6011e2b97c48b86fea6c48408aa6b0e9d20c8c520d3c7cf2L23' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 14003429</div><div id='project'> Project Name: augustwester/transformer-xl</div><div id='commit'> Commit Name: 57fbeff47839f3e9f29dfaf34449752b2d4807e6</div><div id='time'> Time: 2022-11-24</div><div id='author'> Author: august.wester@gmail.com</div><div id='file'> File Name: attention.py</div><div id='m_class'> M Class Name: Attention</div><div id='n_method'> N Class Name: MultiHeadAttention</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: attention.py</div><div id='n_file'> N File Name: attention.py</div><div id='m_start'> M Start Line: 34</div><div id='m_end'> M End Line: 61</div><div id='n_start'> N Start Line: 23</div><div id='n_end'> N End Line: 59</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x<a id="change"> = </a><a id="change">(attn @ v).transpose(1, 2).reshape(</a>B, N, C<a id="change">)</a>
        x = self.proj(x)
        x = self.proj_drop(x)
        return x
</code></pre><h3>After Change</h3><pre><code class='java'>
    def forward(self, x):
        B, N, C = x.shape

        qkv = <a id="change">self.qkv(x).chunk(3</a><a id="change">, dim = -1)</a>
        q, k, v = map(lambda t: rearrange(t, &quotb n (h d) -&gt; b h n d&quot, h = self.heads), qkv)

        q = q * self.scale
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/vit-pytorch/commit/cb6d749821bbf3b0bd17c9e8e64eb343f40b3f69#diff-d9cf888a006662bd788cc31712f154ca4e86227d51998c7d5bca6e17d1197c07L78' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 14003425</div><div id='project'> Project Name: lucidrains/vit-pytorch</div><div id='commit'> Commit Name: cb6d749821bbf3b0bd17c9e8e64eb343f40b3f69</div><div id='time'> Time: 2022-10-29</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: vit_pytorch/cct.py</div><div id='m_class'> M Class Name: Attention</div><div id='n_method'> N Class Name: Attention</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: vit_pytorch/cct.py</div><div id='n_file'> N File Name: vit_pytorch/cct.py</div><div id='m_start'> M Start Line: 80</div><div id='m_end'> M End Line: 90</div><div id='n_start'> N Start Line: 99</div><div id='n_end'> N End Line: 111</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        v = self.NIN_2(h)

        w = torch.einsum(&quotbchw,bcij-&gt;bhwij&quot, q, k) * (int(C) ** (-0.5))
        w<a id="change"> = </a><a id="change">torch.reshape(</a>w, (B, H, W, H * W)<a id="change">)</a>
        w = F.softmax(w, dim=-1)
        w = torch.reshape(w, (B, H, W, H, W))
        h = torch.einsum(&quotbhwij,bcij-&gt;bchw&quot, w, v)
        h = self.NIN_3(h)</code></pre><h3>After Change</h3><pre><code class='java'>

        norm = self.norm(input)
        qkv = self.qkv(norm).view(batch, n_head, head_dim * 3, height, width)
        query, key, value = <a id="change">qkv.chunk(3</a><a id="change">, dim=2)</a>  &#47&#47 bhdyx

        attn = torch.einsum(
            "bnchw, bncyx -&gt; bnhwyx", query, key</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/janspiry/image-super-resolution-via-iterative-refinement/commit/1a5a8bc409a8b8e072eb2bf25330d4662fd279c4#diff-8f5ccf6b45f0af5991c777ec4c13895ab7943f03df8982ff0e1edd6b462d8d35L123' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 14003423</div><div id='project'> Project Name: janspiry/image-super-resolution-via-iterative-refinement</div><div id='commit'> Commit Name: 1a5a8bc409a8b8e072eb2bf25330d4662fd279c4</div><div id='time'> Time: 2021-08-09</div><div id='author'> Author: lw_jiang@foxmail.com</div><div id='file'> File Name: model/ddpm_modules/unet.py</div><div id='m_class'> M Class Name: AttnBlock</div><div id='n_method'> N Class Name: SelfAttention</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: model/ddpm_modules/unet.py</div><div id='n_file'> N File Name: model/ddpm_modules/unet.py</div><div id='m_start'> M Start Line: 125</div><div id='m_end'> M End Line: 136</div><div id='n_start'> N Start Line: 111</div><div id='n_end'> N End Line: 128</div><BR>