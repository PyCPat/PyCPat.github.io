<html><h3>Pattern ID :676
</h3><img src='2614327.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            x, (mem_out, lmem_out) = attn(x, memories = memories, calc_memory = use_memory, input_mask = mask, pos_emb = pos_emb)
            x, = ff(x)

            <a id="change">if </a>use_memory<a id="change">:
                </a>next_mem.append(mem_out)
                next_lmem.append(lmem_out)

        out = self.to_logits(x)</code></pre><h3>After Change</h3><pre><code class='java'>
        mem = default(mem, init_mem)
        lmem = default(lmem, init_mem)

        mem_len<a id="change">, lmem_len = </a><a id="change">map(</a>lambda t: t.shape[2], (mem<a id="change">, lmem</a>)<a id="change">)</a>
        total_len = mem_len + lmem_len + self.seq_len

        pos_emb = self.pos_emb[:, (self.seq_len - t):total_len]
        mem_iter, lmem_iter = map(iterate_tensor, (mem, lmem))</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/memory-transformer-xl/commit/cbabe1ae6fa311092a9d0a88116c079a5ad8d790#diff-a0bf60cdef5ad628f9a83f0454c9c66a535268529835e839bb54088805dba051L255' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2614327</div><div id='project'> Project Name: lucidrains/memory-transformer-xl</div><div id='commit'> Commit Name: cbabe1ae6fa311092a9d0a88116c079a5ad8d790</div><div id='time'> Time: 2020-07-22</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: memory_transformer_xl/memory_transformer_xl.py</div><div id='m_class'> M Class Name: MemoryTransformerXL</div><div id='n_method'> N Class Name: MemoryTransformerXL</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: memory_transformer_xl/memory_transformer_xl.py</div><div id='n_file'> N File Name: memory_transformer_xl/memory_transformer_xl.py</div><div id='m_start'> M Start Line: 255</div><div id='m_end'> M End Line: 296</div><div id='n_start'> N Start Line: 306</div><div id='n_end'> N End Line: 345</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        q = merge_heads(q)

        <a id="change">if </a>not self.one_kv_head<a id="change">:
            </a>k, v = map(merge_heads, (k, v))

        out = []
</code></pre><h3>After Change</h3><pre><code class='java'>
            lq, q = split_index_fn(q)

            split_kv_fn = partial(split_at_index, 1, int(self.local_attn_heads &gt; 0))
            (<a id="change">lk, k), (lv, v) = </a><a id="change">map(</a>split_kv_fn, (k<a id="change">, v</a>)<a id="change">)</a>

            local_expand_heads_fn = lambda t: expand_dim(t, 1, self.local_attn_heads, unsqueeze=False)
            lk, lv = map(local_expand_heads_fn, (lk, lv))
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/linear-attention-transformer/commit/75a6cefd9d7facce1ff162dc70138a6e32358f3c#diff-eee6cf669463e9683bcf9643c4b19a493056f98ded32854287144d47f02a7032L295' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2614326</div><div id='project'> Project Name: lucidrains/linear-attention-transformer</div><div id='commit'> Commit Name: 75a6cefd9d7facce1ff162dc70138a6e32358f3c</div><div id='time'> Time: 2020-06-29</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: linear_attention_transformer/linear_attention_transformer.py</div><div id='m_class'> M Class Name: SelfAttention</div><div id='n_method'> N Class Name: SelfAttention</div><div id='m_method'> M Method Name: forward(5)</div><div id='n_method'> N Method Name: forward(5)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: linear_attention_transformer/linear_attention_transformer.py</div><div id='n_file'> N File Name: linear_attention_transformer/linear_attention_transformer.py</div><div id='m_start'> M Start Line: 304</div><div id='m_end'> M End Line: 320</div><div id='n_start'> N Start Line: 308</div><div id='n_end'> N End Line: 327</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        if self.training and self.layer_dropout &gt; 0:
            to_drop = torch.empty(len(self.blocks)).uniform_(0, 1) &lt; self.layer_dropout
            blocks = [block for block, drop in zip(self.blocks, to_drop) if <a id="change">not drop</a>]
            blocks = self.blocks[:1] if len(blocks) == 0 else blocks

        block_args = list(map(lambda x: {&quotf_args&quot: x[0], &quotg_args&quot: x[1]}, block_args))</code></pre><h3>After Change</h3><pre><code class='java'>

        if self.training and self.layer_dropout &gt; 0:
            layers_and_args = layer_drop(layers_and_args, self.layer_dropout)
            blocks<a id="change">, args = </a><a id="change">map(</a>lambda ind: list(map(itemgetter(ind), layers_and_args)), (0<a id="change">, 1</a>)<a id="change">)</a>

        out =  _ReversibleFunction.apply(x, blocks, args)
        return torch.stack(out.chunk(2, dim=-1)).sum(dim=0)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/sinkhorn-transformer/commit/d5b9c649e59290b15c15f85d0bb182cb20b699fb#diff-27d52caf7b4f725ff9fe7d9e57172c62a7c8744ebafb23663512e7990a5ed131L132' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2614325</div><div id='project'> Project Name: lucidrains/sinkhorn-transformer</div><div id='commit'> Commit Name: d5b9c649e59290b15c15f85d0bb182cb20b699fb</div><div id='time'> Time: 2020-04-15</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: sinkhorn_transformer/reversible.py</div><div id='m_class'> M Class Name: ReversibleSequence</div><div id='n_method'> N Class Name: ReversibleSequence</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: sinkhorn_transformer/reversible.py</div><div id='n_file'> N File Name: sinkhorn_transformer/reversible.py</div><div id='m_start'> M Start Line: 133</div><div id='m_end'> M End Line: 142</div><div id='n_start'> N Start Line: 161</div><div id='n_end'> N End Line: 174</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 pad for last token in video

        <a id="change">if </a>padding &gt; 0<a id="change">:
            </a>x = F.pad(x, (0, 0, 0, padding), value = 0.)

        &#47&#47 derive queries / keys / values
</code></pre><h3>After Change</h3><pre><code class='java'>
        k, v = map(lambda t: rearrange(t, &quotb (f h w) d -&gt; b d f h w&quot, f  = num_frames, h = fmap_size), (k, v))
        k, v = map(lambda t: unfoldNd(t, kernel_size = kernel_size, padding = kernel_size // 2), (k, v))
        k, v = map(lambda t: rearrange(t, &quotb (d j) i -&gt; b i j d&quot, j = kernel_size ** 3), (k, v))
        k<a id="change">, v = </a><a id="change">map(</a>lambda t: t[:, :(n - 1)], (k<a id="change">, v</a>)<a id="change">)</a>

        &#47&#47 append bos keys and values

        k_bos, v_bos = map(lambda t: repeat(t, &quotb 1 d -&gt; b n 1 d&quot, n = k.shape[1]), (k_bos, v_bos))</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/nuwa-pytorch/commit/7d904d1fdd0776ffb9e0a20d0857d47c7988f90a#diff-47b637f0a46bb7e4f245f382a9b47f0351b6a5fda8ab6476a0476b4677441eddL412' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2614323</div><div id='project'> Project Name: lucidrains/nuwa-pytorch</div><div id='commit'> Commit Name: 7d904d1fdd0776ffb9e0a20d0857d47c7988f90a</div><div id='time'> Time: 2022-01-03</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: nuwa_pytorch/nuwa_pytorch.py</div><div id='m_class'> M Class Name: Sparse3DNA</div><div id='n_method'> N Class Name: Sparse3DNA</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: nuwa_pytorch/nuwa_pytorch.py</div><div id='n_file'> N File Name: nuwa_pytorch/nuwa_pytorch.py</div><div id='m_start'> M Start Line: 423</div><div id='m_end'> M End Line: 494</div><div id='n_start'> N Start Line: 423</div><div id='n_end'> N End Line: 494</div><BR>