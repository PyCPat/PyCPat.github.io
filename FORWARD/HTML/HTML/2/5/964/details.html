<html><h3>Pattern ID :964
</h3><img src='3485429.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                          mode=&quotnearest&quot, recompute_scale_factor=False)
        b = F.interpolate(b, scale_factor=1./self.rate,
                          mode=&quotnearest&quot, recompute_scale_factor=False)
        int_fs = <a id="change">list(</a>f.size()<a id="change">)</a>     &#47&#47 b*c*h*w
        int_bs = list(b.size())
        &#47&#47 split tensors along the batch dimension
        f_groups = torch.split(f, 1, dim=0)
        &#47&#47 w shape: [N, C*k*k, L]
        w = extract_image_patches(b, ksizes=[self.ksize, self.ksize],
                                  strides=[self.stride, self.stride],
                                  rates=[1, 1],
                                  padding=&quotsame&quot)
        &#47&#47 w shape: [N, C, k, k, L]
        w = w.view(int_bs[0], int_bs[1], self.ksize, self.ksize, -1)
        w = w.permute(0, 4, 1, 2, 3)    &#47&#47 w shape: [N, L, C, k, k]
        w_groups = torch.split(w, 1, dim=0)

        &#47&#47 process mask
        if mask is None:
            mask = torch.zeros([int_bs[0], 1, int_bs[2], int_bs[3]])
            mask = mask.to(device)
        else:
            mask = F.interpolate(
                mask, scale_factor=1./((2**self.n_down)*self.rate), mode=&quotnearest&quot, recompute_scale_factor=False)
        int_ms = list(mask.size())
        &#47&#47 m shape: [N, C*k*k, L]
        m = extract_image_patches(mask, ksizes=[self.ksize, self.ksize],
                                  strides=[self.stride, self.stride],
                                  rates=[1, 1],
                                  padding=&quotsame&quot)
        &#47&#47 m shape: [N, C, k, k, L]
        m = m.view(int_ms[0], int_ms[1], self.ksize, self.ksize, -1)
        m = m.permute(0, 4, 1, 2, 3)    &#47&#47 m shape: [N, L, C, k, k]
        m = m[0]    &#47&#47 m shape: [L, C, k, k]
        &#47&#47 mm shape: [L, 1, 1, 1]

        mm = (torch.mean(m, axis=[1, 2, 3], keepdim=True) == 0.).to(
            torch.float32)

        mm = mm.permute(1, 0, 2, 3)  &#47&#47 mm shape: [1, L, 1, 1]

        y = []
        offsets = []
        k = self.fuse_k
        scale = self.softmax_scale    &#47&#47 to fit the PyTorch tensor image value range
        fuse_weight = torch.eye(k).view(1, 1, k, k)  &#47&#47 1*1*k*k
        fuse_weight = fuse_weight.to(device)

        for xi, wi, raw_wi in zip(f_groups, w_groups, raw_w_groups):
            &quot&quot&quot
            O =&gt; output channel as a conv filter
            I =&gt; input channel as a conv filter
            xi : separated tensor along batch dimension of front; (B=1, C=128, H=32, W=32)
            wi : separated patch tensor along batch dimension of back; (B=1, O=32*32, I=128, KH=3, KW=3)
            raw_wi : separated tensor along batch dimension of back; (B=1, I=32*32, O=128, KH=4, KW=4)
            &quot&quot&quot
            &#47&#47 conv for compare
            escape_NaN = torch.Tensor([1e-4])
            escape_NaN = escape_NaN.to(device)
            wi = wi[0]  &#47&#47 [L, C, k, k]
            max_wi = torch.sqrt(
                torch.sum(torch.pow(wi, 2) + escape_NaN, dim=[1, 2, 3], keepdim=True))

            wi_normed = wi / max_wi
            &#47&#47 xi shape: [1, C, H, W], yi shape: [1, L, H, W]
            xi = same_padding(xi, [self.ksize, self.ksize], [
                              1, 1], [1, 1])  &#47&#47 xi: 1*c*H*W
            yi = F.conv2d(xi, wi_normed, stride=1)   &#47&#47 [1, L, H, W]
            &#47&#47 conv implementation for fuse scores to encourage large patches
            if self.fuse:
                &#47&#47 make all of depth to spatial resolution
                &#47&#47 (B=1, I=1, H=32*32, W=32*32)
                yi = yi.view(1, 1, int_bs[2]*int_bs[3], int_fs[2]*int_fs[3])
                yi = same_padding(yi, [k, k], [1, 1], [1, 1])
                &#47&#47 (B=1, C=1, H=32*32, W=32*32)
                yi = F.conv2d(yi, fuse_weight, stride=1)
                &#47&#47 (B=1, 32, 32, 32, 32)
                yi = yi.contiguous().view(
                    1, int_bs[2], int_bs[3], int_fs[2], int_fs[3])
                yi = yi.permute(0, 2, 1, 4, 3)

                yi = yi.contiguous().view(
                    1, 1, int_bs[2]*int_bs[3], int_fs[2]*int_fs[3])
                yi = same_padding(yi, [k, k], [1, 1], [1, 1])
                yi = F.conv2d(yi, fuse_weight, stride=1)
                yi = yi.contiguous().view(
                    1, int_bs[3], int_bs[2], int_fs[3], int_fs[2])
                yi = yi.permute(0, 2, 1, 4, 3).contiguous()

            &#47&#47 (B=1, C=32*32, H=32, W=32)
            yi = yi.view(1, int_bs[2] * int_bs[3], int_fs[2], int_fs[3])
            &#47&#47 softmax to match
            yi = yi * mm
            yi = F.softmax(yi*scale, dim=1)
            yi = yi * mm  &#47&#47 [1, L, H, W]

            offset = torch.argmax(yi, dim=1, keepdim=True)  &#47&#47 1*1*H*W

            if int_bs != int_fs:
                &#47&#47 Normalize the offset value to match foreground dimension
                times = float(int_fs[2] * int_fs[3]) / \
                    float(int_bs[2] * int_bs[3])
                offset = ((offset + 1).float() * times - 1).to(torch.int64)
            offset = torch.cat([torch.div(offset, int_fs[3], rounding_mode=&quottrunc&quot),
                                offset % int_fs[3]], dim=1)  &#47&#47 1*2*H*W

            &#47&#47 deconv for patch pasting
            wi_center = raw_wi[0]
            yi = F.conv_transpose2d(
                yi, wi_center, stride=self.rate, padding=1) / 4.  &#47&#47 (B=1, C=128, H=64, W=64)
            y.append(yi)
            offsets.append(offset)

        y = torch.cat(y, dim=0)  &#47&#47 back to the mini-batch
        y = y.contiguous().view(raw_int_fs)

        if not self.return_flow:
            return y, None

        offsets = torch.cat(offsets, dim=0)
        offsets = offsets.view(int_fs[0], 2, *int_fs[2:])

        &#47&#47 case1: visualize optical flow: minus current position
        h_add = torch.arange(int_fs[2]).view(
            [1, 1, int_fs[2], 1]).expand(int_fs[0], -1, -1, int_fs[3])
        w_add = torch.arange(int_fs[3]).view(
            [1, 1, 1, int_fs[3]]).expand(int_fs[0], -1, int_fs[2], -1)
        ref_coordinate = torch.cat([h_add, w_add], dim=1)
        ref_coordinate<a id="change"> = </a>ref_coordinate.to(device)

        offsets<a id="change"> = </a>offsets - ref_coordinate
        &#47&#47 flow = pt_flow_to_image(offsets)

        flow = torch.from_numpy(flow_to_image(</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 background for matching and use original background for reconstruction.
        f = downsampling_nn_tf(f, n=self.rate)
        b = downsampling_nn_tf(b, n=self.rate)
        int_fs, int_bs = <a id="change">list(</a>f.size()<a id="change">), list(b.size())</a>   &#47&#47 b*c*h*w
        &#47&#47 split tensors along the batch dimension
        f_groups = torch.split(f, 1, dim=0)
        &#47&#47 w shape: [N, C*k*k, L]</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/nipponjo/deepfillv2-pytorch/commit/b56ad8569aeea17343cedc9c7331223e134c228f#diff-e9366d4b0e74c6f58afc0e8ec64f4603739e8d75e0c63da5e728893a16a8c92dL309' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 3485429</div><div id='project'> Project Name: nipponjo/deepfillv2-pytorch</div><div id='commit'> Commit Name: b56ad8569aeea17343cedc9c7331223e134c228f</div><div id='time'> Time: 2021-12-02</div><div id='author'> Author: 28433296+nipponjo@users.noreply.github.com</div><div id='file'> File Name: model/networks_tf.py</div><div id='m_class'> M Class Name: ContextualAttention</div><div id='n_method'> N Class Name: ContextualAttention</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: model/networks_tf.py</div><div id='n_file'> N File Name: model/networks_tf.py</div><div id='m_start'> M Start Line: 309</div><div id='m_end'> M End Line: 467</div><div id='n_start'> N Start Line: 338</div><div id='n_end'> N End Line: 464</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                          mode=&quotnearest&quot, recompute_scale_factor=False)
        b = F.interpolate(b, scale_factor=1./self.rate,
                          mode=&quotnearest&quot, recompute_scale_factor=False)
        int_fs = <a id="change">list(</a>f.size()<a id="change">)</a>     &#47&#47 b*c*h*w
        int_bs = list(b.size())
        &#47&#47 split tensors along the batch dimension
        f_groups = torch.split(f, 1, dim=0)
        &#47&#47 w shape: [N, C*k*k, L]
        w = extract_image_patches(b, ksize=self.ksize,
                                  stride=self.stride,
                                  rate=1, padding=&quotauto&quot)
        &#47&#47 w shape: [N, C, k, k, L]
        w = w.view(int_bs[0], int_bs[1], self.ksize, self.ksize, -1)
        w = w.permute(0, 4, 1, 2, 3)    &#47&#47 w shape: [N, L, C, k, k]
        w_groups = torch.split(w, 1, dim=0)

        &#47&#47 process mask
        if mask is None:
            mask = torch.zeros([int_bs[0], 1, int_bs[2], int_bs[3]])
            mask = mask.to(device)
        else:
            mask = F.interpolate(
                mask, scale_factor=1./((2**self.n_down)*self.rate), mode=&quotnearest&quot, recompute_scale_factor=False)
        int_ms = list(mask.size())
        &#47&#47 m shape: [N, C*k*k, L]
        m = extract_image_patches(mask, ksize=self.ksize,
                                  stride=self.stride,
                                  rate=1, padding=&quotauto&quot)
        &#47&#47 m shape: [N, C, k, k, L]
        m = m.view(int_ms[0], int_ms[1], self.ksize, self.ksize, -1)
        m = m.permute(0, 4, 1, 2, 3)    &#47&#47 m shape: [N, L, C, k, k]
        m = m[0]    &#47&#47 m shape: [L, C, k, k]
        &#47&#47 mm shape: [L, 1, 1, 1]

        mm = (torch.mean(m, dim=[1, 2, 3], keepdim=True) == 0.).to(
            torch.float32)

        &#47&#47mm = (torch.mean(m, dim=[1, 2, 3], keepdim=True) == 0.).to(torch.float32)
        mm = mm.permute(1, 0, 2, 3)  &#47&#47 mm shape: [1, L, 1, 1]

        y = []
        offsets = []
        k = self.fuse_k
        scale = self.softmax_scale    &#47&#47 to fit the PyTorch tensor image value range
        fuse_weight = torch.eye(k).view(1, 1, k, k)  &#47&#47 1*1*k*k
        fuse_weight = fuse_weight.to(device)

        for xi, wi, raw_wi in zip(f_groups, w_groups, raw_w_groups):
            &quot&quot&quot
            O =&gt; output channel as a conv filter
            I =&gt; input channel as a conv filter
            xi : separated tensor along batch dimension of front; (B=1, C=128, H=32, W=32)
            wi : separated patch tensor along batch dimension of back; (B=1, O=32*32, I=128, KH=3, KW=3)
            raw_wi : separated tensor along batch dimension of back; (B=1, I=32*32, O=128, KH=4, KW=4)
            &quot&quot&quot
            &#47&#47 conv for compare
            escape_NaN = torch.Tensor([1e-4])
            escape_NaN = escape_NaN.to(device)
            wi = wi[0]  &#47&#47 [L, C, k, k]
          
            &#47&#47max_wi = torch.max(torch.sqrt(torch.sum(wi**2, dim=[1, 2, 3], keepdim=True)), escape_NaN)
            max_wi = torch.sqrt(torch.sum(torch.pow(wi, 2) + escape_NaN, dim=[1, 2, 3], keepdim=True))

            wi_normed = wi / max_wi
            &#47&#47 xi shape: [1, C, H, W], yi shape: [1, L, H, W]
            yi = F.conv2d(xi, wi_normed, stride=1, padding=(self.ksize-1)//2)   &#47&#47 [1, L, H, W]
            &#47&#47 conv implementation for fuse scores to encourage large patches
            if self.fuse:
                &#47&#47 make all of depth to spatial resolution
                &#47&#47 (B=1, I=1, H=32*32, W=32*32)
                yi = yi.view(1, 1, int_bs[2]*int_bs[3], int_fs[2]*int_fs[3])
                &#47&#47 (B=1, C=1, H=32*32, W=32*32)
                yi = F.conv2d(yi, fuse_weight, stride=1, padding=(k-1)//2)
                &#47&#47 (B=1, 32, 32, 32, 32)
                yi = yi.contiguous().view(1, int_bs[2], int_bs[3], int_fs[2], int_fs[3])
                yi = yi.permute(0, 2, 1, 4, 3)
                
                yi = yi.contiguous().view(1, 1, int_bs[2]*int_bs[3], int_fs[2]*int_fs[3])
                yi = F.conv2d(yi, fuse_weight, stride=1, padding=(k-1)//2)
                yi = yi.contiguous().view(1, int_bs[3], int_bs[2], int_fs[3], int_fs[2])
                yi = yi.permute(0, 2, 1, 4, 3).contiguous()

            &#47&#47 (B=1, C=32*32, H=32, W=32)
            yi = yi.view(1, int_bs[2] * int_bs[3], int_fs[2], int_fs[3])
            &#47&#47 softmax to match
            yi = yi * mm
            yi = F.softmax(yi*scale, dim=1)
            yi = yi * mm  &#47&#47 [1, L, H, W]

            offset = torch.argmax(yi, dim=1, keepdim=True)  &#47&#47 1*1*H*W

            if int_bs != int_fs:
                &#47&#47 Normalize the offset value to match foreground dimension
                times = float(int_fs[2] * int_fs[3]) / \
                    float(int_bs[2] * int_bs[3])
                offset = ((offset + 1).float() * times - 1).to(torch.int64)
            offset = torch.cat([torch.div(offset, int_fs[3], rounding_mode=&quottrunc&quot),
                                offset % int_fs[3]], dim=1)  &#47&#47 1*2*H*W

            &#47&#47 deconv for patch pasting
            wi_center = raw_wi[0]
            yi = F.conv_transpose2d(
                yi, wi_center, stride=self.rate, padding=1) / 4.  &#47&#47 (B=1, C=128, H=64, W=64)
            y.append(yi)
            offsets.append(offset)

        y = torch.cat(y, dim=0)  &#47&#47 back to the mini-batch
        y = y.contiguous().view(raw_int_fs)

        if not self.return_flow:
            return y, None

        offsets = torch.cat(offsets, dim=0)
        offsets = offsets.view(int_fs[0], 2, *int_fs[2:])

        &#47&#47 case1: visualize optical flow: minus current position
        h_add = torch.arange(int_fs[2]).view(
            [1, 1, int_fs[2], 1]).expand(int_fs[0], -1, -1, int_fs[3])
        w_add = torch.arange(int_fs[3]).view(
            [1, 1, 1, int_fs[3]]).expand(int_fs[0], -1, int_fs[2], -1)
        ref_coordinate = torch.cat([h_add, w_add], dim=1)
        ref_coordinate<a id="change"> = </a>ref_coordinate.to(device)

        offsets = offsets - ref_coordinate
        &#47&#47 flow = pt_flow_to_image(offsets)

        flow = torch.from_numpy(flow_to_image(
            offsets.permute(0, 2, 3, 1).cpu().data.numpy())) / 255.
        flow = flow.permute(0, 3, 1, 2)
        flow<a id="change"> = </a>flow.to(device)
        &#47&#47 case2: visualize which pixels are attended
        &#47&#47 flow = torch.from_numpy(highlight_flow((offsets * mask.long()).cpu().data.numpy()))
</code></pre><h3>After Change</h3><pre><code class='java'>
                          mode=&quotnearest&quot, recompute_scale_factor=False)
        b = F.interpolate(b, scale_factor=1./self.rate,
                          mode=&quotnearest&quot, recompute_scale_factor=False)
        int_fs, int_bs = <a id="change">list(</a>f.size()<a id="change">), list(b.size())</a>   &#47&#47 b*c*h*w
        &#47&#47 split tensors along the batch dimension
        f_groups = torch.split(f, 1, dim=0)
        &#47&#47 w shape: [N, C*k*k, L]</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/nipponjo/deepfillv2-pytorch/commit/b56ad8569aeea17343cedc9c7331223e134c228f#diff-3eaaced9361a826c2aad9331cd5df2488e9a4b15f6c936aa46574b67f6649874L326' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 3485460</div><div id='project'> Project Name: nipponjo/deepfillv2-pytorch</div><div id='commit'> Commit Name: b56ad8569aeea17343cedc9c7331223e134c228f</div><div id='time'> Time: 2021-12-02</div><div id='author'> Author: 28433296+nipponjo@users.noreply.github.com</div><div id='file'> File Name: model/networks.py</div><div id='m_class'> M Class Name: ContextualAttention</div><div id='n_method'> N Class Name: ContextualAttention</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: model/networks.py</div><div id='n_file'> N File Name: model/networks.py</div><div id='m_start'> M Start Line: 333</div><div id='m_end'> M End Line: 482</div><div id='n_start'> N Start Line: 352</div><div id='n_end'> N End Line: 474</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            encoder_outputs: encoder outputs
        
        feat_outputs = list()
        stop_outputs<a id="change"> = </a><a id="change">list()</a>
        alignment_energies = list()

        inputs, max_length, train = self.validate_args(inputs, encoder_outputs)
        self._init_decoder_states(encoder_outputs)

        if train:
            inputs = self.prenet(inputs)  &#47&#47 B x T x 256

            for di in range(max_length):
                feat_output, stop_output, alignment_energy = self.forward_step(inputs[:, di, :].unsqueeze(1), encoder_outputs)
                feat_outputs.append(feat_output)
                stop_outputs.append(stop_output)
                alignment_energies.append(alignment_energy)

        else:
            input_var = inputs

            for di in range(max_length):
                input_var = self.prenet(input_var)
                feat_output, stop_output, alignment_energy = self.forward_step(input_var, encoder_outputs)
                feat_outputs.append(feat_output)
                stop_outputs.append(stop_output)
                alignment_energies.append(alignment_energy)

                if torch.sigmoid(stop_output.item()) &gt; self.stop_threshold:
                    break

                input_var = feat_output

        output<a id="change"> = </a>self.parse_decoder_outputs(feat_outputs, stop_outputs, alignment_energies)

        return output
</code></pre><h3>After Change</h3><pre><code class='java'>
            inputs: Optional[Tensor] = None,
            teacher_forcing_ratio: float = 1.0
    ) -&gt; Dict[str, Tensor]:
        feat_outputs, stop_outputs, alignments = list()<a id="change">, list(), list()</a>
        use_teacher_forcing = True if random.random() &lt; teacher_forcing_ratio else False

        inputs, max_decoding_step = self.validate_args(encoder_outputs, inputs, teacher_forcing_ratio)
        decoder_states = self._init_decoder_states(encoder_outputs)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sooftware/tacotron2/commit/79c719bc3e7e65c83c532b668960a62948e2b147#diff-42dffe9ecd8420aebc61fa823d9820e3d9c50c61d5c38d615e0c193cc1898e9eL146' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 3485441</div><div id='project'> Project Name: sooftware/tacotron2</div><div id='commit'> Commit Name: 79c719bc3e7e65c83c532b668960a62948e2b147</div><div id='time'> Time: 2020-10-03</div><div id='author'> Author: sh951011@gmail.com</div><div id='file'> File Name: tacotron2/model/decoder.py</div><div id='m_class'> M Class Name: Decoder</div><div id='n_method'> N Class Name: Decoder</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: tacotron2/model/decoder.py</div><div id='n_file'> N File Name: tacotron2/model/decoder.py</div><div id='m_start'> M Start Line: 156</div><div id='m_end'> M End Line: 189</div><div id='n_start'> N Start Line: 162</div><div id='n_end'> N End Line: 213</div><BR>