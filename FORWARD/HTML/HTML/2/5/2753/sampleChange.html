<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            values_ =  F.relu(self.linear_value(values.float().unsqueeze(1))).squeeze(1)
            &#47&#47 variable_ =  F.relu(self.linear_sensor(variable.float().unsqueeze(1))).squeeze(1)

            unit = <a id="change">torch.cat([</a>pe_, values_, variable_<a id="change"></a>]<a id="change">, dim=1)</a>
            Add Normalization across samples here, to make all 48-dimensions are in similar scale
            unit<a id="change"> = </a>F.normalize(unit, dim=1)

            &#47&#47 use 2-layer transformer to get f&quot
            &#47&#47 trans_unit = self.transformer_encoder_f_prime(unit.unsqueeze(1))</code></pre><h3>After Change</h3><pre><code class='java'>
            pe_ = self.pos_encoder(time_points.unsqueeze(1)).squeeze(1)
            variable = nonzero_index[:,1] &#47&#47 the dimensions of variables. The m value in SEFT paper.

            unit = torch.cat([pe_, <a id="change">values.unsqueeze(1</a><a id="change">)</a>, variable.unsqueeze(1)], dim=1)

            &#47&#47 &#47&#47 positional encoding  AUROC ~0.86 Why positional encoding works?
            &#47&#47 &#47&#47 values_ = self.pos_encoder_value(values.unsqueeze(1)).squeeze(1)</code></pre>