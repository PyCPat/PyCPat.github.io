<html><h3>Pattern ID :3262
</h3><img src='17194567.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        mu = torch.clip(mu, MEAN_MIN, MEAN_MAX)
        log_sigma = self.sigma_head(a)
        log_sigma = torch.clip(log_sigma, LOG_STD_MIN, LOG_STD_MAX)
        sigma = <a id="change">torch.exp(</a>log_sigma<a id="change">)</a>

        a_distribution = Normal(mu, sigma)
        action = a_distribution.rsample()

        logp_pi = a_distribution.log_prob(action).sum(axis=-1)
        logp_pi -= (2 * (np.log(2) - action - F.softplus(-2 * action))).sum(axis=1)
        logp_pi = torch.unsqueeze(logp_pi, dim=1)

        action = self.max_action<a id="change"> * </a>torch.tanh(action)
        mu<a id="change"> = </a>torch.tanh(mu) * self.max_action
        return action<a id="change">, logp_pi, mu</a>

    def get_log_density(self, state, action):
        a = F.relu(self.fc1(state))
        a = F.relu(self.fc2(a))</code></pre><h3>After Change</h3><pre><code class='java'>
        a_dist, a_tanh_mode = self._get_outputs(state)
        action = a_dist.rsample()
        logp_pi = a_dist.log_prob(action).sum(axis=-1)
        return action<a id="change">, logp_pi, a_tanh_mode</a>

    def get_log_density(self, state, action):
        a_dist, _ = self._get_outputs(state)
        action_clip = torch.clip(action, -1. + EPS, 1. - EPS)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/ryanxhr/dwbc/commit/b3791e408af7125fde12cda1cdeaefbaa400aacc#diff-75aa8d0a222c16ec3e4bb78749eef436b3d51c25ea931fe019fed10965e94549L35' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 17194567</div><div id='project'> Project Name: ryanxhr/dwbc</div><div id='commit'> Commit Name: b3791e408af7125fde12cda1cdeaefbaa400aacc</div><div id='time'> Time: 2022-06-30</div><div id='author'> Author: xuhaoran8@jd.com</div><div id='file'> File Name: algos/DWBC.py</div><div id='m_class'> M Class Name: Actor</div><div id='n_method'> N Class Name: Actor</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: algos/DWBC.py</div><div id='n_file'> N File Name: algos/DWBC.py</div><div id='m_start'> M Start Line: 35</div><div id='m_end'> M End Line: 52</div><div id='n_start'> N Start Line: 51</div><div id='n_end'> N End Line: 54</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 w_1 = torch.exp(-0.1*(range_param.unsqueeze(-1) ** -2) * (t - c) ** 2)  &#47&#47 [B, L, T]
        &#47&#47 w_2 = torch.sum(torch.exp(-0.1*(range_param.unsqueeze(-1) ** -2) * (t - c) ** 2), dim=1, keepdim=True)  &#47&#47 [B, 1, T]
        w_1 = <a id="change">torch.exp(</a>-0.1 * (t - c) ** 2<a id="change">)</a>  &#47&#47 [B, L, T]
        w_2 = torch.sum(torch.exp(-0.1 * (t - c) ** 2), dim=1, keepdim=True)  &#47&#47 [B, 1, T]
        w_2[w_2==0.] = 1.

        &#47&#47 w_1 = self.normpdf(t, c, range_param.unsqueeze(-1))  &#47&#47 [B, L, T]
        &#47&#47 w_1 = torch.distributions.normal.Normal(c, 0.1).log_prob(t)  &#47&#47 [B, L, T]
        &#47&#47 w_2 = torch.sum(w_1, dim=1, keepdim=True)  &#47&#47 [B, 1, T]
        &#47&#47 w_2[w_2==0.] = 1.

        w = w_1<a id="change"> / </a>w_2

        out = torch.matmul(w.transpose(1, 2), encoder_outputs)

        return out<a id="change">, w</a>


class DurationPredictor(nn.Module):
     Duration Parameter Predictor </code></pre><h3>After Change</h3><pre><code class='java'>
        c = c.unsqueeze(2)
        s = self.range_param_predictor(encoder_outputs, duration, mask).unsqueeze(-1)

        g<a id="change"> = </a>torch.distributions.normal.Normal(loc=c, scale=s)

        w = self.get_alignment_energies(g, t)  &#47&#47 [B, L, T]

        if mask is not None:
            w = w.masked_fill(mask.unsqueeze(-1), 0.0)

        attn = w / (torch.sum(w, dim=1).unsqueeze(1) + 1e-8)  &#47&#47 [B, L, T]
        out = torch.bmm(attn.transpose(1, 2), encoder_outputs)

        return out<a id="change">, attn</a>


class DurationPredictor(nn.Module):
     Duration Parameter Predictor </code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/keonlee9420/wavegrad2/commit/523ec241c64ab635218f32d071fd85fbc469e178#diff-44dac3f4f06b62a8129520d279f589b0b2144de4f2a7720992f263c496651b43L105' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 17194612</div><div id='project'> Project Name: keonlee9420/wavegrad2</div><div id='commit'> Commit Name: 523ec241c64ab635218f32d071fd85fbc469e178</div><div id='time'> Time: 2021-07-13</div><div id='author'> Author: keonlee9420@gmail.com</div><div id='file'> File Name: model/modules.py</div><div id='m_class'> M Class Name: GaussianUpsampling</div><div id='n_method'> N Class Name: GaussianUpsampling</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: model/modules.py</div><div id='n_file'> N File Name: model/modules.py</div><div id='m_start'> M Start Line: 115</div><div id='m_end'> M End Line: 135</div><div id='n_start'> N Start Line: 120</div><div id='n_end'> N End Line: 134</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        mel_loss = (mel_iter_loss / (self.L * mel_lens_targets)).mean()

        &#47&#47 Duration Loss
        duration_loss = self.lambda_ * (self.mae_loss((<a id="change">torch.exp(</a>log_durations<a id="change">) - </a>1).sum(-1), mel_lens_targets) / src_lens_targets).mean()

        &#47&#47 KL Divergence Loss
        beta = torch.tensor(self.kl_anneal(step))
        log_vars, mus = log_vars.masked_select(src_masks.unsqueeze(-1)), mus.masked_select(src_masks.unsqueeze(-1))
        kl_loss = -0.5 * torch.sum(1 + log_vars - mus.pow(2) - log_vars.exp())

        &#47&#47 Residual Attention Loss
        attn_loss<a id="change"> = </a>self.guided_loss(attns.transpose(-2, -1), src_lens_targets, mel_lens_targets)

        total_loss = (
            mel_loss + duration_loss + beta * kl_loss + attn_loss
        )

        return (
            total_loss<a id="change">,
            mel_loss,
            duration_loss,
            kl_loss,
            attn_loss,
            beta</a>,
        )

</code></pre><h3>After Change</h3><pre><code class='java'>
        )

        return (
            total_loss<a id="change">,
            mel_loss,
            duration_loss,
            kl_loss,
            beta</a>,
        )
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/keonlee9420/parallel-tacotron2/commit/2c2f8c0876192de39c8e6d3c4f579dc68a50a678#diff-5ea2db96a36360fc43da9b22eaf6dcb593b698b98e6bf32ebfdef3a7589dd912L33' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 17194610</div><div id='project'> Project Name: keonlee9420/parallel-tacotron2</div><div id='commit'> Commit Name: 2c2f8c0876192de39c8e6d3c4f579dc68a50a678</div><div id='time'> Time: 2021-05-25</div><div id='author'> Author: keonlee9420@gmail.com</div><div id='file'> File Name: model/loss.py</div><div id='m_class'> M Class Name: ParallelTacotron2Loss</div><div id='n_method'> N Class Name: ParallelTacotron2Loss</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: model/loss.py</div><div id='n_file'> N File Name: model/loss.py</div><div id='m_start'> M Start Line: 50</div><div id='m_end'> M End Line: 91</div><div id='n_start'> N Start Line: 48</div><div id='n_end'> N End Line: 86</div><BR>