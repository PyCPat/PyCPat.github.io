<html><h3>Pattern ID :298
</h3><img src='967407.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        diff = 0
        codes = []
        for vq in self.vqs:
            _xq<a id="change">, _diff, _codes</a> = vq(x - xq)
            diff = diff + _diff
            xq = xq + _xq
            <a id="change">codes.append(_codes</a><a id="change">)</a>
        codes = torch.cat(codes, 1)
        return xq, (x - xq).pow(2).mean(), codes

    def encode(self, x):</code></pre><h3>After Change</h3><pre><code class='java'>
        residual = x

        all_losses = []
        all_indices<a id="change"> = </a>[]

        n_q = n_q or len(self.layers)

        for layer in self.layers[:n_q]:
            quantized, indices, loss = layer(residual)
            residual = residual - quantized
            quantized_out = quantized_out + quantized

            all_indices.append(indices)
            all_losses.append(loss)

        out_losses<a id="change">, out_indices = </a><a id="change">map(</a>torch.stack, (all_losses<a id="change">, all_indices</a>)<a id="change">)</a>
        return quantized_out, sum(out_losses), out_indices.permute(1, 0, 2)

    def encode(self,
               x: torch.Tensor,</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/acids-ircam/rave/commit/b58cba5d330c227f2122bc07fcbf7ed068eb91be#diff-0642a8a2acbceb830eb07caddb975242233188021b9f16fec4b7168271bf7749L106' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 967407</div><div id='project'> Project Name: acids-ircam/rave</div><div id='commit'> Commit Name: b58cba5d330c227f2122bc07fcbf7ed068eb91be</div><div id='time'> Time: 2023-01-24</div><div id='author'> Author: caillon@ircam.fr</div><div id='file'> File Name: rave/quantization.py</div><div id='m_class'> M Class Name: ResidualVQ</div><div id='n_method'> N Class Name: ResidualVectorQuantization</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: rave/quantization.py</div><div id='n_file'> N File Name: rave/quantization.py</div><div id='m_start'> M Start Line: 106</div><div id='m_end'> M End Line: 116</div><div id='n_start'> N Start Line: 289</div><div id='n_end'> N End Line: 307</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        diff = 0
        codes = []
        for vq in self.vqs:
            _xq<a id="change">, _diff, _codes</a> = vq(x - xq)
            diff = diff + _diff
            xq = xq + _xq
            <a id="change">codes.append(</a>_codes<a id="change">)</a>
        codes = torch.cat(codes, 1)
        return xq, (x - xq).pow(2).mean(), codes

    def encode(self, x):</code></pre><h3>After Change</h3><pre><code class='java'>
        residual = x

        all_losses = []
        all_indices<a id="change"> = </a>[]

        n_q = n_q or len(self.layers)

        for layer in self.layers[:n_q]:
            quantized, indices, loss = layer(residual)
            residual = residual - quantized
            quantized_out = quantized_out + quantized

            all_indices.append(indices)
            all_losses.append(loss)

        out_losses<a id="change">, out_indices = </a><a id="change">map(</a>torch.stack, (all_losses<a id="change">, all_indices</a>)<a id="change">)</a>
        return quantized_out, sum(out_losses), out_indices.permute(1, 0, 2)

    def encode(self,
               x: torch.Tensor,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/caillonantoine/rave/commit/b58cba5d330c227f2122bc07fcbf7ed068eb91be#diff-0642a8a2acbceb830eb07caddb975242233188021b9f16fec4b7168271bf7749L106' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 967405</div><div id='project'> Project Name: caillonantoine/rave</div><div id='commit'> Commit Name: b58cba5d330c227f2122bc07fcbf7ed068eb91be</div><div id='time'> Time: 2023-01-24</div><div id='author'> Author: caillon@ircam.fr</div><div id='file'> File Name: rave/quantization.py</div><div id='m_class'> M Class Name: ResidualVQ</div><div id='n_method'> N Class Name: ResidualVectorQuantization</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: rave/quantization.py</div><div id='n_file'> N File Name: rave/quantization.py</div><div id='m_start'> M Start Line: 106</div><div id='m_end'> M End Line: 116</div><div id='n_start'> N Start Line: 289</div><div id='n_end'> N End Line: 307</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            if use_memory:
                memories = (next(mem_iter), next(lmem_iter))

            x, (mem_out<a id="change">, lmem_out</a>) = attn(x, memories = memories, calc_memory = use_memory, input_mask = mask, pos_emb = pos_emb)
            x, = ff(x)

            if use_memory:
                next_mem.append(mem_out)
                <a id="change">next_lmem.append(</a>lmem_out<a id="change">)</a>

        out = self.to_logits(x)

        next_mem, next_lmem = map(torch.stack, (next_mem, next_lmem))</code></pre><h3>After Change</h3><pre><code class='java'>
        num_memory_layers = len(self.memory_layers)
        init_mem = lambda: torch.empty(num_memory_layers, b, 0, d, **to(x))

        mem<a id="change"> = </a>default(mem, init_mem)
        lmem = default(lmem, init_mem)

        mem_len<a id="change">, lmem_len = </a><a id="change">map(</a>lambda t: t.shape[2], (mem<a id="change">, lmem</a>)<a id="change">)</a>
        total_len = mem_len + lmem_len + self.seq_len

        pos_emb = self.pos_emb[:, (self.seq_len - t):total_len]
        mem_iter, lmem_iter = map(iterate_tensor, (mem, lmem))</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/memory-transformer-xl/commit/cbabe1ae6fa311092a9d0a88116c079a5ad8d790#diff-a0bf60cdef5ad628f9a83f0454c9c66a535268529835e839bb54088805dba051L253' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 967418</div><div id='project'> Project Name: lucidrains/memory-transformer-xl</div><div id='commit'> Commit Name: cbabe1ae6fa311092a9d0a88116c079a5ad8d790</div><div id='time'> Time: 2020-07-22</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: memory_transformer_xl/memory_transformer_xl.py</div><div id='m_class'> M Class Name: MemoryTransformerXL</div><div id='n_method'> N Class Name: MemoryTransformerXL</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: memory_transformer_xl/memory_transformer_xl.py</div><div id='n_file'> N File Name: memory_transformer_xl/memory_transformer_xl.py</div><div id='m_start'> M Start Line: 255</div><div id='m_end'> M End Line: 296</div><div id='n_start'> N Start Line: 306</div><div id='n_end'> N End Line: 345</div><BR>