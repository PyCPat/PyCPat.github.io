<html><h3>Pattern ID :3084
</h3><img src='16585182.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        elif self.combining_operation == "sum":
            e = iid_embeddings.sum(dim=1)
        else:
            <a id="change">raise </a>ValueError("combining_operation must be in [&quotsum&quot, &quotmean&quot].")

        embedding = self.fc_subnet(e)
</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 otherwise we need to loop over the batch to account for varying trial lengths
        else:
            combined_embedding<a id="change"> = </a><a id="change">[]</a>
            trial_counts = torch.zeros(batch, 1)
            <a id="change">for </a>i in range(batch)<a id="change">:
                &#47&#47 remove NaNs
                </a>valid_x<a id="change"> = </a>x[i, ~torch.isnan(x[i, :, 0]), :]
                trial_counts[i] = valid_x.shape[0]
                trial_embeddings = self.trial_net(valid_x)
                &#47&#47 apply combining operation over permutation dimension
                <a id="change">combined_embedding.append(
                    </a>self.combining_function(trial_embeddings, dim=0)<a id="change">
                )</a>

            combined_embedding = torch.stack(combined_embedding, dim=0)

        assert not torch.isnan(combined_embedding).any(), "NaNs in embedding."</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/mackelab/sbi/commit/1352e77bdbc47aa4a4130679903b57672e48218c#diff-672ba10e6c3065a6f5554033b9b173c4fe88f4c05ffc31c70a9198691e6c6659L271' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 16585182</div><div id='project'> Project Name: mackelab/sbi</div><div id='commit'> Commit Name: 1352e77bdbc47aa4a4130679903b57672e48218c</div><div id='time'> Time: 2023-03-01</div><div id='author'> Author: jan.boelts@tum.de</div><div id='file'> File Name: sbi/neural_nets/embedding_nets.py</div><div id='m_class'> M Class Name: PermutationInvariantEmbedding</div><div id='n_method'> N Class Name: PermutationInvariantEmbedding</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: sbi/neural_nets/embedding_nets.py</div><div id='n_file'> N File Name: sbi/neural_nets/embedding_nets.py</div><div id='m_start'> M Start Line: 271</div><div id='m_end'> M End Line: 284</div><div id='n_start'> N Start Line: 277</div><div id='n_end'> N End Line: 304</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def forward(self, x, **kwargs):
        &#47&#47 input: T frames: [b, T, c, h, w]
        &#47&#47 output: single frame: [b, c, h, w]
        <a id="change">raise </a>NotImplementedError

    def pred_n(self, x, pred_length=1, **kwargs):
        &#47&#47 input: T frames: [b, T, c, h, w]</code></pre><h3>After Change</h3><pre><code class='java'>
         Predicts pred_length frames into the future. 
        &#47&#47 input: T frames: [b, T, c, h, w]
        &#47&#47 output: pred_length (P) frames: [b, P, c, h, w]
        preds<a id="change"> = </a><a id="change">[]</a>
        loss_dicts = []
        <a id="change">for i</a> in range(pred_length)<a id="change">:
            </a>pred, loss_dict = self.pred_1(x, **kwargs)
            pred<a id="change"> = </a>pred.unsqueeze(dim=1)
            <a id="change">preds.append(</a>pred<a id="change">)</a>
            loss_dicts.append(loss_dict)
            x = torch.cat([x[:, 1:], pred], dim=1)

        pred = torch.cat(preds, dim=1)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/ais-bonn/vp-suite/commit/b1ebac921dc35dcaf5e5c3f9fe803c4c9e2d78f8#diff-5d20954daff7dcc215ae55822581edcf94d33938f852c929207188cdd3385a40L42' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 16585189</div><div id='project'> Project Name: ais-bonn/vp-suite</div><div id='commit'> Commit Name: b1ebac921dc35dcaf5e5c3f9fe803c4c9e2d78f8</div><div id='time'> Time: 2022-01-14</div><div id='author'> Author: boltres@ais.uni-bonn.de</div><div id='file'> File Name: vp_suite/models/_base_model.py</div><div id='m_class'> M Class Name: VideoPredictionModel</div><div id='n_method'> N Class Name: VideoPredictionModel</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: vp_suite/models/_base_model.py</div><div id='n_file'> N File Name: vp_suite/models/_base_model.py</div><div id='m_start'> M Start Line: 45</div><div id='m_end'> M End Line: 45</div><div id='n_start'> N Start Line: 51</div><div id='n_end'> N End Line: 69</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        elif self.combining_operation == "sum":
            e = iid_embeddings.sum(dim=1)
        else:
            <a id="change">raise </a>ValueError("combining_operation must be in [&quotsum&quot, &quotmean&quot].")

        embedding = self.fc_subnet(e)
</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 otherwise we need to loop over the batch to account for varying trial lengths
        else:
            combined_embedding<a id="change"> = </a><a id="change">[]</a>
            trial_counts = torch.zeros(batch, 1)
            <a id="change">for i</a> in range(batch)<a id="change">:
                &#47&#47 remove NaNs
                </a>valid_x = x[i, ~torch.isnan(x[i, :, 0]), :]
                trial_counts[i] = valid_x.shape[0]
                trial_embeddings<a id="change"> = </a>self.trial_net(valid_x)
                &#47&#47 apply combining operation over permutation dimension
                <a id="change">combined_embedding.append(
                    </a>self.combining_function(trial_embeddings, dim=0)<a id="change">
                )</a>

            combined_embedding = torch.stack(combined_embedding, dim=0)

        assert not torch.isnan(combined_embedding).any(), "NaNs in embedding."</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/mackelab/sbi/commit/1352e77bdbc47aa4a4130679903b57672e48218c#diff-672ba10e6c3065a6f5554033b9b173c4fe88f4c05ffc31c70a9198691e6c6659L262' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 16585191</div><div id='project'> Project Name: mackelab/sbi</div><div id='commit'> Commit Name: 1352e77bdbc47aa4a4130679903b57672e48218c</div><div id='time'> Time: 2023-03-01</div><div id='author'> Author: jan.boelts@tum.de</div><div id='file'> File Name: sbi/neural_nets/embedding_nets.py</div><div id='m_class'> M Class Name: PermutationInvariantEmbedding</div><div id='n_method'> N Class Name: PermutationInvariantEmbedding</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: sbi/neural_nets/embedding_nets.py</div><div id='n_file'> N File Name: sbi/neural_nets/embedding_nets.py</div><div id='m_start'> M Start Line: 271</div><div id='m_end'> M End Line: 284</div><div id='n_start'> N Start Line: 277</div><div id='n_end'> N End Line: 304</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        len is equal to the batch size, the second len is equal to the top_k and the third len is equal to the respective
        number of classes for the child FC.
        
        <a id="change">raise </a>NotImplementedError()


class FlowDictDecorator(nn.Module):</code></pre><h3>After Change</h3><pre><code class='java'>
        
        n = len(features)
        parent_indices = parent_flow_dict.activated.argsort(dim=1)[:, :self.top_k]
        res<a id="change"> = </a><a id="change">[]</a>
        <a id="change">for i</a> in range(n)<a id="change">:
            </a>res_for_parent<a id="change"> = </a>[]
            for parent_index in parent_indices[i]:
                res_for_parent.append(self.fcs[parent_index](features[i:i+1]))
            <a id="change">res.append(</a>res_for_parent<a id="change">)</a>
        return res


class FlowDictDecorator(nn.Module):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/hristo-vrigazov/dnn.cool/commit/3782b44e17cff0a1bc35747df9fe88b4bfac72c3#diff-53cd98664c00edca1974faacafa1f6aafcb5756b740af3a59b935bb20d6a1442L60' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 16585184</div><div id='project'> Project Name: hristo-vrigazov/dnn.cool</div><div id='commit'> Commit Name: 3782b44e17cff0a1bc35747df9fe88b4bfac72c3</div><div id='time'> Time: 2020-06-28</div><div id='author'> Author: hvrigazov@gmail.com</div><div id='file'> File Name: dnn_cool/modules.py</div><div id='m_class'> M Class Name: NestedFC</div><div id='n_method'> N Class Name: NestedFC</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: dnn_cool/modules.py</div><div id='n_file'> N File Name: dnn_cool/modules.py</div><div id='m_start'> M Start Line: 70</div><div id='m_end'> M End Line: 70</div><div id='n_start'> N Start Line: 71</div><div id='n_end'> N End Line: 79</div><BR>