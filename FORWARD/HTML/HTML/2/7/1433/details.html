<html><h3>Pattern ID :1433
</h3><img src='4893418.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        weight = weight.masked_fill(mask[..., None], 0.)

        gate = rearrange(gate, &quotb n (h d) -&gt; b h n d&quot, h = h)
        gate = <a id="change">einsum(&quotb h n d, h n m -&gt; b h m d&quot</a>, gate, weight<a id="change">)</a>
        gate<a id="change"> = </a>gate + rearrange(bias, &quoth n -&gt; () h n ()&quot)
        gate<a id="change"> = rearrange(</a>gate, <a id="change">&quotb h n d -&gt; b n (h d)&quot</a><a id="change">)</a>

        return gate * res

def gMLPBlock(</code></pre><h3>After Change</h3><pre><code class='java'>
        res, gate = x.chunk(2, dim = -1)
        gate = self.norm(gate)

        gate = F.pad(gate, (0<a id="change">, 0, 0, 0, 1, 0</a>), value = 0.)
        gate = torch.cat((gate[:, :-1], gate[:, 1:]), dim = 2)

        weight, bias = self.weight, self.bias</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/g-mlp-gpt/commit/a2f065dfcf7be1d1e4b205ac1a55de4ad1b3327d#diff-9e1c762f97dc7e52cf231d6bec8dbb8412bc9383faaf89eadd9b1c02e46fea80L90' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4893418</div><div id='project'> Project Name: lucidrains/g-mlp-gpt</div><div id='commit'> Commit Name: a2f065dfcf7be1d1e4b205ac1a55de4ad1b3327d</div><div id='time'> Time: 2021-05-20</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: g_mlp_gpt/g_mlp_gpt.py</div><div id='m_class'> M Class Name: CausalSpatialGatingUnit</div><div id='n_method'> N Class Name: CausalLocalSGU</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: g_mlp_gpt/g_mlp_gpt.py</div><div id='n_file'> N File Name: g_mlp_gpt/g_mlp_gpt.py</div><div id='m_start'> M Start Line: 90</div><div id='m_end'> M End Line: 100</div><div id='n_start'> N Start Line: 130</div><div id='n_end'> N End Line: 154</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        if exists(mask):
            v.masked_fill_(~mask, 0.)

        context<a id="change"> = </a><a id="change">einsum(&quotb h n d, b h n e -&gt; b h d e&quot</a>, k, v<a id="change">)</a>
        out = einsum(&quotb h d e, b h n d -&gt; b h n e&quot, context, q)
        out<a id="change"> = rearrange(</a>out, <a id="change">&quotb h n d -&gt; b n (h d)&quot</a><a id="change">)</a>
        return self.to_out(out), 0

class EquivariantAttention(nn.Module):
    def __init__(</code></pre><h3>After Change</h3><pre><code class='java'>
    def forward(self, x, queries, mask = None):
        induced = self.attn1(queries, x, mask = mask)
        out     = self.attn2(x, induced)
        return out<a id="change">, 0</a>

class EquivariantAttention(nn.Module):
    def __init__(
        self,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/en-transformer/commit/6bd1817d780502d24a2515e850c9cd1600f24642#diff-f288a1692c1c58a186cb9ac763046f632757db1647271b97852e59e3fc5696b9L131' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4893419</div><div id='project'> Project Name: lucidrains/en-transformer</div><div id='commit'> Commit Name: 6bd1817d780502d24a2515e850c9cd1600f24642</div><div id='time'> Time: 2021-06-04</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: en_transformer/en_transformer.py</div><div id='m_class'> M Class Name: GlobalLinearAttention</div><div id='n_method'> N Class Name: GlobalLinearAttention</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: en_transformer/en_transformer.py</div><div id='n_file'> N File Name: en_transformer/en_transformer.py</div><div id='m_start'> M Start Line: 132</div><div id='m_end'> M End Line: 151</div><div id='n_start'> N Start Line: 160</div><div id='n_end'> N End Line: 162</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        sim = einsum(&quotb h d i, b h d j -&gt; b h i j&quot, q, k)
        attn = sim.softmax(dim = -1)

        out<a id="change"> = </a><a id="change">einsum(&quotb h i j, b h d j -&gt; b h i d&quot</a>, attn, v<a id="change">)</a>
        out<a id="change"> = rearrange(</a>out, <a id="change">&quotb h (x y) d -&gt; b (h d) x y&quot</a><a id="change">, x = h, y = w)</a>
        return self.to_out(out)

class FiLM(nn.Module):
    def __init__(</code></pre><h3>After Change</h3><pre><code class='java'>

        if exists(self.time_cond):
            assert exists(time)
            scale<a id="change">, shift</a> = self.time_cond(time).chunk(2, dim = -1)
            x = (x * (scale + 1)) + shift

        if has_context:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/recurrent-interface-network-pytorch/commit/5cd08b2823cfe105785a525aea43a7396fea07e9#diff-c245517a15c6c34e71df6f08ed422324484176189f4795463b7e58278d35b9cdL144' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4893416</div><div id='project'> Project Name: lucidrains/recurrent-interface-network-pytorch</div><div id='commit'> Commit Name: 5cd08b2823cfe105785a525aea43a7396fea07e9</div><div id='time'> Time: 2022-12-24</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: rin_pytorch/rin_pytorch.py</div><div id='m_class'> M Class Name: Attention</div><div id='n_method'> N Class Name: Attention</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: rin_pytorch/rin_pytorch.py</div><div id='n_file'> N File Name: rin_pytorch/rin_pytorch.py</div><div id='m_start'> M Start Line: 149</div><div id='m_end'> M End Line: 161</div><div id='n_start'> N Start Line: 199</div><div id='n_end'> N End Line: 225</div><BR>