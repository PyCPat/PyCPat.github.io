<html><h3>Pattern ID :2227
</h3><img src='14269085.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        for i, up_conv in enumerate(self.up_convs):
            before_pool = None
            if encoder_outs is not None:
                before_pool<a id="change"> = </a>encoder_outs[-(i+2)]
            x = up_conv(x, before_pool,se=self.im_atts[i])
        x_im = x

        x = input        
        for i, up_conv in enumerate(self.up_convs):
            before_pool = None
            if encoder_outs is not None:
                before_pool = encoder_outs[-(i+2)]
            x = up_conv(x, before_pool, se = self.mask_atts[i])
        x_mask = x

        x = input
        for i, up_conv in enumerate(self.up_convs):
            before_pool = None
            if encoder_outs is not None:
                before_pool = encoder_outs[-(i+2)]
            x = up_conv(x, before_pool, se=self.wm_atts[i])
        x_wm = x

        <a id="change">return </a>x_im<a id="change">,x_mask,x_wm</a>

class CoarseDecoder(nn.Module):
    def __init__(self, args, in_channels=512, out_channels=3, norm=&quotbn&quot,act=F.relu, depth=5, blocks=1, residual=True,</code></pre><h3>After Change</h3><pre><code class='java'>

    def forward(self, input):
        &#47&#47 Encoder convs
        im_encoder_outs<a id="change"> = </a>[]
        mask_encoder_outs<a id="change"> = </a>[]
        x = input
        for i, d_conv in enumerate(self.down_convs):
            &#47&#47 d_conv, attn = nets
            x, before_pool = d_conv(x)
            im_encoder_outs.append(before_pool)
            mask_encoder_outs.append(before_pool)
        x_im = x
        x_mask = x

        &#47&#47 Decoder convs
        x = x_im
        for i, nets in enumerate(zip(self.up_convs, self.up_im_atts)):
            up_conv, attn = nets
            before_pool = None
            if im_encoder_outs is not None:
                before_pool = im_encoder_outs[-(i+2)]
            x<a id="change"> = </a>up_conv(x, before_pool,se=attn)
        x_im = x

        x<a id="change"> = </a>x_mask       
        for i, nets in enumerate(zip(self.up_convs, self.up_mask_atts)):
            up_conv, attn = nets
            before_pool = None
            if mask_encoder_outs is not None:
                before_pool = mask_encoder_outs[-(i+2)]
            x = up_conv(x, before_pool, se = attn)
        x_mask<a id="change"> = </a>x

        <a id="change">return </a>x_im<a id="change">, x_mask</a>

class CoarseDecoder(nn.Module):
    def __init__(self, args, in_channels=512, out_channels=3, norm=&quotbn&quot,act=F.relu, depth=5, blocks=1, residual=True,</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 10</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/bcmi/slbr-visible-watermark-removal/commit/43e84b70895d28955496122816e50857863e5bfd#diff-1599e3fabe6f8d9fdcda23a2f19a46bdeb3b09c3c28b82c490897e88308729a6L84' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 14269085</div><div id='project'> Project Name: bcmi/slbr-visible-watermark-removal</div><div id='commit'> Commit Name: 43e84b70895d28955496122816e50857863e5bfd</div><div id='time'> Time: 2022-01-04</div><div id='author'> Author: lj200820082007@163.com</div><div id='file'> File Name: src/networks/resunet.py</div><div id='m_class'> M Class Name: SharedDecoder</div><div id='n_method'> N Class Name: SharedBottleNeck</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: src/networks/resunet.py</div><div id='n_file'> N File Name: src/networks/resunet.py</div><div id='m_start'> M Start Line: 86</div><div id='m_end'> M End Line: 112</div><div id='n_start'> N Start Line: 84</div><div id='n_end'> N End Line: 114</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        v = F.relu(self.norm_v1(self.fc_v1(out)))
        v = self.fc_v2(v)

        <a id="change">return </a>prob<a id="change">, v, h_out</a>

    def make_batch(self, data):
        &#47&#47 data = [tr1, tr2, ..., tr10] * batch_size
        s_player_batch, s_ball_batch, s_left_batch, s_left_closest_batch, s_right_batch, s_right_closest_batch=  [], [], [], [], [], []</code></pre><h3>After Change</h3><pre><code class='java'>
        left_closest_state = state_dict["left_closest"]
        right_team_state = state_dict["right_team"]  
        right_closest_state = state_dict["right_closest"]
        avail<a id="change"> = </a>state_dict["avail"]
        
        player_embed = self.norm_player(self.fc_player(player_state))
        ball_embed = self.norm_ball(self.fc_ball(ball_state))
        left_team_embed = self.norm_left(self.fc_left(left_team_state))
        left_closest_embed = self.norm_left_closest(self.fc_left_closest(left_closest_state))
        right_team_embed = self.norm_right(self.fc_right(right_team_state))
        right_closest_embed = self.norm_right_closest(self.fc_right_closest(right_closest_state))
        
        left_team_embed = self.pool(left_team_embed).squeeze(2)
        right_team_embed = self.pool(right_team_embed).squeeze(2)

        cat = torch.cat([player_embed, ball_embed, left_team_embed, right_team_embed, left_closest_embed, right_closest_embed], 2)
        cat = F.relu(self.norm_cat(self.fc_cat(cat)))
        h_in = state_dict["hidden"]
        out, h_out = self.lstm(cat, h_in)
        
        a_out = F.relu(self.norm_pi_a1(self.fc_pi_a1(out)))
        a_out<a id="change"> = </a>self.fc_pi_a2(a_out)
        logit<a id="change"> = </a>a_out + (avail-1)*1e8
        prob = F.softmax(logit, dim=2)
        
        prob_m<a id="change"> = </a>F.relu(self.norm_pi_m1(self.fc_pi_m1(out)))
        prob_m<a id="change"> = </a>self.fc_pi_m2(prob_m)
        prob_m<a id="change"> = </a>F.softmax(prob_m, dim=2)

        v = F.relu(self.norm_v1(self.fc_v1(out)))
        v = self.fc_v2(v)

        <a id="change">return </a>prob<a id="change">, prob_m, v, h_out</a>

    def make_batch(self, data):
        &#47&#47 data = [tr1, tr2, ..., tr10] * batch_size
        s_player_batch, s_ball_batch, s_left_batch, s_left_closest_batch, s_right_batch, s_right_closest_batch, avail_batch =  [],[],[],[],[],[],[]</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/seungeunrho/football-paris/commit/98e6f2e9e75b4a124ecd2be32d7ece32abe24101#diff-83673cb4d5ed44d31919eb99e2dbb22fbc3ed2445ddaa2958b711a542e0be39bL48' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 14269084</div><div id='project'> Project Name: seungeunrho/football-paris</div><div id='commit'> Commit Name: 98e6f2e9e75b4a124ecd2be32d7ece32abe24101</div><div id='time'> Time: 2020-10-26</div><div id='author'> Author: seungeun07@snu.ac.kr</div><div id='file'> File Name: Model/ppo.py</div><div id='m_class'> M Class Name: PPO</div><div id='n_method'> N Class Name: PPO</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: Model/ppo.py</div><div id='n_file'> N File Name: Model/ppo.py</div><div id='m_start'> M Start Line: 71</div><div id='m_end'> M End Line: 78</div><div id='n_start'> N Start Line: 59</div><div id='n_end'> N End Line: 88</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        t_0 = self.l_4(t_0, attention_mask=decoder_attention_mask, position_bias=x1, encoder_hidden_states=x0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=x2)
        &#47&#47 returning:
        &#47&#47 T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]
        <a id="change">return </a>(t_0<a id="change"></a>,)

    def state_dict(self,*args,**kwargs):
        &#47&#47 we return the state dict of this part as it should be in the original model</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 moving inputs to current device no op if already on the correct device
        attention_mask, decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x1, attention_mask=attention_mask, position_bias=x0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        t_0<a id="change"> = </a>self.l_1(t_0, attention_mask=attention_mask, position_bias=x0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        t_0<a id="change"> = </a>self.l_2(t_0)
        t_0<a id="change"> = </a>self.l_3(t_0)
        t_1<a id="change"> = </a>self.l_4(x2)
        t_1 = self.l_5(t_1, attention_mask=decoder_attention_mask, position_bias=None, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=None)
        t_2 = t_1[0]
        t_3<a id="change"> = </a>t_1[1]
        t_1<a id="change"> = </a>t_1[2]
        t_2 = self.l_6(t_2, attention_mask=decoder_attention_mask, position_bias=t_3, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=t_1)
        t_2 = self.l_7(t_2, attention_mask=decoder_attention_mask, position_bias=t_3, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=t_1)
        t_2 = self.l_8(t_2, attention_mask=decoder_attention_mask, position_bias=t_3, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=t_1)
        t_2 = self.l_9(t_2, attention_mask=decoder_attention_mask, position_bias=t_3, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=t_1)
        t_2 = self.l_10(t_2, attention_mask=decoder_attention_mask, position_bias=t_3, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=t_1)
        t_2 = self.l_11(t_2, attention_mask=decoder_attention_mask, position_bias=t_3, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=t_1)
        &#47&#47 returning:
        &#47&#47 T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        &#47&#47 T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130
        &#47&#47 T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132
        &#47&#47 T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]
        <a id="change">return </a>list(flatten((t_0<a id="change">, t_3, t_1, t_2</a>)))

    def state_dict(self,*args,**kwargs):
        &#47&#47 we return the state dict of this part as it should be in the original model</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/e959641a351e7827214f66e1a3a0675b9059392e#diff-d0be6aca0bdc49b9a5cfa74ef6f4d9a78ebbf8a7f6c923df203d594715ce1306L1065' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 14269075</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: e959641a351e7827214f66e1a3a0675b9059392e</div><div id='time'> Time: 2020-08-17</div><div id='author'> Author: saareliad@campus.technion.ac.il</div><div id='file'> File Name: models/partitioned/t5_large_tied_lmhead_8p_bw12_async_squad1.py</div><div id='m_class'> M Class Name: Partition6</div><div id='n_method'> N Class Name: Partition6</div><div id='m_method'> M Method Name: forward(1)</div><div id='n_method'> N Method Name: forward(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: models/partitioned/t5_large_tied_lmhead_8p_bw12_async_squad1.py</div><div id='n_file'> N File Name: models/partitioned/t5_large_tied_lmhead_8p_bw12_async_squad1.py</div><div id='m_start'> M Start Line: 1079</div><div id='m_end'> M End Line: 1087</div><div id='n_start'> N Start Line: 963</div><div id='n_end'> N End Line: 984</div><BR>