<html><h3>Pattern ID :2110
</h3><img src='13968917.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        t_0 = self.l_4(t_0, attention_mask=decoder_attention_mask, position_bias=x1, encoder_hidden_states=x0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=x2)
        &#47&#47 returning:
        &#47&#47 T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]
        <a id="change">return </a>(t_0,)

    def state_dict(self,*args,**kwargs):
        &#47&#47 we return the state dict of this part as it should be in the original model</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 moving inputs to current device no op if already on the correct device
        attention_mask, decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x1, attention_mask=attention_mask, position_bias=x0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        t_0<a id="change"> = </a>self.l_1(t_0, attention_mask=attention_mask, position_bias=x0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        t_0 = self.l_2(t_0)
        t_0<a id="change"> = </a>self.l_3(t_0)
        t_1 = self.l_4(x2)
        t_1 = self.l_5(t_1, attention_mask=decoder_attention_mask, position_bias=None, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=None)
        t_2<a id="change"> = </a><a id="change">t_1[0]</a>
        t_3 = t_1[1]
        t_1 = t_1[2]
        t_2 = self.l_6(t_2, attention_mask=decoder_attention_mask, position_bias=t_3, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=t_1)
        t_2 = self.l_7(t_2, attention_mask=decoder_attention_mask, position_bias=t_3, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=t_1)
        t_2 = self.l_8(t_2, attention_mask=decoder_attention_mask, position_bias=t_3, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=t_1)
        t_2 = self.l_9(t_2, attention_mask=decoder_attention_mask, position_bias=t_3, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=t_1)
        t_2<a id="change"> = </a>self.l_10(t_2, attention_mask=decoder_attention_mask, position_bias=t_3, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=t_1)
        t_2<a id="change"> = </a>self.l_11(t_2, attention_mask=decoder_attention_mask, position_bias=t_3, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=t_1)
        &#47&#47 returning:
        &#47&#47 T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        &#47&#47 T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130
        &#47&#47 T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132
        &#47&#47 T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]
        <a id="change">return </a>list(flatten((t_0, t_3, t_1, t_2)))

    def state_dict(self,*args,**kwargs):
        &#47&#47 we return the state dict of this part as it should be in the original model</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 8</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/e959641a351e7827214f66e1a3a0675b9059392e#diff-d0be6aca0bdc49b9a5cfa74ef6f4d9a78ebbf8a7f6c923df203d594715ce1306L963' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 13968917</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: e959641a351e7827214f66e1a3a0675b9059392e</div><div id='time'> Time: 2020-08-17</div><div id='author'> Author: saareliad@campus.technion.ac.il</div><div id='file'> File Name: models/partitioned/t5_large_tied_lmhead_8p_bw12_async_squad1.py</div><div id='m_class'> M Class Name: Partition6</div><div id='n_method'> N Class Name: Partition6</div><div id='m_method'> M Method Name: forward(1)</div><div id='n_method'> N Method Name: forward(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: models/partitioned/t5_large_tied_lmhead_8p_bw12_async_squad1.py</div><div id='n_file'> N File Name: models/partitioned/t5_large_tied_lmhead_8p_bw12_async_squad1.py</div><div id='m_start'> M Start Line: 1079</div><div id='m_end'> M End Line: 1087</div><div id='n_start'> N Start Line: 963</div><div id='n_end'> N End Line: 984</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        u = x.mean(-1, keepdim=True)
        s = (x - u).pow(2).mean(-1, keepdim=True)
        x = (x - u) / torch.sqrt(s + self.variance_epsilon)
        <a id="change">return </a>self.gamma * x + self.beta


</code></pre><h3>After Change</h3><pre><code class='java'>
            x = (x - u) / torch.sqrt(s + self.variance_epsilon)
            return self.weight * x + self.bias
        else :
            inputs<a id="change"> = </a><a id="change">x[0]</a>
            cond<a id="change"> = </a>x[1]
            for _ in range(len(inputs.shape) - len(cond.shape)):
                cond = cond.unsqueeze(dim=1)
           
            weight<a id="change"> = </a>self.weight + self.weight_dense(cond)
            bias = self.bias + self.bias_dense(cond)
            u = inputs.mean(-1, keepdim=True)
            s<a id="change"> = </a>(inputs - u).pow(2).mean(-1, keepdim=True)
            x<a id="change"> = </a>(inputs - u) / torch.sqrt(s + self.variance_epsilon)
           
            <a id="change">return </a>weight * x + bias


</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/920232796/bert_seq2seq/commit/76a9cf45cad987e423a9cc511eef0d4eb49620ac#diff-fb4c9ea7027099b8043655760b5df4374d6e6d5e94db0fa50111bfeee0ad7912L64' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 13968855</div><div id='project'> Project Name: 920232796/bert_seq2seq</div><div id='commit'> Commit Name: 76a9cf45cad987e423a9cc511eef0d4eb49620ac</div><div id='time'> Time: 2020-06-22</div><div id='author'> Author: xingzhaohu@xingzhaohudeMacBook-Pro.local</div><div id='file'> File Name: bert_seq2seq/model/bert_model.py</div><div id='m_class'> M Class Name: BertLayerNorm</div><div id='n_method'> N Class Name: BertLayerNorm</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: bert_seq2seq/model/bert_model.py</div><div id='n_file'> N File Name: bert_seq2seq/model/bert_model.py</div><div id='m_start'> M Start Line: 65</div><div id='m_end'> M End Line: 68</div><div id='n_start'> N Start Line: 72</div><div id='n_end'> N End Line: 93</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

			content_code = content_code + noise

		x<a id="change"> = </a>content_code
		for block in self.decoder:
			x<a id="change"> = </a>block(x, style_code)

		<a id="change">return </a>self.to_rgb(x)


class ContentEncoder(nn.Module):</code></pre><h3>After Change</h3><pre><code class='java'>
		nn.init.uniform_(self.class_embedding.weight, a=-0.05, b=0.05)

	def forward(self, content_code, class_id):
		batch_size<a id="change"> = </a><a id="change">content_code.shape[0]</a>

		if self.training and self.config[&quotcontent_std&quot] != 0:
			noise = torch.zeros_like(content_code)
			noise.normal_(mean=0, std=self.config[&quotcontent_std&quot])

			content_code = content_code + noise

		class_code = self.class_embedding(class_id)
		class_code_repeated<a id="change"> = </a>class_code.view((batch_size, -1, 1, 1)).repeat((1, 1, 16, 16))
		x<a id="change"> = </a>torch.cat((content_code, class_code_repeated), dim=1)

		for block in self.adains:
			x = block(x, class_code)

		<a id="change">return </a>self.convs(x)


class ContentEncoder(nn.Module):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/avivga/overlord/commit/3ed2f68af62bf260d2133b58ec59b0a19e7dd2c0#diff-44dac3f4f06b62a8129520d279f589b0b2144de4f2a7720992f263c496651b43L30' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 13968977</div><div id='project'> Project Name: avivga/overlord</div><div id='commit'> Commit Name: 3ed2f68af62bf260d2133b58ec59b0a19e7dd2c0</div><div id='time'> Time: 2020-07-19</div><div id='author'> Author: avivga@gmail.com</div><div id='file'> File Name: model/modules.py</div><div id='m_class'> M Class Name: Generator</div><div id='n_method'> N Class Name: Generator</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: model/modules.py</div><div id='n_file'> N File Name: model/modules.py</div><div id='m_start'> M Start Line: 35</div><div id='m_end'> M End Line: 41</div><div id='n_start'> N Start Line: 48</div><div id='n_end'> N End Line: 63</div><BR>