<html><h3>Pattern ID :713
</h3><img src='2712617.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        out = torch.matmul(w.transpose(1, 2), encoder_outputs)

        <a id="change">return </a>out<a id="change">, w</a>


class DurationPredictor(nn.Module):
     Duration Parameter Predictor </code></pre><h3>After Change</h3><pre><code class='java'>
        c = c.unsqueeze(2)
        s = self.range_param_predictor(encoder_outputs, duration, mask).unsqueeze(-1)

        g<a id="change"> = </a>torch.distributions.normal.Normal(loc=c, scale=s)

        w = self.get_alignment_energies(g, t)  &#47&#47 [B, L, T]

        if mask is not None:
            w<a id="change"> = </a>w.masked_fill(mask.unsqueeze(-1), 0.0)

        attn = w / (<a id="change">torch.sum(w, dim=1).unsqueeze(1</a><a id="change">)</a> + 1e-8)  &#47&#47 [B, L, T]
        out<a id="change"> = </a>torch.bmm(attn.transpose(1, 2), encoder_outputs)

        <a id="change">return </a>out<a id="change">, attn</a>


class DurationPredictor(nn.Module):
     Duration Parameter Predictor </code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 8</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/keonlee9420/wavegrad2/commit/523ec241c64ab635218f32d071fd85fbc469e178#diff-44dac3f4f06b62a8129520d279f589b0b2144de4f2a7720992f263c496651b43L115' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2712617</div><div id='project'> Project Name: keonlee9420/wavegrad2</div><div id='commit'> Commit Name: 523ec241c64ab635218f32d071fd85fbc469e178</div><div id='time'> Time: 2021-07-13</div><div id='author'> Author: keonlee9420@gmail.com</div><div id='file'> File Name: model/modules.py</div><div id='m_class'> M Class Name: GaussianUpsampling</div><div id='n_method'> N Class Name: GaussianUpsampling</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: model/modules.py</div><div id='n_file'> N File Name: model/modules.py</div><div id='m_start'> M Start Line: 115</div><div id='m_end'> M End Line: 135</div><div id='n_start'> N Start Line: 120</div><div id='n_end'> N End Line: 134</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        stats = self.proj(x) * x_mask

        m, logs = torch.split(stats, self.out_channels, dim=1)
        <a id="change">return </a>x<a id="change">, m, logs, x_mask</a>
</code></pre><h3>After Change</h3><pre><code class='java'>
        text_padded_embedded = torch.transpose(text_padded_embedded, 1, -1)
        &#47&#47text_padded_embedded.size() : torch.Size([batch_size, self.hidden_channels, text_length])
        &#47&#47マスクの作成
        max_text_length<a id="change"> = </a>text_padded_embedded.size(2)
        progression<a id="change"> = </a>torch.arange(max_text_length, dtype=text_lengths.dtype, device=text_lengths.device)
        text_mask<a id="change"> = </a>(<a id="change">progression.unsqueeze(0</a><a id="change">)</a> &lt; text_lengths.unsqueeze(1))
        text_mask = torch.unsqueeze(text_mask, 1).to(text_padded_embedded.dtype)
        &#47&#47text_mask.size() : torch.Size([batch_size, 1, text_length])

        text_encoded = self.encoder(text_padded_embedded * text_mask, text_mask)
        &#47&#47text_encoded.size() : torch.Size([batch_size, self.hidden_channels, text_length])
        stats = self.proj(text_encoded) * text_mask

        m, logs = torch.split(stats, self.out_channels, dim=1)
        <a id="change">return </a>text_encoded<a id="change">, m, logs, text_mask</a>
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/zassou65535/vits/commit/00ade3b39bd67dc21d25b09c5b535374f2c8bd1d#diff-16d3f9fbcca22429a76565eb201733ad0eb6eba76cf3d99a5c8b02ee599dc37bL290' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2712619</div><div id='project'> Project Name: zassou65535/vits</div><div id='commit'> Commit Name: 00ade3b39bd67dc21d25b09c5b535374f2c8bd1d</div><div id='time'> Time: 2022-01-17</div><div id='author'> Author: gravity@kuf.biglobe.ne.jp</div><div id='file'> File Name: module/model_component/text_encoder.py</div><div id='m_class'> M Class Name: TextEncoder</div><div id='n_method'> N Class Name: TextEncoder</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: module/model_component/text_encoder.py</div><div id='n_file'> N File Name: module/model_component/text_encoder.py</div><div id='m_start'> M Start Line: 292</div><div id='m_end'> M End Line: 304</div><div id='n_start'> N Start Line: 287</div><div id='n_end'> N End Line: 303</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            raise RuntimeError("Unknown value for attention norm type")
        context = torch.bmm(alignment.unsqueeze(1), inputs)
        context = context.squeeze(1)
        <a id="change">return </a>context<a id="change">, alignment</a>


class Postnet(nn.Module):
    def __init__(self, mel_dim, num_convs=5):</code></pre><h3>After Change</h3><pre><code class='java'>
            raise RuntimeError("Unknown value for attention norm type")
        if self.forward_attn:
            &#47&#47 forward attention
            prev_alpha<a id="change"> = </a>F.pad(self.alpha[:, :-1].clone(), (1, 0, 0, 0)).to(inputs.device)
            self.alpha<a id="change"> = </a>(((1-self.u) * self.alpha.clone().to(inputs.device) + self.u * prev_alpha) + 1e-7) * alignment
            alpha_norm<a id="change"> = </a>self.alpha / <a id="change">self.alpha.sum(dim=1).unsqueeze(1</a><a id="change">)</a>
            &#47&#47 compute context
            context = torch.bmm(alpha_norm.unsqueeze(1), inputs)
            context = context.squeeze(1)
            <a id="change">return </a>context<a id="change">, alpha_norm, alignment</a>
        else:
            context = torch.bmm(alignment.unsqueeze(1), inputs)
            context = context.squeeze(1)
            return context, alignment, alignment</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/coqui-ai/tts/commit/961af0f5cdefbb5f267671f6847cf05659962d6c#diff-98fd28c1559c72435d1c59024b2baa25ece39483c2196f2377bfc9ee6c7f183bL145' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2712622</div><div id='project'> Project Name: coqui-ai/tts</div><div id='commit'> Commit Name: 961af0f5cdefbb5f267671f6847cf05659962d6c</div><div id='time'> Time: 2019-04-05</div><div id='author'> Author: egolge@mozilla.com</div><div id='file'> File Name: layers/tacotron2.py</div><div id='m_class'> M Class Name: Attention</div><div id='n_method'> N Class Name: Attention</div><div id='m_method'> M Method Name: forward(6)</div><div id='n_method'> N Method Name: forward(6)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: layers/tacotron2.py</div><div id='n_file'> N File Name: layers/tacotron2.py</div><div id='m_start'> M Start Line: 173</div><div id='m_end'> M End Line: 175</div><div id='n_start'> N Start Line: 193</div><div id='n_end'> N End Line: 208</div><BR>